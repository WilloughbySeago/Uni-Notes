\documentclass[fleqn]{NotesClass}

\usepackage{csquotes}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage{enumitem}

% Tikz stuff
\usepackage{tikz}
\tikzset{>=latex}
% external
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
%\tikzexternaldisable
% other libraries
\usetikzlibrary{shapes.misc}
\usetikzlibrary{angles}
\usetikzlibrary{quotes}
\usetikzlibrary{calc}

% PGF plots
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% References, should be last things loaded
\usepackage{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{mathtools}
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

% Title page info
\title{General Relativity}
\author{Willoughby Seago}
\date{February 17, 2022}
% \subtitle{}
% \subsubtitle{}

% Highlight colour
\definecolor{highlight}{HTML}{FFC800}
\definecolor{my blue}{HTML}{001BEB}
\definecolor{my red}{HTML}{EB3300}
\definecolor{my green}{HTML}{6AEB00}
\definecolor{my purple}{HTML}{CF00EB}

\hyphenation{Schwarz-schild}

% Commands
% Maths
\newcommand*{\dalembertian}{\mathop{\square}}
\newcommand*{\christoffel}[3]{\tensor{\Gamma}{^{#1}_{{#2}{#3}}}}
\newcommand*{\order}{\mathcal{O}}
\newcommand*{\lagrangian}{\mathcal{L}}
\newcommand*{\category}[1]{\mathbf{#1}}
\DeclareMathOperator{\End}{End}
\newcommand*{\cv}[1]{\bm{\tilde{#1}}}
\newcommand*{\cve}[1]{\bm{\tilde{e}^{#1}}}
\newcommand*{\vep}[1]{\bm{e'_{#1}}}
\newcommand*{\cvep}[1]{\bm{\tilde{e}'^{#1}}}
\newcommand*{\trans}{\top}
\newcommand*{\covariantDerivative}[1]{\nabla_{\!#1}}
\newcommand*{\e}{\mathrm{e}}
\diffdef{covariant}{
    op-symbol=\mathrm{D},
    op-symbol-alt=\mathrm{d}
}
\NewDocumentCommand{\diffcov}{}{\diff.covariant.}
\newcommand*{\christoffeldef}[4]{%
    \frac{1}{2} g^{#4#1} ( \partial_{#2} g_{#3#4} + \partial_{#3}  g_{#2#4} - \partial_{#4} g_{#3#2} )
}
\ExplSyntaxOn
\prop_new:N \l_willoughby_color_prop
\prop_clear:N \l_willoughby_color_prop
\prop_set_from_keyval:Nn \l_willoughby_color_prop {
    i = red,
    j = blue,
    k = green,
    l = magenta,
    m = orange,
    n = yellow
}

\cs_new:Npn \willoughby_index_color:n #1 {
    \str_set:Nn \l_tmpa_str { #1 }
    \prop_if_in:NVTF \l_willoughby_color_prop \l_tmpa_str {
        \textcolor{
            \exp_args:NNV \prop_item:Nn \l_willoughby_color_prop { \l_tmpa_str }
        } {
            #1
        }
    } { #1 }
}

\NewDocumentCommand{\indexcolor}{ m }{ {\willoughby_index_color:n { #1 }} }
\NewDocumentCommand{\setindexcolor}{ m }{
    \prop_clear:N \l_willougbhy_color_prop
    \prop_set_from_keyval:Nn \l_willoughby_color_prop { #1 }
}
\ExplSyntaxOff

% Include
\includeonly{parts/graphical-notation-appendix}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/effective-schwarzschild-potential}
    \tableofcontents
    \listoffigures
    \mainmatter
    \chapter{Introduction}
    General relativity (GR) generalises special relativity (SR).
    SR starts with the requirement that there no preferred frame of reference, which means there is no absolute rest or motion.
    It follows from this that there is no absolute time.
    In SR we mostly concern ourselves with observers in uniform motion, although it is possible to treat certain accelerating cases.
    In GR we will expand relativity to observers in arbitrary motion.
    An implication that we shall find is that spacetime is curved and that this curvature determines particle dynamics.
    As a sort of secondary function GR gives us a relativistic theory of gravity, which arises as the cause of the curvature in GR.
    As the physicist John Archibald Wheeler put it
    \begin{displayquote}
        Matter tells spacetime how to curve, and curved spacetime tells matter how to move.
    \end{displayquote}

    \section{Preliminary Matters}
    Greek indices, \(\mu\), \(\nu\), etc.\@ run from 0 to 3, with 0 being the time component.
    Latin indices, \(i\), \(j\), etc.\@ run from 1 to 3, so only for the spacial components.
    Unless stated otherwise the Einstein summation convention is in effect, meaning that we sum over repeated indices.
    Four-vector indices must be one up and one down to be summed over whereas three-vector indices don't need to be on different levels.
    We use the metric signature \(({+}{-}{-}{-})\), meaning that the Minkowski metric is \(\eta = \diag(1, -1, -1, -1)\).
    
    \chapter{Special Relativity Tools}
    \begin{rmk}
        For more details on special relativity see the relativity section of the notes from relativity, nuclear, and particle physics.
    \end{rmk}
    \section{Vectors}
    In special relativity (SR) we combine space and time into one object, known collectively as \defineindex{spacetime}.
    The position and time of something are combined into a single object called an \defineindex{event}.
    Given two events we can write the interval between them as a \defineindex{contravariant} \define{four-vector}\index{four!vector}:
    \begin{equation}
        \dl{x^\mu} = (\dl{x^0}, \dl{x^1}, \dl{x^2}, \dl{x^3}) = (c \dd{t}, \dl{x}, \dl{y}, \dl{z}) = (c\dd{t}, \dl{\vv{x}}).
    \end{equation}
    As the notation suggests we will often consider the case of this interval being infinitesimal, although this needn't always be the case.
    
    Given a four vector, \(\dl{x^\mu}\), we define a \defineindex{norm}, which is \defineindex{invariant}, meaning the same in all reference frames.
    To do so we introduce a different kind of vector with lower indices, called a \defineindex{covariant} four-vector:
    \begin{equation}
        \dl{x_\mu} = (c\dd{t}, -\dl{x}, -\dl{y}, -\dl{z}) = (c\dd{t}, -\dl{\vv{x}}).
    \end{equation}
    These two quantities are related by
    \begin{equation}
        \dl{x^\mu} = \eta^{\mu\nu}\dl{x_\nu}, \qqand \dl{x_\mu} = \eta_{\mu\nu}\dl{x^\nu}
    \end{equation}
    where \(\eta\) is the Minkowski metric
    \begin{equation}
        \eta_{\mu\nu} = \eta^{\mu\nu} = \diag(1, -1, -1, -1).
    \end{equation}   
    The invariant norm that we can define from these quantities is then
    \begin{multline}
        \dl{x^\mu}\dl{x_\mu} = \dl{x_\mu}\dl{x^\mu} = \eta_{\mu\nu}\dl{x^\mu}\dl{x^\nu} = \eta^{\mu\nu}\dl{x_\mu}\dl{x_\nu}\\
        = c^2\dd{t}^2 - \dl{x}^2 - \dl{y}^2 - \dl{z}^2 = c^2\dd{t}^2 - \dl{\vv{x}} \cdot \dl{\vv{x}} = c^2\dl{\tau}^2.
    \end{multline}
    If this is invariant and the speed of light is invariant then \(\dl{\tau}\) must also be invariant.
    We call \(\dl{\tau}\) the \defineindex{proper time}.
    It is the time between two events which occur at the same spacial location in some frame.
    An alternative way to think about it is the time an observer measures on a clock that they carry with them.
    
    This suggests a more general scalar product for four vectors \(A^\mu\) and \(B^\mu\), defined by
    \begin{multline}
        A^\mu B_\mu = A_\mu B^\mu = \eta_{\mu\nu}A^\mu B^\nu = \eta^{\mu\nu} A_\mu B_\nu\\
        = A^0B^0 - A^1B^1 - A^2B^2 - A^3B^3 = A^0B^0 - \vv{A} \cdot \vv{B}.
    \end{multline}
    
    This norm is chosen so that events that are connected by a light signal, say the events \enquote{turned on the torch} and \enquote{the torch light reached the sensor} have \(\dl{x^\mu}\dl{x_\mu} = 0\).
    Requiring that this holds for all observers is a key step in deriving the results of special relativity.
    In order to derive these consequences we need to know how to compare events between observers by transforming between frames.
    
    Suppose now that an event is measured in two frames.
    In the unprimed frame the event is \(\dl{x^\mu}\).
    In the primed frame the event is \(\dl{x'^\mu}\).
    Suppose that the frames are in the \defineindex{standard configuration}, meaning that both frames coincide at \(t = 0\) and the motion of the primed frame is along the \(x\)-axis of the unprimed frame at speed \(V\).
    Before special relativity it was assumed that a Galilean transform related the two frames:
    \begin{equation}
        \dl{x'^\mu} = (c\dd{t'}, \dl{x'}, \dl{y'}, \dl{z'}) = (c\dd{t}, \dl{x} - V\dl{t}, \dl{y}, \dl{z}).
    \end{equation}
    Notice that the time component is unchanged.
    By comparing the second component of these four-vectors and dividing by \(\dl{t}\) we get
    \begin{equation}
        \diff{x'}{t} = \diff{x}{t} - V.
    \end{equation}
    This tells us that velocities simply combine additively in a Galilean transform.
    
    Problems arise when we start trying to include electrodynamics.
    One of the fundamental results of electrodynamics being that the speed of light, \(c\), is constant and independent of the observer.
    The Galilean transformation suggests that by having the second frame move at speed \(V\) along the direction the light travels then an observer in this frame would view the speed of light as \(c - V\).
    
    The solution that physicists eventually settled on was that the Galilean transform is wrong, and only holds approximately when \(V \ll c\).
    Instead we assume that the frames are related by a linear transform
    \begin{equation}
        c\dd{t'} = Ac\dd{t} + B\dd{x}, \qqand \dd{x'} = Cc\dd{t} + D\dd{x}.
    \end{equation}
    Since all motion is along the \(x\)-axis we can neglect the final two components since \(\dl{y'} = \dl{y}\) and \(\dl{z'} = \dl{z}\).
    
    Consider the case where the event occurs at the origin of the primed frame.
    In this case we have \(\dl{x'} = 0\).
    If the speed of the frame is \(V\) then we also have \(\diff{x}/{t} = V\) and \(\diff{x'}/{t'} = -V\).
    From this it is possible to show that the transformation takes the form
    \begin{equation}
        \dl{x'^\mu} = \diffp{x'^\mu}{x^\nu} \dd{x^\nu} = \tensor{\Lambda}{^\mu_\nu}.
    \end{equation}
    Here \(\tensor{\Lambda}{^\mu_\nu}\) is a Lorentz transform, which, in the standard configuration, takes the form
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = 
        \begin{pmatrix}
            \gamma & -\beta\gamma & 0 & 0\\
            -\gamma\beta & \gamma & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    Here \(\beta \coloneqq V/c\) and \(\gamma \coloneqq 1/\sqrt{1 - \beta^2}\).
    From this we can show that \(\dl{x^\mu}\dl{x_\mu}\) is invariant under Lorentz transformations and so the proper time is a relativistic invariant.
    
    \subsection{Why do we Need These Upper and Lower Indices?}
    Normally we work with an orthonormal basis, \(\{\ve{i}\}\), where we can write an arbitrary vector as
    \begin{equation}
        \vv{A} = A^i\ve{i}.
    \end{equation}
    We then extract the components by taking the scalar product with the relevant basis vectors, since by definition for orthonormal basis vectors \(\ve{i} \cdot \ve{j} = \delta_{ij}\), and so
    \begin{equation}
        \ve{j} \cdot \vv{A} = A^i \ve{j} \cdot \ve{i} = A^i\delta_{ji} = A^j.
    \end{equation}
    
    Now suppose that \(\{\ve{i}\}\) is instead a skew basis, meaning it isn't orthonormal.
    In this case we can still write an arbitrary vector as
    \begin{equation}
        \vv{A} = A^i\ve{i},
    \end{equation}
    but now \(\ve{i} \cdot \ve{j} \ne \delta_{ij}\) and so
    \begin{equation}
        \ve{j} \cdot \vv{A} = A^i\ve{j} \cdot \ve{i} \eqqcolon A_j \ne A^j
    \end{equation}
    where we have introduced a new type of component with lower indices which \emph{is} equal to \(\ve{j} \cdot \vv{A}\).
    
    We can then define an invariant quantity as
    \begin{equation}
        A^2 = \vv{A} \cdot \vv{A} = A^iA^j\ve{i} \cdot \ve{j} = A^iA_i.
    \end{equation}
    Notice here that the action of \(\ve{i} \cdot \ve{j}\) is to lower the index and so we can interpret \(\ve{i} \cdot \ve{j}\) as the metric tensor.
    
    \subsection{Derivatives}
    The derivative \(\partial_\mu = \diffp{}/{x^\mu}\) is an example of a covariant four-vector.
    Notice that the index is an upper index in the denominator, which gives a lower index in the numerator.
    To see that this is sensible consider the scalar field \(\varphi\).
    A change in this field is given by
    \begin{equation}
        \dl{\varphi} = \diffp{\varphi}{x^\mu} \dd{x^\mu} = \partial_\mu \varphi \dl{x^\mu}
    \end{equation}
    which is an invariant quantity, which we should expect, since the value of a scalar is invariant, and so the difference in a scalar field between two points should also be invariant.
    
    We can then identify
    \begin{equation}
        \partial_\mu = \left( \frac{1}{c}\diffp{}{t}, \grad \right)
    \end{equation}
    where \(\grad\) is the normal gradient as defined for three-vectors.
    We then get the contravariant quantity
    \begin{equation}
        \partial^\mu = \left( \frac{1}{c}\diffp{}{t}, -\grad \right).
    \end{equation}
    This allows us to define an invariant operator, called the \defineindex{d'Alembertian}:
    \begin{equation}
        \partial_\mu\partial^\mu = \frac{1}{c^2}\diffp[2]{}{t} - \laplacian \eqqcolon \dalembertian.
    \end{equation}
    This same operator is sometimes denoted \(\partial^2\) or, confusingly, \(\square^2\).
    
    Notice that by using the d'Alembertian we can can write the wave equation as
    \begin{equation}
        \dalembertian u = 0 \iff \laplacian u = \frac{1}{c^2}\diffp[2]{u}{t}.
    \end{equation}
    
    Let \(\vv{j}\) be a three-current and \(\rho\) the charge density.
    Then the continuity equation is
    \begin{equation}
        \diffp{\rho}{t} + \div \vv{j} = 0.
    \end{equation}
    Defining the \define{four-current}\index{four!current}, \(J^\mu = (c\rho, \vv{j})\), we can write the continuity equation compactly as
    \begin{equation}
        \partial_\mu J^\mu = 0.
    \end{equation}
    This shows that this quantity is invariant and hence the related conserved quantity, in this case the total charge, is conserved.
    This also applies to other types of current and density where the continuity equation holds, such as the probability current in quantum mechanics\footnote{see the notes for quantum theory, particularly the final section on relativistic quantum mechanics}.
    
    \section{Principle of General Covariance}
    \begin{rmk}
        The \enquote{covariance} in this principle and the \enquote{covariance} in vectors with lower indices are different concepts and not to be confused.
    \end{rmk}
    The \defineindex{principle of general covariance} is
    \begin{important}
        Valid laws of physics are independent of the coordinates.
    \end{important}
    An alternative formulation is
    \begin{important}
        Valid laws of physics hold for all observers.
    \end{important}
    
    In practice this means that the forms of equations shouldn't change between frames.
    To do this we apply two methods.
    First, we can write laws in terms of four vectors (and higher rank tensors), such as the law that \(A^\mu = B^\mu\).
    This law is naturally covariant (in the sense of the principle of general covariance, not covariant vectors, since these are contravariant vectors) since both sides of the equation transform in the same way under a Lorentz transformation, and so if it holds in one frame it holds in all frames.
    Second, we can use invariant quantities, such as \(A^\mu B_\mu\), and \(c\) to write our equations, since all observers will agree on the value of these quantities as measured in their frames.
    
    For example, suppose we have a collection of particles undergoing some interaction.
    If the momentum of the \(i\)th particle is \(\vv{p_i}\) then conservation of momentum gives us \(\sum_i \Delta\vv{p_i} = \vv{0}\).
    This is \emph{not} covariant.
    The natural way to generalise the momentum is to define the four-momentum through the \define{four-velocity}\index{four!velocity}, which is defined in the natural way as the proper time derivative of the position:
    \begin{equation}
        U^\mu \coloneqq \diffp{x^\mu}{\tau} = \gamma \diffp{x^\mu}{t} = (\gamma c, \gamma\vv{v}).
    \end{equation}
    The \define{four-momentum}\index{four!momentum} is then defined as
    \begin{equation}
        P^\mu = mU^\mu = m(\gamma c, \gamma\vv{v}).
    \end{equation}
    Conservation of momentum then becomes \(\sum_i \Delta P_i^\mu = 0\).
    This is covariant since \(P_i^\mu\) is a four-vector since it is defined in terms of the four-velocity, which is a four-vector, and the mass of the particle, which is an invariant quantity.
    Note that from this we get a natural definition for the \defineindex{relativistic three-momentum}, \(\vv{p_i} \coloneqq \gamma m\vv{v_i}\).
    We then get two conservation laws, conservation of three-momentum, \(\sum_i \Delta \vv{p_i} = 0\), and we also have conservation of the 0 component: \(\sum_i \Delta P_i^0 = 0\).
    
    To see what this second conservation law means consider the proper time derivative of the four-momentum, which, for a constant mass, is related to the \define{four-acceleration}\index{four!acceleration}, \(A^\mu\):
    \begin{equation}
        \diff{}{\tau} P^\mu = m \diff{}{\tau} U^\mu = m A^\mu = \left( \gamma \diff{}{t}(\gamma mc), \gamma \diff{\vv{p}}{t} \right).
    \end{equation}
    Wishing to keep the usual definition of force as the rate of change of momentum we define the \define{four-force}\index{four!force}, \(F^\mu\), to be
    \begin{equation}
        F^\mu \coloneqq \diff{}{\tau} P^\mu = mA^\mu,
    \end{equation}
    which is the relativistic generalisation of Newton's second law.
    
    We now ask what the time component of \(F^\mu\) is.
    It must satisfy
    \begin{equation}
        F^0 = mA^0 = \gamma \diff{}{t}(\gamma mc),
    \end{equation}
    which we can write in terms of the three-force, \(\vv{f}\), using the invariant quantity \(A^\mu U_\mu = 0\).
    We can prove this in the rest frame of the particle, where \(U^\mu = (U^0, \vv{0})\), and \(\diff{\gamma}/{t} = \diff{1}/{t} = 0\) when the velocity is zero, meaning that \(A^0 = 0\).
    It follows that in the rest frame of the particle \(A^\mu U_\mu =  0\).
    Since this quantity is invariant this must hold in all frames.
    
    Hence in an arbitrary frame of reference we have
    \begin{equation}
        A^\mu U_\mu = \gamma c A^0 - \gamma \vv{v} \cdot \gamma \diff{}{t} (\gamma\vv{v}) = 0.
    \end{equation}
    From this we get
    \begin{equation}
        F^0 = mA^0 = \frac{\gamma}{c}\vv{v} \cdot \diff{}{t}(\gamma m\vv{v}) = \frac{\gamma}{c}\vv{v} \cdot \vv{f}.
    \end{equation}
    From this we get that the time component of \(F^\mu = mA^\mu\) gives us
    \begin{equation}
        \frac{\gamma}{c}\vv{v} \cdot \vv{f} = \gamma \diff{}{t}(\gamma m c) \implies \vv{v} \cdot \vv{f} = \diff{}{t}(\gamma mc^2).
    \end{equation}
    From this we can identity
    \begin{equation}
        \vv{v} \cdot \vv{f} = \diff{}{t}(\vv{x} \cdot \vv{f})
    \end{equation}
    as the rate at which work is done by a constant force, \(\vv{f}\).
    This allows us to identify \(\gamma mc^2\) as the total energy.
    We can then interpret the conservation law \(\sum_i \Delta P_i^0\) as conservation of energy.
    We see that energy conservation is a natural consequence of momentum conservation when the four-momentum is \(P^\mu = (E/c, \vv{p})\).
    
    \chapter{From SR to GR}
    \section{A Problem}
    Consider an observer in arbitrary motion, that is they might be accelerating.
    The coordinates in two different frames in SR are related by
    \begin{equation}
        \dl{x'^\mu} = \tensor{\Lambda}{^\mu_\nu} \dl{x^\nu}.
    \end{equation}
    Dividing by \(\dl{\tau}\) we get
    \begin{equation}
        \diff{x'^\mu}{\tau} = U'^\mu = \tensor{\Lambda}{^\mu_\nu} \diff{x^\nu}{\tau} = \tensor{\Lambda}{^\mu_\nu}U^\nu.
    \end{equation}
    This is all fine so far, we've simply found that the four-velocity is a four-vector in arbitrary motion.
    The problem occurs when we consider quantities based on the second derivative of position.
    Taking another derivative we have
    \begin{equation}
        A'^\mu = \diff[2]{x'^\mu}{\tau} = \diff{}{\tau}U'^\mu = \diff{}{\tau}(\tensor{\Lambda}{^\mu_\nu} U^\mu) = \tensor{\Lambda}{^\mu_\nu}\diff{}{\tau} U^\nu +  U^\nu\diff{}{\tau} \tensor{\Lambda}{^\mu_\nu} \ne \tensor{\Lambda}{^\mu_\nu}A^\nu.
    \end{equation}
    We see that for an observer in arbitrary motion there is no need for \(\tensor{\Lambda}{^\mu_\nu}\) to be constant and therefore the four-acceleration as defined in SR is not necessarily a four-vector.
    
    \section{Equivalence Principle}
    The equivalence principle is at the heart of general relativity.
    It is a remarkably simple observation which generalises Galileo's observation that all objects fall at the same rate.
    
    \subsection{Mass in Newtonian Mechanics}
    Newton defined force as the rate of change of an objects momentum.
    For an object with constant mass \(m_{\mathrm{i}}\), the force is therefore
    \begin{equation}
        \vv{F} = \diff{}{t} m_{\mathrm{i}}\vv{v} = m_{\mathrm{i}}\ddot{\vv{x}}.
    \end{equation}
    Here \(m_{\mathrm{i}}\) is the \defineindex{inertial mass}\index{mass!inertial}.
    We can think of \(m_{\mathrm{i}}\) as an objects resistance to changing its speed.
    
    Newton also observed that the force on an object of constant mass \(m_{\mathrm{gp}}\) due to a gravitational field is
    \begin{equation}
        \vv{F} = m_{\mathrm{gp}}\vv{g}
    \end{equation}
    where \(\vv{g}\) is the constant acceleration due to gravity and \(m_{\mathrm{gp}}\) is the \defineindex{gravitational potential mass}\index{mass!gravitational potential}.
    We can think of \(m_{\mathrm{gp}}\) as how strongly a gravitational field can accelerate an object.
    
    From these two equations we can readily see that
    \begin{equation}
        \ddot{\vv{x}} = \frac{m_{\mathrm{gp}}}{m_{\mathrm{i}}} \vv{g}
    \end{equation}
    and that in order for all objects to fall at the same rate we therefore need \(m_{\mathrm{gp}}/m_{\mathrm{i}}\) to be constant, regardless of the composition of the object.
    We can further choose our units such that \(m_{\mathrm{gp}}/m_{\mathrm{i}} = 1\).
    
    There is a third type of mass in Newtonian physics, also associated with gravity.
    This is the \defineindex{active gravitational mass}\index{mass!active gravitational}, \(m_{\mathrm{ga}}\), which we can think of as the ability of an object to create a gravitational field.
    This is defined according to the equation
    \begin{equation}
        \vv{g} = -\frac{Gm_{\mathrm{ga}}}{r^2}\vh{r}
    \end{equation}
    where \(\vv{r}\) is the vector pointing from the object generating the field to the object being attracted, \(r = \abs{\vv{r}}\), and \(\vh{r} = \vv{r}/r\).
    \(G\) is a constant.
    
    Newton's third law gives \(\vv{F_{12}} = -\vv{F_{21}}\), where \(\vv{F_{ij}}\) is the force on body \(i\) due to body \(j\).
    Adding an index \(i\) to all types of mass for object \(i\) this gives
    \begin{equation}
        \frac{Gm_{\mathrm{ga1}}m_{\mathrm{gp2}}}{r^2} = \frac{Gm_{\mathrm{ga2}}m_{\mathrm{gp1}}}{r^2} \implies \frac{m_{\mathrm{ga1}}}{m_{\mathrm{gp1}}} = \frac{m_{\mathrm{ga2}}}{m_{\mathrm{gp2}}} \implies m_{\mathrm{gp}} \propto m_{\mathrm{ga}}.
    \end{equation}
    We can further choose the value of \(G\) such that these two different gravitational masses aren't just proportional but equal.
    We then refer to them as a single quantity the \defineindex{gravitational mass}\index{mass!gravitational}, \(m_{\mathrm{g}}\).
    
    \subsection{E\"otv\"os Experiment}
    In the 1890s E\"otv\"os used a torsion balance to test the theory that \(m_{\mathrm{i}} = m_{\mathrm{g}}\).
    In the experiment masses of different materials but the same gravitational mass, \(m_{\mathrm{g}}\), were used.
    It was possible to ensure the masses were the same, within the tolerance of the experiment, using a spring balance.
    The force on each mass is then the gravitational force plus the force that the balance exerts on it.
    However, these forces don't match exactly since the balance rotates around the Earth with (very small) acceleration \(\vv{a}\).
    The difference in force between the different objects must supply a force of \(m_{\mathrm{i}}\vv{a}\) for each mass.
    Therefore if the inertial masses are different it will be possible to tell by measuring the force supplied to counteract the acceleration of Earth.
    Since the Earth rotates daily \(\vv{a}\) oscillates with a period of \qty{24}{\hour}.
    The balance would also have to oscillate with this period if the inertial masses were different.
    
    No such oscillations were measured and E\"otv\"os determined that gravitational and inertial masses are equal to 1 part in 20 million.
    Modern experiments have improved this such that variations in the ratio can be no larger than \(10^{-13}\).
    
    \subsection{Inertial Frames}
    Another problem with Newtonian mechanics is that \(\vv{F} = m\vv{a}\) only applies in inertial frames.
    So, what is an inertial frame?
    Well, its a frame in which \(\vv{F} = m\vv{a}\) applies.
    The definition is circular, however, it is easy enough given an inertial frame to construct some noninertial frame.
    A simple example is to have a frame accelerating at a constant acceleration \(\vv{g}\), such as if the entire frame were falling.
    We then find that
    \begin{equation}
        \vv{F} = m\vv{a} + m\vv{g}.
    \end{equation}
    Another example is a frame rotating at angular velocity \(\vv{\omega}\), so that the point \(\vv{r}\) has linear velocity \(\vv{v}\).
    In this frame we have
    \begin{equation}
        \vv{F} = m\vv{a} + m\vv{\omega} \times (\vv{\omega} \times \vv{r}) - 2m(\vv{v} \times \vv{\omega}) + m\dot{\vv{\omega}} \times \vv{r}.
    \end{equation}	
    
    Both of these examples give the force as \(m\vv{a}\) plus some other forces.
    These forces are often called \define{fictitious forces}\index{fictitious force|see{inertial force}}, but perhaps a better name is \define{inertial forces}\index{inertial force}.
    The forces are real and their effects can be measured, there is just no observable physical cause of the forces, they arise due to the acceleration of the frame relative to some inertial frame.
    
    In relativity we aim to find a theory that works in all frames and therefore we must find a way to include inertial forces in our theory.
    
    Notice that in an inertial frame most of the mass of the universe is at rest.
    This suggests Mach's principle:
    \begin{important}
        All acceleration is relative and hence the inertia of an object depends on the existence of other bodies.
    \end{important}
    This was an influence on Einstein when he was developing GR but the final theory of GR turned out to violate this principle entirely.
    In particular the rest mass of an object, which is measure of its inertia, is invariant and independent of the gravitational environment that the object is in.
    
    The important take away is that inertial forces are proportional to the mass of the object, and so are gravitational forces, suggesting a link between inertial and gravitational forces.
    
    \subsection{Weak Equivalence Principle}
    Consider a particle inside a freely-falling box in a gravitational field, \(\vv{g}\).
    The equation of motion for the particle is
    \begin{equation}
        m\diff[2]{\vv{x}}{t} = m\vv{g} + \vv{F}.
    \end{equation}
    Here \(\vv{F}\) is the net non-gravitational force.
    Moving to the rest frame of the box, which can be done by extending Galilean transformations in the logical way to accelerating frames, we get new primed coordinates:
    \begin{equation}
        \vv{x}' = \vv{x} - \frac{1}{2}\vv{g}t^2, \qqand t' = t.
    \end{equation}
    The equation of motion is them
    \begin{equation}
        m \diff[2]{\vv{x}}{t} = m\diff[2]{\vv{x}}{t} - m\vv{g} = \vv{F}.
    \end{equation}
    So we see that by making an appropriate transformation we have gotten rid of inertial forces.
    
    In terms of an observer in the box it is impossible for them to tell that the box is falling since in their frame there are no inertial forces from the falling of the box.
    
    \begin{rmk}
        The assumption here is that the box is small enough that over the box \(\vv{g}\) is constant as far as any one can tell by making measurements.
        That is \(\vv{g}\) is locally constant.
    \end{rmk}
    
    This leads to the \define{weak equivalence principle}\index{equivalnce principle!weak} (WEP):
    \begin{important}
        At any point in spacetime in an arbitrary gravitational field it is possible to choose a freely-falling \defineindex{locally inertial frame} (LIF) in which the laws of motion are the same as if gravity were absent.
    \end{important}
    
    There are actually infinitely many LIFs at any one spacetime point, related by Lorentz transformations.
    The WEP holds only if \(m_{\mathrm{g}} = m_{\mathrm{i}}\).
    Einstein later made a stronger claim in 1907, namely the \define{strong equivalence principle}\index{equivalence principle!strong} (SEP):
    \begin{important}
        In a locally inertial frame all SR laws of physics apply.
    \end{important}
    This essentially amounts to a claim that in a LIF the gravitational field doesn't exist.
    
    From now on we simply assume that the strong equivalence principle holds, and refer to it as \emph{the} \defineindex{equivalence principle} (EP).
    The EP gives us a starting point for anything in GR, no matter what the gravitational field we can always consider a frame falling in the field and ignore the effects of the field.
    
    One caveat is that while we can always transform away the gravitational acceleration, \(\vv{g} = -\grad\Phi\), for some gravitational potential, \(\Phi\), it is not always possible to transform away higher derivatives simultaneously, for example the \defineindex{tidal tensor},
    \begin{equation}
        \diffp{\Phi}{x^i, x^j},
    \end{equation}
    may not vanish when \(\grad\Phi\) does.
    By accepting the EP we are implicitly accepting the \defineindex{principle of minimal gravitational coupling}, which states that the laws of SR as deduced in a laboratory on Earth have no explicit dependence on higher order derivatives of \(\Phi\).
    While this is not guaranteed it has never been observed to be broken.
    
    \section{Gravitational Time Dilation}\label{sec:gravitational time dilation}
    As a test of our new tool the EP we will derive a remarkable property of spacetime.
    
    Consider an observer in a box which is accelerating upwards.
    Suppose the box is of height \(h\) and a clock is mounted at the top of the box.
    Attach to this clock a light source which pulses every time the clock ticks.
    
    If the rocket is accelerating upwards at acceleration \(g\) then the speed of the floor increases by \(v = gh/c\) in the time taken for the light to reach the floor.
    There will then be a blueshift in the frequency of the photons such that if the photons started with frequency \(\nu\) their frequency will change by \(\Delta \nu\), which is given by
    \begin{equation}
        \frac{\Delta \nu}{\nu} = \frac{v}{c} = \frac{gh}{c^2}
    \end{equation}
    The rate at which the photons are received by an observer at the bottom of the box will increase by the same factor.
    
    Since the rocket can keep accelerating forever, and photons can't be stockpiled somewhere we have to conclude that, from the point of view of an observer on the floor of the box, the clock really is running fast.
    If the rocket stops accelerating then the clock will have gained a time \(\Delta t\) compared to a clock kept on the floor of the box such that if the floor clock reads time \(t\) the ceiling clock reads \(t + \Delta t\), which is such that
    \begin{equation}
        \frac{\Delta t}{t} = \frac{gh}{c^2}.
    \end{equation}
    
    The EP then states that the same effect must occur for a stationary box in a gravitational potential \(\Phi\).
    In particular we can identify \(gh = \Delta \Phi\) as the difference in gravitational potential between the top and bottom of the box and we have
    \begin{equation}
        \frac{\Delta t}{t} = \frac{\Delta \Phi}{c^2}.
    \end{equation}
    
    The effect is that clocks in a gravitational field run slower, a clock at the top of a mountain will run faster than one at the base of the mountain.
    This effect is small but not negligible in all circumstances.
    Famously GPS satellites have to account for it.
    
    \subsection{Gravity Bends Light}
    A similar thought experiment has a beam of light crossing the accelerating box.
    In the time taken to move across the box the box will have moved up slightly.
    This means the light will reach the opposite wall slightly lower than if the box was stationary.
    By the EP we then conclude that the same must happen in a gravitational field, meaning that light is bent downwards by a gravitational field.
    
    \chapter{GR Spacetime and Equations of Motion}
    In special relativity the equation of motion of a free particle is
    \begin{equation}
        \diff[2]{x^\mu}{\tau} = 0
    \end{equation}
    where \(x^\mu\) is the four-position of the particle.
    This is simply a statement that the particle isn't accelerating.
    This statement is not covariant (in the sense of the principle of general covariance).
    It will be our goal to come up with an equivalent statement in general relativity.
    To do so we will use the equivalence principle.
    
    \section{Affine Connection}
    Consider some freely moving particle in a gravitational field.
    By the EP there is some locally inertial frame in which the four-position of the particle is \(\xi^\mu = (ct, \vv{x})\).
    We can parametrise this with the proper time, \(\tau\), so that the coordinates are a function of \(\tau\), \(\xi^\mu(\tau)\).
    In this locally inertial frame SR holds and so
    \begin{equation}\label{eqn:LIF EOM}
        \diff[2]{\xi^\alpha}{\tau} = 0.
    \end{equation}
    As well as this we have that the SR spacetime interval is
    \begin{equation}
        c^2 \dd{\tau}^2 = c^2\dd{t}^2 - \dl{\vv{x}} \cdot \dl{\vv{x}} = \eta_{\mu\nu}\dl{\xi^\mu}\dl{\xi^\nu}.
    \end{equation}
    
    In a general frame denote the four-position of the particle by \(x^\mu\).
    The chain rule gives
    \begin{equation}
        \dl{\xi^\mu} = \diffp{\xi^\mu}{x^\nu} \dd{x^\nu}.
    \end{equation}
    Recall that we are employing the Einstein summation convention so the right hand side is summed over \(\nu\) from 0 to 3.
    We can also express the time derivative in terms of these arbitrary-frame coordinates:
    \begin{equation}
        \diff{}{\tau} = \diff{x^\mu}{\tau} \diffp{}{x^\mu}.
    \end{equation}
    Since \(x^\mu = x^\mu(\tau)\) is parametrised by \(\tau\) the first derivative is a total derivative whereas the second is a partial derivative because in general the function to be differentiated will be a function of all four spacetime coordinates.
    
    Applying this derivative to \(\xi^\alpha\) we get
    \begin{equation}
        \diff{\xi^\alpha}{\tau} = \diff{x^\mu}{\tau}\diffp{\xi^\alpha}{x^\mu}.
    \end{equation}
    We want the second proper-time derivative of \(\xi^\alpha\) since this is what appears in \cref{eqn:LIF EOM}.
    Applying the proper-time derivative a second time and using the product rule we get
    \begin{align}
        0 &= \diff[2]{\xi^\alpha}{\tau}\\
        &= \diff{}{\tau}\left( \diffp{\xi^\alpha}{x^\mu} \diff{x^\mu}{\tau} \right)\\
        &= \diff{x^\nu}{\tau}\diffp{}{x^\nu}\left( \diffp{\xi^\alpha}{x^\mu} \diff{x^\mu}{\tau} \right)\\
        &= \diffp{\xi^\alpha}{x^\mu}\diff[2]{x^\mu}{\tau} + \diff{x^\mu}{\tau} \diff{x^\nu}{\tau} \diffp{\xi^\alpha}{x^\mu, x^\nu}.
    \end{align}
    
    We can identify \(\diff[2]{x^\mu}/{\tau}\) as the acceleration in the general frame.
    We want this quantity on it's own in order to find the equation of motion.
    However, we can't just divide through by \(\diffp{\xi^\alpha}/{x^\mu}\) since the \(\mu\) is summed over.
    Instead we multiply the entire equation by \(\diffp{x^\lambda}{\xi^\alpha}\), which seems awfully like dividing by \(\diffp{\xi^\alpha}{x^\mu}\), and then we use
    \begin{equation}
        \diffp{\xi^\alpha}{x^\mu}\diffp{x^\lambda}{\xi^\alpha} = \diffp{x^\lambda}{x^\mu} = \tensor{\delta}{^\lambda_\mu}.
    \end{equation}
    We can then use \(\tensor{\delta}{^\lambda_\mu}V^\mu = V^\lambda\).
    Doing this we have
    \begin{align}
        0 &= \diffp{x^\lambda}{\xi^\alpha} \diffp{\xi^\alpha}{x^\mu}\diff[2]{x^\mu}{\tau} + \diffp{x^\lambda}{\xi^\alpha} \diff{x^\mu}{\tau} \diff{x^\nu}{\tau} \diffp{\xi^\alpha}{x^\mu, x^\nu} \\
        &= \tensor{\delta}{^\lambda_\mu} \diff[2]{x^\mu}{\tau} + \diffp{x^\lambda}{\xi^\alpha} \diff{x^\mu}{\tau} \diff{x^\nu}{\tau} \diffp{\xi^\alpha}{x^\mu, x^\nu} \\
        &= \diff[2]{x^\lambda}{\tau} + \diff{x^\mu}{\tau} \diff{x^\nu}{\tau} \diffp{x^\lambda}{\xi^\alpha} \diffp{\xi^\alpha}{x^\mu, x^\nu} \\
        &= \diff[2]{x^\lambda}{\tau} + \christoffel{\lambda}{\mu}{\nu} \diff{x^\lambda}{\tau} \diff{x^\nu}{\tau}.
    \end{align}
    This final equation is called the \defineindex{geodesic equation} and is the equation of motion for an otherwise free particle in a gravitational field.
    We identify
    \begin{equation}\label{eqn:affine connection}
        \christoffel{\lambda}{\mu}{\nu} \coloneqq \diffp{x^\lambda}{\xi^\alpha}\diffp{\xi^\alpha}{x^\nu, x^\mu}.
    \end{equation}
    This is called the \defineindex{affine connection}\index{\(\christoffel{\lambda}{\mu}{\nu}\), affine connection} or \define{Christoffel symbol}\index{Christoffel symbol|see{affine connection}}.
    This same quantity is sometimes denoted \(\left\{\begin{smallmatrix} \lambda\\ \mu\nu \end{smallmatrix}\right\}\).
    Notice that due to the commutativity of partial derivatives the affine connection is symmetric in its lower indices so
    \begin{equation}
        \christoffel{\lambda}{\mu}{\nu} = \diffp{x^\lambda}{\xi^\alpha} \diffp{\xi^\alpha}{x^\nu, x^\mu} = \diffp{x^\lambda}{\xi^\alpha} \diffp{\xi^\alpha}{x^\mu, x^\nu} = \christoffel{\lambda}{\nu}{\mu}.
    \end{equation}
    For future reference the affine connection is \emph{not} a tensor.
    
    \subsection{Massless Particles}
    For a massless particle we cannot use \(\dl{\tau}\) as it s zero.
    Instead we can use \(\sigma = \xi^0\), which is \(ct\) in the LIF.
    The exact same logic then leads us to the equation of motion
    \begin{equation}
        \diff[2]{x^\lambda}{\sigma} + \christoffel{\lambda}{\mu}{\nu} \diff{x^\mu}{\sigma} \diff{x^\nu}{\sigma} = 0.
    \end{equation}
    Notice that we don't need to know what \(\sigma\) or \(\tau\) is in either the massive or massless case since we have four equations here so we can eliminate \(\sigma\) or \(\tau\) and still have enough equations to find the position of the particle, \(\vv{x}\), which is what we really care about.
    
    The equation of motion will be the same when ever we replace \(\tau\) with an \defineindex{affine parameter} which is linearly related to \(\tau\).
    
    \subsection{Physical Implications}
    The mathematics that lead to the affine connection and geodesic equation is fiddly, but not that difficult.
    However, the physical implications of this small amount of maths are profound.
    
    Notice that in the arbitrary frame the four-acceleration is quadratic in the four-velocity.
    So the gravitational force is velocity dependent.
    Compare this to the Lorentz force, \(\vv{F} = q(\vv{E} + \vv{v} \times \vv{B})\).
    There is a strong analogy between Newtonian gravity and non-relativistic electrostatics, namely that both follow an inverse square law.
    If this were to extend to a relativistic theory of gravity then we would expect the existence of \define{gravomagnetic fields}\index{gravomagnetic field}, which are generated by the motion of the mass, in an analogous way to how the magnetic field can be thought of a due to the motion of charge.
    We would also expect that gravitational effects would propagate as waves at the speed of light, and indeed we will see that this is true.
    
    Another even more profound implication is that spacetime is (probably) curved.
    This is the topic of the next section.
    
    \section{Metric Tensor}
    Consider the spacetime interval from SR in the LIF:
    \begin{align}
        c^2\dd{\tau}^2 &= \eta_{\alpha\beta}\dd{\xi^\alpha}\dd{\xi^\beta}\\
        &= \eta_{\alpha\beta}\left( \diffp{\xi^\alpha}{x^\mu}\dd{x^\mu} \right)\left( \diffp{\xi^\beta}{x^\nu}\dd{x^\nu} \right)\\
        &= g_{\mu\nu}\dd{x^\mu}\dd{x^\nu}.
    \end{align}
    Here we define the \defineindex{metric tensor}
    \begin{equation}\label{eqn:metric tensor}
        g_{\mu\nu} \coloneqq \diffp{\xi^\alpha}{x^\mu}\diffp{\xi^\beta}{x^n\nu}\eta_{\alpha\beta}.
    \end{equation}
    Notice that since \(\eta_{\alpha\beta}\) is symmetric by relabelling \(\alpha \leftrightarrow \beta\) we see that \(g_{\mu\nu}\) is symmetric.
    
    By the equivalence principle this metric structure of space carries over to the general frame but the metric is now \(g_{\mu\nu}\), instead of the simpler \(\eta_{\mu\nu}\) of SR.
    The existence of a nontrivial metric is a big step towards showing that spacetime is curved, but it doesn't quite prove this definitively yet.
    Consider the example of a sphere of radius \(r\) in polar coordinates a length element on the sphere is \(\dl{l}^2 = r^2(\dl{\vartheta}^2 + \sin^2\vartheta\dl{\varphi}^2) \ne r^2(\dl{\vartheta}^2 + \dl{\varphi}^2)\), so the metric in this case is
    \begin{equation}
        g_{\mu\nu} = 
        \begin{pmatrix}
            r^2 & 0\\
            0 & r^2\sin^2\vartheta
        \end{pmatrix}
        .
    \end{equation}
    The complexity of this is linked to the fact that the sphere is curved.
    
    However, consider now the plane parametrised by plane polar coordinates.
    A line element is \(\dl{l}^2 = \dl{r}^2 + r^2\dl{\varphi}^2\).
    The metric in this case is
    \begin{equation}
        g_{\mu\nu} = 
        \begin{pmatrix}
            1 & 0\\
            0 & r^2
        \end{pmatrix}
        .
    \end{equation}
    But we know that the plane is flat.
    Hence the existence of a complicated metric does not prove curvature.
    
    We need a way to discuss curvature independent of the coordinate choice, only then can we be sure that spacetime is truly curved, rather than just using coordinates in which the metric looks complicated.
    We will do this later in the course.
    
    \subsection{The Metric and Gravity}
    Currently the metric and affine connection are expressed as a relation between coordinates in two frames.
    We will now show that they can be expressed entirely in terms of a single frame in such a way that the metric determines the affine connection.
    This means that the metric determines particle dynamics and since the only force on the particle is gravity we can identify the metric tensor as a measure of gravity.
    
    First recall that
    \begin{equation}
        g_{\mu\nu} = \diffp{\xi^\alpha}{x^\mu}\diffp{\xi^\beta}{x^\nu}\eta_{\alpha\beta}.
    \end{equation}
    The affine connection has second order derivatives so we differentiate the metric:
    \begin{equation}
        \diffp{g_{\mu\nu}}{x^\lambda} = \diffp{\xi^\alpha}{x^\lambda, x^\mu} \diffp{\xi^\beta}{x^\nu} \eta_{\alpha\beta} + \diffp{\xi^\alpha}{x^\mu}\diffp{\xi^\alpha}{x^\lambda, x^\nu} \eta_{\alpha\beta}.
    \end{equation}
    Recall that the Minkowski metric, \(\eta_{\alpha\beta}\), is constant.
    
    Now consider the definition of the affine connection:
    \begin{equation}
        \christoffel{\lambda}{\mu}{\nu} = \diffp{x^\lambda}{\xi^\alpha} \diffp{\xi^\alpha}{x^\nu, x^\mu}.
    \end{equation}
    We can see that this term almost appears in the derivative of the metric tensor but in this we have terms like \(\diffp{\xi^\alpha}/{x^\mu}\), which is the wrong way compared to the \(\diffp{x^\lambda}{x^\alpha}\) in the affine connection and the indices don't quite match.
    We can get around this by multiplying by \(\diffp{\xi^\alpha}{x^\lambda}\) to get a derivative that does match:
    \begin{equation}
        \diffp{\xi^\alpha}{x^\lambda} \christoffel{\lambda}{\mu}{\nu} = \diffp{\xi^\alpha}{\lambda} \diffp{x^\lambda}{x^\alpha} \diffp{\xi^\alpha}{x^\nu, x^\mu}.
    \end{equation}
    We can then identify this term in the derivative of the metric:
    \begin{align}
        \diffp{g_{\mu\nu}}{x^\lambda} &= \christoffel{\rho}{\lambda}{\mu} \diffp{\xi^\alpha}{x^\rho} \diffp{\xi^\beta}{x^\nu} \eta_{\alpha\beta} + \christoffel{\rho}{\lambda}{\nu} \diffp{\xi^\alpha}{x^\mu} \diffp{\xi^\beta}{x^\rho} \eta_{\alpha\beta}\\
        &= \christoffel{\rho}{\lambda}{\mu} g_{\rho\nu} + \christoffel{\rho}{\nu}{\lambda} g_{\mu\rho}.\label{eqn:christoffel in terms of metric 1}
    \end{align}
    Here we have recognised the metric tensor appearing after substituting in the derivative-times-a-Christoffel-symbol.
    
    We are free to relabel the free indices as long as we are consistent.
    Exchanging \(\mu\) and \(\lambda\) we have
    \begin{equation}
        \diffp{g_{\lambda\nu}}{x^\mu} = \christoffel{\rho}{\mu}{\lambda} g_{\rho\nu} + \christoffel{\rho}{\lambda}{\nu} g_{\lambda\rho}.\label{eqn:christoffel in terms of metric 2}
    \end{equation}
    Notice that in the first term on the right hand side \(\christoffel{\rho}{\mu}{\lambda} = \christoffel{\rho}{\lambda}{\mu}\) by the symmetry of the affine connection in the lower indices.
    Instead exchanging \(\nu\) and \(\lambda\) we get
    \begin{equation}
        \diffp{g_{\mu\lambda}}{x^\nu} = \christoffel{\rho}{\nu}{\mu} g_{\rho \lambda} + \christoffel{\rho}{\nu}{\lambda} g_{\mu\rho}.\label{eqn:christoffel in terms of metric 3}
    \end{equation}
    Adding \cref{eqn:christoffel in terms of metric 1,eqn:christoffel in terms of metric 2} and subtracting \cref{eqn:christoffel in terms of metric 3} we get
    \begin{equation}
        \diffp{\gamma_{\mu\nu}}{x^\lambda} + \diffp{g_{\lambda\nu}}{x^\mu} - \diffp{g_{\mu\lambda}}{x^\nu} = 2\christoffel{\rho}{\lambda}{\mu}g_{\rho\nu}.
    \end{equation}
    
    We now define the inverse of the metric tensor to be \(g^{\mu\nu}\) which is such that
    \begin{equation}
        g^{\mu\nu}g_{\nu\lambda} = \tensor{\delta}{^\mu_\lambda}.
    \end{equation}
    We then have
    \begin{align}
        \christoffel{\sigma}{\lambda}{\mu} &= \frac{1}{2}g^{\nu\sigma} \left( \diffp{g_{\mu\nu}}{x^\lambda} + \diffp{g_{\lambda\nu}}{x^\mu} - \diffp{g_{\mu\lambda}}{x^\nu} \right)\\
        &= \frac{1}{2}g^{\nu\sigma} (\partial_\lambda g_{\mu\nu} + \partial_\mu g_{\lambda\nu} - \partial_\nu g_{\mu\lambda}).
    \end{align}
    
    We see that the affine connection, and hence the geodesic equation and the motion of the particle, depends on gradients of \(g_{\mu\nu}\).
    This justifies us describing \(g_{\mu\nu}\) as gravitational potentials.
    Notice that there are 10 potentials instead of the single gravitational in Newtonian gravity.
    There are 10 since \(g_{\mu\nu}\) has \(4\times 4 = 16\) components but setting the upper right triangle fixes the lower left triangle as \(g_{\mu\nu}\) is symmetric, and so there are \(16 - 6 = 10\) free components.
    
    If we have \(\gamma_{\mu\nu}\) as a function of position then we can, in theory, solve the geodesic equation for the motion of the particle.
    If there is some symmetry to the problem there are often simpler approaches, but in general the metric contains all of the information we need.
    So the only question left is where does the metric come from?
    The answer is that it is a solution to Einstein's field equations, which we will see later.
    
    Note that the inverse metric, \(g^{\mu\nu}\), also forms a metric structure in the sense that
    \begin{equation}
        c^2\dd{\tau}^2 = g^{\mu\nu}\dd{x_\mu}\dd{x_\nu} = g_{\mu\nu}\dd{x^\mu}\dd{x^\nu}.
    \end{equation}

    \chapter{Consequences of the Geodesic Equation}
    \section{Newtonian Limit}
    In this section we will consider the Newtonian limit of the geodesic equation.
    That is we assume that the three-velocity of the particle, \(\vv{v}\), is such that \(v \ll c\).
    We also assume that the gravitational field is weak and stationary, meaning that any time derivatives disappear (but not proper time derivatives).
    In full the geodesic equation is
    \begin{equation}
        \diff[2]{x^\lambda}{\tau} + \christoffel{\lambda}{\mu}{\nu} \diff{x^\mu}{\tau} \diff{x^\nu}{\tau} = 0.
    \end{equation}
    In the Newtonian limit \(\diff{x^i}/{\tau}\) is small compared to \(c\diff{t}/{\tau}\) and so dropping terms where \(\diff{x^i}/{\tau}\) appears we get
    \begin{equation}
        \diff[2]{x^\lambda}{\tau} + \christoffel{\lambda}{0}{0} c^2 \left( \diff{t}{\tau} \right)^2 \approx 0.
    \end{equation}
    
    For a stationary field \(\diffp{g_{\mu\nu}}{t} = 0\) and so
    \begin{equation}
        \christoffel{\lambda}{0}{0} = \frac{1}{2}g^{\mu\nu} \left( \partial_0 g_{0\nu} + \partial_0 g_{0\nu} - \partial_\nu g_{00} \right) = -\frac{1}{2}g^{\mu\nu}\partial_\nu g_{00}.
    \end{equation}
    
    We can always write
    \begin{equation}
        g_{\mu\nu} = \eta_{\mu\nu} + h_{\mu\nu}
    \end{equation}
    for some \(h_{\mu\nu}\).
    The assumption that the gravitational field is weak means that \(\abs{h_{\mu\nu}} \ll 1\), that is the components of \(h\) are small compared to the components of \(\eta\).
    We therefore have
    \begin{align}
        \christoffel{\lambda}{0}{0} &= -\frac{1}{2}g^{\mu\nu}\partial_\nu g_{00}\\
        &= -\frac{1}{2}(\eta^{\lambda\nu} + h^{\lambda\nu})\partial_\nu(\eta_{00} + h_{00})\\
        &= -\frac{1}{2} \eta^{\lambda\nu} \partial_\nu h_{00} - \frac{1}{2} h^{\lambda\nu} \partial_\nu h_{00}\\
        &= -\frac{1}{2} \eta^{\lambda\nu} \partial_\nu h_{00} + \order(h^2).
    \end{align}
    For a stationary field \(\partial_0 h_{00} = 0\) and so
    \begin{equation}
        \christoffel{i}{0}{0} = -\frac{1}{2}\eta^{ij}\partial_j h_{00}
    \end{equation}
    and \(\christoffel{0}{0}{0} = 0\).
    We then have
    \begin{equation}
        \eta^{ij}\partial_j = \partial^i = -\partial_j.
    \end{equation}
    The geodesic equation in this approximation is then reduced to
    \begin{equation}
        \diff[2]{x^\lambda}{\tau} + \frac{1}{2} c^2 \left( \diff{t}{\tau} \right)^2 \diffp{h_{00}}{x^\lambda}.
    \end{equation}
    Considering just the spatial parts we can write this as a three-vector equation:
    \begin{equation}\label{eqn:newtonian limit of geodesic}
        \diff[2]{\vv{x}}{\tau} = -\frac{1}{2}c^2 \left( \diff{t}{\tau} \right)^2 \grad h_{00}.
    \end{equation}
    The time component is
    \begin{equation}
        \diff[2]{x^0}{\tau} = 0
    \end{equation}
    since \(\christoffel{0}{0}{0}\) vanishes.
    The solution to this is \(\dl{t} = A\dd{\tau}\) for some constant \(A\).
    Substituting \(\dl{t}/A\) for \(\dl{\tau}\) in \cref{eqn:newtonian limit of geodesic} we get
    \begin{equation}
        A^2\diff[2]{\vv{x}}{t} = -\frac{1}{2}c^2\left( A\diff{t}{t} \right)^2 \grad h_{00} \implies \diff[2]{\vv{x}}{t} = -\frac{1}{2}c^2 \grad h_{00}.
    \end{equation}
    
    Compare this to the Newtonian equation of motion in a gravitational field, which is \(\diff[2]{\vv{x}}{t} = -\grad \Phi\), where \(\Phi\) is the gravitational field and so \(-\grad \Phi\) is the acceleration due to gravity.
    We see that
    \begin{equation}
        h_{00} = \frac{2\Phi}{c^2} + C.
    \end{equation}
    Here \(C\) is some constant.
    It is conventional to assume that fields vanish at infinity in which case at infinity we should have \(g_{\mu\nu} \to \eta_{\mu\nu}\) and so \(h_{\mu\nu} \to 0\), meaning \(C = 0\).
    Therefore in the weak field limit we have
    \begin{equation}
        g_{00} = 1 + \frac{2\Phi}{c^2}.
    \end{equation}
    This is only one of the \enquote{potentials} which contributes.
    In general all 10 independent components of \(g_{\mu\nu}\) contribute to the affine connection and hence the geodesic equation and particle dynamics.
    
    \section{Gravitational Time Dilation}
    The fact that \(g_{00}\) is not 1 relates to our earlier discovery of gravitational time dilation in \cref{sec:gravitational time dilation}.
    
    Consider a particle carrying a clock, this clock defines the proper time elapsed in the particles rest frame.
    We know that the proper time is given by
    \begin{equation}
        c^2\dd{\tau}^2 = g_{\mu\nu}\dd{x^\mu}\dd{x^\nu}.
    \end{equation}
    For a stationary clock \(\dl{x^i} = 0\) and so
    \begin{equation}
        c^2\dd{\tau}^2 = g_{00}c^2\dd{t^2}.
    \end{equation}
    We therefore have
    \begin{equation}
        \dl{\tau} = \sqrt{g_{00}} \dd{t}.
    \end{equation}
    Therefore for a clock in a gravitational field, in the Newtonian limit, we have
    \begin{equation}
        \dl{\tau} \approx \sqrt{1 + \frac{2\Phi}{c^2}} \dd{t} \approx \left( 1 + \frac{\Phi}{c^2} \right)\dd{t}
    \end{equation}
    where we have used the binomial expansion to expand the square root.
    
    From this we see that the proper time and coordinate time coincide only if there is no gravitational field.
    The coordinates \(x^\mu = (xt, \vv{x})\) are considered to be global and hence we can determine the spacetime interval between any two events.
    Further this interval will be agreed upon by any observer in any frame.
    We can therefore consider \(\dl{t}\) to be the time elapsed on a stationary clock at infinity, where \(\Phi = 0\).
    From the perspective of this observer the clock in the gravitational well, which is measuring the proper time, \(\dl{\tau}\), runs slow since \(\Phi < 0\) and so \(1 + \Phi/c^2 < 1\).
    This difference in clock speed is exactly the same as the difference computed using the equivalence principle.
    
    \subsection{Gravitational Redshift}
    As well as gravitational time dilation we argued that the equivalence principle leads to gravitational redshifting.
    We can explain this also using the Newtonian limit of the geodesic equation.
    
    Consider a stationary light emitter at \(\vv{x}_{\mathrm{e}}\) and a stationary observer at \(\vv{x}_{\mathrm{o}}\).
    Suppose that the time between peaks of the EM radiation emitted is \(\dl{t_{\mathrm{e}}}\), that is if one peak is emitted at time \(t_{\mathrm{e}}\) the next peak is emitted at \(t_{\mathrm{e}} + \dl{t_{\mathrm{e}}}\).
    This means \(\dl{t_{\mathrm{e}}}\) is the period of the radiation.
    This light signal propagates at the speed of light.
    The two peaks arrive at the observer at times \(t_{\mathrm{o}}\) and \(t_{\mathrm{o}} + \dl{t_{\mathrm{o}}}\).
    That is the observer measures the period of the radiation to be \(\dl{t_{\mathrm{o}}}\).
    
    Radiation must follow the \defineindex{null trajectory}, where \(\dl{\tau} = 0\).
    For a time independent metric we can write the time interval \(t_{\mathrm{o}} - t_{\mathrm{e}}\) as
    \begin{equation}
        t_{\mathrm{o}} - t_{\mathrm{e}} = \int f(x) \dd{x}
    \end{equation}
    for some function \(f\) of the spatial coordinates and the metric.
    Since the metric is time independent this coordinate time interval must be the same for all journeys and so \(\dl{t_{\mathrm{e}}} = \dl{t_{\mathrm{o}}} = \dl{t}\).
    Writing this in terms of proper time intervals in both frames we have
    \begin{equation}
        \dl{t} = \frac{\dl{\tau}}{\sqrt{g_{00}}} = \frac{\dl{\tau_{\mathrm{e}}}}{g_{00}(\vv{x}_{\mathrm{e}})} = \frac{\dl{\tau_{\mathrm{o}}}}{\sqrt{g_{00}(\vv{x}_{\mathrm{o}})}}.
    \end{equation}
    Rearranging this we have
    \begin{equation}
        \diff{\tau_{\mathrm{e}}}{\tau_{\mathrm{o}}} = \sqrt{\frac{g_{00}(\vv{x}_{\mathrm{e}})}{\vv{x}_{\mathrm{o}}}}.
    \end{equation}
    
    This is the ratio of emitted and observed periods and so is the ratio of observed and emitted frequencies, that is
    \begin{equation}
        \diff{\tau_{\mathrm{e}}}{\tau_{\mathrm{o}}} = \frac{\nu_{\mathrm{o}}}{\nu_{\mathrm{e}}}.
    \end{equation}
    In the Newtonian limit we therefore have
    \begin{equation}
        \frac{\nu_{\mathrm{o}}}{\nu_{\mathrm{e}}} \approx \sqrt{\frac{1 + 2\Phi_{\mathrm{o}}/c^2}{1 + 2\Phi_{\mathrm{e}}/c^2}} \approx 1 + \frac{\Phi_{\mathrm{e}}}{c^2} - \frac{\Phi_{\mathrm{o}}}{c^2}
    \end{equation}
    where \(\Phi_{\mathrm{e}}\) and \(\Phi_{\mathrm{o}}\) are the gravitational potentials at the emitter and observer, respectively.
    
    Defining the \defineindex{gravitational redshift}, \(z_{\mathrm{grav}}\), such that \(1 + z_{\mathrm{grav}} = \nu_{\mathrm{o}}/\nu_{\mathrm{e}}\) we see that
    \begin{equation}
        z_{\mathrm{grav}} \approx \frac{\Phi_{\mathrm{e}} - \Phi_{\mathrm{o}}}{c^2}.
    \end{equation}
    
    \chapter{Variational Formulation of GR}
    \section{Variational Principle}
    \begin{rmk}
        For more on variational methods and the Lagrangian method see thee notes from the Lagrangian Dynamics course.
    \end{rmk}
    The geodesic equation is named as such as the solutions are geodesics, by which we mean they are the shortest paths when accounting for curvature.
    Solving the equations of motion for a particle the correspond to extremising the path.
    Consider the SR situation of finding the path taken between two points.
    A straight line is the shortest path between the points.
    If the particle somehow made the same journey in the same time but the path is not straight then the particle must necessarily be moving faster on this non-straight path and hence will have \(\gamma > 1\), and so the proper time will be less since \(\Delta t = \gamma \Delta \tau\) is fixed.
    
    We can extract from this special relativity case of flat spacetime a general principle that \(\Delta \tau\) is maximised by whatever trajectory the particle takes.
    We can state this as a variational principle that
    \begin{equation}
        \delta \int \dl{\tau} = 0.
    \end{equation}
    We now notice an analogy with Lagrangian dynamics where we have Hamilton's principle, which states that the action \(S\) is maximised, and the action is given by an integral over the Lagrangian, \(\lagrangian(q, \dot{q})\), here \(q\) and \(\dot{q}\) are a generalised coordinate and generalised velocity.
    Hamilton's principle is then
    \begin{equation}
        \delta S = \int \lagrangian \dd{t} = 0.
    \end{equation}
    
    In classical mechanics the Lagrangian is given by \(\lagrangian = T - V\), where \(T\) is the kinetic energy and \(V\) is the (conservative) potential.
    For a free particle in one dimension we have \(T = m\dot{x}^2/2\) and \(V = V(x)\).
    We can then apply the Euler--Lagrange equations,
    \begin{equation}
        \diff*{\left( \diffp{\lagrangian}{\dot{q}} \right)}{t} - \diffp{\lagrangian}{q} = 0.
    \end{equation}
    In this one-dimensional free case we get
    \begin{equation}
        \diffp{\lagrangian}{x} =  -\diffp{V}{x} = m\ddot{x} = \diff*{(m\dot{x})}{t} = \diff*{\left( \diffp{\lagrangian}{\dot{x}} \right)}{t}.
    \end{equation}
    This is simply Newton's second law.
    
    In general relativity we can write the proper time between events \(A\) and \(B\) as an integral.
    We can then perform a change of variables to some parameter, \(p\), which increases monotonically along the world line:
    \begin{equation}
        c\tau_{AB} = c\int_{A}^{B} \dl{\tau} = c\int_{A}^{B} \diff{\tau}{p}\dd{p} = \int_{A}^{B} \lagrangian(x^\mu, \dot{x}^\mu) \dd{p}.
    \end{equation}
    Here we know we are looking for an equivalent to the Lagrangian in Hamilton's principle and so we identify the integrand as the Lagrangian:
    \begin{equation}
        \lagrangian(x^\mu, \dot{x}^\mu) = c\diff{\tau}{p}.
    \end{equation}
    We use the notation \(\dot{x} = \diff{x}/{p}\) in analogy to classical mechanics where \(\dot{x} = \diff{x}{t}\), since \(t\) plays the role of our parameter, \(p\), as it monotonically increases along the path in non-relativistic mechanics.
    
    In special relativity we can show that the Lagrangian takes the form
    \begin{equation}
        \lagrangian = c\diff{\tau}{p} = \sqrt{\eta_{\mu\nu} \diff{x^\mu}{p} \diff{x^\nu}{p}} = \sqrt{c^2\dot{t}^2 - \dot{x}^2 - \dot{y}^2 - \dot{z}^2}.
    \end{equation}
    For a free particle the Euler--Lagrange equations should give a solution of the form \(x^i = A + Bt\) for some constants \(A\) and \(B\).
    That is we should have motion in a straight line.
    
    The square root complicates the algebra somewhat so it would be nice to get rid of it.
    Fortunately we can do this by considering \(\lagrangian^2\).
    The justification for this is that if
    \begin{equation}
        c\Delta \tau = c\int \dl{\tau}
    \end{equation}
    then taking \(p = \tau\) we have \(\lagrangian = c\).
    In this case \(\lagrangian^2 = c^2\).
    We therefore have
    \begin{equation}
        \int \lagrangian^2 \dd{\tau} = c^2\int \dd{\tau} \implies \delta \int \lagrangian^2 \dd{\tau} = c^2 \, \delta\int \dd{\tau} = 0
    \end{equation}
    so \(\lagrangian^2\) is also such as to find a stationary path, meaning we can apply the Euler--Lagrange equations.
    
    First notice that
    \begin{equation}
        \diffp{\lagrangian^2}{x^\mu} = 0
    \end{equation}
    and so the Euler--Lagrange equations for \(\lagrangian^2\) are
    \begin{equation}
        \diff*{\left( \diffp{\lagrangian^2}{\dot{x}^\mu} \right)}{\tau} - \diffp{\lagrangian^2}{x^\mu} = \diff*{\left( \diffp{\lagrangian^2}{\dot{x}^\mu} \right)}{\tau} = 0.
    \end{equation}
    We can integrate this to get
    \begin{equation}
        \diffp{\lagrangian^2}{\dot{x}^\mu} = \text{constant}.
    \end{equation}
    The solution to this is then that \(x^\mu = A^\mu + B^\mu\tau\) for some constants \(A^\mu\) and \(B^\mu\) which are specific to each coordinate but don't necessarily form four-vectors.
    This means that we get motion along a straight line at constant velocity.
    Any accelerated trajectory differing from this will have a shorter proper time.
    
    At this point one may wonder why we introduced the parameter \(p\) if we were just immediately going to replace it with \(\tau\).
    The answer is we have to consider massless particles.
    Since these travel at the speed of light \(\tau = 0\), and so \(\tau\) cannot be used as a parameter.
    Instead we can use an affine parameter, which is a linear function of \(\tau\), say \(p \propto \tau\).
    In particular we could choose \(p \propto \gamma \tau = t\), since for massless particles \(t\) increases monotonically along the trajectory.
    The only difference is we instead have \(\lagrangian^2 = 0\) for massless particles.
    The result is the same and we usually aren't explicit about what \(p\) is since we eliminate it from any equations.
    
    By the equivalence principle in general relativity we can find the path by maximising \(\tau\) in a locally inertial frame.
    However, since \(\tau\) is an invariant this is equivalent to maximising \(\tau\) in any frame.
    The Lagrangian generalises in the obvious way, replacing \(\eta_{\mu\nu}\) with \(g_{\mu\nu}\), and the geodesic principle for \(\lagrangian^2\) holds so
    \begin{equation}
        \delta \int \lagrangian \dd{p} = 0, \qqwhere \lagrangian^2 = g_{\mu\nu} \diff{x^\mu}{p} \diff{x^\nu}{p}.
    \end{equation}
    
    \section{Calculating the Affine Connection}
    We now have a general scheme for calculating the affine connection, \(\christoffel{\lambda}{\mu}{\nu}\), without needing to compute derivatives of the metric.
    The process is as follows:
    \begin{enumerate}
        \item Write down the relativistic line element associated with the given spacetime.
        This is equal to \(c^2\dd{\tau}^2\).
        \item Convert this to \(\lagrangian^2\) by identifying \(\dl{x^\mu} \to \dot{x}^\mu\).
        \item Write down the Euler--Lagrange equations for \(\lagrangian^2\), remember that \(x^\mu\) and \(\dot{x}^\mu\) are independent variables.
        \item Rearrange these equations to the form \(\ddot{x}^\lambda + \tensor{[\text{something}]}{^\lambda_{\mu\nu}} \dot{x}^\mu \dot{x}^\nu = 0\).
        \item Identify \(\christoffel{\lambda}{\mu}{\nu} = \tensor{[\text{something}]}{^\lambda_{\mu\nu}}\).
        \item When \(\mu \ne \nu\) divide by 2.
        This is because \(\christoffel{\lambda}{\mu}{\nu} = \christoffel{\lambda}{\nu}{\mu}\) and in the geodesic equation we sum over \(\mu\) and \(\nu\) but in the Euler--Lagrange equations we don't, therefore both \(\christoffel{\lambda}{\mu}{\nu}\) and \(\christoffel{\lambda}{\nu}{\mu}\) are combined into one term in the Euler--Lagrange equations.
        \item If any set of indices for \(\Gamma\) don't appear in the equations then \(\Gamma\) is zero for these indices.
    \end{enumerate}
    
    \begin{exm}{Geodesics on a Sphere}{}
        On a sphere of radius \(R\) we can parametrise points by the polar coordinates \((\vartheta, \varphi)\).
        The line element between \((\vartheta, \varphi)\) and \((\vartheta + \dl{\vartheta}, \varphi + \dl{\varphi})\) is then
        \begin{equation}
            \dl{\ell}^2 = R^2 \dd{\vartheta}^2 + R^2\sin^2(\vartheta) \dd{\varphi}^2.
        \end{equation}
        Converting this to \(\lagrangian^2\) we get
        \begin{equation}
            \lagrangian^2 = R^2 \dot{\vartheta}^2 + R^2\sin^2(\vartheta) \dot{\varphi}^2.
        \end{equation}
        
        Applying the Euler--Lagrange equations for \(\lagrangian^2\) with respect to \(\vartheta\) we get
        \begin{align}
            \diffp{\lagrangian^2}{\vartheta} &= \diffp*{(R^2\dot{\vartheta}^2 + R^2\sin^2(\vartheta) \dot{\varphi}^2)}{\vartheta} = 2R^2\sin(\vartheta)\cos(\vartheta) \dot{\varphi}^2,\\
            \diff*{\left( \diffp{\lagrangian^2}{\dot{\vartheta}} \right)}{p} &= \diff*{\left( \diffp*{(R^2\dot{\vartheta}^2 + R^2\sin^2(\vartheta) \dot{\varphi}^2)}{\dot{\vartheta}} \right)}{p} = \diff*{(2R^2\dot{\vartheta})}{p} = 2R^2\ddot{\vartheta},\notag
        \end{align}
        and so
        \begin{equation}
            2R^2\ddot{\vartheta} - 2R^2\sin(\vartheta)\cos(\vartheta) \dot{\varphi}^2 = 0 \implies \ddot{\vartheta} - \sin(\vartheta)\cos(\vartheta) \dot{\varphi}^2 = 0.
        \end{equation}
        To aid in reading off the affine connection we write this with \(x^\vartheta = \vartheta\) and \(x^\varphi = \varphi\):
        \begin{equation}
            \ddot{x}^\vartheta - \sin(x^\vartheta)\cos(x^\vartheta) \dot{x}^\varphi \dot{x}^\varphi = 0.
        \end{equation}
        From this we compare to \(\ddot{x}^\lambda + \christoffel{\lambda}{\mu}{\nu} \dot{x}^\mu\dot{x}^\nu\) and we see that
        \begin{equation}
            \christoffel{\vartheta}{\varphi}{\varphi} = -\sin(\vartheta)\cos(\vartheta).
        \end{equation}
        
        Applying the Euler--Lagrange equations for \(\lagrangian^2\) with respect to \(\varphi\) we get
        \begin{align}
            \diffp{\lagrangian^2}{\varphi} &= \diffp*{(R^2\dot{\vartheta}^2 + R^2\sin^2(\vartheta)\dot{\varphi}^2)}{\varphi} = 0,\\
            \diff*{\left( \diffp{\lagrangian^2}{\dot{\varphi}} \right)}{p} &= \diff*{\left( \diffp*{(R^2\dot{\vartheta}^2 + R^2\sin^2(\vartheta)\dot{\varphi}^2)}{\dot{\varphi}} \right)}{p}\\
            &= \diff*{(2R^2\sin^2(\vartheta)\dot{\varphi})}{p} = 2R^2\sin^2(\vartheta)\ddot{\varphi} + 4R^2\sin(\vartheta)\cos(\vartheta) \dot{\vartheta}\dot{\varphi},
        \end{align}
        and so
        \begin{equation}
            2R^2\sin^2(\vartheta) \ddot{\varphi} + 4R^2\sin(\vartheta)\cos(\vartheta) \dot{\vartheta}\dot{\varphi} = 0 \implies \ddot{\varphi} + \cot(\vartheta) \dot{\vartheta} \dot{\varphi} = 0.
        \end{equation}
        Which we can rewrite as
        \begin{equation}
            \ddot{x}^\varphi + 2\cot(\vartheta) \dot{x}^\vartheta \dot{x}^\varphi = 0.
        \end{equation}
        We can then read off
        \begin{equation}
            \christoffel{\varphi}{\vartheta}{\varphi} = \christoffel{\varphi}{\varphi}{\vartheta} = \cot\vartheta.
        \end{equation}
        Be careful since in the Euler--Lagrange equation as written what we identify as \(\tensor{[\text{something}]}{^\vartheta_{\varphi\vartheta}}\) is actually \(\christoffel{\vartheta}{\varphi}{\vartheta} + \christoffel{\vartheta}{\vartheta}{\varphi} = 2\christoffel{\vartheta}{\varphi}{\vartheta}\), so we need to divide by 2.
        
        All affine connections which we haven't seen must be zero since we have accounted for all degrees of freedom in the Euler--Lagrange equations.
        
        For future use we write these affine connections as \(2 \times 2\) matrices, \(\Gamma_\mu\), where \(\tensor{(\Gamma_\mu)}{^\lambda_\nu} = \christoffel{\lambda}{\mu}{\nu}\):
        \begin{equation}\label{eqn:christoffel matrices S1}
            \Gamma_\vartheta = 
            \begin{pmatrix}
                0 & 0\\
                0 & \cot\vartheta
            \end{pmatrix}
            , \qqand \Gamma_\varphi = 
            \begin{pmatrix}
                0             & -\sin\vartheta\cos\vartheta\\
                \cot\vartheta & 0
            \end{pmatrix}
            .
        \end{equation}
    \end{exm}
    
    \chapter{Schwarzschild Metric}
    As an example of the tools we have developed so far we will consider the Schwarzschild metric.
    In October of 1915 Einstein published his field equations, which we will see in detail later, these relate gravity and energy and their solutions are the metric for the spacetime in question.
    In December 1915, while fighting in World War I, Karl Schwarzschild solved Einstein's field equations for the case of a spherically symmetric point mass.
    This was the first exact solution any one found to Einstein's field equations.
    In analogy with Gauss' law in electromagnetism this solution also holds outside of a spherically symmetric mass distribution, this is known as Birkhoff's theorem.
    However, unlike in electromagnetism we cannot then integrate over point masses to get all solutions since this only works if the superposition principle holds, which it doesn't for gravitational fields, which are nonlinear in nature.
    While Schwarzschild's original solutions allowed for a time dependent mass distribution we will assume here that the mass distribution is static.
    
    \section{Coordinate Freedom}
    \subsection{Distances on the 2-Sphere}
    In order to start our derivation of the Schwarzschild metric we will make use of the coordinate freedom that we have in general relativity, namely that we can choose various coordinate systems to describe the same spacetime.
    As an example of this we consider again a two-dimensional sphere of radius \(R\).
    We previously used the polar coordinates \((\vartheta, \varphi)\) to parametrise the sphere.
    Instead we can let \(r = R\vartheta\), \(r\) here is called the \defineindex{geodesic distance}.
    We can then parametrise the surface with the coordinates \((r, \varphi)\).
    
    The line element is then transformed from
    \begin{equation}
        \dl{\ell}^2 = R^2\dd{\vartheta}^2 + R^2\sin^2(\vartheta)\dd{\varphi}^2
    \end{equation}
    to
    \begin{equation}
        \dl{\ell}^2 = \dl{r}^2 + R^2\sin^2(r/R)\dd{\varphi}^2.
    \end{equation}
    The first term is now as simple as possible, and the second is more complicated.
    In the limit of \(R \to \infty\) we have
    \begin{align}
        \lim_{R \to \infty} R\sin(r/R) &= \lim_{R \to \infty} \frac{\sin(r/R)}{1/R}\\
        &= \lim_{R\to\infty} \frac{-rR^{-2}\cos(r/R)}{-R^{-2}}\\
        &= \lim_{R \to \infty} r\cos(r/R) = r
    \end{align}
    and we get
    \begin{equation}
        \dl{\ell}^2 = \dl{r}^2 + r^2\dd{\varphi}^2.
    \end{equation}
    This is the metric for a plane in polar coordinates, we can think of this as the generalisation of a straight line being a circle with infinite radius.
    
    There is no reason why we should make the first term simple and the second complicated.
    There is actually an infinite number of ways we can write the line element.
    For example if we want to make the second term simpler we might take \(\rho = R\sin(r/R)\) and so \(\dl{\rho} = \cos(r/R)\dd{r} = \sqrt{1 - \rho^2/R^2}\dl{r}\), which gives the line element as
    \begin{equation}
        \dl{\ell}^2 = \frac{\dl{\rho}^2}{1 0 \kappa \rho^2} + \rho^2 \dd{\varphi}^2
    \end{equation}
    where \(\kappa \coloneqq 1/R^2\).
    This form has the advantage that we can identify \(\kappa\) as the curvature of the sphere.
    We call \(\rho\) the \defineindex{angular diameter distance} since it can be be shown that the angle subtended by a line of length \(\dl{\ell_\perp}\) perpendicular to the line-of-sight is \(\dl{\varphi} = \dl{\ell_\perp}/\rho\).
    
    This form of the metric also generalises, clearly it holds for spheres, \(\kappa > 0\), but it also holds for planes, \(\kappa = 0\), and hyperbolic surfaces, \(\kappa < 0\).
    
    \subsection{SR Metric in Spherical Coordinates}
    In special relativity the line element is
    \begin{equation}
        c^2\dd{\tau}^2 = c^2\dd{t}^2 - \dl{\ell}^2.
    \end{equation}
    The spatial part can take the form
    \begin{equation}
        \dl{\ell}^2 = \dl{x}^2 + \dl{y}^2 + \dl{z}^2
    \end{equation}
    in Cartesian coordinates, \((x, y, z)\), or
    \begin{equation}
        \dl{\ell}^2 = \dl{r}^2 + r^2 \dd{\vartheta}^2 + r^2\sin^2(\vartheta) \dd{\varphi}^2
    \end{equation}
    in spherical coordinates, \((r, \vartheta, \varphi)\).
    We can also define \(\dl{\psi}^2 \coloneqq \dl{\vartheta}^2 + \sin^2(\vartheta) \dd{\varphi}^2\) and so
    \begin{equation}
        \dl{\ell}^2 = \dl{r}^2 + r^2 \dd{\psi}^2.
    \end{equation}
    We can identify \(\dl{\psi}\) as the angle between radial lines separated by \((\dl{\vartheta}, \dl{\varphi})\) on the surface of the sphere.
    The SR metric can then be written as
    \begin{equation}
        c^2\dd{\tau}^2 = c^2\dd{t}^2 - [\dl{r}^2 + r^2(\dl{\vartheta}^2 + \sin^2(\vartheta) \dd{\varphi}^2)].
    \end{equation}
    
    \section{Schwarzschild Metric in Spherical Coordinates}
    The \defineindex{Schwarzschild metric} applies outside of the point mass/spherically symmetric mass distribution, and hence in an empty, but curved, spacetime.
    The spherical symmetry of this setup gets us a fair way in finding the form of the metric.
    Clearly in a spherical geometry like this we should work in spherical coordinates.
    
    We can then assume that the Schwarzschild metric is a modified version of the SR metric in spherical polars, in particular that it takes the form
    \begin{align}
        c^2\dd{\tau}^2 &= e(r)c^2\dd{t}^2 - f(r)^2\dd{r}^2 - g(r)^2 r^2(\dl{\vartheta}^2 + \sin^2(\vartheta)\dd{\varphi}^2)\\
        &= e(rc^2\dd{t})^2 - f(r)^2\dd{r}^2 - g(r)^2 r^2 \dd{\psi}^2.
    \end{align}
    Here \(e\), \(f\), and \(g\) are spherically symmetric functions, hence depend only on \(r\), which we need to determine.
    They are also independent of time since we are assuming a stationary mass distribution.
    
    This is where the freedom of coordinates comes in.
    If \(e\), \(f\), and \(g\) are monotonic we are free to define \(r'\) such that \(r'^2 = g(r)^2r^2\).
    This works since our radial coordinate only needs to be monotonically increasing with distance from the point mass, so we can adopt \(r'\) as our radial coordinate.
    We can then rewrite the metric in terms of \(r'\) and the \(g(r)^2r^2\) term becomes unity.
    Doing so and dropping the prime we get
    \begin{equation}
        c^2\dd{\tau}^2 = e(r)c^2\dd{t}^2 - f(r)\dd{r}^2 - r^2(\dd{\vartheta}^2 + \sin^2(\vartheta) \dd{\varphi}^2).
    \end{equation}
    
    At large distances it is physically reasonable to assume the effect of the mass becomes small and we should recover the weak field approximation, and at infinity we should recover the special relativity metric.
    Recall that in the weak field limit \(g_{00} \approx 1 + 2\Phi/c^2\).
    
    Without using Einstein's field equations this is as far as we can get.
    We therefore simply quote the full solution which we will derive later:
    \begin{equation}
        c^2\dd{\tau}^2 = \alpha c^2\dd{t}^2 - \frac{1}{\alpha}\dd{r}^2 - r^2(\dl{\vartheta}^2 + \sin^2(\vartheta) \dd{\varphi}^2).
    \end{equation}
    Here
    \begin{equation}
        \alpha \coloneqq 1 + \frac{2\Phi}{c^2} = 1 - \frac{2GM}{c^2r} = 1 - \frac{r_{\mathrm{s}}}{r}
    \end{equation}
    where \(r_{\mathrm{s}} = 2GM/c^2\) is the \defineindex{Schwarzschild radius}.
    
    Some comments on the Schwarzschild metric:
    \begin{itemize}
        \item \(t\) is the coordinate time, corresponding to the time measured on a stationary clock at infinity.
        The proper time for a stationary clock at \((r, \vartheta, \varphi)\) is
        \begin{equation}
            \dl{\tau} = \dl{t}\sqrt{1 - \frac{2GM}{rc^2}} = \dl{t}\sqrt{1 + \frac{2\Phi}{c^2}} = \dl{t}\sqrt{1 - \frac{r_{\mathrm{s}}}{r}}
        \end{equation}
        which coincides with the weak field approximation when \(r \gg GM/c^2\), since we can then apply the binomial expansion to the square root to get
        \begin{equation}
            \dl{\tau} \approx \dl{t}\left( 1 - \frac{GM}{rc^2} \right) = \dl{t}\left( 1 + \frac{\Phi}{c^2} \right) = \dl{t}\left( 1 - \frac{r_{\mathrm{s}}}{2r} \right).
        \end{equation}
        \item Time is curved, since \(g_{00} = 1 + 2\Phi/c^2 \ne 1\).
        \item Space is curved a similar amount, since \(g_{rr} = -1/\alpha \approx -1 - GM/(rc^2)\) to first order in \(1/r\), which has a similar absolute value to \(g_{00}\).
        This surprised Einstein who expected that the curvature would occur for time only.
        \item Something strange occurs when \(r = r_{\mathrm{s}}\).
        Setting \(\dl{\tau} = 0\), so considering massless particles, we see that the coordinate speed of light is \(\diff{r}/{t} = 0\) at \(r = r_{\mathrm{s}}\), meaning that light signals cannot reach \(r < r_{\mathrm{s}}\), we have an event horizon.
    \end{itemize}

    \chapter{Orbits in the Schwarzschild Metric}
    \section{Basic Orbits}
    We can now start deriving equations of motion for particles orbiting a point mass or some other spherically symmetric mass distribution.
    We start by writing the squared Lagrangian in the Schwarzschild metric, which we do by taking the metric,
    \begin{equation}
        c^2 \dd{\tau}^2 = \alpha c^2 \dd{t}^2 - \frac{1}{\alpha}\dd{r}^2 - r^2(\dd{\vartheta}^2 + \sin^2(\vartheta)\dd{\varphi}^2)
    \end{equation}
    and changing \((\dd{x}^\mu)^2 \to (\dot{x}^\mu)^2\):
    \begin{equation}
        \lagrangian^2 = \alpha c^2 \dot{t}^2 - \frac{1}{\alpha} \dot{r}^2 - r^2 (\dot{\vartheta}^2 + \sin^2(\vartheta)\dot{\varphi}^2).
    \end{equation}
    We now apply the Euler--Lagrange equations to this.
    Starting with \(t\) we have
    \begin{equation}
        \diffp{\lagrangian^2}{t} = 0 \implies \diffp{\lagrangian^2}{\dot{t}} = 2\alpha c^2\dot{t} = \text{constant}.
    \end{equation}
    Remembering that \(\alpha\) depends on \(r\) we have
    \begin{equation}
        \alpha \dot{t} = k = \text{constant}.
    \end{equation}

    We interpret this as saying that there is time dilation, since if there wasn't then we would expect \(\dot{t} = 0\).
    This form makes sense when considering non-relativistic weak fields.
    We would expect both special-relativistic time dilation, providing a factor of \(\gamma\), and gravitational time dilation, providing a factor of \(1 + GM/(c^2r)\).
    As a result we expect
    \begin{equation}
        \dl{t} = \gamma \left( 1 + \frac{GM}{c^2r} \right) \dl{\tau} \implies \diff{t}{\tau} = \gamma \left( 1 + \frac{GM}{c^2r} \right).
    \end{equation}
    Expanding \(\gamma \approx 1 + v^2/(2c^2)\) we get
    \begin{equation}
        \dot{t} \approx 1 + \frac{v^2}{2c^2} + \frac{GM}{c^2r}.
    \end{equation}
    Noticing that \(v^2/2 - GM/r = E\) is the total energy per unit mass this result is equal to the full GR time dilation equation to first order when we set \(k = 1 + E/c^2\) or \(k = E/c^2\) if we include the rest mass in the total energy.
    
    The \(\varphi\) Euler--Lagrange equations give
    \begin{equation}
        \diffp{\lagrangian^2}{\varphi} = 0 \implies \diffp{\lagrangian^2}{\dot{\varphi}} = -2r^2\sin^2(\vartheta)\dot{\varphi} = \text{constant}.
    \end{equation}
    We are free to choose the orbit to occur in the \(\vartheta = \pi/2\) plane and hence this becomes
    \begingroup
    \setlength{\abovedisplayskip}{-5pt}
    \begin{equation}
        -2r^2\dot{\varphi} = \text{constant}
    \end{equation}
    \endgroup
    so we define
    \begin{equation}
        r^2\dot{\varphi} = h = \text{constant}.
    \end{equation}
    We can identify \(h\) as the \defineindex{specific angular momentum}, which is the angular momentum per unit mass and this equation is therefore conservation of angular momentum.
    
    The radial Euler--Lagrange equation is significantly more complicated, since \(\alpha\) depends on \(r\).
    Fortunately we can use the fact that \(\lagrangian^2 = c^2\) for a massive particle to rearrange the Lagrangian-squared into a more familiar form.
    Currently, after inserting our newly defined constants, the Lagrangian-squared equation reads
    \begin{equation}
        \lagrangian^2 = c^2 = c^2\alpha\frac{k^2}{\alpha^2} - \frac{\dot{r}^2}{\alpha} - r^2\frac{h^2}{r^4}.
    \end{equation}
    Rearranging this we get
    \begin{equation}
        \dot{r}^2 + \alpha\frac{h^2}{r^2}  = c^2(k^2 - \alpha) = c^2k^2 - c^2 + \frac{2GM}{r}.
    \end{equation}

    Compare this to the Newtonian result for orbits
    \begin{equation}
        \diff[2]{r}{t} = -\frac{GM}{r^2} + \frac{v_{\perp}^2}{r} = -\frac{GM}{r^2} + \frac{h_{\mathrm{N}}^2}{r^3}.
    \end{equation}
    Here \(v_{\perp}\) is the velocity perpendicular to \(\ve{r}\) and so \(h_{\mathrm{N}} = v_{\perp}r\) is the Newtonian specific angular momentum.
    Multiplying this by \(\diff{r}/{t}\) we get
    \begin{equation}
        \diff[2]{r}{t}\diff{r}{t} + \frac{GM}{r^2}\diff{r}{t} - \frac{h_{\mathrm{N}}^2}{r^3}\diff{r}{t} = 0.
    \end{equation}
    Integrating this with respect to time we get
    \begin{equation}
        \frac{1}{2}\left( \diff{r}{t} \right)^2 + \frac{h_{\mathrm{N}}^2}{r^2} - \frac{GM}{r} = \text{constant}.
    \end{equation}
    
    We can write the GR result as
    \begin{equation}\label{eqn:schwarzschild geodesic}
        \frac{\dot{r}^2}{2} + \frac{h^2}{2r^2} - \frac{GM}{r} - \frac{GMh^2}{r^3c^2} = \frac{1}{2}c^2(k^2 - 1) = \text{constant}.
    \end{equation}
    We see that the form of these equations is the same except that time is replaced with proper time in the GR case and the Newtonian specific angular momentum is replaced with the GR specific angular momentum.
    There is also an extra term in the GR equation which couples the gravitational field and angular momentum.
    This term has the same sign as the gravitational field and so acts as an additional attractive radial force.
    The origin of this term is the factor of \(1/\alpha\) in the radial component of the metric.
    This in turn arises because spacetime experiences spatial curvature as well as the time dilation predicted by the equivalence principle.
    
    \section{Effective Potentials}
    We can identify the first term of the GR radial energy equation as the kinetic energy per unit mass to first order and write the equation as
    \begin{equation}
        \frac{\dot{r}}{2} + \tilde{V}_{\mathrm{eff}} = \text{constant}.
    \end{equation}
    Here we've defined an effective potential per unit mass,
    \begin{equation}
        \tilde{V}_{\mathrm{eff}} \coloneqq -\frac{GM}{r} + \frac{h^2}{2r^2} - \frac{GMh^2}{r^3c^2}.
    \end{equation}
    
    We can make this equation dimensionless by defining a \enquote{gravitational radius}, \(r_{\mathrm{g}} \coloneqq GM/c^2 = r_{\mathrm{s}}/2\), a dimensionless radius, \(R \coloneqq r/r_{\mathrm{g}}\), a dimensionless angular momentum, \(H \coloneqq h / (r_{\mathrm{g}} c)\), and a dimensionless effective potential, \(V_{\mathrm{eff}} = \tilde{V}_{\mathrm{eff}}/c^2\).
    We then have the dimensionless equation
    \begin{equation}
        V_{\mathrm{eff}} = -\frac{1}{R} + \frac{H^2}{2R^2} - \frac{H^2}{R^3}.
    \end{equation}
    The third term means that our analysis of an orbit depends on the value of \(H\).
    
    This equation for the radial energy with an effective potential is very informative.
    The simplest orbits are circular, with \(\dot{r} = 0\).
    These orbits occur at the minima of the effective potential.
    For the Newtonian case there is always a minimum.
    Coming in from infinity the potential becomes gradually more negative before the centrifugal barrier, \(H^2/(2R^2)\), begins to dominate at small distances and so the particle never reaches the origin.
    The energy in this case will be whatever the energy is at the minimum of the potential.
    If we raise the energy above the minimum while keeping \(H\) fixed then we get radial oscillations which lead to an elliptical orbit in the Newtonian case.
    
    We can differentiate the radial energy equation to get a force law (per unit mass).
    Since
    \begin{equation}
        \diff{}{r} = \diff{\tau}{r}\diff{}{\tau} = \frac{1}{\dot{r}}\diff{}{\tau}
    \end{equation}
    differentiating the radial energy equation gives
    \begin{equation}
        \ddot{r} + \diff{V_{\mathrm{eff}}}{r} = 0.
    \end{equation}
    So it is not enough to have \(\dot{r} = 0\), we also need the gradient of \(V_-\mathrm{eff}\) to vanish to have a stable circular orbit.
    Solving for when this happens we find that
    \begin{equation}
        H = \frac{R}{\sqrt{R - 3}} \implies R = \frac{1}{2}(H^2 \pm \sqrt{H^4 - 12H^2}).
    \end{equation}
    In the Newtonian case we had \(R = H^2\).
    
    The fact that we have two different values of \(R\) for a given value of \(H\) is confusing.
    The reason is clear if we differentiate again the derivative will have opposite signs at these two positions and we find that the stable orbit occurs when \(R = (H^2 + \sqrt{H^4 - 12H^2})/2\).
    The inner orbit is a maximum and so is unstable.
    At \(H = \sqrt{12}\) these two orbits coincide.
    For smaller values of \(H\) there are no stable orbits as there is no longer a minimum in \(V_{\mathrm{eff}}\).
    This means there is an \defineindex{innermost stable orbit}, which is not something we see in Newtonian orbits.
    This innermost stable orbit occurs at \(R = 6\), any circular orbit closer to the origin is unstable.
    The effective potential, and its Newtonian counterpart, are plotted in \cref{fig:effective potential}.
    
    \begin{figure}
        \tikzsetnextfilename{effective-schwarzschild-potential}
        \begin{tikzpicture}
            \begin{semilogxaxis}[
                no markers,
                domain=1:100,
                ymin = -0.2,
                ymax = 0.2,
                xlabel = {Radius \(r/(GM/c^2)\)},
                ylabel = {Potential \(V_{\mathrm{eff}}/c^2\)},
                title = {Effective Potential as a Function of Radial Distance},
                width=0.9\textwidth,
                samples=100,
                legend style = {font = \footnotesize},
                legend entries = {1,,2,,\(\sqrt{12}\),,4,,5,,6,,},
                log ticks with fixed point
                ]
                \addplot[highlight] {-1/x + 1 / (2*x*x) - 1 / (x*x*x)};
                \addplot[highlight, dashed] {-1/x + 1 / (2*x*x)};
                \addplot[my blue] {-1/x + 4 / (2*x*x) - 4 / (x*x*x)};
                \addplot[my blue, dashed] {-1/x + 4 / (2*x*x)};
                \addplot[my red] {-1/x + 12 / (2*x*x) - 12 / (x*x*x)};
                \addplot[my red, dashed] {-1/x + 12 / (2*x*x)};
                \addplot[my green] {-1/x + 16 / (2*x*x) - 16 / (x*x*x)};
                \addplot[my green, dashed] {-1/x + 16 / (2*x*x)};
                \addplot[my purple] {-1/x + 25 / (2*x*x) - 25 / (x*x*x)};
                \addplot[my purple, dashed] {-1/x + 25 / (2*x*x)};
                \addplot[orange, domain=2:3] {-1/x + 36 / (2*x*x) - 36 / (x*x*x)};
                \addplot[orange, domain=4:100] {-1/x + 36 / (2*x*x) - 36 / (x*x*x)};
                \addplot[orange, dashed, domain=6:100] {-1/x + 36 / (2*x*x)};
                
                \tikzset{min node/.style={#1, fill=#1, inner sep=0pt, minimum width=5pt, circle}}
                \tikzset{cross/.style={cross out, draw=#1, minimum size=2*(2.5pt-\pgflinewidth), inner sep=0pt, outer sep=0pt}}
                
                \node[cross=highlight] at (axis cs: 1, -0.5) {};
                \node[cross=my blue] at (axis cs: 4, -1/8) {};
                \node[cross=my red] at (axis cs: 12, -1/24) {};
                \node[cross=my green] at (axis cs: 16, -1/32) {};
                \node[cross=my purple] at (axis cs: 25, -1/50) {};
                \node[cross=orange] at (axis cs: 36, -1/72) {};
                
                \node[min node=my red] at (axis cs: 6, -1/18) {};
                \node[min node=my green] at (axis cs: 12, -1/27) {};
                \node[min node=my purple] at (axis cs: 21.51, -0.022) {};
                \node[min node=orange] at (axis cs: 32.7, -0.015) {};
            \end{semilogxaxis}
        \end{tikzpicture}
        \caption[Effective Schwarzschild potential.]{The effective potential, \(V_{\mathrm{eff}}\), plotted for \(H = 1, 2, \sqrt{12}, 4, 5, 6\), increasing in energy as \(H\) increases. The dashed line is the Newtonian effective potential at the same angular momentum. The minima are indicated with circles (GR) and crosses (Newtonian). Strictly for the \(H = \sqrt{12}\) case its actually a point of inflection rather than a minimum.}
        \label{fig:effective potential}
    \end{figure}
    
    In a Newtonian orbit the velocity is \(v = L/r\), where \(L\) is the Newtonian specific angular momentum.
    We know that \(L \propto \sqrt{r}\) and so \(v \propto 1/\sqrt{r}\) so the velocity increases as \(r\) becomes smaller.
    It is a sensible assumption then that the \(R = 3\) orbit corresponds to the point where a particle in a Newtonian orbit must achieve light speed travel.
    That is, the radius \(R = 3\) corresponds to the radius of the \defineindex{photon sphere}, which is the radius at which light orbits the mass.
    We can verify this fairly easily.
    The geodesic equation for photons are almost the same as for massive particles but instead of \(\lagrangian^2 = c^2\) we have \(\lagrangian^2 = 0\), which means that our equations have one fewer term and the radial energy equation for light is simply
    \begin{equation}
        \frac{\dot{r}^2}{2} = \frac{1}{2}\left( c^2k^2 - \frac{h^2\alpha}{r^2} \right) = 0.
    \end{equation}
    The effective potential is then
    \begin{equation}
        V_{\mathrm{eff}} = \frac{h^2\alpha}{2r^2}
    \end{equation}
    and the condition for a circular orbit is still that the gradient of the potential vanishes.
    This can be shown to happen at \(R = 3\).
    The second derivative at \(R = 3\) is still negative and so this is an unstable orbit.
    Photons can orbit at \(R = 3\) but will eventually either fall into the centre or escape to infinity.
    
    It's worth noting that for the sun \(R = 3\) corresponds to an orbiting distance of only \qty{4430}{\metre}, and that the Schwarzschild metric doesn't apply inside the mass.
    This means that light orbiting a mass only really applies when said mass is a black hole.
    
    \section{Binding Energy and Accretion Efficiency}
    In the Lagrangian formalism if the Lagrangian doesn't depend explicitly on a given coordinate, \(q\), so \(\diffp{\lagrangian}/{q} = 0\), there is a conserved quantity, \(\diffp{\lagrangian}/{\dot{q}}\).
    In addition if the Lagrangian doesn't depend on time then the Hamiltonian is conserved, which in most cases is the total energy.
    In GR time is a coordinate and if the Lagrangian doesn't depend explicitly on the time coordinate there is an energy-like conserved quantity, \(\diffp{\lagrangian}/{\dot{t}}\).
    We can compute this quantity using the chain rule:
    \begin{equation}
        \diffp{\lagrangian^2}{\dot{t}} = \diffp{\lagrangian^2}{\lagrangian} \diffp{\lagrangian}{\dot{t}} = 2\lagrangian \diffp{\lagrangian}{\dot{t}} \implies \diffp{\lagrangian}{\dot{t}} = \frac{1}{2\lagrangian}\diffp{\lagrangian^2}{\dot{t}} = \frac{1}{2c}\diffp{\lagrangian^2}{\dot{t}}
    \end{equation}
    where the last equality holds for a massive particle.
    
    Since \(\lagrangian^2 = g_{\mu\nu}\dot{x}^\mu \dot{x}^\nu\) only the terms with \(\dot{t}\) contribute so for the purposes of this derivative \(\lagrangian^2 = 2g_{0\mu} c\dot{t}\dot{x}^\mu\) plus terms independent of \(\dot{t}\).
    Here we have used the fact that \(g_{\mu\nu}\) is symmetric so both terms where \(\dot{x}^\mu = c\dot{t}\) and terms where \(\dot{x}^\nu = c\dot{t}\) can be written in this form, hence the factor of 2.
    We then have
    \begin{equation}
        \diffp{\lagrangian^2}{\dot{t}} = \diffp{}{\dot{t}} (2c g_{0\mu} \dot{t} \dot{x}^\mu) = 2cg_{0\mu}\dot{x}^\mu = 2c\dot{x}_0.
    \end{equation}
    In the SR limit we identify \(\diffp{\lagrangian}/{\dot{t}} = \gamma c\), since \(\dot{x}_\mu = (\gamma c, -\gamma\vv{v})\).
    This SR result is \(1/c\) times the energy per unit mass.
    In the GR Schwarzschild case we have \(\diffp{\lagrangian}/{\dot{t}} = \alpha c\dot{t}\) so we can identify \(k = \alpha\dot{t}\) as \(E/(mc^2)\), the ratio of the total energy to the rest mass.
    It then follows that
    \begin{equation}
        \alpha \frac{h^2}{r^2} = c^2(k^2 - \alpha) \implies k = \sqrt{\alpha\left( 1 + \frac{h^2}{r^2c^2} \right)}.
    \end{equation}
    For the innermost stable orbit, \(R = 6\) and \(H = \sqrt{12}\), we get
    \begin{equation}
        k = \sqrt{\frac{8}{9}} \approx 0.943.
    \end{equation}
    
    What this means is that a particle at the innermost stable orbit has a total energy about \qty{6}{\percent} less than its rest mass.
    To have reached this point the particle must have somehow converted its missing energy into another form.
    A black hole will usually have an \defineindex{accretion disc} of orbiting matter which is flattened by rotating.
    Different orbital angular velocities at different radii leads to friction in the accretion disc which radiates away energy as heat.
    We typically assume that the accretion disc has its innermost inner edge at the innermost stable orbit, beyond this matter rapidly falls into the black hole.
    This accretion process therefore liberates approximately \qty{6}{\percent} of the mass energy of the accreted matter as radiation.
    It is possible to achieve even higher efficiencies if the black hole is spinning, but for this we need the \defineindex{Kerr metric} instead of the Schwarzschild metric.
    
    \chapter{GR in the Solar System}
    In this chapter we will look at evidence for general relativity based on measurements taken within the solar system of various effects which cannot be explained with just Newtonian gravity.
    
    \section{Advance of the Perihelion of Mercury}
    We can apply the previous chapter on orbits to the Solar System.
    This was first done by Einstein in 1916 for Mercury.
    He wanted to know if GR could account for a peculiarity in Mercury's orbit which was unexplained at the time.
    There are two arguments to follow, the simple one and the hard one.
    The simple one treats orbits as perturbations on circular orbits, and the harder one treats orbits as perturbations on elliptical orbits.
    The simple one contains all the important ideas but the harder one is important to verify that what we are calculating for circular orbits does really generalise.
    
    \subsection{Simple Version}
    Start with the radial force equation
    \begin{equation}
        \ddot{r} = -\diff{V_{\mathrm{eff}}}{r}.
    \end{equation}
    Suppose we have an orbit that is very nearly circular, that is it's radial component is \(r = r_{\mathrm{c}} + \varepsilon\) where \(r_{\mathrm{c}}\) is a constant and \(\varepsilon\) is a time dependent parameter such that \(\varepsilon \ll r_{\mathrm{c}}\).
    We can then expand the potential in a Taylor series given
    \begin{equation}
        V_{\mathrm{eff}}(r) = V_0 + \diff{V_{\mathrm{eff}}}{r}\varepsilon + \order(\varepsilon^2).
    \end{equation}
    The left hand side of the force equation gives us \(\ddot{r} = \ddot{\varepsilon}\), and so\\
    \begin{align}
        \ddot{\varepsilon} &= -\diff{}{r}\left[ V_0 + \diff{V_{\mathrm{eff}}}{r}\varepsilon + \order(\varepsilon^2) \right]\\
        &= =\diff[2]{V_{\mathrm{eff}}}{r}\varepsilon + \order(\varepsilon^2).
    \end{align}
    So, to first order, the radius undergoes harmonic oscillations with angular frequency
    \begin{equation}
        \omega = \sqrt{\diff[2]{V_{\mathrm{eff}}}{r}}.
    \end{equation}
    
    The second derivative of \(V_{\mathrm{eff}}\) is not difficult to compute.
    The first derivative gives
    \begin{equation}
        \diff{V_{\mathrm{eff}}}{r} = \frac{GM}{r^2} - \frac{h^2}{r^3} + \frac{3GMh^2}{r^4c^2}
    \end{equation}
    We can eliminate one since we know \(\diff{V_{\mathrm{eff}}}/{r} = 0\) at \(r_{\mathrm{c}}\) and so
    \begin{equation}
        \frac{GM}{r^2} = \textcolor{my blue}{\frac{h^2}{r^3} - \frac{3GMh^2}{r^4c^2}}.
    \end{equation}
    The second derivative is
    \begin{equation}
        \diff[2]{V_{\mathrm{eff}}}{r} = -\frac{2GM}{r^3} + \frac{3h^2}{r^4} - \frac{12GMh^2}{r^5c^2}.
    \end{equation}
    Which we can then write as
    \begin{equation}
        \diff[2]{V_{\mathrm{eff}}}{r} = -\frac{2}{r}\left( \textcolor{my blue}{\frac{h^2}{r^3} - \frac{3GMh^2}{r^4c^2}} \right) + \frac{3h^2}{r^4} - \frac{12GMh^2}{r^5c^2}.
    \end{equation}
    A bit of rearranging then gives
    \begin{equation}
        \diff[2]{V_{\mathrm{eff}}}{r} = \frac{h^2}{r^4}\left( 1 - \frac{6GM}{c^2r} \right).
    \end{equation}
    
    For a Newtonian circular orbit with angular frequency \(\omega\) we would have \(h = r^2\omega\) and so \(\omega^2 = h^2/r^4\).
    This means that if the \(6GM/(c^2 r)\) term were absent or negligible the orbital and radial oscillation frequencies would be the same and so the orbit would close into an ellipse.
    However, if the \(6GM/(c^2 r)\) term is not negligible then this equality fails and so the perturbation reduces the frequency of the radial oscillations and the orbit has to make more than a whole rotation between two occurrences of \defineindex{perihelion}\footnote{closest approach to the sun}.
    The resulting orbit is a \defineindex{rosette orbit} undergoing \defineindex{precession}, meaning the perihelion point moves around as the body orbits, an example is shown in \cref{fig:rosette orbit}.
    Since \(h^2 \approx GMr\) the change in phase per orbit, which is \(\delta\omega (2\pi/\omega)\), can be written as
    \begin{equation}
        \Delta\varphi = \frac{6\pi GM}{c^2 r} \text{ radians per orbit}.
    \end{equation}
    
    \begin{figure}
        \tikzsetnextfilename{rosette-orbit}
        \begin{tikzpicture}
            \draw[domain=0:15*180, samples=1000, very thick] plot (\x:{1/(1+0.82*cos(\x*0.82))});
        \end{tikzpicture}
        \caption[Rosette orbit.]{A rosette orbit. Notice that the orbit doesn't close. This is far more exaggerated than any real orbit in the solar system.}
        \label{fig:rosette orbit}
    \end{figure}
    
    \subsection{Hard Version}
    The simple version assumes circular orbits.
    A more general approach assumes elliptical orbits.
    It is convenient to make a change of parameters by defining \(u \coloneqq 1/r\).
    Since we are interested in the shape of orbits, rather than how they evolve, we want to eliminate time and parametrise the orbit by \(\varphi\).
    We can do so by noticing that
    \begin{equation}
        \dot{r} = \diff{r}{\tau} = \diff{r}{\varphi}\diff{\varphi}{\tau} = \diff{r}{u}\diff{u}{\varphi}\diff{\varphi}{\tau} = \diff*{\left( \frac{1}{u} \right)}{u}\diff{u}{\varphi}\diff{\varphi}{\tau} = -\frac{1}{u^2}\diff{u}{\varphi}hu^2 = -h\diff{u}{\varphi}
    \end{equation}
    where we have used \(h = r^2\dot{\varphi}\) so \(\dot{\varphi} = h/r^2 = hu^2\).
    \Cref{eqn:schwarzschild geodesic} then becomes
    \begin{equation}
        \frac{h^2}{2}\left( \diff{u}{\varphi} \right)^2 + \frac{h^2u^2}{2} - GMu - \frac{GMh^2u^3}{c^2} = \frac{1}{2}c^2(k^2 - 1) = \text{constant}.
    \end{equation}
    Differentiating with respect to \(\varphi\) we get
    \begin{equation}
        h^2\diff{u}{\varphi}\diff[2]{u}{\varphi} + h^2u\diff{u}{\varphi} - GM\diff{u}{\varphi} - \frac{3GMh^2}{c^2}u^2\diff{u}{\varphi} = 0.
    \end{equation}
    Dividing by \(\diff{u}/{\varphi}\) we then have
    \begin{equation}
        \diff[2]{u}{\varphi} + u = \frac{GM}{h^2} + \frac{3GM}{c^2}u^2.
    \end{equation}
    In a Newtonian calculation the final term would be absent and this would give the equation for an elliptical orbit,
    \begin{equation}
        u(\varphi) = \frac{GM}{h^2}(1 + e\cos(\varphi))
    \end{equation}
    where \(e\) is the \defineindex{eccentricity} or \define{ellipticity}\index{ellipticity|see{eccentricity}} of the orbit and we have chosen \(\varphi = 0\) to be the perihelion.
    
    The final term corresponding to the GR correction is very small for Mercury's orbit, its approximately 10 million times smaller than the \(GM/h^2\) term.
    This means we can treat it as a perturbation.
    In order to do this we first define a dimensionless quantity
    \begin{equation}
        U \coloneqq u\frac{h^2}{GM}
    \end{equation}
    and so the equation of motion becomes
    \begin{equation}
        \diff[2]{U}{\varphi} + U = 1 + \varepsilon U^2 \qqwhere \varepsilon = \frac{3G^2M^2}{c^2h^2}.
    \end{equation}
    We can expand the solution as \(U = U_0 + U_1\), where \(U_0\) is the unperturbed Newtonian orbit, the solution for \(\varepsilon = 0\), which we know to be an ellipse given by \(U_0 = 1 + e\cos\varphi\).
    The perturbation, \(U_1\), is \(\order(\varepsilon)\) and so to linear order in \(\varepsilon\) we can replace \(U\) in \(\varepsilon U^2\) with \(\varepsilon U_0^2\), since any terms with \(\varepsilon U_1^2\) are \(\order(\varepsilon^3)\).
    The unperturbed equation, \(\diff[2]{U_0}/{\varphi} + U_0 = 1\), can be subtracted from the equation of motion above giving the equation
    \begin{align}
        \diff[2]{U_1}{\varphi^2} + U_1 &= \varepsilon U_0^2\\
        &= \varepsilon(1 + e\cos\varphi)^2\\
        &= \varepsilon(1 + 2e\cos\varphi + e^2\cos^2\varphi)\\
        &= \varepsilon\left( 1 + 2e\cos\varphi + \frac{e^2}{2}[1 + \cos(2\varphi)] \right),
    \end{align}
    where we have used the trig identity \(\cos^2\varphi = (1 + \cos(2\varphi))/2\) in the last step.
    The complementary function, that is the solution to the homogeneous equation, doesn't add anything new since it is proportional to \(U_0\) and so the solution is
    \begin{equation}
        U_1 = A + B\varphi \sin\varphi + C\cos(2\varphi).
    \end{equation}
    We need the extra factor of \(\varphi\) since \(\sin\varphi\) appears in the complementary function.
    The solution is then
    \begin{equation}
        U_1 = \varepsilon\left[ \left( 1 + \frac{e^2}{2} \right) + e\varphi\sin\varphi - \frac{e^2}{6}\cos(2\varphi) \right].
    \end{equation}
    We can take \(\varphi\) to be arbitrarily large so we can ignore all terms not proportional to \(\varphi\) and we get
    \begin{align}
        U &\approx 1 + e\cos\varphi + \varepsilon e \varphi \sin \varphi\\
        &\approx 1 _ e \cos[\varphi(1 - \varepsilon)].
    \end{align}
    This is accurate to \(\order(\varepsilon)\).
    The orbit is periodic in \(\varphi\) with period \(2\pi/(1 - \varepsilon)\).
    The result is that the perihelion moves forward through an angle of \(2\pi\varepsilon\) per orbit.
    This agrees with the circular orbit case to first order in \(\varepsilon\) and is independent of \(e\).
    
    It was known from about 1860 that the perihelion of Mercury was precessing and many solutions were sought before the existence of GR.
    The total observed effect is approximately \num{5600} arc seconds\footnote{1 arcsecond is \(1/3600\)th of a degree} per century.
    The majority of this, approximately 5026 arcseconds per century, can be explained by the precession of Earth's spin axis.
    The next largest contribution is explained by perturbations from the gravitational fields of the other planets, this explains another 531 arcseconds per century.
    There is a resulting 43 arcseconds per century drift that couldn't be explained before GR.
    One attempted fix was to posit a planet orbiting within the orbit of Mercury, this idea was popular enough that they named the planet, Vulcan, but this planet was obviously never observed.
    Including the GR correction for Mercury's orbit of \(T = 88\) days at \(r = \qty{5.8e10}{\metre}\) and \(e = 0.2\) we predict the required advance of Mercury's perihelion by 43 arcseconds per century.
    This was an early success for GR.
    
    Sceptics of GR could claim that all this calculation shows is the need for an extra \(1/r^3\) term in the potential, as might arise from a quadrupole moment, which would occur if the sun were less spherical, and it wouldn't even have to be that much less spherical to get the size of quadrupole moment needed.
    However, GR also predicts deviations for light which this quadrupole momentum couldn't explain.
    
    \section{Bending of Light around the Sun}
    \define{Gravitational lensing}\index{gravitational lensing} is the process by which light bends around a massive body, as if it was passing through a convex lens.
    We've discussed the bending of light qualitatively from the equivalence principle.
    Suppose a box contains a laser which points from one side of the box to the other, and the box accelerates perpendicular to the laser beam.
    Then in the time taken for the light to cross the box the wall of the box on the opposite side will have moved slightly and the laser will hit slightly lower than it would have had the box not been accelerating.
    By the equivalence principle a gravitational field will therefore result in light following a curved path.
    Of course, we know that light follows geodesics in spacetime, so we can interpret this curved path as the spatial part of a straight spacetime world line through a curved spacetime.
    
    The calculation for the magnitude of the bending of the light is not quite as simple as other effects of GR which can be seen from the equivalence principle.
    First, we consider the Newtonian argument which Einstein made in about 1912.
    Think of the photon as a massive particle moving at speed \(c\) and apply the equivalence principle to Galileo's \enquote{all objects fall equally fast}.
    We are interested in the deflection of the light, rather than the total effect on its velocity, and so we consider the gravitational acceleration, \(a_{\perp}\), perpendicular to the straight path that the light would follow absent the massive body.
    The equivalence principle suggests that this acceleration will correspond to a change in the perpendicular momentum, according to Newton's second law, with the \enquote{mass} of the light played by \(E/c^2 = p/c\):
    \begin{equation}
        \diff{p_{\perp}}{t} = a_\perp \frac{E}{c^2} = a_{\perp} \frac{p}{c}.
    \end{equation}
    To calculate the angle of deflection we just have to integrate this over the path of the light.
    The deflection angle is then given by
    \begin{equation}
        \vartheta = \frac{\delta p_{\perp}}{p} = \frac{1}{p} \int \diff{p_{\perp}}{t}t = \frac{1}{c}\int a_{\perp} \dd{t}.
    \end{equation}
    
    We assume that the deflection angle is small, which is normally the case.
    For small deflections we can use the \defineindex{Born approximation} to calculate this integral.
    This entails assuming that the photon follows some straight unperturbed trajectory and the acceleration at the photons actual position is close enough to the acceleration at the corresponding point on this straight trajectory that the difference is negligible.
    
    \begin{figure}
        \tikzsetnextfilename{light-bend}
        \begin{tikzpicture}
            \fill[yellow] (0, 0) circle [radius = 0.5];
            \draw[very thick, red] (-3, 1) -- (3, 1) node[text=black, right] {Unperturbed Path};
            \draw[red, domain=-3:3] plot (\x, -0.6/9*\x*\x + 1) node[right, text=black] {Perturbed Path};
            \draw (0, 0) coordinate (A) -- (0, 1) coordinate(C) node[pos=0.6, right] {\(R\)};
            \draw (0, 0) -- (-2, 1) coordinate(B) node[midway, below] {\(r\)};
            \path pic[draw, "\(\varphi\)", angle eccentricity=1.3] {angle};
            \path (0, 1) -- (-2, 1) node[midway, above] {\(x\)};
        \end{tikzpicture}
        \caption[Light bending around the sun.]{Light passing the sun follows a curved path, which we can approximate as a straight path. This path can be parametrised by \(x\). Defining the distance of closest approach to be \(R\) we then get that the distance between the mass and the photon is \(r = \sqrt{x^2 + R^2}\).}
    \end{figure}
    
    We consider the case of a point mass \(M\) being the gravitational source and, borrowing more terminology from scattering experiments we define the \defineindex{impact parameter}, \(R\), to be the closest distance of approach (this is usually called \(b\) in a scattering experiment).
    We parametrise the straight path with \(x\), defining \(x = 0\) to be the point of closest approach with \(x < 0\) before this point is reached and \(x > 0\) after the photon passes this point.
    The distance from the mass to the photon is then \(r\), which we can easily find from Pythagoras: \(r^2 = x^2 + R^2\).
    The perpendicular component of the acceleration is \(a_{\perp} = a\sin\varphi\) where \(\sin\varphi = R / r\)/
    It then follows that
    \begin{equation}
        a_{\perp} = a\sin\varphi = \frac{GM}{r^2} \frac{R}{r}.
    \end{equation}
    The position along the unperturbed path and the time are simply related by \(x = ct\), and so \(\dl{t} = \dl{t}/c\).
    The deflection angle is then given by integrating over the entire straight path:
    \begin{align}
        \vartheta &= \frac{1}{c^2}\int_{-\infty}^{\infty} \frac{GM}{r^2}\frac{R}{r} \dd{x}\\
        &= \frac{GM}{c^2R} \int_{-\infty}^{\infty} \frac{R^2}{r^3} \dd{x}\\
        &= \frac{GM}{c^2R} \int_{-\infty}^{\infty} \frac{(x^2 + R^2)^{3/2}}{R^2} \dd{x}\\
        &= \frac{GM}{c^2R} \int_{-\infty}^{\infty} \frac{1}{(1 + y^2)^{3/2}} \dd{y}
    \end{align}
    where we have used a substitution \(y = x/R\).
    we can then do a further substitution, \(y = \tan u\), so \(\dl{y} = \sec^2(u) \dd{u}\).
    We then have
    \begin{equation}
        (1 + y^2)^{3/2} = (1 + \tan^2 u)^{3/2} = (\sec^2 u)^{3/2} = \sec^3u.
    \end{equation}
    The integrand is even so we can modify the limits to \((0, \infty)\) and double the integral.
    The limits after substitution become the solutions to \(0 = \tan u\), which gives \(u = 0\), and \(\infty = \tan u\), which gives \(u = \pi/2\).
    Hence,
    \begin{align}
        \vartheta &= \frac{2GM}{c^2R} \int_{0}^{\pi/2} \frac{1}{\sec^3 u} \sec^2(u) \dd{u}\\
        &= \frac{2GM}{c^2R} \int_{0}^{\pi/2} \cos u \dd{u}\\
        &= \frac{2GM}{c^2R}[\sin u]_{0}^{\pi/2}\\
        &= \frac{2GM}{c^2R}.
    \end{align}
    
    The problem with this is that its wrong.
    It turns out to be half the size of the actual deflection.
    This is because we have failed to take into account the curvature of space.
    We are interested in weak gravitational fields outside a spherically symmetric mass.
    Spacetime is then described by the Schwarzschild metric
    \begin{equation}
        c^2\dd{\tau}^2 = \left( 1 + \frac{2\Phi}{c^2} \right)c^2\dd{t}^2 - \left( 1 - \frac{2\Phi}{c^2} \right)\dd{r}^2 - r^2(\dl{\vartheta}^2 + \sin^2\vartheta \dd{\varphi}^2).
    \end{equation}
    Here \(\Phi = -GM/r\) is the Newtonian potential.
    
    This form has been useful so far but it has one peculiarity.
    The curvature of space is constrained to the radial direction, the angular terms appear exactly the same as they do in a flat spacetime.
    This is due to our choice of radial coordinate, a mostly arbitrary choice which we can change.
    Suppose we define \(r' = r(1 + \Phi/c^2)\).
    Then since \(r\Phi = -GM\) is constant it follows that \(\dl{r'} = \dl{r}\).
    Changing the radial coordinate to \(r'\) also includes a factor of \(1(1 + \Phi/c^2)^2\) in the angular part of the metric, which we can expand for weak fields to give a factor of \(1 - 2\Phi/c^2\).
    The resulting form of the metric is called the \defineindex{isotropic form} of the metric, dropping the prime on the \(r\) this is:
    \begin{align}
        c^2\dd{\tau}^2 &= \left( 1 + \frac{2\Phi}{c^2} \right)c^2\dd{t}^2 - \left( 1 - \frac{2\Phi}{c^2} \right) \dd{r}^2 - \left( 1 - \frac{2\Phi}{c^2} \right) r'^2 (\dl{\vartheta}^2 + \sin^2 \vartheta \dd{\varphi}^2)\\
        &= \left( 1 + \frac{2\Phi}{c^2} \right) c^2 \dd{t}^2 - \left( 1 - \frac{2\Phi}{c^2} \right)(\dl{r}^2 + r^2[\dl{\vartheta}^2 + \sin^2\vartheta \dd{\varphi}^2])\\
        &= \left( 1 + \frac{2\Phi}{c^2} \right) c^2 \dd{t}^2 + \left( 1 - \frac{2\Phi}{c^2} \right)(\dl{x}^2 + \dl{y}^2 + \dl{z}^2).
    \end{align}
    In the last step we identified \(\dl{r}^2 + r^2(\dl{\vartheta}^2 + \sin^2\vartheta \dd{\varphi}^2)\) as the spatial component of a flat metric in spherical polars, which is necessarily the same as the spatial part of a flat metric in Cartesian coordinates, so we switch back to Cartesian coordinates.
    The curvature in this form is the same in all three Cartesian directions, as it must be since the mass is spherically symmetric.
    It turns out that this form of the metric actually applies to any static weak gravitational field, not just to the spherically symmetric mass.
    
    Now consider this metric for light, we then have \(\dl{\tau} = 0\), and so
    \begin{equation}
        \left( 1 + \frac{2\Phi}{c^2} \right) c^2 \dd{t}^2 = \left( 1 - \frac{2\Phi}{c^2} \right)\dl{\vv{x}}^2.
    \end{equation}
    This then gives us
    \begin{equation}
        \left( 1 + \frac{2\Phi}{c^2} \right) \left( 1 - \frac{2\Phi}{c^2} \right)^{-1} c^2 = \left( \diff{\vv{x}}{t} \right)^2.
    \end{equation}
    Expanding the second bracket we have \((1 - 2\Phi/c^2)^{-1} \approx 1 + 2\Phi/c^2\) and so taking the square root we get
    \begin{equation}
        \abs*{\diff{\vv{x}}{t}} = c\left( 1 + \frac{2\Phi}{c^2} \right).
    \end{equation}
    We can interpret this as the \defineindex{coordinate speed} of light.
    Notice that since \(\Phi < 0\) this is less than the speed of light.
    We can think of this as spacetime behaving like a medium with a refractive index \(1 + 2\Phi/c^2\), which depends on the gravitational field and hence, in general, on the position.
    
    It's clear from the derivation here that the temporal and spatial curvature give equal contributions to the change in the speed.
    In our earlier derivation of the deflection of light we considered only the temporal curvature.
    This explains why we were a factor of 2 too low.
    We were lead astray because our equivalence principle argument failed to account for the curvature of the spatial part of the metric since non-relativistic particles are not sensitive to the spatial curvature.
    
    We can now go through a full derivation of the light bending using the geodesic equation for a massless particle.
    The proper time for the photon is zero, \(\dl{\tau} = 0\), and so we instead use some affine parameter, \(p\).
    For a massless particle \(\lagrangian^2 = 0\) and we can apply the Euler--Lagrange equations to the Lagrangian
    \begin{equation}
        \lagrangian^2 = \alpha c^2\dot{t}^2 - \frac{1}{\alpha}\dot{r}^2 - r^2(\dot{\vartheta}^2 + \sin^2(\vartheta) \dot{\varphi}^2),
    \end{equation}
    which is exactly the same form as for a massive particle, except now \(\dot{x} = \diff{x}/{p}\).
    Applying the Euler--Lagrange equations for \(t\) and \(\varphi\) we get the same results,
    \begin{equation}
        \alpha\dot{t} = k, \qqand r^2\dot{\varphi} = h.
    \end{equation}
    The next step is slightly simpler since \(\lagrangian^2 = 0\) and so our radial energy equation is
    \begin{equation}
        0 = \alpha c^2 \dot{t}^2 - \frac{\dot{r}^2}{\alpha} - r^2 \dot{\varphi}^2,
    \end{equation}
    again choosing the orbit to be in the \(\vartheta = \pi/2\) plane.
    Rearranging this and writing \(u = 1/r\) we get
    \begin{equation}
        h^2 = \left( \diff{u}{\varphi} \right)^2 = c^2k^2 - \alpha h^2 u^2 = c^2k^2 - h^2u^2 + \frac{2GM}{c^2}h^2u^3.
    \end{equation}
    This is the same as the massive case except with one fewer term since \(\lagrangian^2 = 0\).
    Differentiating with respect to \(\varphi\) and dividing through by \(\diff{u}/{\varphi}\) we get
    \begin{equation}
        \diff[2]{u}{\varphi} + u = \frac{3GM}{c^2}u^2.
    \end{equation}
    
    We can treat this as a perturbation on the straight line orbit, which is given by \(u_0 = 1/r_0 = \sin(\varphi)/R\), where \(r_0\) is the distance to the unperturbed path and \(R\) is the distance of closest approach.
    We can then write \(u = u_0 + u_1\) for some small perturbation \(u_1\).
    Putting this into the differential equation above and using
    \begin{equation}
        \diff[2]{u_0}{\varphi} + u_0 = 0,
    \end{equation}
    as this is the unperturbed path we get
    \begin{equation}
        \diff[2]{u_1}{\varphi} + u_1 = \frac{3GM}{c^2R^2} \sin^2\varphi = \frac{3GM}{2c^2R^2}(1 - \cos(2\varphi)).
    \end{equation}
    It can be shown that to first order the solution to this is
    \begin{equation}
        u = \frac{\sin\varphi}{R} + \frac{3GM}{2c^2R^2} \left( 1 + \frac{1}{3}\cos(2\varphi) \right).
    \end{equation}
    
    At a large distance before the light passes the mass \(u \approx 0\) and \(\varphi\) is small so \(\sin\varphi \approx \varphi\) and \(\cos(2\varphi) \approx 1\).
    We can then consider when the photon is at negative infinity setting \(u_{-\infty} = 0\) and expanding the trig gives
    \begin{equation}
        0 = \frac{\varphi_{-\infty}}{R} + \frac{3GM}{2c^2R^2} \frac{4}{3} \implies \varphi_{-\infty} = -\frac{2GM}{c^2R}.
    \end{equation}
    Similarly once the light passes the mass at large distances \(u \approx 0\) and \(\varphi - \pi \approx 0\) so we can expand \(\sin(\varphi) = -\sin(\varphi - \pi) \approx \pi - \varphi\), and \(\cos(2\varphi) = \cos(2\varphi + 2\pi) \approx 1\) giving
    \begin{equation}
        0 = \frac{\pi - \varphi_{+\infty}}{R} + \frac{3GM}{2c^2R^2} \frac{4}{3} \implies \varphi_{+\infty} = \pi + \frac{2GM}{c^2R}.
    \end{equation}
    This angle would be \(\pi\) if not for the deflection, so the deflection is \(2GM/(c^2R)\).
    The total deflection is then
    \begin{equation}
        \Delta \varphi_{\mathrm{GR}} = \frac{4GM}{c^2R}.
    \end{equation}
    This is, as expected, twice as large as our more naive calculation.
    This result agrees well with 1919 measurements during a solar eclipse, the eclipse being necessary to see stars behind the sun.
    Putting in the mass of the sun as \(M\) and the radius of the sun as \(R\) we see that for light that just grazes the surface of the sun as it passes we expect an deflection of \(1.75\) arcseconds, a very small amount that is not easy to measure.
    
    \section{Time Delay of Light}
    The final test of GR in the solar system was proposed later than the two previous tests, since it required human inventions, rather than the normal dynamics of the solar system.
    In 1964 Irwin Shapiro suggested that a radar signal could be bounced off of Venus and the time for the round trip measured.
    He then went on to complete exactly this measurement in 1966.
    He proposed that if this was done at a time when the radar signal had to pass the sun to reach Venus the signal would be slowed by the previously mentioned changing coordinate speed of light and that this could be measured.
    
    Since the change in the coordinate speed of light is a factor of \(1 + 2\Phi/c^2 = 1 + 2GM/(c^2r)\) the accumulated time delay over a journey through a distance \(\dl{x}\) is
    \begin{equation}
        \Delta t = \frac{2GM}{c^2r} \frac{\dl{x}}{c}.
    \end{equation}
    Taking \(x\) as a parameter increasing along the straight line between the two planets this is related to the distance to the sun by \(r^2 = x^2 + R^2\), where once again \(R\) is the distance of closest approach.
    We then have \(\dl{x} = r \dd{r} / \sqrt{r^2 - R^2}\).
    
    The time delay for the entire journey is given by integrating the path from Earth to Venus, and then doubling to account for the return journey.
    The one subtlety is that this is a principle value integral since at \(r = R\) the integrand diverges.
    So we split the integral at this point and consider a limiting process of the limits tending to \(R\).
    The resulting time delay can then be calculated as
    \begin{equation}
        \Delta t = \frac{4GM}{c^3} \left( \int_R^{r_{\mathrm{E}}} \frac{1}{\sqrt{r^2 - R^2}} \dd{r} + \int_{R}^{r_{\mathrm{V}}} \frac{1}{\sqrt{r^2 - R^2}} \dd{r} \right).
    \end{equation}
    Here \(r_{\mathrm{E}}\) and \(r_{\mathrm{V}}\) are the radii of Earth and Venus's orbits respectively, these are the end points since we are assuming that Earth and Venus are approximately on opposite sides of the sun and hence the first path takes us from Earth to the sun and the second from the sun to the Earth.
    
    Notice that both integrals have \(R\) as a lower boundary.
    This is because for the first integral the path goes from Earth to the sun, but since \(x\) is negative here we get \(\dl{r}\) as the negative root but then use this negative sign to reverse the order of the integration limits.
    
    These integrals aren't too difficult and we can define \(u = r / R\) and recognise the resulting integral as inverse hyperbolic cosine and so the first integral gives
    \begin{equation}
        \ln\left[ \frac{r_{\mathrm{E}}}{R} + \sqrt{ \frac{r_{\mathrm{E}}}{R} - 1} \right],
    \end{equation}
    and the second integral is the same substituting for the radius of Venus's orbit.
    
    In the limit of where the orbital radii are much greater than the distance of closest approach these integrals simplify to
    \begin{equation}
        \ln\left( \frac{2r_{\mathrm{E}}}{R} \right),
    \end{equation}
    and similar for the second integral.
    A reasonable approximation for the total delay is then
    \begin{equation}
        \Delta t \approx \frac{4GM}{c^3} \ln\left( \frac{4r_{\mathrm{E}}r_{\mathrm{V}}}{R^2} \right).
    \end{equation}
    
    This is the dominant term but there are other terms of order \(GM/c^3\).
    One of these terms appears because this is a delay in the elapsed coordinate time but we want the elapsed proper time on Earth, which is given by
    \begin{equation}
        c^2 \dd{\tau}^2 = \left( 1 - \frac{2GM}{r_{\mathrm{E}}c^2} \right)c^2\dd{t}^2 - r_{\mathrm{E}}^2\dd{\varphi}^2.
    \end{equation}
    Here we are treating the orbit as circular so \(\dl{r} = 0\).
    The proper time elapsed on Earth during the experiment is then
    \begin{equation}
        \tau = \sqrt{\alpha(r_{\mathrm{E}}) - \frac{r_{\mathrm{E}}^2}{c^2} \left( \diff{\varphi}{t} \right)^2 t}.
    \end{equation}
    The angular velocity of the Earth obeys, at least in the Newtonian approximation, the relation \((\diff{\varphi}/{t})^2 = GM/r_{\mathrm{E}}^3\) and so
    \begin{equation}
        \frac{\tau}{t} = \sqrt{1 - \frac{2GM}{r_{\mathrm{E}}c^2} - \frac{GM}{r_{\mathrm{E}}c^2}} = \sqrt{1 - \frac{3GM}{r_{\mathrm{E}}c^2}}.
    \end{equation}
    The total journey time is approximately \(r_{\mathrm{E}}/c\) and so the offset between the coordinate time and proper time at the end of the experiment is approximately \(GM/c^3\).
    This is negligible compared to the log term we first derived.
    
    We used the Born approximation, that the deviation from a straight path is sufficiently small for us to integrate along the unperturbed path.
    This is justified only by the fact that it works, as can be demonstrated using the geodesic equations.
    As before we have \(\alpha\dot{t} = k\) and \(r^2\dot{\varphi} = h\), with dots denoting derivatives with respect to some affine parameter, and the \(\lagrangian^2 = 0\) radial energy equation is
    \begin{equation}
        \alpha c^2 \dot{t}^2 - \frac{\dot{r}^2}{\alpha} - r^2\dot{\varphi}^2 = 0 \implies \dot{r}^2 + \frac{\alpha h^2}{r^2} = k^2c^.
    \end{equation}
    The point of closest approach, known as the perihelion since we are considering an orbit rather than the Born approximation, is a turning point and so \(\dot{r} = 0\).
    We then have \(k^2c^2 = h^2 \alpha(R)/R^2\) and we can use this to eliminate \(k\).
    We can also use the time equation to convert
    \begin{equation}
        \diff{r}{t} = \diff{r}{p} \diff{p}{t} = \frac{\dot{r}}{\dot{t}}
    \end{equation}
    and so
    \begin{equation}
        \diff{r}{t} = \pm \frac{\alpha(r)Rc}{\sqrt{\alpha(R)}} \sqrt{\frac{\alpha(R)}{R^2} - \frac{\alpha(r)}{r^2}}.
    \end{equation}
    We can expand this to first order in \(M\), which is the same order as the logarithmic answer we got with the Born approximation, and we find that, after some algebra or use of a computer,
    \begin{equation}
        \diff{t}{r} = \frac{r}{\sqrt{r^2 - R^2}} + \frac{2r + 3R}{(r + R)\sqrt{r^2 - R^2}} \frac{GM}{c^3}.
    \end{equation}
    Integrating this from \(R\) to \(r\) we get
    \begin{equation}
        \Delta t = \frac{\sqrt{r^2 - R^2}}{c} + \left( \frac{\sqrt{r^2 - R^2}}{r + R} - 2\ln R + 2\ln (r + \sqrt{r^2 - R^2}) \right) \frac{GM}{c^3}.
    \end{equation}
    The first term is the time taken in the absence of gravitational effects. The GR correction, for \(r \gg R\), is dominated by
    \begin{equation}
        2\ln\left( \frac{2r}{R} \right)\frac{GM}{c^3}.
    \end{equation}
    This is for a one way trip from the perihelion to one of the planets.
    Doubling this for a two way trip and also including both parts of the journey, from a planet to the perihelion and then onto the next planet, we get \(4\ln(4r_{\mathrm{E}}r_{\mathrm{V}}/R^2)GM/c^3\) as before.
    The delay is therefore dominated by the reduced coordinate speed of light.
    
    When \(R\) is small, say \(R\) is the radius of the sun, so the radar signal just grazes the sun, the total delay is around \qty{200}{\micro\second}.
    In the 1960s when the experiment was first performed this could be verified to about \qty{5}{\percent}.
    Now using various solar system space craft the agreement with the GR prediction has been shown to be accurate to about \qty{0.002}{\percent}, or one part in \num{50000}.
    
    \chapter{Mathematical Formalism}
    So far we have been quite informal with the maths we've used.
    We can get quite a long way using just the maths of special relativity, but at some point we need to introduce new concepts, and that is the purpose of this chapter.
    Our goal is now to develop the mathematics of tensor calculus.
    We follow a mostly coordinate based approach emphasising the geometric nature of the mathematics that we do.
    
    We will not make assumptions about dimensions here apart from where specifically stated so indices run from \(1\) to \(n\), where \(n \in \integers_{>0}\) is some positive integer, namely the dimension of the space in question.
    We also apply the Einstein summation convention for repeated indices.
    
    I go into far more detail here than the original course does.
    I also make statements like \enquote{\(\category{Top}\) is the category of topological spaces with continuous functions as morphisms and homeomorphisms as isomorphisms} without explanation, these sorts of statement aren't important if you don't understand them.
    
    \section{Manifolds}
    Manifolds are the stage for the maths of GR, and in fact most physics.
    We start with a rough definition of a manifold which lacks a few details which we will fill out later.
    For our purposes this rough definition is sufficient and the full definition is for completeness.
    \begin{dfn}{Manifold}{}
        A \defineindex{manifold} is a space where we can locally introduce Cartesian coordinates such that different choices of local coordinates will give compatible descriptions and by defining sets of local coordinates covering the whole space we have complete information about the space.
    \end{dfn}
    
    A manifold is parametrised continuously and differentiably as an \(n\)-tuple of real numbers, \((x^1, \dotsc, x^n) \in \reals^n\), which are the coordinates of the point.
    We can connect two points with a curve parametrised by some \(\lambda \in \reals\) such that derivatives of the coordinates along this path exist, that is \(\diff{x^i}/{\lambda}\) exists for all \(i = 1, \dotsc, n\).
    
    By this definition it should be clear that Euclidean space, where non-relativistic mechanics takes place, is a manifold.
    However, not all manifolds are as nice as Euclidean space, for example Euclidean space has a metric (way to measure distance), which not all manifolds do.
    Locally all manifolds behave like a subset of Euclidean space due to their differentiable nature.
    
    \subsection{Topology}
    \begin{dfn}{Topological Space}{}
        A \define{topological space}\index{topological!space}, \((X, \mathcal{T})\), is a set, \(X\), and a collection of subsets of \(X\), \(\mathcal{T} \subset \mathcal{P}(X)\), called the \defineindex{topology}, such that
        \begin{itemize}
            \item \(X \in \mathcal{T}\),
            \item \(\emptysetAlt \in \mathcal{T}\),
            \item \(\mathcal{T}\) is closed under unions, that is if \(\{U_i\}_{i \in I} \subseteq \mathcal{T}\) then\vspace{-1ex}
            \begin{equation}
                \bigcup_{i\in I} U_i \in \mathcal{T},
            \end{equation}\vspace{-4ex}
            \item \(\mathcal{T}\) is closed under finite intersections, that is if \(\{U_i, \dotsc, U_N\} \subseteq \mathcal{T}\) then\vspace{-4ex}
            \begin{equation}
                \bigcap_{i = 1}^{N} U_i \in \mathcal{T}.
            \end{equation}
        \end{itemize}\vspace{-1ex}
        We call the elements of \(\mathcal{T}\) \define{open sets}\index{open set}.
    \end{dfn}
    
    It is common to refer to \(X\) as a topological space and then specify \(\mathcal{T}\) separately, or just leave \(\mathcal{T}\) implicit, much in the same way that we might define a group \(G\) and operation \(\cdot\), when more formally it might be more correct to call \((G, \cdot)\) and \(G\) the underlying set.
    
    Topological spaces can be quite abstract, for example the following topology makes \((\{a, b, c, d\}, \mathcal{T})\) a topological space:
    \begin{equation}
        \mathcal{T} = \{\emptysetAlt, \{a, b, c, d\}, \{a, b\}, \{c, d\}\}.
    \end{equation}
    We only deal with more familiar topological spaces.
    We can define topologies through the notion of a basis.
    
    \begin{dfn}{Topological Basis}{}
        A \define{basis}\index{topological!basis} for a topology on some set \(X\) is a collection of sets, \(\mathcal{B} \subseteq \mathcal{P}(X)\) such that
        \begin{itemize}
            \item the elements of \(\mathcal{B}\) cover \(X\), that is for all \(x \in X\) there exists some \(B \in \mathcal{B}\) such that \(x \in B\).
            \item given \(B_1, B_2 \in \mathcal{B}\) for all \(x \in B_1 \cap B_2\) there exists some \(B \in \mathcal{B}\) such that \(x \in B\) and \(B_3 \subseteq B_1 \cap B_2\).
        \end{itemize}
        The topology generated by a basis, \(\mathcal{B}\), is then defined to be the set of all subsets \(U \subseteq X\) such that\vspace{-0.5ex}
        \begin{equation}
            U = \bigcup_{B_i \in B \subseteq \mathcal{B}} B_i.
        \end{equation}\vspace{-0.5ex}
        That is all elements of the topology are unions of basis sets.
    \end{dfn}
    
    Using this definition we can define the \defineindex{standard topology} on \(\reals^n\) as the topology generated by the basis of open balls in \(\reals^n\), where by an open ball we mean
    \begin{equation}
        B = \{\vv{r} \in \reals^n \mid \abs{\vv{r} - \vv{a}}^{n} \text{ for some } \vv{a} \in \reals^n\}
    \end{equation}
    with \(\abs{\vv{r}} = \sum_i x_i^2\) being the usual Euclidean metric.
    This is not the only topology on \(\reals^n\) but it is the only one we care about.
    
    Let \(X\) and \(Y\) be topological spaces with topologies \(\mathcal{T}_X\) and \(\mathcal{T}_Y\) respectively.
    Then we can define a function, \(f \colon X \to Y\).
    The \defineindex{preimage} of some \(S \subseteq Y\) under this function is the set
    \begin{equation}
        f^*(S) = f^{-1}(S) \coloneqq \{a \in X \mid f(a) \in S\} \subseteq X.
    \end{equation}
    The notation \(f^{-1}\) suggests that this is related to the inverse of the function \(f\), and indeed if the inverse exists then it is, as the preimage is the image of the inverse restricted to \(S\).
    
    The function \(f \colon X \to Y\) is \defineindex{continuous} if all preimages of open sets of \(Y\) are open sets of \(X\), that is if \(f^*(U_Y) \in \mathcal{T}_X\) for all \(U_Y \in \mathcal{T}_Y\).
    With the standard topology for \(\reals^n\) this definition coincides with the normal \(\varepsilon\)-\(\delta\) definition of continuity from analysis.
    
    An invertible (and hence bijective) continuous function between topological spaces is called a \defineindex{homeomorphism}.
    If there exist homeomorphisms between two topological spaces we say those spaces are \defineindex{homeomorphic}.
    \(\category{Top}\) is the category of topological spaces with continuous functions as morphisms and homeomorphisms as isomorphisms.
    
    \subsection{Charts}
    In order to be able to define differentiation in topological spaces we need them to look locally like \(\reals^n\).
    By \enquote{look like} we mean we want there to be a homeomorphism between neighbourhoods of a topological space and \(\reals^n\).
    Homeomorphisms preserve topological properties so this allows us to translate things into Euclidean terms, perform calculations, and then convert the results back to the topological space.
    This isn't possible for all topological spaces but is a defining property of manifolds.
    
    To better define the notion of \enquote{looks like} we define charts.
    
    \begin{dfn}{Chart}{}
        Given a topological space, \((X, \mathcal{T})\), a \defineindex{chart}, \(C\), is an ordered pair \(C = (U, \varphi)\), where \(U \in \mathcal{T}\) and \(\varphi\) is a homeomorphism
        \begin{equation}
            \varphi \colon U \to \mathop{\mathrm{im}}\varphi \subseteq \reals^n.
        \end{equation}
        We call \(n \in \integers_{>0}\) the \defineindex{dimension} of \(U\), it is independent of \(\varphi\).
    \end{dfn}
    
    Simply put a chart associates with each neighbourhood in a topological space an open subset of \(\reals^n\) and gives us a way to move between \(U\) and this open subset while preserving key topological properties.
    
    \subsection{Topological Manifolds}
    For our first definition of a manifold we will need a few preliminary definitions.
    \begin{dfn}{Locally Euclidean}{}
        A topological space, \(X\), is \defineindex{locally Euclidean} if there exists \(n \in \integers_{>0}\) such that for every point, \(p \in X\) there is a chart, \(C = (U, \varphi)\), such that \(p \in U\).
    \end{dfn}
    
    That is \(X\) must be covered by the open sets \(\{U_i\}_{i\in I}\) and for each open set in this covering there must be an associated chart, \(C_i = (U_i, \varphi_i)\).

    \begin{dfn}{Hausdorff}{}
        Let \(X\) be a topological space.
        Then \(x, y \in X\) are separated by neighbourhoods if there exists some open sets \(U \ni x\) and \(V \ni y\) such that \(U \cap V = \emptysetAlt\).
        
        \(X\) is a \defineindex{Hausdorff space} if all distinct points in \(X\) are pairwise neighbourhood separable.
    \end{dfn}
    Roughly put the condition of being Hausdorff means for any two points we can find sufficiently small neighbourhoods around the two points such that the two points aren't both in the neighbourhoods.
    Take for example, \(\reals^2\), the plane with the standard topology.
    Since the standard topology here is the topology generated by the open discs we can draw two discs around any two points such that the discs don't overlap and so \(\reals^2\) is Hausdorff.
    
    \begin{dfn}{Topological Manifold}{}
        A \define{topological manifold}\index{topological!manifold}\index{manifold!topological} is a Hausdorff space, \(M\), which is locally Euclidean.
    \end{dfn}
    
    It is common to include extra conditions, such as being \defineindex{second-countable}, which means that the topology is generated by a countable basis.

    A topological manifold is the simplest manifold, and all manifolds are topological manifolds.
    However, topological manifolds don't have the required structure for our purposes, since there is still no sense of differentiation on general topological manifolds.
    We will need a more specific type of manifold, which we will develop in the next few sections.
    
    \subsection{Local Coordinates}
    Charts allow us to rigorously define the notion local coordinates.
    Intuitively local coordinates are a way to parametrise a neighbourhood with tuples in \(\reals^n\), formally local coordinates are defined as follows.
    
    \begin{dfn}{Local Coordinates}{}
        Let \(M\) be a topological manifold and \(U\) an open subset of \(M\) such that \((U, \varphi)\) is a chart.
        The coordinates of some point \(p \in U\) are the Cartesian coordinates, \(\vv{x}_p\), given by \(\varphi(p) \in \reals^n\).
    \end{dfn}
    
    In general charts are not unique and it is important that the physics we do doesn't depend on the choice of coordinate system, since coordinates are a man made construction and don't actually reflect nature, they just let us apply maths.
    This means we need charts to be compatible in the following sense.
    
    \begin{dfn}{Compatible Charts}{}
        Let \(M\) be a topological manifold with charts \(C_1 = (U_1, \varphi_1)\) and \(C_2 = (U_2, \varphi_2)\).
        If \(U_1 \cap U_2 \ne \emptysetAlt\) we can define the \defineindex{transition maps} to be
        \begin{align}
            \varphi_1 \circ \varphi_2^{-1} \colon \varphi_2(U_1 \cap U_2) &\to \varphi_1(U_1 \cap U_2),\\
            \varphi_2 \circ \varphi_1^{-1} \colon \varphi_1(U_1 \cap U_2) &\to \varphi_2(U_1 \cap U_2).
        \end{align}
        We say that \(C_1\) and \(C_2\) are \define{compatible}\index{compatible!charts} if either the transition maps are homeomorphisms or \(U_1 \cap U_2 = \emptysetAlt\).
        
        We say that \(C_1\) and \(C_2\) are \defineindex{smoothly compatible} if they are compatible and the transitions functions, if defined, are smooth.
    \end{dfn}
    
    We can think of the two charts as two separate sets of local coordinates and the transition maps as coordinate transformations, that is
    \begin{align}
        (\varphi_1 \circ \varphi_2^{-1})(\vv{x}_p^{(2)}) &= \varphi_1(p) = \vv{x}_p^{(1)},\\
        (\varphi_2 \circ \varphi_1^{-1})(\vv{x}_p^{(1)}) &= \varphi_1(p) = \vv{x}_p^{(2)}.
    \end{align}
    
    We can use local coordinates to express functions on some manifold in terms of local coordinates.
    
    \begin{dfn}{Local Function}{}
        Given a continuous function, \(f \colon M \to \reals\) for some topological manifold, \(M\), and a chart, \(C = (U, \varphi)\), we can associate the restriction of \(f\) to \(U\) with the function \(f_U = f \circ \varphi^{-1}\) so
        \begin{equation}
            f_U \colon \varphi(U) \subseteq \reals^n \to \reals
        \end{equation}
        is defined by
        \begin{equation}
            f(p) = f_U(\vv{x}_p)
        \end{equation}
        for \(p \in U\).
    \end{dfn}
    
    This is useful since we can deal with functions \(\reals^n \to \reals\) using the normal tools of vector calculus.
    
    For compatible charts we can represent the function in two different ways in terms of local coordinates.
    namely by the functions \(f_{U_i}\) with \(i = 1, 2\).
    On the intersection \(U_1 \cap U_2\) we define \(f = f_1 \circ \varphi_1 = f_2 \circ \varphi_2\) so rearranging this final equality \(f_2 = f_1 \circ \varphi_1 \circ \varphi_2^{-1}\) and \(f_1 = f_2 \circ \varphi_2 \circ \varphi_1^{-1}\).
    We can think of these as the same function with a change of variables, so \(f_1(\vv{x}_p^{(1)}) = f_2(\vv{x}_p^{(2)})\).
    
    \subsection{Smooth Manifold}
    \begin{dfn}{Smooth Atlas}{}
        A \define{smooth atlas}\index{smooth!atlas}, \(\mathcal{A}(M)\), for a topological manifold, \(M\), is a family of charts, \(\{C_a = (U_a, \varphi_a)\}\), which cover \(M\), so \(p \in U_a\) for some \(a\) for all \(p \in M\), such that the charts are all mutually smoothly compatible.
        
        Two smooth atlases, \(\mathcal{A}_1(M)\) and \(\mathcal{A}_2(M)\), are said to be \define{compatible}\index{compatible!atlases} if all of the charts of \(\mathcal{A}_1\) are compatible with all of the charts of \(\mathcal{A}_2\).
    \end{dfn}
    
    We can think of charts as mapping the manifold and then the name atlas is naturally a collection of maps.
    Compatibility defines an equivalence relation on the set of all atlases for a given manifold.
    
    \begin{dfn}{Smooth Manifold}{}
        A \define{smooth structure}\index{smooth!structure} on a topological manifold, \(M\), is an equivalence class of smooth atlases:
        \begin{equation}
            \mathcal{S}(M) = [\mathcal{A}(M)],
        \end{equation}
        that is \(\mathcal{S}(M)\) is the set of all atlases compatible with \(\mathcal{A}(M)\).
        
        A \define{smooth manifold}\index{smooth!manifold}\index{manifold!smooth} is a topological manifold, \(M\), equipped with a smooth structure, \(\mathcal{S}\).
    \end{dfn}
    
    \begin{dfn}{Smooth Functions and Maps}{}\
        A function, \(f \colon M \to \reals\), on a smooth manifold, \(M\), is said to be \define{smooth}\index{smooth!function} if all of its local coordinate representatives, 
        \begin{equation}
            f_a = f \circ \varphi_a^{-1} \colon \varphi(U_a) \to \reals,
        \end{equation}
        are smooth.
        
        Let \(M\) and \(V\) be smooth manifolds of dimensions \(m\) and \(n\) with charts \((U_a, \varphi_a)\) and \((V_b, \psi_b)\) respectively.
        Then the map \(\mu \colon M \to N\) is \define{smooth}\index{smooth!map} if all of its local coordinate representatives,
        \begin{equation}
            \mu_{ab} = \psi_b \circ \mu \circ \varphi_a^{-1} \colon \varphi_a(U_a) \subseteq \reals^m \to \psi_b(V_b) \subseteq \reals^n
        \end{equation}
        are smooth.
        Smooth invertible maps between manifolds are also called \define{diffeomorphisms}\index{diffeomorphism}.
    \end{dfn}
    
    It is possible to differentiate smooth functions and maps by first differentiating the local coordinates and then mapping the result back to the manifolds.
    
    \(\category{SmoothMan}\) is the category of smooth manifolds with smooth maps as homomorphisms and diffeomorphisms as isomorphisms.
    
    Smooth manifolds are the most common type of manifolds, and are the type we will be using, but there are other types.
    For example, we can define \(C^k\)-differentiable manifolds by replacing the requirement that the transition functions be smooth (\(C^{\infty}\)) with requirements for transition functions to be \(C^k\)-differentiable.
    We can define real analytic manifolds by requiring that transition functions are real analytic functions.
    We can define complex smooth manifolds by replacing \(\reals\) with \(\complex\) and requiring that transition functions be holomorphic.
    The list of possible manifold types goes on.
    
    \subsection{Examples of Manifolds}
    Now that we've had quite an abstract introduction to manifolds it helps to discuss a few examples.
    The first and most obvious example is Euclidean space itself, which has an atlas consisting or a single chart, \((\reals^n, \mathrm{id})\), where \(\mathrm{id}\) is the identity function.
    
    Our next example is the circle, \(S^1\).
    This is a one-dimensional manifold.
    It takes at least two charts to form an atlas.
    One chart can be given by defining \(\vartheta = 0\) to be the top of the circle, and to wrap around to \(2\pi\).
    The definition of a chart requires that the image of an open set is open in \(\reals\), in this case this means we can't include \(0\) or \(2\pi\) in the neighbour hood, so our neighbour hood is \((0, 2\pi)\).
    We can introduce another chart that defines \(\vartheta' = 0\) to be the right most point of the circle and similarly wraps around to \(2\pi\), this again maps to the neighbourhood \((0, 2\pi)\) and covers the point excluded in the first chart.
    
    The circle is fairly obviously a manifold since it is usually viewed embedded in Euclidean space.
    This isn't necessary for all manifolds however.
    A more abstract manifold is \(\specialOrthogonal(3)\), the group of rotations in three dimensions.
    This is continuously parametrised by three Euler angles, and so is a three-dimensional manifold.
    The Lorentz group, \(\orthogonal(1, 3)\), is a three dimensional manifold parametrised by the three components of the velocity of the frame.
    
    For a system of \(N\) particles their phase space, consisting of their positions and momenta, is a \(6N\)-dimensional manifold.
    
    Given an equation with two variables, \(x\) and \(y\), we can define the set of all \((x, y)\) to be a manifold and any particular solution to the equation is a curve in this manifold.
    
    Vector spaces are manifolds, in particular real finite-dimensional vector spaces.
    To see this we simply construct a basis and then parametrise the vectors by the coefficients of the basis vectors.
    
    \section{Vectors, Covectors, and Tensors}
    \subsection{Tangent Space}
    One of the main changes when moving from discussing vectors in Euclidean space to vectors in some manifold is that it becomes important where the vector is on the manifold.
    In Euclidean geometry we usually think of vectors as being at the origin, but we can also move them around freely, say to add them by combining them tip to tail.
    This doesn't work on a general manifold as we will see later.
    
    Let \(M\) be a smooth manifold.
    For each point, \(p \in M\), we define a \defineindex{tangent space} at \(p\), \(T_pM\), and we can treat the vectors in this tangent space as Euclidean vectors.
    A tangent vector is a vector in the tangent space.
    The formal definition of a tangent space is a bit abstract:
    \begin{dfn}{Tangent Space}{}
        Let \(M\) be a smooth manifold and \(f \colon M \to \reals\) a smooth function on \(M\).
        The set of all smooth functions on \(M\), denoted \(\End(M)\) or \(C^{\infty}(M)\), is a real associative algebra under point wise addition and products, that is for \(f, g \in \End(M)\) we define \((f + g)(p) = f(p) + g(p)\) and \(fg(p) = f(p)g(p)\).
        
        A \defineindex{derivation} at \(p \in M\) is a linear map, \(D \colon \End(M) \to \reals\) satisfying the Leibniz identity,
        \begin{equation}
            D(fg) = D(f)g(p) + f(p)D(g)
        \end{equation}
        for all \(f, g \in \End(M)\).
        Note the similarity to the product rule.
        
        Define addition and scalar multiplication of derivations at \(p\) by
        \begin{equation}
            (D_1 + D_2)(f) = D_1(f) + D_2(f), \qand (\lambda D)(f) = \lambda D(f)
        \end{equation}
        for derivations at \(p\) \(D_1\), \(D_2\), and \(D\), and some \(\lambda \in \reals\).
        This makes the space of derivations a real vector space and this is the space we define as the tangent space, \(T_pM\).
        
        We can then define the \defineindex{tangent bundle}, \(TM\), as the disjoint union of all tangent spaces,
        \begin{equation}
            TM \coloneqq \bigsqcup_{p \in M} T_pM = \{(p, x) \mid p \in M, x \in T_pM\}.
        \end{equation}
        That is we combine all vectors in any tangent space, \(T_pM\), into a single set and tag each one with the point \(p\), to keep track of \(p\), since this is important.
    \end{dfn}
    
    Given this definition we can define a vector field as an assignment of a tangent vector to each point in the manifold.
    More formally we define a vector field, \(V \colon M \to TM\) such that \(\pi \circ V\) is the identity mapping on \(M\) where \(\pi \colon TM \to M\) is defined to be the projection \(\pi(p, x) = p\).
    
    So far these definitions of tangent vectors have been coordinate free.
    This is how mathematicians like to do general relativity, but its not the easiest way to do calculations so we introduce coordinates.
    
    For an \(n\)-dimensional manifold we can define \(n\) coordinates, \(x^i\).
    We can also define a path parametrised smoothly by \(\lambda\) such that \(\diff{x^i}{\lambda}\) exists.
    We can then identify tangent vectors as derivatives, \(\diff{}/{\lambda}\) evaluated at a point, hence the name \enquote{derivations}.
    This can then be expanded as
    \begin{equation}
        \diff{}{\lambda} = \diff{x^i}{\lambda} \diffp{}{x^i} = \diff{x^i}{\lambda}\partial_i
    \end{equation}
    and we can identify \(\partial_i \coloneqq \diffp{}/{x^i}\) as basis tangent vectors.
    Notice that this expansion is simply the chain rule, which it can be shown holds for all derivations.
    The basis, \(\{\partial_i\}\), is derived from a set of coordinates and so is called a \defineindex{coordinate basis}.
    There are other non-coordinate bases but they are harder to find and coordinate bases will be enough for us.
    
    The good thing about this definition of a basis is that the transformation laws for the components of tangent vectors are immediately obvious.
    Suppose we have some other set of coordinates, \(x'^i\).
    It follows by the chain rule that
    \begin{equation}
        \diffp{}{x'^i} = \diffp{x^j}{x'^i}\diffp{}{x^j}.
    \end{equation}
    Denoting an arbitrary vector's components as \(V^i\), so the vector is \(V = V^i\partial_i\), it follows that
    \begin{equation}
        V = V'^i\diffp{}{x'^i} = V'^i \diffp{x^j}{x'^i}\diffp{}{x^j} = V^j \diffp{}{x^j}
    \end{equation}
    and so we can identify the transformation law for the components as
    \begin{equation}
        V'^i = \diffp{x'^i}{x^j}V^j.
    \end{equation}
    We'll come back to this when we discuss tensors.
    
    \subsection{Covectors}
    Given a vector space, \(V\), over some field, \(\field\), we can define its dual space, \(V^*\) as the space of linear forms on \(V\), which is to say the space of maps \(V \to \field\).
    \(V^*\) is also a vector space over \(\field\) and has the same dimension as \(V\).
    Vectors in \(V^*\) are sometimes called \define{covectors}\index{covector}.
    
    \begin{dfn}{Cotangent Space}{}
        Given a manifold, \(M\), and some point \(p \in M\) we define the \defineindex{cotangent space} at \(p\), \(T_p^*M\), to be the dual space to the tangent space at \(p\), that is \(T_p^*M = (T_pM)^*\).
    \end{dfn}
    
    The elements of \(T_p^*M\) are then linear maps \(T_pM \to \reals\), in this context we call elements of \(T_p^*M\) \define{one-forms}\index{one-form}\index{1-form|see{one-form}}.
    This means if \(\vv{v}, \vv{w} \in T_pM\), \(\cv{\omega} \in T_p^*M\), and \(a, b \in \reals\) then
    \begin{equation}
        \omega(a\vv{v} + b\vv{w}) = a\cv{\omega}(\vv{v}) + b\cv{\omega}(\vv{w}) \in \reals.
    \end{equation}
    Since \(T_p^*M\) is itself a vector space if \(\cv{\psi} \in T_p^*M\) also we have
    \begin{equation}
        (a\cv{\omega} + b\cv{\psi})(\vv{v}) = a\cv{\omega}(\vv{v}) + b\cv{\psi}(\vv{v}),
    \end{equation}
    which gives a nice symmetry between \(T_pM\) and \(T_p^*M\).
    
    This symmetry suggests it is sensible to define the action of \(T_pM\) on \(T_p^*M\), and indeed it is.
    We can define \(\vv{v}(\cv{\omega}) \coloneqq \cv{\omega}(\vv{v})\).
    
    Geometrically we can think of vectors as lines with direction and length.
    The dual concept to a line in three dimensions is a plane.
    We can think of one-forms as planes arranged in space.
    The value \(\cv{\omega}(\vv{v}) = \vv{v}(\cv{\omega})\) is then a measure of how many planes associated with \(\cv{\omega}\) the vector \(\vv{v}\) cuts through.
    
    \subsection{Tensors}
    Tensors generalise the notion of one-forms and vectors.
    
    \begin{dfn}{Tensor}{}
        A \define{\(\bm{(n, m)}\)-tensor}\index{tensor} or \(\left( \begin{smallmatrix} n \\ m \end{smallmatrix} \right)\)-tensor is a multilinear map
        \begin{equation}
            T \colon T_p^*M^n \times T_pM^m \to \reals.
        \end{equation}
    \end{dfn}
    By multilinear we mean
    \begin{align*}
        T(\cv{\omega}_1, \dotsc, a\cv{\omega}_i + b\cv{\omega}_i' ,\dotsc, \cv{\omega}_n, \vv{v}_1, \dotsc, \vv{v}_m) &= aT(\cv{\omega}_1, \dotsc, \cv{\omega}_i, \dotsc, \cv{\omega}_n, \vv{v}_1, \dotsc, \vv{v}_m)\\
        &+ bT(\cv{\omega}_1, \dotsc, \cv{\omega}_i', \dotsc, \cv{\omega}_n, \vv{v}_1, \dotsc, \vv{v}_m),\\
        T(\cv{\omega}_1, \dotsc, \cv{\omega}_n, \vv{v}_1, \dotsc, a\vv{v}_i + b\vv{v}_i', \dotsc \vv{v}_m) &= aT(\cv{\omega}_1, \dotsc, \cv{\omega}_n, \vv{v}_1, \dotsc, \vv{v}_i, \dotsc, \vv{v}_m)\\
        &+ bT(\cv{\omega}_1, \dotsc, \cv{\omega}_n, \vv{v}_1, \dotsc, \vv{v}_i', \dotsc \vv{v}_m).
    \end{align*}
    For example, this means for a \((1, 1)\) tensor
    \begin{equation}
        T(a\cv{\omega} + b\cv{\psi}, c\vv{v} + d\vv{w}) = acT(\cv{\omega}, \vv{v}) + adT(\cv{\omega}, \vv{w}) + bcT(\cv{\psi}, \vv{v}) + bdT(\cv{\psi}, \vv{w}).
    \end{equation}
    
    We can then identify a scalar as a \((0, 0)\)-tensor, a vector as a \((1, 0)\)-tensor, and a covector as a \((0, 1)\)-tensor.
    
    \subsection{Bases}
    Let \(\{\ve{i}\}\) be a basis for vectors.
    We can then write \(\vv{v} = v^i\ve{i}\).
    Using the linearity of the one-form we can write
    \begin{equation}
        \cv{\omega}(\vv{v}) = \cv{\omega}(v^i\ve{i}) = v^i\cv{\omega}(\ve{i}) = v^i\omega_i
    \end{equation}
    where we define \(\omega_i \coloneqq \cv{\omega}(\ve{i})\).
    We can define a \defineindex{dual basis}, \(\{\cve{i}\}\), which is such that
    \begin{equation}
        \cve{i}(\ve{j}) = \ve{j}(\cve{i}) = \tensor{\delta}{^i_j}.
    \end{equation}
    We then have
    \begin{align}
        \cv{\omega}(\ve{i}) &= \omega_j\cve{j}(\ve{i}) = \omega_j\tensor{\delta}{^j_i} = \omega_i,\\
        \vv{v}(\cve{i}) &= v^j\ve{j}(\cve{i}) = v^j\tensor{\delta}{^i_j} = v^i.
    \end{align}
    So, we can identify the components of a vector as the result of acting on the dual basis, and the components of a dual vector as the result of acting on the basis.
    
    We can generalise this to tensors and define the \define{components of a tensor}\index{tensor components} as the result of evaluating the tensor on the dual basis and basis.
    Keeping the notation consistent with the vectors and covectors above we denote the result of acting on the dual basis with upper indices acting on the basis with lower indices, so for an \((n, m)\)-tensor, \(T\), we define the components
    \begin{equation}
        \tensor{T}{^{i_1i_2\dotso i_n}_{j_1j_2\dotso j_m}} \coloneqq T(\ve{i_1}, \ve{i_2}, \dotsc, \ve{i_n}, \cve{j_1}, \cve{j_2}, \dotsc, \cve{j_m}).
    \end{equation}
    
    For completeness we now mention that we can interpret these components as the coefficients of basis tensors, which are given by tensor products of basis vectors and covectors and so
    \begin{equation}
        T = \tensor{T}{^{i_1i_2\dotso i_n}_{j_1j_2\dotso j_m}} \ve{i_1} \otimes \ve{i_2} \otimes \dotsb \otimes \ve{i_n} \otimes \cve{j_1} \otimes \cve{j_2} \otimes \dotsb \otimes \cve{j_m}.
    \end{equation}
    The tensor product in term is a way of combining a \((m, n)\)-tensor, \(T\), and a \((k, l)\)-tensor, \(S\), to produce a \((m + k, n + l)\)-tensor, \(T \otimes S\), defined by
    \begin{multline}
        (T\otimes S) (\cv{\omega}_1, \dotsc, \cv{\omega}_{m + k}, \vv{v}_1, \dotsc, \vv{v}_{n + l})\\
        \coloneqq T(\cv{\omega}_1 \dotsc, \cv{\omega}_{m}, \vv{v_1}, \dotsc, \vv{v}_n) S(\cv{\omega}_{m + 1}, \dotsc, \cv{\omega}_{m + k}, \vv{v}_{n + 1}, \dotsc, \vv{v}_{n + l}).
    \end{multline}
    
    In general the order of the indices is important, that is \(T^{ij} \ne T^{ij}\) and \(\tensor{T}{^i_j^k} \ne \tensor{T}{^i_k^j}\).
    If the order doesn't matter between two indices then we say that the tensor is \define{symmetric}\index{symmetric tensor} in those indices, and we may be a bit more sloppy about separating upper and lower indices.
    For example, the Kronecker delta is symmetric in both of its indices and so \(\tensor{\delta}{^i_j} = \tensor{\delta}{_j^i} = \delta^i_j\).
    
    \subsubsection{Metric}
    \begin{dfn}{Metric Tensor}{}
        The \defineindex{metric tensor}, \(g\), is defined to be a \((0, 2)\)-tensor which is symmetric in its arguments, so \(g(\vv{v}, \vv{u}) = g(\vv{u}, \vv{v})\).
    \end{dfn}
    We then have
    \begin{equation}
        g(\vv{v}, \vv{u}) = g(v^i\ve{i}, u^j\ve{j}) = v^iu^jg(\ve{i}, \ve{j}) = v^iu^jg_{ij}.
    \end{equation}
    Previously we have called \(g_{\mu\nu}\) the metric tensor, but we now see that these are actually the components of the metric tensor, although we will most likely refer to \(g_{\mu\nu}\) as the metric tensor in the future.
    
    We can use the metric to define a basis for the dual space, although it is not necessarily a dual basis, since the vectors in this basis won't be dual to the vectors in the normal basis unless the metric happens to be diagonal.
    We can think of the function one-form \(f = g(\ve{j}, -)\), defined by\(f(\ve{i}) \coloneqq g(\ve{j}, \ve{i})\) as the basis one-form \(\cve{j}\).
    
    It is possible to construct a dual basis even without a metric structure however.
    
    \subsection{Manifolds in GR}
    \begin{dfn}{Riemannian Manifold}{}
        A \defineindex{Riemannian manifold}\index{manifold!Riemannian}, \((M, g)\), is a real, smooth manifold, \(M\), equipped with a positive definite inner-product, \(g_p\), defined on the tangent space, \(T_pM\), at each point \(p\).
        The family \(g_p\) is called a \defineindex{Riemannian metric}.
        
        If we relax the condition that \(g_p\) be positive definite, but keep the requirement that \(g_p\) is everywhere non-degenerate, so \(g_p(\vv{v}, \vv{w}) = 0\) only if \(\vv{v} = \vv{w}\), then we call the resulting space a \defineindex{pseudo-Riemannian manifold}\index{manifold!pseudo-Riemannian} and \(g_p\) is called a \defineindex{pseudo-Riemannian metric}.
        It should be noted that \(g_p\) is technically no longer a metric if it is allowed to be negative.
    \end{dfn}

    \begin{dfn}{Metric Signature}{}
        Given a manifold, \(M\), and an inner product, \(g_p\), defined on the tangent space, \(T_pM\), for \(p \in M\) we define the \defineindex{metric signature} to be \((p, q, r)\) for non-negative integers \(p\), \(q\), and \(r\), defined such that the quadratic form defined by \(Q(\vv{v}) \coloneqq g(\vv{v}, \vv{v})\) gives \(p\) positive results, \(q\) negative results, and \(r\) zero results when applied to the basis vectors.
        The metric signature is independent of the choice of basis.
        When \(r = 0\) it is common to write \((p, q)\).
    \end{dfn}
    
    In Newtonian mechanics, SR, and GR everything happens in three very similar manifolds.
    We shall call these \(\reals^{1, 3}\), where the \(1\) corresponds to the single time dimension and the \(3\) to the three spatial dimensions.
    These spaces differ in their metric structure.
    In Newtonian mechanics we consider Galilean transformations, which are transformations preserving Euclidean distance.
    The Euclidean group consists of all maps preserving Euclidean distance and it is generated by the rotations, reflections, and translations.
    
    In SR we consider the Poincar\'e group, which consists of all maps preserving the invariant spacetime interval.
    This is generated by translations, rotations, and boosts.
    The metric structure in this case is simply the Minkowski metric, \(\eta\), defined by \(\eta = \diag(-1, 1, 1, 1)\).
    
    A Riemannian manifold has a metric signature \((n, 0, 0)\), since the metric is positive definite.
    A pseudo Riemannian manifold has a metric signature \((p, n - p)\), since the metric is non-degenerate.
    A \defineindex{Lorentzian manifold}\index{manifold!Lorentzian} is a pseudo-Riemannian manifold with a metric signature \((1, n - 1)\).
    
    General relativity takes place on a Lorentzian manifold with a smooth, symmetric, non-degenerate metric.
    
    \subsection{Transformation Laws}
    Consider a basis transformation, which we take to be linear so that the new basis is a complete set, this can be written as \(\vep{j} = \alpha^i_j \ve{i}\).
    Note that \(\alpha^i_j\) here is \emph{not} a tensor, it is just a matrix defining this coordinate transformation.
    We can similarly define a way to transform the dual basis, \(\cvep{j} = \beta^j_i\cve{i}\).
    
    To maintain duality we require that
    \begin{equation}
        \tensor{\delta}{^i_j} = \cvep{i}(\vep{j}) = \beta^i_k\cve{k}(\alpha^l_i\ve{l}) = \beta^i_k \alpha^l_i\cve{k}(\ve{l}) = \beta^i_k\alpha^l_i \tensor{\delta}{^k_l} = \beta^i_k\alpha^k_i.
    \end{equation}
    This shows that \(\beta = (\alpha^\trans)^{-1}\).
    
    For a coordinate basis we saw that \(\ve{i} = \partial_i\) and then the transformation matrices are just derivatives between the old and new coordinates.
    \begin{equation}
        \alpha^i_j = \diffp{x^i}{x'^j}.
    \end{equation}
    From this we then have that
    \begin{equation}
        \beta^i_j = \diffp{x'^i}{x^j}.
    \end{equation}
    
    It follows then that
    \begin{equation}
        v'^j = \cvep{j}(\vv{v}) = \beta^j_k \cve{k}(\vv{v}) = \beta^j_k v^k,
    \end{equation}
    so the vector components transform in the opposite ways to the basis vectors, which is why we call them \emph{contra}variant.
    Similarly the components of a dual vector transform as
    \begin{equation}
        \omega'_j = \ve{j}(\cv{\omega}) = \alpha^k_j \ve{k}(\cve{\omega}) = \alpha^k_j \omega_k.
    \end{equation}

    \section{Other Matters}
    \subsection{Affine Geometry}
    \begin{dfn}{Affine Space}{}
        An \defineindex{affine space} is a set, \(A\), together with a vector space, \(V\), which acts freely and transitively as an additive group on the set \(A\).
        The action is a mapping \(A \times V \to A\) such that \((a, v) \mapsto a + v\).
        This has the following properties:
        \begin{itemize}
            \item Right identity: for all \(a \in A\) \(a + 0 = a\), where \(0 \in V\) is the zero vector.
            \item Associativity: for all \(v, w \in V\) and for all \(a \in A\) \(a + (v + w) = (a + v) + w\), where the first, third and fourth \(+\) are the group action and the second \(+\) is vector addition.
            \item The action is free and transitive: For all \(a \in A\) the mapping \(V \to  A\) defined by \(v \mapsto a + v\) is a bijection.
        \end{itemize}
        The first two properties define a right group action and the third characterises a free and transitive action.
    \end{dfn}
    
    This immediately defines two new properties:
    \begin{itemize}[resume]
        \item Existence of bijective translations: for all \(v \in V\) the mapping \(A \to A\) defined by \(a \mapsto a + v\) is a bijection.
        \item Subtraction: for all \(a, b \in A\) there exists unique \(v \in V\) such that \(b = a + v\), we denote \(v\) as \(b - a\).
    \end{itemize}
    
    Most of the time when we consider affine spaces \(A\) and \(V\) have the same underlying set and so the right action is the obvious extension of vector addition considered as a group action.
    
    In the same way that invariance of length under rotation defines a vector space we can think of an affine space as the space that is invariant under \define{affine transformations}\index{affine transformation}, which are rotations and translations, with a general affine transformation taking the form \(\vv{x} \mapsto A\vv{x} + \vv{y}\).
    This is more general than a vector space since we can think of it as a vector space but dropping the requirement that there is a zero vector, since we now allow translations.
    
    Poincar\'e transformations are examples of affine transformations.
    Recall that Poincar\'e transformations are Lorentz transformations plus translations\footnote{Where by \enquote{plus} we really mean a semidirect product, see the symmetries of quantum mechanics notes.}.
    
    Affine spaces don't require a metric structure to exist, in fact affine translations don't in general preserve lengths.
    However, affine translations do keep parallel lines parallel, which means distances along parallel lines are preserved relative to each other.
    
    \tikzexternaldisable
    An example of an affine space is colour space.
    We can think of this as a three dimensional affine space with basis \(\{\textcolor{red}{\text{red}}, \textcolor{green}{\text{green}}, \textcolor{blue}{\text{blue}}\}\).
    We can compare the intensity when the colour balance is unchanged, for example \((\textcolor{red}{1}, \textcolor{green}{0.75}, \textcolor{blue}{0.5})\) (\definecolor{rgb1}{rgb}{1,0.75,0.5}\tikz{\fill[rgb1] (0, 0) circle [radius = 0.75ex];}) is more intense than \((\textcolor{red}{0.8}, \textcolor{green}{0.6}, \textcolor{blue}{0.4})\) (\definecolor{rgb2}{rgb}{0.8,0.6,0.4}\tikz{\fill[rgb2] (0, 0)circle [radius = 0.75ex];}), which is the result of scaling all components by \(0.8\), but there is no intensity comparison with \((\textcolor{red}{0.3}, \textcolor{green}{0.5}, \textcolor{blue}{0.96})\) (\definecolor{rgb3}{rgb}{0.3,0.8,0.96}\tikz{\fill[rgb3] (0, 0) circle [radius = 0.75ex];}).
    Even though \(\sqrt{1^2 + 0.75^2 + 0.5^2} \approx \sqrt{0.3^2 + 0.8^2 + 0.96^2}\).
    \tikzexternalenable
    
    \subsection{Differential Forms}
    The name \enquote{one-form} suggests the existence of \(k\)-forms, for some \(k \in \integers_{>0}\).
    More generally we call these \define{differential forms}\index{differential form}.
    These are \((0, k)\)-tensors which are completely antisymmetric in their arguments, for example if \(\cv{\beta}\) is a two-form then \(\cv{\beta}(\vv{v}, \vv{w}) = -\cv{\beta}(\vv{w}, \vv{v})\).
    It is common to write the basis \(1\)-forms in this context as \(\{\dl{x^i}\}\).
    
    A simple example is the area of a parallelogram.
    We can define a parallelogram by taking two vectors, \(\vv{a}\) and \(\vv{b}\), as two adjacent sides.
    There must then be some function \(\mathop{\mathrm{area}} \colon \reals^2 \to \reals\) such that \(\mathop{\mathrm{area}}(\vv{a}, \vv{b})\) is the area of the parallelogram defined by \(\vv{a}\) and \(\vv{b}\).
    Making the only sensible choice of defining \(\mathop{\mathrm{area}}(\vv{a}, \vv{a}) = 0\) we must then have
    \begin{align}
        0 &= \mathop{\mathrm{area}}(\vv{a} + \vv{b}, \vv{a} + \vv{b})\\
        &= \mathop{\mathrm{area}}(\vv{a}, \vv{a} + \vv{b}) + \mathop{\mathrm{area}}(\vv{b}, \vv{a} + \vv{b})\\
        &= \mathop{\mathrm{area}}(\vv{a}, \vv{a}) + \mathop{\mathrm{area}}(\vv{a}, \vv{b}) + \mathop{\mathrm{area}}(\vv{b}, \vv{a}) + \mathop(\vv{b}, \vv{b})\\
        &= \mathop{\mathrm{area}}(\vv{a}, \vv{b}) + \mathop{\mathrm{area}}(\vv{b}, \vv{a}).
    \end{align}
    This can hold only if
    \begin{equation}
        \mathop{\mathrm{area}}(\vv{a}, \vv{b}) = -\mathop{\mathrm{area}}(\vv{b}, \vv{a}),
    \end{equation}
    that is, \(\mathop{\mathrm{area}}\) is antisymmetric.
    Since \(\mathop{\mathrm{area}}\) is antisymmetric and takes two vectors to a scalar we can identify it as a two-form.
    
    Given a basis of one-forms we can define higher order forms using the \defineindex{wedge product}, \(\wedge\), as a linear combination of basis \(k\)-forms, where the basis \(k\)-forms are all distinct combinations of basis one-forms.
    In three dimensions there are three basis one-forms, \(\{\dl{x}, \dl{y}, \dl{z}\}\), there are then three basis two-forms, \(\{\dl{x} \wedge \dl{y}, \dl{y} \wedge \dl{z}, \dl{z} \wedge \dl{x}\}\).
    Other possible combinations, such as \(\dl{x} \wedge \dl{z}\), are proportional to one of these by the antisymmetric property, \(\dl{x} \wedge \dl{z} = - \dl{z} \wedge \dl{x}\).
    There is only one basis three-form in three dimensions, namely \(\dl{x} \wedge \dl{y} \wedge \dl{z}\).
    There are \(m\) choose \(n\) basis \(m\)-forms in \(n\) dimensions.
    
    The wedge product can then be extended linearly to all \(k\)-forms.
    It turns out that this can be written in terms of the tensor product:
    \begin{equation}
        \cv{\alpha} \wedge \cv{\beta} = \cv{\alpha} \otimes \cv{\beta} - \cv{\beta} \otimes \cv{\alpha}.
    \end{equation}
    
    The area example may seem familiar.
    In \(\reals^3\) we can define a parallelogram like this and the oriented area is given by \(\vv{a} \times \vv{b}\).
    We can now see that this is just a special case of the wedge product using the fact that \({\reals^3}^* = \reals^3\) with the normal dot product defining \(\cv{a}(\vv{b}) = \vv{b}(\cv{a}) = \cv{a} \cdot \vv{b}\).
    
    \chapter{Tensor Analysis}
    In GR we consider a Lorentzian manifold, \(M\), and define vectors to be elements of the tangent space, \(T_pM\), at some point \(p \in M\).
    We then define covectors to be elements of the dual space, \(T^*PM\), which we can think of as the space of functions \(T_pM \to \reals\).
    We call elements of \(\reals\) scalars.
    We can then define more general \((n, m)\)-tensors as multilinear maps \(T_p^*M^n \times T_pM \to \reals\).
    We define the components of these objects by evaluating them on a given basis.
    This definition is far too abstract for day to day use and so in this section we start by introducing the \enquote{physicist's definition} of scalars, vectors, covectors, and tensors.
    This definition works in GR because of the structure of pseudo-Riemannian manifolds, but isn't quite as general as the previous definition.
    We'll start with the simplest, scalars.
    
    All of these definitions are based on the way the components of these objects transform under some coordinate transformation, \(x \mapsto x'\).
    
    \section{Scalars}
    \begin{dfn}{Scalar}{}
        A scalar is an object with no indices which is invariant under a general coordinate transformation.
    \end{dfn}
    
    A \defineindex{scalar field} is then a scalar valued function, \(\varphi\), such that \(\varphi'(x') = \varphi(x)\), here the two primes on the left hand side means that under the coordinate transformation \(x \mapsto x'\) the scalar field, \(\varphi\), transforms as \(\varphi \mapsto \varphi'\), in such a way that the resulting scalar value is unchanged.
    
    Scalars are relativistic invariants.
    An important example is the proper time, \(\tau(x)\).
    Another example is the mass of an object, \(m\).
    
    Not all numbers are scalars.
    For example, the number density, \(n\), is not a scalar due to length contraction, which means that the volume can change without the number of particles changing resulting in different densities.
    The same applies to other densities, such as the energy density, \(\rho c^2\) (where \(\rho\) is the mass density), here the problem is even worse since energy is also a frame dependent quantity, as well as volume.
    Another important example of a non-scalar are the components of other higher order tensors, as we will see.
    
    \section{Vectors}
    \subsection{Contravariant Vectors}
    \begin{dfn}{Contravariant Vectors}{}
        A \defineindex{contravariant vector}, or simply  \define{vector}\index{vector|see{contravariant vector}} is an object with components \(V^\mu\) which transform according to the following equation under a coordinate transformation \(x \mapsto x'\):
        \begin{equation}
            V'^\mu = \diffp{x'^\mu}{x^\nu} V^\nu.
        \end{equation}
    \end{dfn}
    
    This follows by the application of the chain rule to the four-vector line element, \(\dl{x^\mu}\):
    \begin{equation}
        \dl{x'^\mu} = \diffp{x'^\mu}{x^\nu}\dd{x^\nu}.
    \end{equation}
    
    It is common to call \(V^\mu\) a vector, when what we really mean is that it is a component of a vector.
    
    For a Lorentz transform the coordinate transformation is \(x^\mu \mapsto x'^\mu = \tensor{\Lambda}{^\mu_\nu}x^\nu\) and so for constant boosts
    \begin{equation}
        \diffp{x'^\mu}{x^\nu} = \tensor{\Lambda}{^\mu_\nu} \implies V'^\mu = \tensor{\Lambda}{^\mu_\nu} V^\nu,
    \end{equation}
    which is the usual SR definition of a four-vector.
    
    These vectors are called contravariant because the components transform (vary) in the opposite (contra) way to the basis vectors.
    
    Many familiar vectors are contravariant vectors, including the four-position, four-velocity, four-momentum, and most other four-vectors.
    
    \subsection{Covariant Vectors}
    \begin{dfn}{Covariant Vectors}{}
        A \defineindex{covariant vector}, or \defineindex{covector}, is an object with components \(U_\mu\) which transform according to the following equation under a coordinate transformation \(x \mapsto x'\):
        \begin{equation}
            U'_\mu = \diffp{x^\nu}{x'^\mu}U_\nu.
        \end{equation}
    \end{dfn}
    
    It is common to call \(U_\mu\) a covariant vector, when what we really mean is that it is a component of a covariant vector.
    
    These vectors are called covariant because the components transform (vary) in the same way (co, with) as the basis vectors.
    
    \begin{exm}{}{}
        Let \(\varphi\) be a scalar field.
        Then \(\diffp{\varphi}/{x^\mu}\) is a covector.
        This follows since
        \begin{equation}
            \dl{\varphi} = \diffp{\varphi}{x^\nu}\dd{x^\nu},
        \end{equation}
        by the chain rule and since \(\dl{\varphi}\) inherits its scalar nature from \(\varphi\), since if \(\varphi\) is invariant so are changes in \(\varphi\), we must then have that
        \begin{equation}
            \diffp{\varphi}{x'^\mu} = \diffp{x^\nu}{x'^\mu} \diffp{\varphi}{x^\nu}
        \end{equation}
        so that
        \begin{equation}
            \dl{\varphi'} = \diffp{\varphi}{x'^\nu}\dd{x'^\nu} = \diffp{x^\rho}{x^\mu} \diffp{\varphi}{x^\rho} \diffp{x'^\nu}{x^\mu} \dd{x^\mu} = \diffp{\varphi}{x^\mu}\dd{x^\mu} = \dl{\varphi}.
        \end{equation}
    \end{exm}
    
    \begin{ntn}{}{}
        We write
        \begin{equation}
            \partial_\mu \coloneqq \diffp{}{x^\mu}.
        \end{equation}
        Notice that a lower index is equivalent to an upper index in the denominator.
        We also use a comma notation\index{\(,-\), derivative notation} to write coordinate derivatives, so
        \begin{equation}
            \varphi_{,\mu} \coloneqq \partial_\mu \varphi = \diffp{\varphi}{x^\mu}.
        \end{equation}
        This notation extends not just to scalars, so, for example, we may write
        \begin{equation}
            \tensor{T}{^\mu_{\nu\rho,\sigma}} = \partial_\sigma \tensor{T}{^\mu_{\nu\rho}} = \diffp*{\tensor{T}{^\mu_{\nu\rho}}}{x^\sigma}.
        \end{equation}
        
        Similarly for derivatives with respect to contravariant coordinates we write
        \begin{equation}
            \partial^\mu \coloneqq \diffp{}{x_\mu}
        \end{equation}
        and
        \begin{equation}
            \varphi^{,\mu} = \partial^\mu \varphi = \diffp{\varphi}{x_\mu}.
        \end{equation}
    \end{ntn}
    
    \subsection{Scalar Product}
    Given a contravariant vector with components \(V^\mu\) and a covariant vector with components \(U_\mu\) we can construct a scalar value \(V^\mu U_\mu\).
    This works because
    \begin{align}
        V'^\mu U'_\mu &= \diffp{x'^\mu}{x^\nu} V^\nu \diffp{x^\rho}{x'^\mu} U_\rho\\
        &= \diffp{x'^\mu}{x^\nu} \diffp{x^\rho}{x'^\mu} V^\nu U_\rho\\
        &= \tensor{\delta}{^\rho_\nu} V^\nu U_\rho\\
        &= V^\nu U_\nu.
    \end{align}
    It is important that this involves contraction (summing over) indices of both a co- and contravariant nature, since the two transformations cancel out to give \(\tensor{\delta}{^\mu_\nu}\).
    This means that quantities like \(V^\mu V^\mu\), or \(U_\mu U_\mu\) aren't, in general, invariant.
    
    \section{Tensors of Arbitrary Rank}
    \begin{dfn}{Tensors}{}
        A rank \(n\) \defineindex{tensor} is an object with \(n\) indices, more specifically an \((k, l)\)-tensor has \(k\) upper indices and \(l\) lower indices, with \(k + l = n\).
        Upper indices transform like contravariant vectors and lower indices like covariant vectors, so a tensor transforms like
        \begin{equation}
            \tensor{{T'}}{^{\mu_1 \dotso \mu_k}_{\nu_1 \dotso \nu_k}} = \diffp{x'^{\mu_1}}{x^{\rho_1}} \dotsm \diffp{x'^{\mu_m}}{x^{\rho_m}} \diffp{x^{\sigma_1}}{x'^{\nu_1}} \dotsm \diffp{x^{\sigma_l}}{x'^{\nu_l}} \tensor{T}{^{\rho_1\dotso \rho_k}_{\sigma_1\dotso \sigma_l}}.
        \end{equation}
    \end{dfn}
    
    The above definition is probably clearer with a specific example of a rank 3 tensor with components \(\tensor{T}{^\mu_{\nu\rho}}\), which transforms like
    \begin{equation}
        \tensor{{T'}}{^{\mu}_{\nu\rho}} = \diffp{x'^\mu}{x^\alpha} \diffp{x^\beta}{x'^\nu} \diffp{x^\gamma}{x'^\rho} \tensor{T}{^\alpha_{\beta\gamma}}.
    \end{equation}
    
    In terms of transforming we can think of tensors as being a product of vector components, so \(\tensor{T}{^\mu_{\nu\rho}}\) transforms like \(V^\mu U_\nu W_\rho\).
    However, not all tensors can actually be written as a product of vectors\footnote{This is the difference between non-entangled multi-particle states like \(\ket{1} \otimes \ket{0} = \ket{10}\), and entangled states, like \((\ket{1}\otimes \ket{0} + \ket{0}\otimes\ket{1})/\sqrt{2} = (\ket{10} + \ket{01})/\sqrt{2}\), in quantum mechanics.}.
    
    \begin{exm}{Kronecker Delta}{}
        The \defineindex{Kronecker delta} is a rank 2 tensor of type \((1, 1)\), \(\tensor{\delta}{^\mu_\nu}\) defined the same in all frames by \(\tensor{\delta}{^\mu_\nu} = 1\) if \(\mu = \nu\) and \(\tensor{\delta}{^\mu_\nu} = 0\) if \(\mu \ne \nu\).
        This transforms according to
        \begin{equation}
            \tensor{{\delta'}}{^\mu_\nu} = \diffp{x'^\mu}{x^\alpha} \diffp{x^\beta}{x'^\nu} \tensor{\delta}{^\alpha_\beta} = \diffp{x'^\mu}{x^\alpha}\diffp{x^\alpha}{x'^\nu} = \diffp{x'^\mu}{x'^\nu} = \tensor{\delta}{^\mu_\nu}.
        \end{equation}
        So the Kronecker delta is the same in all frames as required.
    \end{exm}
    
    For a general tensor the location of the indices is important, for example \(\tensor{T}{^\mu_{\nu\rho}} \ne \tensor{T}{_\nu^\mu_\rho} \ne \tensor{T}{_{\nu\rho}^\mu}\).
    This isn't the case for symmetric tensors.
    Recall that a tensor is symmetric in two indices if swapping the indices doesn't change the value.
    In this case we might be more sloppy about the locations, for example, we may write \(\tensor{\delta}{^\mu_\nu} = \tensor{\delta}{_\nu^\mu} = \delta^\mu_\nu\).
    
    In general if we contract all covariant indices with a contravariant index and vice versa we will get a scalar.
    For example,
    \begin{align}
        \tensor{{T'}}{_{\mu\nu}}{A'}^{\mu}{B'}^{\nu} = \tensor{{T'}}{_{\mu\nu}} \diffp{x'^\mu}{x^\alpha} A^\alpha \diffp{x'^\nu}{x^\beta}B^\beta.
    \end{align}
    Demanding that this is equal to \(T_{\alpha\beta}A^\alpha B^\beta\) we must have
    \begin{equation}
        \tensor{{T'}}{_{\mu\nu}} = \diffp{x^\alpha}{x'^\mu} \diffp{x^\beta}{x'^\nu} \tensor{T}{_{\alpha\beta}}.
    \end{equation}
    This then gives
    \begin{align}
        \tensor{{T'}}{_{\mu\nu}}{A'}^{\mu}{B'}^{\nu} &= \diffp{x^\alpha}{x'^\mu} \diffp{x^\beta}{x'^\nu} \tensor{T}{_{\alpha\beta}} \diffp{x'^\mu}{x^\gamma} A^\gamma \diffp{x'^\nu}{x^\delta} B^\delta\\
        &= \diffp{x^\alpha}{x^\gamma} \diffp{x^\beta}{x^\delta} T_{\alpha\beta} A^\gamma B^\delta\\
        &= \tensor{\delta}{^\alpha_\gamma} \tensor{\delta}{^\beta_\delta} T_{\alpha\beta} A^\gamma B^\delta\\
        &= T_{\alpha\beta} A^\alpha B^\beta.
    \end{align}
    
    Physical laws cannot depend on the frame of reference.
    We can use this to inform the structure of physical laws.
    The \defineindex{principle of manifest covariance} says that equations should be written in terms of tensors such that free indices match on both sides of the equation and non-free indices are contracted with an index of the opposite type.
    For example, under this the following would be a valid physical law: \(A^{\alpha\beta}B_\alpha C_\beta D^\gamma = 17 E^\gamma\), of course to be useful we also have to identify \(A\), \(B\), \(C\), \(D\), and \(E\) with physical quantities.
    
    The converse to this principle is that if we are told an equation is covariant (in the sense that it follows the above principle) and we know all but one object are some form of tensor then the unknown object must be a suitable type of tensor for covariance to hold, so for example in \(F^\mu = m A^\mu\) we know that if \(m\) is a scalar (the mass) and \(A\) is a vector (the four-acceleration) then \(F\) must be a vector (the four-force).
    
    As a final note not all objects with indices are tensors.
    For example, the affine connections, \(\christoffel{\alpha}{\beta}{\gamma}\), are not tensors.
    This is because these appear in the geodesic equation of motion,
    \begin{equation}
        \diff{U^\mu}{\tau} + \christoffel{\mu}{\alpha}{\beta} U^\alpha U^\beta = 0,
    \end{equation}
    which is covariant, but \(\diff{U^\mu}/{\tau}\) is not a four-vector in general and so \(\christoffel{\mu}{\alpha}{\beta}\) is also not a four vector in such a way that the entire equation remains covariant.
    
    \section{Metric Tensor}
    We have seen that the metric tensor, with components \(g_{\mu\nu}\), is such that
    \begin{equation}
        c^2 \dd{\tau^2} = g_{\mu\nu} \dd{x^\mu} \dd{x^\nu}.
    \end{equation}
    We defined the metric tensor in \cref{eqn:metric tensor} to be such that
    \begin{equation}
        g_{\mu\nu} = \diffp{\xi^\alpha}{x^\mu} \diffp{\xi^\beta}{x^\nu} \eta_{\alpha\beta}.
    \end{equation}
    Identifying the Minkowski metric as the metric tensor in a locally inertial frame with coordinates \(\xi^\alpha\) we see that this is just the tensor transformation law after we change \(g_{\mu\nu} \to g'_{\mu\nu}\), \(\eta_{\alpha\beta} \to g_{\alpha\beta}\), \(\xi^\alpha \to x^\alpha\), and \(x^\mu \to x'^\mu\).
    
    Since \(\eta_{\mu\nu}\) is symmetric the we must also have that \(g_{\mu\nu}\) is symmetric.
    This is good since a metric tensor is required to be symmetric.
    
    In special relativity we wrote \(c^2 \dd{\tau^2} = \dl{x_\mu}\dd{x^\mu}\) and following along with this notation we defined \(V_\mu = \eta_{\mu\nu}V^\nu\).
    We can generalise this to general relativity defining
    \begin{equation}
        V_\mu \coloneqq g_{\mu\nu}V^\nu,
    \end{equation}
    which allows us to define a covariant vector from a contravariant vector.
    We can readily check that the resulting vector is indeed covariant:
    \begin{align}
        V'_\mu &= g'_{\mu\nu}V'^\nu\\
        &= \diffp{x^\alpha}{x'^\mu} \diffp{x^\beta}{x'^\nu} g_{\alpha\beta} \diffp{x'^\nu}{x^\gamma} V^\gamma\\
        &= \diffp{x^\alpha}{x'^\mu} \diffp{x^\beta}{x^\gamma} g_{\alpha\beta} V^\gamma\\
        &= \diffp{x^\alpha}{x'^\mu} \tensor{\delta}{^\beta_\gamma} g_{\alpha\beta} V^\gamma\\
        &= \diffp{x^\alpha}{x'^\mu} g_{\alpha\beta} V^\beta,
    \end{align}
    which shows that \(V_\alpha = g_{\alpha\beta}V^\beta\) transforms like a covariant vector.
    
    We say that \(g_{\mu\nu}\) \define{lowers indices}\index{lowering indices}.
    This generalises to all tensors, for example
    \begin{equation}
        T_{\mu\nu} = g_{\mu\alpha}\tensor{T}{^\alpha_\nu} = g_{\mu\alpha}g_{\nu\nu}T^{\alpha\beta}.
    \end{equation}
    
    Now define \(g^{\mu\nu}\) to be the components of the inverse of the metric tensor, such that
    \begin{equation}
        g^{\mu\alpha}g_{\nu\alpha} = \tensor{\delta}{^\mu_\nu}.
    \end{equation}
    We can then use this to \define{raise indices}\index{raising indices}:
    \begin{equation}
        V^\mu = g^{\mu\nu}V_\nu = g^{\mu\nu}g_{\nu\rho}V^\rho = \tensor{\delta}{^\mu_\rho} V^\rho = V^\mu.
    \end{equation}
    
    These equations, along with the principle of manifest invariance, show that \(g^{\mu\nu}\) are the components of a tensor, and that said tensor is symmetric.
    
    \chapter{Covariant Derivative}
    \section{The Problem}
    Our goal is to define the derivative of a vector field, \(V\).
    We will suppose that our vector field is parametrised by some affine parameter, \(\lambda\).
    The obvious definition is to define the derivative as a vector with components given by the following limit, where it exists, 
    \begin{equation}
        \lim_{\Delta \lambda \to 0} \frac{V^\mu(\lambda + \Delta \lambda) - V^\mu(\lambda)}{\Delta \lambda}.
    \end{equation}
    
    However, there is a subtlety to vectors on a manifold which is not present in Euclidean geometry and means this doesn't work.
    The issue is that in this limit we are comparing two vectors (\(V^\mu(\lambda + \Delta \lambda)\) and \(V^\mu(\lambda)\)) which are defined at different points on the manifold.
    In Euclidean geometry this is fine since the point at which a vector is defined isn't important.
    On a general manifold it is important, since we consider tangent spaces, \(T_pM\), defined at a point.
    We say that in Euclidean geometry it is possible to \defineindex{parallel transport} vectors, moving the base point where they are defined.
    We will see that this is possible by moving along a geodesic.
    
    Consider the example of the two-dimensional manifold \(S^2\), a sphere.
    This can be embedded in \(\reals^3\) with a general vector given by \(\vv{r} = R(\sin\vartheta\cos\varphi, \sin\vartheta\sin\varphi, \cos\vartheta)\), where \(R\) is the constant radius of the sphere and \(\vartheta\) and \(\varphi\) are the coordinates.
    A point on the sphere can be given as a linear combination of the two basis vectors
    \begin{align}
        \ve{\vartheta} &= \diffp{\vv{r}}{\vartheta} = R(\cos\vartheta\cos\varphi, \cos\vartheta\sin\varphi, -\sin\vartheta),\\
        \ve{\varphi} &= \diffp{\vv{r}}{\varphi} = R(-\sin\vartheta\sin\varphi, \sin\vartheta\cos\varphi, 0).
    \end{align}
    Consider changing \(\vartheta\) by a small amount.
    The change in \(\ve{\vartheta}\) is perpendicular to \(\ve{\varphi}\) and the change in \(\ve{\varphi}\) is perpendicular to \(\ve{\vartheta}\).
    This means that transporting these vectors along a geodesic (which is a great circle on the sphere) at constant \(\varphi\) will not rotate the basis.
    On the other hand moving along a non-great circle at constant \(\vartheta\) will rotate the vectors.
    The dot product between the change in one basis vector and the other basis vector will be non-zero if \(\vartheta \ne \pi/2\).
    
    \section{Covariant Derivative}\label{sec:covariant derivative}
    We now assume that there is a means of parallel transport, which allows for comparison of vectors at different points.
    This allows us to define the \defineindex{covariant derivative} of a vector, \(\vv{u}\), which we can take to be a tangent vector, \(\diff{}/{\mu}\), to a curve parametrised by \(\mu\), along a curve parametrised by \(\lambda\), that is in the direction of a tangent vector, \(\vv{v} = \diff{}/{\lambda}\).
    The covariant derivative is defined by the following limit, when it exists:
    \begin{equation}
        \covariantDerivative{\vv{v}} \vv{u} \coloneqq \lim_{\Delta \lambda \to 0} \frac{\vv{u}(\lambda + \Delta \lambda) - \vv{u}_{\|}(\lambda + \Delta \lambda)}{\Delta \lambda}.
    \end{equation}
    Here \(\vv{u}_{\|}(\lambda + \Delta \lambda)\) denotes the vector \(\vv{u}(\lambda)\) parallel transported to \(\lambda + \Delta\lambda\).
    
    We can also define the covariant derivative of a scalar field, \(f\), which are trivial to parallel transport since they don't change so \(f_{\|}(\lambda + \Delta \lambda) = f(\lambda)\) and hence
    \begin{equation}
        \covariantDerivative{\vv{v}} f = \lim_{\Delta \lambda \to 0} \frac{f(\lambda + \Delta \lambda) - f_{\|}(\lambda + \Delta \lambda)}{\Delta \lambda} = \lim_{\Delta \lambda \to 0} \frac{f(\lambda + \Delta \lambda) - f(\lambda)}{\Delta \lambda} = \diff{f}{\lambda}.
    \end{equation}
    
    The covariant derivative should not be confused with the \defineindex{Lie derivative}, which is just the commutator of the two tangent vectors:
    \begin{equation}
        \mathcal{L}_{\vv{v}} \vv{u} \coloneqq [\vv{v}, \vv{u}] = \diff{}{\lambda}\diff{}{\mu} - \diff{}{\mu}\diff{}{\lambda}.
    \end{equation}

    Parallel transport is reversible, in the sense that if you parallel transport a vector along a path and then back along the same path it returns to its initial value.
    The same is not true of parallel transporting a vector around a loop.
    For example, suppose you stand at the North pole looking south along the Greenwich Meridian.
    Walk south to the equator, then walk \ang{90} west, still looking south.
    Walk North until you reach the North pole again, still looking south.
    This is equivalent to having turned \ang{90} on the spot.
    
    \section{Components of the Covariant Derivative}
    We define the covariant derivative of a basis vector, \(\ve{j}\), with respect to another basis vector, \(\ve{i}\), to be
    \begin{equation}\label{eqn:affine connection again}
        \covariantDerivative{\ve{i}} \ve{j} \coloneqq \christoffel{k}{j}{i} \ve{k}.
    \end{equation}
    We take this here as the definition of \(\christoffel{k}{j}{i}\), which we will soon show is equal to the \defineindex{affine connection} which appears in the geodesic equation, this also justifies the name \enquote{affine connection} since the covariant derivative compares vectors at two positions, so connected by an affine transformation.
    This definition makes sense since we can think of \(\Gamma^k\ve{k}\) as a vector given by a linear combination of basis vectors and these components depend in general on \(i\) and \(j\) through \(\covariantDerivative{\ve{i}}\ve{j}\) and \(\christoffel{k}{j}{i}\).
    
    For a general vector, \(\vv{v} = v^i\ve{i}\), we expect the covariant derivative to be linear in the components, meaning \(\covariantDerivative{\vv{v}}\vv{u} = v^i\covariantDerivative{\ve{i}}\vv{u}\).
    We also expect that the normal product rule should hold, also known as the \defineindex{Leibniz rule}.
    This means that \(\covariantDerivative{\vv{v}} (u^i\ve{i}) = \ve{i}\covariantDerivative{\vv{v}} u^i + u^i\covariantDerivative{\vv{v}} \ve{i}\).
    We also use the fact that \(\covariantDerivative{\ve{i}} f = \diffp{f}/{x^i}\) for all scalar fields, \(f\).
    Putting this together we get
    \begin{align}
        \covariantDerivative{\vv{v}}\vv{u} &= \covariantDerivative{v^j\ve{j}} (u^i\ve{i})\\
        &= v^j\covariantDerivative{\ve{j}} (u^i\ve{i})\\
        &= v^ju^i \covariantDerivative{\ve{j}} \ve{i} + v^j\ve{i} \covariantDerivative{\ve{j}} u^i\\
        &= v^ju^i \christoffel{k}{i}{j} + v^j\ve{i}\diffp{u^i}{x^j}.
    \end{align}
    With some relabelling changing \(i \to k\) in the second term we get
    \begin{equation}
        \covariantDerivative{\vv{v}}\vv{u} = \left( v^ju^i\christoffel{k}{i}{j} + v^j\diffp{u^k}{x^j} \right) \ve{k}.
    \end{equation}
    
    Now, consider the case when both vectors involved are the four-velocity.
    The components in the above equation then become
    \begin{align}
        U^\alpha \diffp{U^\mu}{x^\alpha} + \christoffel{\mu}{\alpha}{\beta} U^\alpha U^\beta  &= \diff{x^\alpha}{\tau} \diffp{U^\mu}{x^\alpha} + \christoffel{\mu}{\alpha}{\beta} U^\alpha U^\beta\\
        &= \diff{U^\mu}{\tau} + \christoffel{\mu}{\alpha}{\beta} U^\alpha U^\beta\\
        &= 0.
    \end{align}
    We can identify this as the geodesic equation, which demands that this quantity vanishes.
    This also shows that \(\christoffel{\mu}{\alpha}{\beta}\) is indeed the affine connection.
    
    This allows us to write the geodesic equation in coordinate free notation as
    \begin{equation}
        \covariantDerivative{\vv{v}} \vv{v} = 0.
    \end{equation}
    This shows that we can parallel transport a vector by moving it along a geodesic to which it is tangent at the point it is defined.
    We can think of the covariant derivative as modifying the derivative to subtract the extra term added by parallel transporting the vector.
    For an infinitesimal parallel transportation the amount the vector changes is proportional to the vector and to the distance travelled, hence it is of the form
    \begin{equation}
        \dl{V_{\|}^\mu} = -\christoffel{\mu}{\alpha}{\beta} V^\alpha \dd{x^\beta}.
    \end{equation}
    We can take this as the definition of the affine connection.
    The total change when the vector is moved around a closed loop is then
    \begin{equation}
        \delta V_{\|}^\mu = -\oint \christoffel{\mu}{\alpha}{\beta} V^\alpha \dd{x^\beta}.
    \end{equation}
    This does not, in general, vanish, consider the example of walking on the surface of Earth at the end of \cref{sec:covariant derivative}.
    Compare the result above to rotation of a vector, where the change in the vector is proportional to the length of the vector and the angle rotated through, for sufficiently small angles.
    
    We can now construct the components of the covariant derivative of a vector.
    These components are given by taking the limit of \(\delta V^\mu / \delta x^\nu\).
    The observable change is \(V^\mu(x^\nu + \delta x^\nu)\) minus \(V^\mu(x^\nu)\) after parallel transporting to \(x^\nu + \delta x^\nu\).
    We therefore have that the components of the covariant derivative of \(V\) are
    \begin{equation}
        \covariantDerivative{\nu} V^\mu = \diffp{V^\mu}{x^\nu} + \christoffel{\mu}{\alpha}{\nu} V^\alpha
    \end{equation}
    
    \begin{ntn}{}{}
        We write
        \begin{equation}
            \tensor{V}{^\mu_{;\nu}} \coloneqq \covariantDerivative{\nu} V^\mu
        \end{equation}
        for the \(\nu\) component of the covariant derivative of \(V^\mu\).
    \end{ntn}
    
    With the above notation, and the notation \(\diffp{\varphi}/{x^\nu} = \partial_\nu \varphi = \varphi_{,\nu}\) the components of the covariant derivative are given by
    \begin{equation}
        \covariantDerivative{\nu} V^\mu = \tensor{V}{^\mu_{;\nu}} = \partial_\nu V^\mu + \christoffel{\mu}{\alpha}{\nu} V^\alpha = \tensor{V}{^\mu_{,\nu}} + \christoffel{\mu}{\alpha}{\nu} V^\alpha.
    \end{equation}
    
    \subsection{Covariant Derivatives of Other Quantities}
    The result \(\tensor{V}{^\mu_{;\nu}} = \tensor{V}{^\mu_{,\nu}} + \christoffel{\mu}{\alpha}{\nu}\) only holds for a contravariant vector.
    For other objects the components of their covariant derivatives are different, but they can fairly easily be worked out using the two following facts:
    \begin{itemize}
        \item The covariant derivative is a derivation, that is its linear and the Leibniz rule applies, \(D(fg) = fD(g) + D(f)g\), and
        \item The covariant derivative of a scalar field is just a normal derivative.
    \end{itemize}
    
    For example, suppose we wish to know the components of a covariant vector, \(V_\mu\).
    We can use the fact that \(U^\mu V_\mu\) is a scalar for a contravariant vector \(U^\mu\).
    By the second property above we then have
    \begin{equation}
        (U^\mu V_\mu)_{;\nu} = (U^\mu V_\mu)_{,\nu} = \tensor{U}{^\mu_{,\nu}}V_\mu + U^\mu\tensor{V}{_{\mu,\nu}}.
    \end{equation}
    By the first property we have
    \begin{equation}
        (U^\mu V_\mu)_{;\nu} = U^\mu \tensor{V}{_{\mu;\nu}} + \tensor{U}{^\mu_{;\nu}}V_\mu = U^\mu \tensor{V}{_{\mu;\nu}} + \tensor{U}{^\mu_{,\nu}}V_\mu + \christoffel{\mu}{\alpha}{\nu}U^\alpha V_\mu.
    \end{equation}
    Reindexing in the last term, \(\alpha \leftrightarrow \mu\), we then have
    \begin{equation}
        (U^\mu V_\mu)_{;\nu} = U^\mu V_{\mu;\nu} + \tensor{U}{^\mu_{,\nu}} V_\mu + \christoffel{\alpha}{\mu}{\nu} U^\mu V_\alpha.
    \end{equation}
    Equating these two results we have
    \begin{equation}
        \tensor{U}{^\mu_{,\nu}} V_{\mu} + U^\mu V_{\mu,\nu} = U^\mu V_{\mu;\nu} + \tensor{U}{^\mu_{,\nu}} V_\mu + \christoffel{\alpha}{\mu}{\nu} U^\mu V_\alpha.
    \end{equation}
    Rearranging this we have
    \begin{equation}
        U^\mu V_{\mu;\nu} = U^\mu V_{\mu,\nu} - \christoffel{\alpha}{\mu}{\nu} U^\mu V_\alpha.
    \end{equation}
    Demanding that this holds for all contravariant vectors, \(U\), we must have that
    \begin{equation}
        \covariantDerivative{\nu} V_\mu = V_{\mu;\nu} = V_{\mu,\nu} - \christoffel{\alpha}{\mu}{\nu} V_\alpha.
    \end{equation}

    This is very similar to the contravariant result, the main difference is the sign of the second term, and due to the different index placement we now sum over the upper index of the affine connection instead of one of the lower indices.
    
    Suppose instead that we want to know the the components of the covariant derivative of some tensor with components \(T^{\mu\nu}\).
    We can work this out using the fact that all such tensors transform the same as \(V^\mu U^\nu\) for contravariant vectors \(V\) and \(U\), even though not all such tensors can be written as a product of vectors.
    The transformation rules are what informs the parallel transport and so are the only things important in computing the components of the covariant derivative for a general tensor.
    Applying the Leibniz rule we have
    \begin{align}
        \covariantDerivative{\alpha} (V^\mu U^\nu) &= (V^\mu U^\nu)_{;\alpha}\\
        &= \tensor{V}{^\mu_{;\alpha}} U^\nu + V^\mu \tensor{U}{^\nu_{;\alpha}}\\
        &= (\tensor{V}{^\mu_{,\alpha}} + \christoffel{\mu}{\beta}{\alpha} V^\beta) U^\nu + V^\mu (\tensor{U}{^\nu_{,\alpha}} + \christoffel{\nu}{\beta}{\alpha}U^\alpha)\\
        &= \tensor{V}{^\mu_{,\alpha}}U^\nu + V^\mu\tensor{U}{^\nu_{,\alpha}} + \christoffel{\mu}{\beta}{\alpha} V^\beta U^\nu + \christoffel{\nu}{\beta}{\alpha} V^\mu U^\beta\\
        &= (V^\mu U^\nu)_{,\alpha} + \christoffel{\mu}{\beta}{\alpha} V^\beta U^\nu + \christoffel{\nu}{\beta}{\alpha} V^\mu U^\beta.
    \end{align}
    It then follows that for any tensor with components \(T^{\mu\nu}\), even non-simple tensors, we have
    \begin{equation}
        \covariantDerivative{\alpha} = \tensor{T}{^{\mu\nu}_{;\alpha}} = \tensor{T}{^{\mu\nu}_{,\alpha}} + \christoffel{\mu}{\beta}{\alpha} T^{\beta\nu} + \christoffel{\nu}{\beta}{\alpha} T^{\mu\beta}.
    \end{equation}
    
    This same logic can be applied to tensors with an arbitrary number of upper indices.
    We can also use the contraction-of-indices-to-give-a-scalar-for-which-we-just-get-a-normal-derivative trick to find the components of covariant derivatives of tensors with lower indices.
    In general the result will be that we have a normal derivative plus a term for each index proportional to the affine connection with an appropriate sum over indices, with plus signs for upper indices and minus signs for lower indices.
    For example, the covariant derivative of a tensor with components \(T_{\mu\nu}\) is
    \begin{equation}
        \covariantDerivative{\alpha} T_{\mu\nu} = T_{\mu\nu;\alpha} = T_{\mu\nu,\alpha} - \christoffel{\beta}{\alpha}{\mu} T_{\beta\nu} - \christoffel{\beta}{\alpha}{\nu} T_{\mu\beta},
    \end{equation}
    and the covariant derivative of the mixed tensor with components \(\tensor{T}{^\mu_\nu}\) is
    \begin{equation}
        \covariantDerivative{\alpha} \tensor{T}{^\mu_\nu} = \tensor{T}{^\mu_{\nu;\alpha}} = \tensor{T}{^\mu_{\nu,\alpha}} - \christoffel{\mu}{\beta}{\alpha}\tensor{T}{^\beta_\nu} - \christoffel{\beta}{\alpha}{\nu} \tensor{T}{^\mu_\beta}.
    \end{equation}
    
    \subsection{Covariant Derivative Along a Curve}
    Some quantities only make sense to consider along the world line of a particle.
    For example, the momentum, \(p^\mu(\tau)\), only has meaning along the world line \(x^\mu(\tau)\).
    Consider some four-vector \(A^\mu\).
    We can project this along a path using the four-velocity, \(U^\mu\), so that
    \begin{equation}
        U^\nu \partial_\nu A^\mu = \diff{x^\nu}{\tau} \diffp{A^\mu}{x^\nu} = \diff{A^\nu}{\tau}.
    \end{equation}
    This is not, in general, a covariant quantity.
    We can make it generally covariant by replacing \(\partial_\nu\) with \(\covariantDerivative{\nu}\).
    This allows us to define another derivative type, \(\diffcov{}/{\tau}\)\index{D/dt@\(\diffcov{}/{\tau}\)}:
    \begin{equation}
        \diffcov{A^\mu}{\tau} \coloneqq U^\nu \covariantDerivative{\nu} A^\mu = \diff{A^\mu}{\tau} + \christoffel{\mu}{\sigma}{\nu} \diff{x^\nu}{\tau} A^\sigma.
    \end{equation}
    If \(A^\mu(\tau)\) is a contravariant vector then so is \(\diffcov{A^\mu}/{\tau}\).
    Similarly for a covariant vector we can define
    \begin{equation}
        \diffcov{A_\mu}{\tau} \coloneqq U^\nu \covariantDerivative{\nu} A_\mu = \diff{A_\mu}{\tau} - \christoffel{\nu}{\mu}{\sigma} \diff{x^\sigma}{\tau} A_\nu
    \end{equation}
    
    \subsection{Covariant Derivative of the Metric}
    In \cref{eqn:affine connection} at the start of the course we defined the affine connection as
    \begin{equation}
        \christoffel{\lambda}{\mu}{\nu} \coloneqq \diffp{x^\lambda}{\xi^\alpha} \diffp{\xi^\alpha}{x^\nu, x^\mu}
    \end{equation}
    where \(x^\mu\) are arbitrary coordinates and \(\xi^\mu\) are coordinates in a locally inertial frame.
    Using this we showed that the affine connections naturally appear in derivatives of the metric.
    We then redefined the affine connection in \cref{eqn:affine connection again} by the equation
    \begin{equation}
        \covariantDerivative{\ve{i}}\ve{j} \coloneqq \christoffel{k}{j}{i} \ve{k}.
    \end{equation}
    In order to complete this section on the covariant derivative we will show that the affine connections as defined here cause contravariant derivatives of the metric to vanish.
    
    To show this we start by requiring that parallel transport doesn't change the \enquote{length} of a four-vector.
    By which we mean \(\sqrt{g_{\mu\nu}V^\mu V^\nu}\) should be constant.
    If this is constant then \(g_{\mu\nu}V^\mu V^\nu\) should also be constant, and this is easier to work with.
    This tells us that the covariant derivative of this quantity should vanish, so
    \begin{equation}
        \covariantDerivative{\nu} g_{\alpha\beta} V^\alpha_{\|} V^\beta_{\|} = 0,
    \end{equation}
    here \(V^\alpha_{\|}\) are the components of \(V^\alpha\) after parallel transportation.
    We know that \(\covariantDerivative{\nu}V^\alpha_{\|} = 0\) by the definition of the covariant derivative.
    We also require the Leibniz rule to hold and so
    \begin{equation}
        \covariantDerivative{\nu} g_{\alpha\beta} V^\alpha_{\|}V^\beta_{\|} = (\covariantDerivative{\nu}g_{\alpha\beta}) V^\alpha_{\|}V^\beta_{\|} + g_{\alpha\beta}(\covariantDerivative{\nu}V^\alpha_{\|})V^\beta_{\|} + g_{\alpha\beta}V^\alpha_{\|}\covariantDerivative{\nu}V^\beta_{\|} = V^\alpha_{\|}V^\beta_{\|}\covariantDerivative{\nu}g_{\alpha\beta}.
    \end{equation}
    Demanding that this quantity vanishes and holds for a general four-vector \(V^\mu\) we must have that
    \begin{equation}
        \covariantDerivative{\sigma}g_{\mu\nu} = \diffp{g_{\mu\nu}}{x^\sigma}  - \christoffel{\rho}{\sigma}{\mu} g_{\rho\nu} - \christoffel{\rho}{\sigma}{\nu} g_{\mu\rho} = 0
    \end{equation}
    where the middle term is simply the definition of the components of the covariant derivative of a tensor with two covariant indices, which the metric is.
    
    This property is useful since it shows that raising and lowering indices commutes with the covariant derivative since
    \begin{align}
        \covariantDerivative{\nu} V_\mu &= \covariantDerivative{\nu} g_{\mu\sigma}V^\sigma\\
        &= (\covariantDerivative{\nu} g_{\mu\sigma}) V^\sigma + g_{\mu\sigma} \covariantDerivative{\nu} V^\sigma\\
        &= g_{\mu\sigma} \covariantDerivative{\nu} V^\sigma.
    \end{align}
    
    It is also possible to show that the covariant derivative of the metric vanishes by writing out the components of the covariant derivative using
    \begin{equation}
        \christoffel{\sigma}{\lambda}{\mu} = \frac{1}{2}g^{\nu\sigma} ( \partial_\lambda g_{\mu\nu} + \partial_\mu g_{\lambda\nu} - \partial_\nu g_{\mu\lambda} ).
    \end{equation}
    This then gives
    \begingroup
    \setindexcolor{\sigma = highlight!90!black, \mu = my blue, \nu = my red, \rho = my green, \alpha = my purple}
    \allowdisplaybreaks
    \begin{align}
        \covariantDerivative{\indexcolor{\sigma}} g_{\indexcolor{\mu}\indexcolor{\nu}} &= \partial_{\indexcolor{\sigma}} g_{\indexcolor{\mu}\indexcolor{\nu}} - \christoffel{\indexcolor{\rho}}{\indexcolor{\sigma}}{\indexcolor{\mu}} g_{\indexcolor{\rho}\indexcolor{\nu}} - \christoffel{\indexcolor{\rho}}{\indexcolor{\sigma}}{\indexcolor{\nu}} g_{\indexcolor{\mu}\indexcolor{\rho}}\\
        &= \partial_{\indexcolor{\sigma}} g_{\indexcolor{\mu}\indexcolor{\nu}} - \christoffeldef{\indexcolor{\rho}}{\indexcolor{\sigma}}{\indexcolor{\mu}}{\indexcolor{\alpha}} g_{\indexcolor{\rho}\indexcolor{\nu}}\notag\\
        &\qquad- \christoffeldef{\indexcolor{\rho}}{\indexcolor{\sigma}}{\indexcolor{\nu}}{\indexcolor{\alpha}} g_{\indexcolor{\mu}\indexcolor{\rho}}\\
        &= \partial_{\indexcolor{\sigma}} g_{\indexcolor{\mu}\indexcolor{\nu}} - \frac{1}{2} \tensor{\delta}{^{\indexcolor{\alpha}}_{\indexcolor{\nu}}} (\partial_{\indexcolor{\sigma}} g_{\indexcolor{\mu} \indexcolor{\alpha}} + \partial_{\indexcolor{\mu}} g_{\indexcolor{\sigma}\indexcolor{\alpha}} - \partial_{\indexcolor{\alpha}} g_{\indexcolor{\mu}\indexcolor{\sigma}})\notag\\
        &\qquad- \frac{1}{2}\tensor{\delta}{^{\indexcolor{\alpha}}_{\indexcolor{\mu}}} (\partial_{\indexcolor{\sigma}} g_{\indexcolor{\nu}\indexcolor{\alpha}} + \partial_{\indexcolor{\nu}} g_{\indexcolor{\sigma}\indexcolor{\alpha}} - \partial_{\indexcolor{\alpha}} g_{\indexcolor{\nu} \indexcolor{\sigma}})\\
        &= \partial_{\indexcolor{\sigma}} g_{\indexcolor{\mu}\indexcolor{\nu}} - \frac{1}{2}(\partial_{\indexcolor{\sigma}} g_{\indexcolor{\mu} \indexcolor{\nu}} + \partial_{\indexcolor{\mu}} g_{\indexcolor{\sigma}\indexcolor{\nu}} - \partial_{\indexcolor{\nu}} g_{\indexcolor{\mu} \indexcolor{\sigma}})\notag\\
        &\qquad- \frac{1}{2}(\partial_{\indexcolor{\sigma}} g_{\indexcolor{\nu} \indexcolor{\mu}} + \partial_{\indexcolor{\nu}} g_{\indexcolor{\sigma} \indexcolor{\mu}} - \partial_{\indexcolor{\mu}} g_{\indexcolor{\nu} \indexcolor{\sigma}})\\
        &= \partial_{\indexcolor{\sigma}} g_{\indexcolor{\mu}\indexcolor{\nu}} - \frac{1}{2}(\partial_{\indexcolor{\sigma}} g_{\indexcolor{\mu} \indexcolor{\nu}} + \partial_{\indexcolor{\mu}} g_{\indexcolor{\sigma}\indexcolor{\nu}} - \partial_{\indexcolor{\nu}} g_{\indexcolor{\mu} \indexcolor{\sigma}})\notag\\
        &\qquad- \frac{1}{2}(\partial_{\indexcolor{\sigma}} g_{\indexcolor{\nu} \indexcolor{\mu}} + \partial_{\indexcolor{\nu}} g_{\indexcolor{\sigma} \indexcolor{\mu}} - \partial_{\indexcolor{\mu}} g_{\indexcolor{\nu} \indexcolor{\sigma}})\\
        &= 0.
    \end{align}
    \endgroup
    
    \section{Why Modify the Derivative?}
    The need to modify the derivative to get the covariant derivative is related to a very fundamental fact in physics.
    Changes in the maths that don't result in physical changes cannot be allowed to change equations.
    The classic example of this is electromagnetism.
    
    The Schrödinger equation is
    \begin{equation}
        \left[ \frac{\vecoperator{p}^2}{2m} + V \right]\psi = \left[ -\frac{\hbar^2}{2m} + V \right] \psi = i\hbar\diffp{\psi}{t}.
    \end{equation}
    In quantum mechanics the only physical meaning of the wave function is the resulting probability density, \(\abs{\psi}^2\).
    This means that the physics doesn't depend on the phase of \(\psi\), only its absolute value.
    Therefore the physics doesn't change under the transformation
    \begin{equation}
        \psi \to \e^{i\alpha} \psi.
    \end{equation}
    Indeed, if \(\varphi\) is constant, in which case we call it a global phase, then this is easy to show:
    \begin{equation}
        \laplacian (\e^{i\alpha} \psi) = \e^{i\alpha}\laplacian \psi
    \end{equation}
    and so we can divide a factor of \(\e^{i\alpha}\) out of the Schrödinger equation.
    
    However, a global phase is non-relativistic.
    If we make a change to the phase at one place this cannot propagate instantly across the entire universe.
    Therefore, a better model is a local phase, \(\alpha(\vv{r}, t)\).
    The problem with this is we then get
    \begin{equation}
        \grad(\e^{i\alpha}\psi) = \e^{i\alpha}\grad\psi + \psi \grad\e^{i\alpha} = \e^{i\alpha}\grad\psi + i\e^{i\alpha}\psi\grad\alpha.
    \end{equation}
    The problem then gets even worse when we take the divergence to get the Laplacian.
    We also have a problem with the other side of the Schrödinger equation:
    \begin{equation}
        \diffp*{(\e^{i\alpha}\psi)}{t} = \e^{i\alpha}\diffp{\psi}{t} + i\e^{i\alpha}\diffp{\alpha}{t}.
    \end{equation}
    
    The solution is to modify the derivatives in such a way that when the transformation \(\psi \to \e^{i\alpha}\psi\) occurs this exactly cancels with the modification of the derivative.
    We can recognise the effect of changing the phase as the same as making the substitutions
    \begin{equation}
        \grad \to \grad + i\grad \alpha, \qqand \diffp{}{t} \to \diffp{}{t} + i \diffp{\alpha}{t}.
    \end{equation}
    Therefore we can define new, \define{gauge-covariant derivatives}\index{gauge-covariant derivative}
    \begin{equation}
        \grad \to \grad + \vv{F}(\vv{r}, t), \qqand \diffp{}{t} \to \diffp{}{t} + f(\vv{r}, t)
    \end{equation}
    where \(\vv{F}\) and \(f\) are called the \define{gauge fields}\index{gauge field} and need to transform as follows:
    \begin{equation}
        \vv{F} \to \vv{F} - i\grad\alpha, \qqand f \to f - i \diffp{\alpha}{t}.
    \end{equation}
    We can identify these as exactly the gauge freedom of the vector and scalar potential in electromagnetism, \(\vv{A}\) and \(\varphi\), respectively.
    That is, if we add the gradient of a function to the vector potential and the time derivative of a function to the scalar potential the resulting electric and magnetic fields are unchanged.
    
    We can therefore see electromagnetism as a consequence of requiring that the wave functions phase is \defineindex{gauge invariant}.
    It can also be shown, although not as simply, that the weak and strong force exist due to similar gauge invariance requirements, just for more complicated gauge transformations.
    The symmetry\footnote{See the notes from the symmetries of quantum mechanics course} associated with the phase transformation is \(\unitary(1)\).
    The symmetry of the electroweak interaction is \(\unitary(1)\times\specialUnitary(2)\) and the symmetry of the strong force is \(\specialUnitary(3)\).
    Combined, the symmetry of the standard model is \(\unitary(1)\times\specialUnitary(2)\times\specialUnitary(3)\).
    
    In the same way general relativity can be said to be a gauge theory of gravity with the symmetry being invariance under diffeomorphisms.
    This is why we add something to the derivatives, in this case a term proportional to the affine connection, in order to have the resulting equations be gauge invariant.
    
    \section{Principle of Manifest Covariance}
    We can now give an algorithm for generating covariant equations in general relativity.
    This is based on the observation that a covariant equation must be written in terms of tensors (including scalars and vectors) and must reduce to special relativity in the absence of gravity, when \(g_{\mu\nu} = \eta_{\mu\nu}\) and \(\christoffel{\rho}{\mu}{\nu} = 0\).
    The process for creating an equation which holds in all frames in general relativity is then as follows:
    \begin{itemize}
        \item Take an equation from special relativity which holds in the absence of gravity.
        \item Replace \(g_{\mu\nu}\) with \(\eta_{\mu\nu}\).
        \item Replace coordinate derivatives, \(\partial_\mu\), with covariant derivatives, \(\covariantDerivative{\mu}\).
    \end{itemize}
    
    An example of this would be the continuity equation from electromagnetism.
    Recall that for a conserved quantity with density \(\rho\) and current density \(\vv{j}\) we define the four current to be \(J^\mu = (c\rho, \vv{j})\).
    The continuity equation for \(\rho\) and \(\vv{j}\) is
    \begin{equation}
        \diffp{\rho}{t} + \div\vv{j} = 0.
    \end{equation}
    This can be written with the notation of special relativity as
    \begin{equation}
        \partial_\mu J^\mu = 0.
    \end{equation}
    We then get the general relativistic version of this by replacing the derivative with the covariant derivative:
    \begin{equation}
        \covariantDerivative{\mu} J^\mu.
    \end{equation}
    
    This principle works in reverse also.
    Suppose we have some quantity, \(T\), which we know to be a tensor with components \(T^{\alpha\beta\gamma}\) and we want to know if \(T^{\alpha\beta\gamma} = 0\) is a valid law of physics.
    We can write this quantity in a locally inertial frame and it will probably simplify since we can replace covariant derivatives with coordinate derivatives, the general metric with the Minkowski metric, and all affine connections will vanish (although derivatives of affine connections won't).
    If \(T^{\alpha\beta\gamma} = 0\) holds in this locally inertial frame then it must hold in all frames and so we know that the full version with covariant derivatives, the general metric and affine connections must hold in all frames.
    
    An example where we have already used this is in deriving the geodesic equation,\vspace{-2.5ex}
    \begin{equation}
        \diff{U^\mu}{\tau} + \christoffel{\mu}{\alpha}{\beta} U^\alpha U^\beta = 0,
    \end{equation}
    which is covariant but not manifestly since \(\diff{U^\mu}/{\tau}\) and \(\christoffel{\mu}{\alpha}{\beta}\) are not tensors on their own, but they do for a tensor when combined in this way.
    
    \chapter{Curvature}
    \section{Riemann Curvature Tensor}
    \begin{figure}
        \tikzsetnextfilename{parallel-transport}
        \begin{tikzpicture}
            \draw[->] (0, 0) -- ++ (2, 3) node[pos=0.6, left] {\(b^\mu\)};
            \draw[->] (2, 3) -- ++ (2, 1) node[midway, above] {\(a^\mu\)};
            \draw[->] (0, 0) -- ++ (2, 1) node[midway, below] {\(a^\mu\)};
            \draw[->] (2, 1) -- ++ (2, 3) node[pos=0.4, right] {\(b^\mu\)};
            \node[below left] at (0, 0) {\(A\)};
            \node[below right] at (2, 1) {\(B\)};
            \node[above right] at (4, 4) {\(C\)};
            \node[above left] at (2, 3) {\(D\)};
        \end{tikzpicture}
        \caption{Parallel transport around a parallelogram to define curvature.}
        \label{fig:parallel transport}
    \end{figure}
    
    We introduced parallel transport to get around the effects of curvature.
    We can also use parallel transport as a way to quantify curvature.
    The more we have to change the vector as we parallel transport it the more curvature there is.
    Consider what happens when we parallel transport a vector, \(V^\mu\), around a small parallelogram with sides \(a^\mu\) and \(b^\mu\) as shown in \cref{fig:parallel transport}.
    The change in a vector due to parallel transporting along a displacement \(\delta x^\beta\) is
    \begin{equation}
        \delta V^\mu = -\christoffel{\mu}{\alpha}{\beta} V^\alpha \delta x^\beta.
    \end{equation}
    The total change due to parallel transporting around the parallelogram is equal to the difference in going along \(ABC\) or \(ADC\).
    This is given by
    \begin{multline}
        \delta V^\mu = -\christoffel{\mu}{\alpha}{\beta}(x) V^\alpha(x) a^\beta - \christoffel{\mu}{\alpha}{\beta}(x + a) V_{\|}^{\alpha}(x + a) b^\beta\\
        + \christoffel{\mu}{\alpha}{\beta} V^\alpha(x)b^\beta + \christoffel{\mu}{\alpha}{\beta}(x + b) V_{\|}^\alpha (x + b) a^\beta
    \end{multline}
    where we make explicit where the vector fields and affine connections are evaluated.
    The first term is the change in the vector due to the parallel transport along \(AB\), the next term is for \(BC\), the third term is for \(CD\) and the final term for \(DA\).
    
    For a small parallelogram the form of the second and fourth terms suggests we should Taylor expand.
    Doing so we get
    \begin{equation}
        \christoffel{\mu}{\alpha}{\beta} (x + a) V_{\|}^{\alpha} (x + a) \approx \christoffel{\mu}{\alpha}{\beta} V^\alpha + a^\nu \partial_\nu \christoffel{\mu}{\alpha}{\beta}V_{\|}^{\alpha} + \order(a^2),
    \end{equation}
    where when not explicitly written we evaluate quantities at \(x\).
    We drop the \(\|\) subscript on the first occurrence of \(V^\alpha\) since \(V_{\|}^\alpha(x) = V^\alpha(x)\).
    We will get the same for \(b\):
    \begin{equation}
        \christoffel{\mu}{\alpha}{\beta}(x + b)V_{\|}^\alpha(x + b) \approx \christoffel{\mu}{\alpha}{\beta}(x) V^\alpha(x) + b^\nu \partial_\nu \christoffel{\mu}{\alpha}{\beta} V_{\|}^\alpha + \order(b^2).
    \end{equation}
    Consider the first order term:
    \begin{align}
        a^\nu \partial_\nu \christoffel{\mu}{\alpha}{\beta}V_{\|}^{\alpha} &= a^\nu V^\alpha \partial_\nu \christoffel{\mu}{\alpha}{\beta} + \alpha^\nu \christoffel{\mu}{\alpha}{\beta} \partial_\nu V_{\|}^\alpha\\
        &= a^\nu V^\alpha \partial_\nu \christoffel{\mu}{\alpha}{\beta} + \alpha^\nu \christoffel{\mu}{\alpha}{\beta} (-\christoffel{\alpha}{\sigma}{\nu} V^\sigma).
    \end{align}
    Here we have used the fact that the action of the derivative on \(V_{\|}\) is to parallel transport it, giving the affine connection.
    Doing the same with the fourth term for \(b\) we get
    \begin{equation}
        b^\nu \partial_\nu \christoffel{\mu}{\alpha}{\beta} V_{\|}^\alpha = b^\nu V^\alpha\partial_\nu \christoffel{\mu}{\alpha}{\beta} b^\nu \christoffel{\mu}{\alpha}{\beta}(-\christoffel{\alpha}{\sigma}{\nu}).
    \end{equation}
    
    Now put this back in the calculation of \(\delta V^\mu\) and change the dummy indices \(\alpha \leftrightarrow \beta\) in the final term to get matching factors of \(a^\beta b^\alpha\) giving
    \begin{equation}
        \delta V^\mu = [V^\nu\partial_\alpha \christoffel{\mu}{\beta}{\nu} - \christoffel{\mu}{\beta}{\nu} \christoffel{\nu}{\sigma}{\alpha} V^\sigma - V^\nu\partial_\beta \christoffel{\mu}{\alpha}{\nu} + \christoffel{\mu}{\alpha}{\nu} \christoffel{\nu}{\sigma}{\beta} V^\sigma] a^\beta b^\alpha.
    \end{equation}
    We now relabel dummy indices to make all \(V\) terms \(V^\sigma\) and we get
    \begin{equation}
        \delta V^\mu = \tensor{R}{^\mu_{\sigma\alpha\beta}} V^\sigma a^\beta b^\alpha
    \end{equation}
    where
    \begin{equation}
        \tensor{R}{^\mu_{\sigma\alpha\beta}} \coloneqq \partial_\alpha \christoffel{\mu}{\beta}{\sigma} - \partial_\beta \christoffel{\mu}{\alpha}{\sigma} + \christoffel{\mu}{\alpha}{\nu} \christoffel{\nu}{\sigma}{\beta} - \christoffel{\mu}{\beta}{\nu} \christoffel{\nu}{\sigma}{\alpha}
    \end{equation}
    is the \defineindex{Riemann curvature tensor}.
    
    Since the Riemann curvature tensor is written in terms of affine connections, which are not themselves tensors, it is not immediately obvious that the Riemann curvature tensor is a tensor, but it is.
    One way to show this is through the following identity:
    \begin{equation}
        [\covariantDerivative{\gamma}\covariantDerivative{\beta} - \covariantDerivative{\beta}\covariantDerivative{\gamma}] V_\alpha = \tensor{R}{^\mu_{\alpha\beta\gamma}} V_\mu.
    \end{equation}
    This is covariant and since \(\covariantDerivative{\mu}\) and \(V_\mu\) are both tensorial quantities \(\tensor{R}{^\mu_{\alpha\beta\gamma}}\) is also a tensor.
    This identity can be proven by expanding in a locally inertial frame, where \(\christoffel{\mu}{\alpha}{\beta}\) vanish, but derivatives of \(\christoffel{\mu}{\alpha}{\beta}\) don't necessarily.
    
    A mathematician would define the Riemann tensor geometrically as
    \begin{equation}
        \mathop{\mathrm{Rm}}(\vv{v}, \vv{u}, -, -) \coloneqq [\covariantDerivative{\vv{v}}, \covariantDerivative{\vv{u}}] - \covariantDerivative{[\vv{v}, \vv{u}]}
    \end{equation}
    where \([-,-]\) is the commutator.
    The two empty slots are for a vector and a one-form.
    The covariant derivatives turn the vector into another vector and then it combines with the one-form to produce a scalar, making the Riemann curvature tensor a multilinear map
    \begin{equation}
        \mathop{\mathrm{Rm}}\colon T_pM \times T_pM \times T_pM \times T^*_pM \to \reals.
    \end{equation}
    That is, the Riemann curvature tensor is a \((1, 3)\)-tensor.
    It can alternatively be defined as a multilinear map
    \begin{equation}
        \mathop{\mathrm{Rm}}\colon T_pM \times T_pM \times T_pM \to T_pM
    \end{equation}
    by dropping the slot for the one-form.
    
    The interpretation of the Riemann curvature tensor is that if there is no curvature then the vector parallel transported around a loop will not change, for any loop at any position.
    If there is some non-zero curvature then there will be a loop and a vector such that parallel transporting the vector around the loop results in a non-zero change in the components of the vector.
    If the Riemann curvature tensor is zero in one frame then it will be zero in all frames, meaning that all observers agree upon whether space is flat or curved, although they may measure different amounts of non-zero curvature.
    
    \section{Calculating the Riemann Curvature Tensor}
    At first glance \(\tensor{R}{^\mu_{\alpha\beta\gamma}}\) has four indices, each of which can take on four values, for a total of \(4^4 = 256\) components.
    Fortunately there are several symmetries which reduces the number of degrees of freedom.
    We can also define matrices from the affine connections and Riemann tensor which make computations easier.
    
    Starting with the Christoffel symbols, \(\christoffel{\alpha}{\beta}{\gamma}\), we can define matrices \(\Gamma_\beta\) which have components \(\tensor{(\Gamma_\beta)}{^\alpha_\gamma} \coloneqq \christoffel{\alpha}{\beta}{\gamma}\).
    For example, see \cref{eqn:christoffel matrices S1}.
    There are four such matrices, and each one is a \(4 \times 4\) matrix.
    
    From the Riemann tensor, \(\tensor{R}{^\alpha_{\gamma\rho\sigma}}\) we can define 16 matrices, \(B_{\rho\sigma}\), which are \(4\times 4\) matrices with components
    \begin{equation}
        \tensor{(B_{\rho\sigma})}{^\alpha_\gamma} \coloneqq \tensor{R}{^\alpha_{\gamma\rho\sigma}}.
    \end{equation}
    We can then relate these matrices through the matrix equation
    \begin{equation}
        B_{\rho\sigma} = \partial_\rho \Gamma_\sigma - \partial_\sigma \Gamma_\rho + \Gamma_\rho \Gamma_\sigma - \Gamma_\sigma \Gamma_\rho.
    \end{equation}
    From this it is clear that \(B_{\rho\sigma}\) is antisymmetric in \(\rho\) and \(\sigma\).
    This means of the 16 matrices we actually only need to compute 6, since \(B_{\rho\sigma} = -B_{\sigma\rho}\) also implies \(B_{\rho\rho} = 0\) (no summation convention).
    Constructing \(B\) as a block matrix with the matrices \(B_{\rho\sigma}\) as the components we can think of the antisymmetry as fixing the diagonal to zero and the lower triangle must be equal to the negative of the upper triangle.
    
    Further simplifications come due to symmetries of the Riemann tensor.
    We will state some here without proof.
    To prove these we would work in a locally inertial frame with the affine connections vanishing, but not their derivatives.
    Since the Riemann tensor is defined in terms of affine connections and we can express affine connections in terms of derivatives of the metric we can express the Riemann tensor in terms of derivatives of the metric:
    \begin{equation}
        2R_{\alpha\beta\gamma\delta} = g_{\alpha\delta,\beta\gamma} - g_{\beta\delta,\alpha\gamma} + g_{\beta\gamma,\alpha\delta} - g_{\alpha\gamma,\beta\delta}.
    \end{equation}
    This leads to the following symmetry properties:
    \begingroup
    \setindexcolor{\rho = highlight!80!black, \sigma = my red, \mu = my blue, \alpha = my purple}
    \begin{align}
        \tensor{R}{^\alpha_{\mu\indexcolor{\rho}\indexcolor{\sigma}}} &= - \tensor{R}{^\alpha_{\mu\indexcolor{\sigma}\indexcolor{\rho}}},\\
        \tensor{R}{^\alpha_{\indexcolor{\rho}\indexcolor{\sigma}\indexcolor{\mu}}} + \tensor{R}{^\alpha_{\indexcolor{\sigma}\indexcolor{\mu}\indexcolor{\rho}}} + \tensor{R}{^\alpha_{\indexcolor{\mu}\indexcolor{\rho}\indexcolor{\sigma}}} &= 0,\\
        R_{\indexcolor{\alpha}\indexcolor{\mu}\rho\sigma} &= -R_{\indexcolor{\mu}\indexcolor{\alpha}\rho\sigma},\\
        \setindexcolor{\rho = highlight!80!black, \sigma = highlight!80!black, \mu = my red, \alpha = my red}
        R_{\indexcolor{\alpha}\indexcolor{\mu}\indexcolor{\rho}\indexcolor{\sigma}} = R_{\indexcolor{\rho}\indexcolor{\sigma}\indexcolor{\alpha}\indexcolor{\mu}}.
    \end{align}
    \endgroup
    These symmetries reduce the number of independent components from 256 to 20.
    In general in \(n\) dimensions the Riemann curvature tensor has \(n^2(n^2 - 1) / 12\) degrees of freedom.
    
    From the Riemann curvature tensor we can define two new quantities by contracting the Riemann tensor with itself.
    Since a contraction must occur across an upper and lower index there are three choices for which index pair to contract.
    However, since \(R_{\mu\alpha\beta\gamma} = -R_{\alpha\mu\beta\gamma}\) we know that \(\tensor{R}{^\mu_{\mu\beta\gamma}} = 0\), so this isn't interesting.
    Then since \(\tensor{R}{^\mu_{\alpha\beta\gamma}} = -\tensor{R}{^\mu_{\alpha\gamma\beta}}\) the other choices only differ by a minus sign.
    It is then a matter of convention which we choose, and in this course we choose to define
    \begin{equation}
        R_{\alpha\beta} \coloneqq \tensor{R}{^\mu_{\alpha\beta\mu}}
    \end{equation}
    as the \defineindex{Ricci tensor}.
    This quantity is a symmetric rank 2 tensor.
    In physics we differentiate between the Riemann tensor and the Ricci tensor by the number of indices.
    In maths or index free notation the Ricci tensor is denoted \(\mathop{\mathrm{Ric}}\), which would be defined as a bilinear map
    \begin{equation}
        \mathop{\mathrm{Ric}}\colon T_p M \times T_p M \to \reals.
    \end{equation}
    The Ricci tensor is important and so we give here an explicit form for its components:
    \begin{align}
        R_{\mu\nu} &= \partial_\nu \christoffel{\alpha}{\mu}{\alpha} - \partial_\alpha \christoffel{\alpha}{\mu}{\nu} + \christoffel{\alpha}{\mu}{\beta}\christoffel{\beta}{\alpha}{\nu} + \christoffel{\alpha}{\alpha}{\beta}\christoffel{\beta}{\mu}{\nu}\\
        &= \christoffel{\alpha}{\mu}{\alpha,\nu} - \christoffel{\alpha}{\mu}{\nu,\alpha} + \christoffel{\alpha}{\mu}{\beta}\christoffel{\beta}{\alpha}{\nu} + \christoffel{\alpha}{\alpha}{\beta}\christoffel{\beta}{\mu}{\nu}
    \end{align}
    
    Further contracting the indices on the Ricci tensor we define its trace as the \defineindex{Ricci scalar}:
    \begin{equation}
        R \coloneqq \tensor{R}{^\alpha_\alpha} = g^{\alpha\beta}R_{\alpha\beta} = g^{\alpha\beta}\tensor{R}{^\mu_{\alpha\beta\mu}} = \tensor{R}{^{\mu\alpha}_{\alpha\mu}}.
    \end{equation}
    Again, we distinguish between the Riemann tensor, Ricci tensor, and Ricci scalar by the number of indices.
    In a given coordinate system the Ricci scalar can be computed as
    \begin{align}
        R &= g^{\alpha\beta} (\partial_\gamma \christoffel{\gamma}{\alpha}{\beta} - \partial_\beta\christoffel{\gamma}{\alpha}{\gamma} + \christoffel{\delta}{\alpha}{\beta}\christoffel{\gamma}{\gamma}{\delta} - \christoffel{\delta}{\alpha}{\gamma}\christoffel{\gamma}{\beta}{\delta})\\
        &= g^{\alpha\beta} (\christoffel{\gamma}{\alpha}{\beta,\gamma} - \christoffel{\gamma}{\alpha}{\gamma,\beta} + \christoffel{\delta}{\alpha}{\beta} \christoffel{\gamma}{\gamma}{\delta} - \christoffel{\delta}{\alpha}{\gamma}\christoffel{\gamma}{\beta}{\delta})\\
        &= 2 g^{\alpha\beta}(\christoffel{\gamma}{\alpha}{[\beta,\gamma]} + \christoffel{\delta}{\alpha}{[b} \christoffel{\gamma}{\gamma]}{d})
    \end{align}
    where \([ij]\) represents the antisymmetrisation of indices \(i\) and \(j\), defined by
    \begin{equation}
        T^{[i_1\dotso i_n]} \coloneqq \frac{1}{n!} \sum_{\sigma \in S_n} \sgn(\sigma) T^{\sigma(i_1) \dotso \sigma(i_n)}.
    \end{equation}
    Here \(S_n = \{\sigma\colon \{1, \dotsc, n\} \to \{1, \dotsc, n\} \mid \sigma \text{ is a bijection}\}\) is the permutation group\footnote{see the notes from symmetries of quantum mechanics.} and \(\sgn(n)\) is \(+1\) for an even permutation or \(-1\) for an odd permutation.
    For example, 
    \begin{equation}
        T^{[ij]} = \frac{1}{2}(T^{ij} - T^{ji}).
    \end{equation}
    
    \chapter{Gravitational Fields}
    \section{Gravitational Tidal Fields and the Geodesic Deviation Equation}
    The Riemann tensor, \(\tensor{R}{^\mu_{\alpha\beta\gamma}}\), is defined in terms of the affine connections, \(\christoffel{\alpha}{\beta}{\Gamma}\), and derivatives of the affine connections.
    The affine connections are in turn defined in terms of the metric, \(g_{\mu\nu}\).
    So the Riemann tensor is defined in terms of second derivatives of the metric.
    
    We can always move to a locally inertial frame removing most effects of gravity locally since the metric will be close to the Minkowski metric and the first derivatives of the metric will vanish, meaning that the affine connections vanish, but not derivatives of the affine connection.
    This means that the gravitational field only appears through non-zero second derivatives of the metric.
    We call these \define{tidal forces}\index{tidal!force}.
    In the presence of tidal forces neighbouring points experience different accelerations.
    
    For Newtonian gravity the acceleration is
    \begin{equation}
        \ddot{\vv{x}} = - \grad \Phi,
    \end{equation}
    and so the relative acceleration of two points with spatial separation \(\Delta\vv{x}\) depends on the difference in \(\grad\Phi\) at these two points.
    We can expand \(\Phi\) to second order in a Taylor series and we find that to lowest order
    \begin{equation}
        \Delta \ddot{x_i} = \mathcal{T}_{ij} \Delta x_j
    \end{equation}
    where
    \begin{equation}
        \mathcal{T}_{ij} = -\diffp{\Phi}{x^i, x^j}
    \end{equation}
    is the Newtonian \define{tidal field}\index{tidal!field}.
    Notice that \(\mathcal{T}_{ii} = \laplacian\Phi\), which means that Laplace's equation is \(\mathcal{T}_{ii} = 0\), and Poisson's equation is \(-\mathcal{T}_{ii} = 4\pi G\rho\).
    
    In general relativity we know that in the Newtonian limit the metric component \(g_{00}/2\) plays the role of the potential.
    We therefore expect tidal forces to arise when second derivatives of the metric are nonzero.
    That is if \(\partial_\alpha \partial_\beta g_{\mu\nu} \ne 0\).
    This works in a locally inertial frame but we want a generally covariant statement.
    The first attempt might be to replace derivatives with covariant derivatives, \(\covariantDerivative{\alpha}\covariantDerivative{\beta} g_{\mu\nu}\), but since \(\covariantDerivative{\beta}g_{\mu\nu}\) vanishes identically this isn't useful.
    Instead it can be proven that there exists a unique tensor that one can construct from the metric as well as first and second derivatives of the metric such that it is linear in the second derivatives.
    This tensor is the Riemann curvature tensor.
    
    We can show more simply that the Riemann curvature tensor arises in expressions for tidal forces by considering two nearby free particles as they move.
    We consider the deviation between the geodesics which they follow.
    Suppose the first particle follows the geodesic \(x^\mu(\tau)\) and the second is separated from the first by \(y^\mu(\tau)\).
    Then the second particle must be following the geodesic \(x^\mu(\tau) + y^\mu(\tau)\).
    We want to see how the small separation \(y^\mu(\tau)\) grows.
    
    Let \(P\) be the spacetime point at \(x^\mu(\tau)\).
    We work first in a locally inertial frame at \(P\).
    We will find an equation in this frame and then write it in terms of tensors, and then it will hold in all frames.
    We know that the geodesics \(x^\mu(\tau)\) and \(x^\mu(\tau) + y^\mu(\tau)\) must follow the geodesic equation, meaning
    \begin{equation}
        \ddot{x}^\mu + \christoffel{\mu}{\alpha}{\beta}(x)\dot{x}^\alpha\dot{x}^\beta = 0,
    \end{equation}
    and
    \begin{equation}
        \ddot{x}^\mu + \ddot{y}^\mu + \christoffel{\mu}{\alpha}{\beta}(x + y)(\dot{x}^\alpha + \dot{y}^\alpha)(\dot{x}^\beta + \dot{y}^\beta) = 0.
    \end{equation}
    In a locally inertial frame at \(P\) the affine connection in the first equation, \(\christoffel{\mu}{\alpha}{\beta}(x)\) vanishes.
    Hence \(\ddot{x}^\mu = 0\), which is what we would expect since in a locally inertial frame there is no acceleration.
    Hence the second equation is
    \begin{equation}
        \ddot{y}^\mu + \christoffel{\mu}{\alpha}{\beta}(x + y)(\dot{x}^\alpha + \dot{y}^\alpha)(\dot{x}^\beta + \dot{y}^\beta) = 0.
    \end{equation}
    We can then Taylor expand the affine connection to first order and we get
    \begin{equation}
        \ddot{y}^\mu + \christoffel{\mu}{\alpha}{\beta}(x)(\dot{x}^\alpha + \dot{y}^\alpha)(\dot{x}^\beta + \dot{y}^\beta) + \diffp{\christoffel{\mu}{\alpha}{\beta}}{x^\nu}(\dot{x}^\alpha + \dot{y}^\alpha)(\dot{x}^\beta + \dot{y}^\beta) y^\nu = 0.
    \end{equation}
    The affine connection again vanishes in the locally inertial frame giving
    \begin{equation}\label{eqn:geodesic deviation derivation}
        \ddot{y}^\mu + \diffp{\christoffel{\mu}{\alpha}{\beta}}{x^\nu} \dot{x}^\alpha\dot{x}^\beta y^\nu = 0
    \end{equation}
    to first order in \(y\) and \(\dot{y}\).
    
    The quantity \(\ddot{y}^\mu = \diff{y^\mu}/{\tau}\) is not a tensor, and so different observers will disagree on its value.
    They will agree on the covariant derivative, \(\diffcov[2]{y^\mu}/{\tau}\), however.
    We can compute this quantity in the locally inertial frame:
    \begin{align}
        \diffcov[2]{y^\mu}{\tau} &= \diffcov*{\diffcov{y^\mu}{\tau}}{\tau}\\
        &= \diff*{\diffcov{y^\mu}{\tau}}{\tau}\\
        &= \diff*{\left[ \diff{y^\mu}{\tau} + \christoffel{\mu}{\alpha}{\beta} \dot{x}^\alpha y^\beta \right]}{\tau}\\
        &= \diff[2]{y^\mu}{\tau} + \diffp{\christoffel{\mu}{\alpha}{\beta}}{x^\nu} \dot{x}^\nu \dot{x}^\alpha y^\beta.
    \end{align}
    Here we have used the fact that the affine connection vanishes in the locally inertial frame and so the second covariant derivative reduces to a normal derivative.
    Similarly when we apply the product rule to the final term we keep only the term with the derivative of the affine connection as the other terms vanish.
    We then use the chain rule to change
    \begin{equation}
        \diff{\christoffel{\mu}{\alpha}{\beta}}{\tau} = \diffp{\christoffel{\mu}{\alpha}{\beta}}{x^\nu}\diff{x^\nu}{\tau} = \diffp{\christoffel{\mu}{\alpha}{\beta}}{x^\nu}\dot{x}^\nu.
    \end{equation}
    
    Using \cref{eqn:geodesic deviation derivation} we then have
    \begin{equation}
        \diffcov[2]{y^\mu}{\tau} = \left( \diffp{\christoffel{\mu}{\alpha}{\nu}}{x^\beta} - \diffp{\christoffel{\mu}{\alpha}{\beta}}{x^\nu} \right)\dot{x}^\alpha\dot{x}^\beta y^\nu.
    \end{equation}
    Comparing this to the equation for the Riemann curvature tensor we see that, in a locally inertial frame where the affine connection vanishes, we have
    \begin{equation}
        \diffcov{y^\mu}{\tau} = (\tensor{R}{^\mu_{\alpha\beta\nu}}\dot{x}^\alpha\dot{x}^\beta) y^\nu.
    \end{equation}
    This is called the \defineindex{geodesic deviation} equation.
    It describes how two adjacent particles in a gravitational field move relative to each other.
    It is the covariant generalisation of the Newtonian tidal tensor.
    
    Since this is a tensor relation it is valid in all frames.
    The important thing is that it establishes a connection between the curvature, given by the Riemann tensor, and gravity, given by the changing separation.
    This means that in a non-flat space time we have to account for curvature to define gravity and vice versa.
    
    \section{Einstein's Field Equations}
    In Newtonian gravity the gravitational potential, \(\Phi\), is related to the matter density, \(\rho\), by a field equation, \defineindex{Poisson's equation},
    \begin{equation}
        \laplacian\Phi = 4\pi G\rho.
    \end{equation}
    This is not a satisfactory relativistic equation.
    First, it violates causality, if \(\rho\) changes then this instantly alters the field throughout the entire universe.
    
    We have this same problem in electrostatics, where the field equation is again Poisson's equation, but now relating the electric potential, \(\varphi\), to the charge density, \(\rho\), by \(\laplacian \varphi = -\rho/\varepsilon_0\).
    The solution in electrostatics is to replace the Laplacian with the negative of the wave operator, \(-\dalembertian \coloneqq -\partial_\mu\partial^\mu\).
    If we did this for gravity we would have
    \begin{equation}
        \partial_\mu\partial^\mu \Phi = -4\pi G\rho.
    \end{equation}
    This looks closer to being correct.
    If we were to replace the coordinate derivatives with covariant derivatives it would be correct provided that \(\Phi\) and \(\rho\) are invariant scalars.
    Again, however, comparison to electromagnetism shows that this isn't the case.
    In electromagnetism charge is an invariant, but the charge density is not, since a coordinate transformation will change the volume element.
    Instead the charge density transforms as one component of the four-current density, \(J^\mu = (\rho c, \vv{j})\).
    We then have to replace the potential with the four-potential, \(A^\mu = (\varphi/c, \vv{A})\), giving the equation
    \begin{equation}
        \partial_\mu \partial^\mu A^\mu = \mu_0 J^\mu.
    \end{equation}
    We need this in order to satisfy the continuity equation,
    \begin{equation}
        \partial_\mu J^\mu = \diffp{\rho}{t} + \div\vv{j} = 0.
    \end{equation}
    
    We could then look for a covariant field equation for gravity in which second (covariant) derivatives of some field is proportional to some measure of the density and the flow of the matter field.
    Unlike electromagnetism this won't involve four-vectors.
    First we know that the metric tensor generates the effective gravitational forces, so we will look for a tensor potential, \(\Phi^{\mu\nu}\), and second there isn't an analogue of the four-current for mass.
    The electromagnetic four-current tells us how the conserved quantity, charge, moves.
    In general relativity we are interested in the conserved quantities energy and the three components of momentum, for a total of four conserved quantities.
    Alternatively, we can view these as the four components of the four-momentum.
    We could therefore look for a tensor expressing the motion of these four quantities.
    
    Instead of this approach however, we will follow the path Einstein took and start with the vacuum case.
    We will then look for some vanishing tensor in terms of second derivatives of the metric.
    We will then see how this generalises to the non-vacuum case.
    
    \subsection{Einstein's Field Equations in Empty Space}
    In this section we will generalise Laplace's equation, \(\laplacian\Phi = 0\).
    We can write Laplace's equation as
    \begin{equation}
        \mathcal{T}^{\mathrm{N}}_{ii} = 0
    \end{equation}
    where \(\mathcal{T}^{\mathrm{N}}_{ii} = \diffp{\Phi}{x^i, x^j}\) is the Newtonian tidal tensor defined by \(\Delta \ddot{x}_i = \mathcal{T}^{\mathrm{N}}_{ij} \Delta x_j\).
    The tidal tensor gives the relative tidal acceleration of neighbouring particles separated by \(\Delta \vv{x}\) in a gravitational field.
    
    The covariant generalisation of this is
    \begin{equation}
        \diffcov[2]{y^\mu}{\tau} = \tensor{R}{^\mu_{\alpha\beta\nu}} \dot{x}^\alpha \dot{x}^\beta y^\nu = \tensor{\mathcal{T}}{^\mu_\nu}.
    \end{equation}
    Here \(\tensor{\mathcal{T}}{^\mu_\nu} \coloneqq \tensor{R}{^\mu_{\alpha\beta\nu}}\dot{x}^\alpha\dot{x}^\beta\) are the components of the \define{tidal tensor}\index{tidal!tensor}.
    
    A covariant generalisation of Laplace's equation is therefore
    \begin{equation}
        \tensor{\mathcal{T}}{^\mu_\nu} = 0,
    \end{equation}
    which we expect to hold in empty space.
    That is in a vacuum
    \begin{equation}
        \tensor{R}{^\mu_{\alpha\beta\mu}}\dot{x}^\alpha\dot{x}^\beta = 0
    \end{equation}
    which in terms of the Ricci tensor, \(R_{\alpha\beta} = \tensor{R}{^\mu_{\alpha\beta\mu}}\), is
    \begin{equation}
        R_{\alpha\beta}\dot{x}^\alpha \dot{x}^\beta = 0.
    \end{equation}
    This must hold for all geodesics, \(x^\mu(\tau)\), and so we have that 
    \begin{equation}
        R_{\alpha\beta} = 0
    \end{equation}
    in empty space.
    This gives us 10 equations\footnote{\(R_{\alpha\beta}\) has 16 components and is symmetric so the six components above the diagonal determine the 6 components below the diagonal.} known as Einstein's field equations in free space.
    
    \subsection{Energy-Momentum Tensor}
    We now want to include the effects of matter on spacetime in our equations.
    The matter density is not a scalar invariant, since changing coordinate frame, in general, changes the volume element.
    Consider electromagnetism again.
    Then an observer travelling relative to some charge distribution which has charge density \(\rho_0\) in its frame of reference will instead measure
    \begin{equation}
        \rho = \gamma\rho_0.
    \end{equation}
    The factor of \(\gamma\) occurs since the volume element will be Lorentz contracted by a factor of \(1/\gamma\) in the direction of travel of the observer.
    
    Now consider a matter distribution.
    The energy of the distribution is \(E_0\) in its rest frame and \(\gamma E_0\) as measured by the observer.
    By \(E = mc^2\) and \(m = \rho V\) we expect that if the mass density is \(\rho_0\) in the rest frame of the matter then an observer would measure
    \begin{equation}
        \rho = \gamma^2\rho_0,
    \end{equation}
    with one factor of \(\gamma\) from length contraction and one factor from transforming the energy.
    
    The reason for the difference between electromagnetism and matter is that the charge is a scalar invariant, so \(\partial_\mu J^\mu = 0\), but the energy isn't.
    Instead we have four conserved quantities, the components of the four-momentum, \(P^\mu\), which is to say the energy and the three components of the relativistic three-momentum.
    The relevant continuity equation is then
    \begin{equation}
        \partial_\nu T^{\mu\nu} = 0.
    \end{equation}
    For this to be covariant \(T^{\mu\nu}\) must be the components of a tensor.
    This tensor is called the \defineindex{energy-momentum tensor}, or sometimes the \define{stress-energy tensor}\index{stress-energy tensor|see{energy-momentum tensor}}.
    
    We can ascribe a physical meaning to each component of the energy-momentum tensor.
    First, \(T^{00}\) is \(c^2\) times the mass density, which is to say the energy density.
    Second, \(T^{0i}\) is the \(x^i\) component of the energy flux density, \(T^{i0}\) is the density of \(x^i\) momentum, and \(T^{ij}\) is the \(x^i\) component of the current of \(x^j\) momentum.
    From these definitions we can see that \(T^{\mu\nu}\) must be symmetric.
    This follows since both momentum density and energy flux density are the product of a mass density and net velocity, so have the same form meaning \(T^{0\mu} = T^{\mu0}\).
    Also, the spatial stress tensor, \(T^{ij}\), is required to be symmetric otherwise a small volume element would undergo infinite angular acceleration, which is clearly non-physical.
    This occurs since an asymmetric stress acting on a cube of side \(L\) will create a torque proportional to \(L^3\), and the momentum of inertia is proportional to \(L^5\).
    
    Consider, for example, a cold fluid with density \(\rho_0\) in its rest frame.
    By cold we mean that the particles aren't moving.
    In this case the energy-momentum tensor has only one non-zero component, \(T^{00} = c^2\rho_0\), in the rest frame of the fluid.
    In a covariant form this is
    \begin{equation}
        T^{\mu\nu} = \rho_0 U^\mu U^\nu.
    \end{equation}
    Here \(U^\mu = \gamma(c, \vv{u})\) is the four-velocity.
    From now we drop the \(0\) subscript so \(\rho\) is the rest-frame density.
    We can see that in special relativity this component will indeed transform by gaining a factor of \(\gamma^2\) as predicted at the start of this section.
    
    The equations for energy and momentum conservation in this case are given by the divergence of this tensor, \(\partial_\nu T^{\mu\nu} = 0\).
    In the limit of \(\gamma \to 1\) considering \(\mu = 0\) we get
    \begin{equation}
        \diffp{\rho}{t} + \div(\rho\vv{u}) = 0,
    \end{equation}
    which is the standard continuity equation identifying \(\rho\vv{u}\) as the matter flux density.
    Similarly the spatial components give
    \begin{equation}
        \diffp*{(\rho u_i)}{t} + \covariantDerivative{k}(\rho u_i u_k) = 0.
    \end{equation}
    Combining these two equations we get Euler's equation for a pressure free fluid:
    \begin{equation}
        \diff{u_i}{t} = \diffp{u_i}{t} + (\vv{u} \cdot \grad)u_i = 0.
    \end{equation}
    
    Now consider an ideal\footnote{an ideal fluid has no viscous stresses, meaning that momentum transport depends only on the velocity in that direction.} fluid with a homogeneous isotropic pressure, \(p\), we then have in the rest frame \(T^{\mu\nu} = \diag(\rho c^2, p, p, p)\).
    This can be made into a manifestly covariant special relativity equation:
    \begin{equation}
        T^{\mu\nu} = \left( \rho + \frac{p}{c^2} \right)U^\mu U^\nu - p \eta^{\mu\nu}.
    \end{equation}
    For general relativity we need to replace \(\eta_{\mu\nu}\) with \(g_{\mu\nu}\):
    \begin{equation}
        T^{\mu\nu} = \left( \rho + \frac{p}{c^2} \right)U^\mu U^\nu - pg^{\mu\nu}.
    \end{equation}
    We should also replace \(\partial_\mu\) in the conservation law with \(\covariantDerivative{\mu}\) so
    \begin{equation}
        \covariantDerivative{\nu} T^{\mu\nu} = 0,
    \end{equation}
    which can be written explicitly as
    \begin{equation}
        \diffp{T^{\mu\nu}}{x^\nu} + \christoffel{\mu}{\alpha}{\nu} T^{\alpha\nu} + \christoffel{\nu}{\alpha}{\nu}T^{\alpha\mu} = 0.
    \end{equation}
    
    \subsection{Einstein's Field Equations With Matter}
    In the previous section we generalised the source term for gravity from the density, \(\rho\), to the energy-momentum tensor, \(T^{\mu\nu}\).
    This generalises the right hand side of Poisson's equation, \(\laplacian \Phi = 4\pi G\rho\).
    We need to generalise the left hand side \(\laplacian\Phi\).
    We have a rank 2 tensor as the source term so we need a rank 2 tensor involving derivatives of the metric to generalise the left hand side.
    This term must involve the curvature of spacetime and in a vacuum should reduce to Laplace's equation \(R_{\alpha\beta} = 0\).
    The obvious choice is 
    \begin{equation}
        R^{\alpha\beta} = k T^{\alpha\beta}
    \end{equation}
    for some constant \(k\), but this fails since we require \(T^{\mu\nu}\) to follow the conservation law \(\tensor{T}{^{\mu\nu}_{;\nu}} = 0\), and in general \(\tensor{R}{^{\mu\nu}_{;\nu}} \ne 0\).
    It can be shown that
    \begin{equation}
        \covariantDerivative{\nu} R^{\mu\nu} = \frac{1}{2} g^{\mu\nu}\covariantDerivative{\nu} R,
    \end{equation}
    which is known as the \defineindex{contracted Binaci identity}.  % TODO: Prove Binaci identity and from it the contracted binaci identity
    This can be proven by considering it in a locally inertial frame.
    Importantly this doesn't vanish in all spacetimes.
    
    We can use this identity to construct a tensor with zero covariant divergence:
    \begin{equation}
        G^{\mu\nu} \coloneqq R^{\mu\nu} - \frac{1}{2}g^{\mu\nu} R
    \end{equation}
    which is called the \defineindex{Einstein tensor}.
    The simplest possible generalisation of Poisson's equation is then
    \begin{equation}
        G^{\mu\nu} = \kappa T^{\mu\nu}
    \end{equation}
    for some constant \(\kappa\).
    As is common in physics coming up with the correct form of the laws is not that difficult, but working out the value of the constant will take some work, and we'll do it in the next section.
    
    \subsubsection{Working Out the Value of the Constant}
    We determine \(\kappa\) in \(G^{\mu\nu} = \kappa T^{\mu\nu}\) by requiring that the non-relativistic limit reduces to Poisson's equation.
    In this limit we consider a weak time-independent field, so the metric is close to the Minkowski metric,
    \begin{equation}
        g_{\mu\nu} = \eta_{\mu\nu} + h_{\mu\nu}
    \end{equation}
    where \(\abs{h_{\mu\nu}} \ll 1\).
    The energy-momentum tensor then has components \(T_{00} = \rho c^2\) and \(T_{ij} \ll T_{00}\) so we approximate \(T_{ij} \approx 0\).
    To first order in \(h\) the affine connections are
    \begin{align}
        \christoffel{\sigma}{\lambda}{\mu} &= \frac{1}{2}g^{\nu\sigma} [\partial_\lambda g_{\mu\nu} + \partial_\mu g_{\lambda\nu} - \partial_\nu g_{\mu\lambda}]\\
        &\approx \frac{1}{2}\eta^{\nu\sigma} [\partial_\lambda h_{\mu\nu} + \partial_\mu h_{\lambda\nu} - \partial_\nu h_{\mu\lambda}].
    \end{align}
    The Riemann curvature tensor is
    \begin{equation}
        \tensor{R}{^\alpha_{\sigma\rho\beta}} = \partial_\rho \christoffel{\alpha}{\beta}{\sigma} - \partial_\beta \christoffel{\alpha}{\rho}{\sigma} + \christoffel{\alpha}{\rho}{\nu}\christoffel{\nu}{\sigma}{\beta} - \christoffel{\alpha}{\beta}{\nu}\christoffel{\nu}{\sigma}{\rho}.
    \end{equation}
    To first order in \(h\) we can ignore the \(\Gamma\Gamma\) terms and we get
    \begin{equation}
        \tensor{R}{^\alpha_{\sigma\rho\beta}} \approx \frac{1}{2}\eta^{\alpha\nu} [\partial_\sigma\partial_\rho h_{\beta\nu} - \partial_\nu\partial_\rho h_{\beta\sigma} - \partial_\sigma\partial_\beta h_{\rho\nu} + \partial_\beta\partial_\nu h_{\rho\sigma}].
    \end{equation}
    The Ricci tensor is then
    \begin{equation}
        R_{\sigma\beta} = \tensor{R}{^\alpha_{\sigma\beta\alpha}} \approx \frac{1}{2}\eta^{\alpha\nu} [\partial_\sigma \partial_\beta h_{\alpha\nu} - \partial_\nu\partial_\beta h_{\alpha\sigma} - \partial_\sigma\partial_\alpha h_{\beta\nu} + \partial_\alpha\partial_\nu h_{\beta\sigma}].
    \end{equation}
    We only need \(G_{00} = R_{00} - g_00 R/2\) to calculate the constant of proportionality and so we only need to determine \(R_{00}\) and \(R\).
    For a static field all time derivatives vanish and so \(\beta\) and \(\sigma\) derivatives vanish in the calculation of \(R_{00}\) using the form above giving
    \begin{equation}
        R_{00} \approx \frac{1}{2}\eta^{\alpha\nu} \partial_\nu\partial_\alpha h_{00} = -\frac{1}{2}\laplacian h_{00}
    \end{equation}
    where we have used the fact that \(\eta^{\alpha\nu}\partial_\nu\partial_\alpha = \partial_\nu\partial^\nu = \diffp{}/{t} - \laplacian\) and time derivatives vanish.
    Previously we found that in this limit
    \begin{equation}
        g_{00} \approx 1 + \frac{2\Phi}{c^2}.
    \end{equation}
    Hence,
    \begin{equation}
        R_{00} = -\frac{1}{c^2} \laplacian \Phi
    \end{equation}
    to first order in \(h\).
    
    To determine \(R\) we use the fact that \(\abs{T_{ij}} \to 0\) in the non-relativistic limit and so \(\abs{G_{ij}} \to 0\), which means
    \begin{equation}
        R_{ij} - \frac{1}{2}g_{ij}R \approx 0.
    \end{equation}
    This means
    \begin{equation}
        R_{ij} \approx \frac{1}{2}\eta_{ij}R \approx -\frac{1}{2}\delta_{ij}R.
    \end{equation}
    Thus, the Ricci scalar is
    \begin{equation}
        R = \tensor{R}{^\mu_\mu} \approx \eta^{\mu\nu}R_{\nu\mu} = R_{00} - R_{ii} = R_{00} + \frac{3}{2}R.
    \end{equation}
    In the last step we used \(R_{ii} \approx -\delta_{ii}R/2\) and \(\delta_{ii} = 3\) in three dimensions.
    Hence we have \(R \approx -2R_{00}\) and \(G_{00} = R_{00} - g_{00}R/2 \approx 2R_{00}\).
    This means that the time-time component of the Einstein field equations, \(G_{00} = \kappa T_{00}\), reduces to
    \begin{equation}
        -\frac{2}{c^2}\laplacian \Phi = \kappa \rho c^2.
    \end{equation}
    In order for this to match Poisson's equations we want
    \begin{equation}
        \kappa = -\frac{8\pi G}{c^4}
    \end{equation}
    giving Einstein's field equations as
    \begin{equation}
        G^{\mu\nu} = -\frac{8\pi G}{c^4} T^{\mu\nu}.
    \end{equation}
    
    \subsection{Sign Conventions}
    Unfortunately there are a lot of different sign conventions in general relativity.
    Misner, Thorne, and Wheeler (1973) analysed a collection of general relativity texts and identified three sign conventions, relating to the definitions of the Minkowski metric, Riemann tensor, and Einstein tensor:
    \begin{align}
        \eta^{\mu\nu} &= [S1] \diag(-1, +1, +1, +1),\\
        \tensor{R}{^\mu_{\alpha\beta\gamma}} &= (\tensor{\Gamma}{^\mu_{\alpha\gamma,\beta}} - \tensor{\Gamma}{^\mu_{\alpha\beta,\gamma}} + \christoffel{\mu}{\sigma}{\beta}\christoffel{\sigma}{\gamma}{\alpha} - \christoffel{\mu}{\sigma}{\gamma}\christoffel{\sigma}{\beta}{\alpha}),\\
        G_{\mu\nu} &= [S3] \frac{8\pi G}{c^4}T^{\mu\nu}.
    \end{align}
    This third sign convention can be replaced with a convention on the choice of the Ricci tensor through
    \begin{equation}
        R_{\mu\nu} = [S2][S3] \tensor{R}{^\alpha_{\mu\alpha\nu}} = -[S2][S3] \tensor{R}{^\alpha_{\mu\nu\alpha}}.
    \end{equation}
    In this text we have been using \(S1 = -1\), \(S2 = +1\), and \(S3 = -1\), or \(({-}{+}{-})\), leading to
    \begin{align}
        \eta^{\mu\nu} &= \diag(+1, -1, -1, -1),\\
        \tensor{R}{^\mu_{\alpha\beta\gamma}} &= (\tensor{\Gamma}{^\mu_{\alpha\gamma,\beta}} - \tensor{\Gamma}{^\mu_{\alpha\beta,\gamma}} + \christoffel{\mu}{\sigma}{\beta}\christoffel{\sigma}{\gamma}{\alpha} - \christoffel{\mu}{\sigma}{\gamma}\christoffel{\sigma}{\beta}{\alpha}),\\
        G_{\mu\nu} &= -\frac{8\pi G}{c^4}T^{\mu\nu},\\
        R_{\mu\nu} &= \tensor{R}{^\mu_{\alpha\beta\mu}} = -\tensor{R}{^\mu_{\alpha\mu\beta}}.
    \end{align}
    Note that prior to 2020 this course used \(({-}{+}{+})\), so beware of this in past papers.
    
    %   Appdendix
    \appendixpage
    \begin{appendices}
        \include{parts/graphical-notation-appendix}
    \end{appendices}
    
    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}
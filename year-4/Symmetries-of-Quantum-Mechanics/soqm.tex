\documentclass[fleqn]{NotesClass}

\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{tikz-cd}
\usepackage{enumitem}
\usepackage{colortbl}
\usepackage[polutonikogreek, english]{babel}
\usepackage{multicol}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{chemfig}
\usepackage{tensor}
\usepackage{ParticlesPackage}

% Tikz stuff
\usepackage{tikz}
\tikzset{>=latex}
% external
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
%\tikzexternaldisable
% other libraries
\usetikzlibrary{calc}
\usetikzlibrary{3d}
\usetikzlibrary{hobby}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{angles}
\usetikzlibrary{quotes}

% Young Tableau
\usepackage{ytableau}
\ytableausetup{smalltableaux}

% References, should be last things loaded
\usepackage[pdfauthor={Willoughby Seago},pdftitle={Symmetries of Quantum Mechanics Notes},pdfkeywords={Group Theory, Representation Theory, Lie Groups, Lie Algebras, Quantum Mechancis},pdfsubject={Quantum Mechanics, Group Theory, Representation Theory, Lie Groups, Lie Algebras}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

\diffdef{p}{long-var-wrap=dv}

% Title page info
\title{Symmetries of Quantum Mechanics}
\author{Willoughby Seago}
\date{February 17, 2022}
% \subtitle{}
% \subsubtitle{}

% Highlight colour
\definecolor{highlight}{HTML}{710D78}
\definecolor{my blue}{HTML}{2A0D77}
\definecolor{my red}{HTML}{770D38}
\definecolor{my green}{HTML}{14770D}
\definecolor{my yellow}{HTML}{E7BB41}

% Commands
% Maths
\NewDocumentCommand{\presentation}{ s o m m }{
    \IfNoValueTF{#2}{
        \IfBooleanTF{#1}{
            \left\langle #3 \, \middle\vert \, #4 \right\rangle
        }{
            \langle #3 \, \vert \, #4 \rangle
        }
    }{
        #2\langle #3 \, #2\vert \, #4 #2\rangle
    }
}
\DeclarePairedDelimiterX{\groupindex}[2]{[}{]}{#1 : #2}
\newcommand*{\union}{\cup}
\newcommand*{\bigunion}{\bigcup}
\newcommand*{\intersection}{\cap}
\newcommand*{\bigintersection}{\bigcap}
\newcommand{\subgroup}{\subset}
\newcommand{\subgroupeq}{\subseteq}
\newcommand{\supgroup}{\supset}
\newcommand{\supgroupeq}{\supseteq}
\renewcommand{\emptyset}{\emptysetAlt}
\newcommand*{\action}{\mathbin{.}}
\newcommand*{\isomorphic}{\cong}
\DeclareMathOperator{\image}{Im}
\DeclarePairedDelimiterX{\innerprod}[2]{\langle}{\rangle}{#1, #2}
\newcommand*{\trans}{\top}
\newcommand*{\hermit}{\dagger}
\newcommand*{\positiveintegers}{\integers_{>0}}
\newcommand*{\hilbert}{\mathcal{H}}
\newcommand*{\ident}{\mathbb{1}}
\DeclareMathOperator{\tr}{tr}
\newcommand*{\directsum}{\oplus}
\newcommand*{\directproduct}{\otimes}
\DeclareMathOperator{\Hom}{Hom}
\newcommand*{\e}{\mathrm{e}}
\ExplSyntaxOn
% Create LaTeX interface command
\NewDocumentCommand{\cycle}{ O{\,} m }{  % optional arg is separator, mandatory
    %arg is comma separated list
    (
    \willoughby_cycle:nn { #1 } { #2 }
    )
}

\clist_new:N \l_willougbhy_cycle_clist  % Create new clist variable
\cs_new_protected:Npn \willoughby_cycle:nn #1 #2 {  % create LaTeX3 function
    \clist_set:Nn \l_willougbhy_cycle_clist { #2 }  % set clist variable with
    %clist #2 passed by user
    \clist_use:Nn \l_willougbhy_cycle_clist { #1 }  % print list separated by #1
}
\ExplSyntaxOff
\DeclareMathOperator{\isometry}{ISO}
\newcommand{\orbit}{\mathit{g}}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\End}{End}
\newcommand*{\category}[1]{\mathbf{#1}}
\newcommand*{\normalsubgroup}{\vartriangleleft}
\newcommand*{\normalsubgroupeq}{\trianglelefteq}
\newcommand*{\longsim}{\scalebox{2.5}[1.1]{\(\sim\)}}
\DeclareMathOperator{\ISO}{ISO}
\newcommand*{\triangleDthree}[3]{%
    \begin{tikzpicture}[baseline=(current bounding box), scale=0.5, font=\tiny]
        \draw[highlight, very thick, rounded corners=0.1, fill=highlight!20]
        (-30:1) -- (90:1) -- (210:1) -- cycle;
        \node at (210:1.3) {#1};
        \node at (90:1.3) {#2};
        \node at (-30:1.3) {#3};
    \end{tikzpicture}%
}
\newcommand*{\tetrahedron}[4]{%
    \begin{tikzpicture}[highlight, ultra thick, rounded corners=0.01,
        baseline=(current bounding box)]
        \pgfmathsetmacro{\r}{1}
        \coordinate (A) at (0, 0, \r);
        \coordinate (B) at ({\r*sqrt(2)/2}, 0, 0);
        \coordinate (C) at ({-\r*sqrt(2)}, 0, 0);
        \coordinate (D) at (0, \r*2, {\r*sqrt(2)});
        \draw (B) -- (A) -- (C);
        \draw[dashed] (B) -- (C);
        \draw (A) -- (B) -- (D) -- cycle;
        \draw (C) -- (D) -- (A) -- cycle;
        \fill[#1] (A) circle[radius=0.1];
        \fill[#2] (B) circle[radius=0.1];
        \fill[#3] (C) circle[radius=0.1];
        \fill[#4] (D) circle[radius=0.1];
    \end{tikzpicture}
}
\newcommand*{\invertedtetrahedron}[4]{%
    \begin{tikzpicture}[highlight, ultra thick, rounded corners=0.01,
        baseline=(current bounding box)]
        \pgfmathsetmacro{\r}{1}
        \coordinate (A) at (0, 0, \r);
        \coordinate (B) at ({\r*sqrt(2)/2}, 0, 0);
        \coordinate (C) at ({-\r*sqrt(2)}, 0, 0);
        \coordinate (D) at (0, -\r*1.2, {\r*sqrt(2)});
        \draw (B) -- (A) -- (C);
        \draw (B) -- (C);
        \draw (A) -- (B) -- (D) -- cycle;
        \draw (C) -- (D) -- (A) -- cycle;
        \fill[#1] (A) circle[radius=0.1];
        \fill[#2] (B) circle[radius=0.1];
        \fill[#3] (C) circle[radius=0.1];
        \fill[#4] (D) circle[radius=0.1];
    \end{tikzpicture}
}
\makeatletter
\newcommand{\Spin}{\@lineargroup{Spin}}
\newcommand*{\USp}{\@lineargroup{USp}}
\newcommand*{\Sp}{\@lineargroup{Sp}}
\makeatother
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1, #2}
\newcommand*{\rep}[1]{\mathbf{#1}}
\DeclarePairedDelimiterX{\liebracket}[2]{[}{]}{#1, #2}
\newcommand*{\order}{\mathcal{O}}
\newcommand*{\Csym}{\mathrm{C}}
\newcommand*{\Psym}{\mathrm{P}}
\newcommand*{\Tsym}{\mathrm{T}}
\newcommand*{\clebschgordan}[6]{\mathcal{C}_{#1#2#3}^{#4#5#6}}

% Include
\includeonly{parts/group-theory, parts/representation-theory, parts/continuous-groups, parts/math-prelim-appendix,
    parts/groups-appendix, parts/manifolds-appendix, parts/algebras-appendix}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/connected} 
    \tableofcontents
    \listoffigures
    \mainmatter
    \include{parts/group-theory}
    \include{parts/representation-theory}
    \include{parts/continuous-groups}
    
    \part{Applications to Quantum Mechanics}
    \chapter{Quantum Mechanics Synopsis}
    In this chapter we give a short synopsis of quantum mechanics.
    For more details see the notes from principles of quantum mechanics.
    
    \section{Rules of Quantum Mechanics}
    A state is described by a vector in some complex Hilbert space, \(\hilbert\), (\cref{def:hilbert space}).
    We denote states with bra-ket notation, in particular a state is denoted by a ket, such as \(\ket{\Psi}\).
    This corresponds to some physical state which follows a dynamical equation.
    For the non-relativistic case the dynamical equation is the (time independent) \defineindex{Schrödinger equation}:
    \begin{equation}
        \operator{H}\ket{\Psi} = E\ket{\Psi}.
    \end{equation}
    Here \(\operator{H}\) is an operator, called the \defineindex{Hamiltonian}, and \(E\) is the energy eigenvalue associated with the eigenstate \(\ket{\Psi}\).
    
    The Hamiltonian, and indeed all observables, are Hermitian operators on \(\hilbert\), so \(\operator{H}^\hermit = \operator{H}\).
    This is required so that the observed value, \(E\), is real, which all eigenvalues of Hermitian operators are since
    \begin{equation}
        E\braket{\Psi}{\Psi} = \bra{\Psi}\operator{H}\ket{\Psi} = \bra{\Psi}\operator{H}^\hermit\ket{\Psi} = \bra{\Psi}H\ket{\Psi}^* = E^*\braket{\Psi}{\Psi} = E^*\braket{\Psi}{\Psi},
    \end{equation}
    which can only hold if \(E^* = E\), so \(E \in \reals\).
    Further, for a Hermitian operator eigenstates with different eigenvalues are orthogonal since
    \begin{equation}
        E_{\Psi}\braket{\Phi}{\Psi} = \bra{\Phi}\operator{H}\ket{\Psi} = \bra{\Phi}\operator{H}^\hermit\ket{\Psi} = \bra{\Psi}\operator{H}\ket{\Phi} = E_{\Phi}\braket{\Psi}{\Phi}.
    \end{equation}
    Hence,
    \begin{equation}
        (E_{\Psi} - E_{\Phi})\braket{\Psi}{\Phi} = 0,
    \end{equation}
    so either \(E_{\Psi} = E_{\Phi}\) or \(\braket{\Psi}{\Phi} = 0\).
    
    In quantum mechanics we can only determine the probability of certain processes occurring.
    One important case is the probability that a system transitions from state \(\ket{\Psi}\) to state \(\ket{\Phi}\), which is given by
    \begin{equation}
        P(\Psi \to \Phi) = \abs{A_{\Psi\to\Phi}}^2, \qqwhere A_{\Psi\to\Phi} = \braket{\Phi}{\Psi}.
    \end{equation}
    In order for the probabilities to be properly normalised we must have \(\braket{\Psi}{\Psi} = \braket{\Phi}{\Phi} = 1\).
    
    \section{From hamiltonian to the Schrödinger Equation}
    We can often find the Hamiltonian, up to ambiguity in operator ordering, from the classical case by replacing observables with the associated operators.
    The distinction being that operators don't commute.
    The most famous example of this being the \defineindex{Heisenberg commutation relation} for the position operator, \(\vecoperator{x}\), and momentum operator, \(\vecoperator{p}\):
    \begin{equation}
        \commutator{\operator{x}_i}{\operator{p}_j} = i\hbar \delta_{ij}, \qquad \commutator{\operator{x}_i}{\operator{x}_j} = 0, \qqand \commutator{\operator{p}_i}{\operator{p}_j} = 0.
    \end{equation}
    From these relations we can derive the Heisenberg uncertainty relation, \(\Delta x \Delta p \ge \hbar/2\).
    
    For a non-relativistic particle of mass \(m\) we can write the Hamiltonian as a kinetic part, \(T\), and a potential part, \(V\), which we assume to be a function of only the position (and not the momentum):
    \begin{equation}
        \operator{H} = \operator{T} + \operator{V} = \frac{\vecoperator{p}^2}{2m} + V(\vecoperator{x}).
    \end{equation}
    
    We wish to use this to perform calculations.
    To do so we need to choose a representation, which corresponds to choosing a maximal set of commuting operators.
    The most common choice is to work in the position representation, where we have as a basis states \(\ket{\vv{x}}\), for which
    \begin{equation}
        \operator{x}_i\ket{\vv{x}} = x_i\ket{\vv{x}}.
    \end{equation}
    Completeness of the Hilbert space ensures that we can write the identity as an integral (or sum in the discrete case) over projection operators:
    \begin{equation}
        \ident_x = \int \dl{^3x} \, \ket{\vv{x}}\bra{\vv{x}}.
    \end{equation}
    Choosing the basis to be orthonormal we have
    \begin{equation}
        \braket{\vv{x}}{\vv{x}'} = \delta(\vv{x} - \vv{x}'),
    \end{equation}
    where \(\delta\) is the Dirac delta distribution.
    
    Using the completeness relation, \(\ident_x = \int \dl{^3x}\,\ket{\vv{x}}\bra{\vv{x}}\), we have
    \begin{equation}
        \ket{\Psi} = \int \dl{^3x} \, \ket{\vv{x}}\braket{\vv{x}}{\Psi} = \int \dl{^3x} \, \Psi(\vv{x})\ket{\vv{x}}
    \end{equation}
    where we define \(\Psi(\vv{x}) \coloneqq \braket{\vv{x}}{\Psi}\).
    
    In the position representation the momentum operator is given by
    \begin{equation}
        \operator{p}_j|_{x} \to -i\hbar\partial_j, \qqwhere \partial_j \coloneqq \diffp{}{x_j}.
    \end{equation}
    By this we mean
    \begin{equation}
        \bra{x}\operator{p}_j\ket{\Psi} = -i\hbar\diffp{\psi(x)}{x_j}.
    \end{equation}
    
    Starting with the Schrödinger equation we act on the left with a bra from the position eigenbasis, \(\bra{\vv{x}}\), giving \(\bra{\vv{x}}\operator{H}\ket{\Psi} = E\braket{\vv{x}}{\Psi} = E\Psi(\vv{x})\).
    Applying the completeness relation to the left hand side of the Schrödinger equation we get
    \begin{equation}
        \int \dl{^3x} \, \bra{\vv{x}} \operator{H} \ket{\vv{x}'}\braket{x'}{\Psi}.
    \end{equation}
    Now inserting the Hamiltonian we get two terms:
    \begin{equation}
        \int \dl{^3x} \sum_{j = 1}^{3} \frac{1}{2m} \bra{\vv{x}}\operator{p}_j\ket{\vv{x}'}\braket{\vv{x}'}{\Psi} + \int \dl{^3x} \bra{\vv{x}} V(\vecoperator{x}) \ket{\vv{x}'}\braket{\vv{x}'}{\Psi}.
    \end{equation}
    We now use the fact that \(\bra{\vv{x}}V(\vecoperator{x})\ket{\vv{x}'} = V(\vv{x}')\braket{\vv{x}}{\vv{x}'} = V(\vv{x}')\delta(\vv{x} - \vv{x}')\) and \(\operator{p}_j \to -i\hbar\partial_j\),
    \begin{equation}
        \int \dl{^3x} \sum_{j = 1}^{3} \frac{-\hbar^2}{2m} \delta(\vv{x} - \vv{x}') \diffp[2]{\psi}{x_j} + \int \dl{^3x} V(\vv{x})\delta(\vv{x} - \vv{x}')\braket{\vv{x}'}{\Psi}.
    \end{equation}
    Now using the sifting property of the Dirac delta distribution we have
    \begin{equation}
        \left[ -\frac{\hbar^2}{2m} \laplacian + V(\vv{x}) \right] \psi(\vv{x}) = E\Psi(\vv{x}).
    \end{equation}

    A common application is calculating transition matrices, \(A_{\Psi \to \Phi}\).
    These arise when a system is described by the Hamiltonian \(\operator{H} = \operator{H}_0 + \operator{H}_{\mathrm{trans}}\).
    If \(\ket{\Psi}\) and \(\ket{\Phi}\) are eigenstates of \(\operator{H}_0\) then we define
    \begin{equation}
        A_{\Psi\to\Phi} \coloneqq \bra{\Phi} \operator{H}_{\mathrm{trans}} \ket{\Psi} + \text{higher order terms}.
    \end{equation}
    This is called \defineindex{Fermi's golden rule}.
    It gives reasonable results for sufficiently small perturbations.
    
    \section{Symmetries and Conservation Laws}
    \begin{thm}{Noether's Theorem}{}
        Each symmetry has an associated conservation law and each conservation law can be seen as the consequence of some symmetry.
    \end{thm}
    
    Suppose we have a system governed by the Hamiltonian \(\operator{H}\).
    If the system is invariant under a unitary symmetry, \(U \in \unitary(\hilbert)\), that is \(\ket{\Psi}\) and \(U\ket{\Psi}\) describe the same state, then
    \begin{equation}
        \bra{\Phi} \operator{H} \ket{\Psi} = \bra{\Phi} U^\hermit \operator{H} U \ket{\Psi}.
    \end{equation}
    This can only be the case if \(U^\hermit \operator{H} U = \operator{H}\), and so we see that \(\commutator{U}{\operator{H}} = 0\).
    That is the unitary symmetry transformation commutes with the Hamiltonian.
    We can write the unitary transformation in terms of a generator, \(T\), as \(U = \exp(i\alpha T)\).
    We then see that the generator must commute with the Hamiltonian, \(\commutator{T}{\operator{H}} = 0\).
    
    Heisenberg's equation of motion for a time independent operator, \(T\), (\(\partial_t T = 0\)) is
    \begin{equation}
        \diff*{\expected{T}}{t} = i\expected{\commutator{\operator{H}}{T}},
    \end{equation}
    where \(\expected{A} \coloneqq \bra{\Psi} A \ket{A}\) denotes the expectation value of an operator.
    This shows us that \(\expected{T}\) is a conserved quantity if it commutes with the hamiltonian.
    
    We therefore conclude that the symmetry generated by \(T\) is linked with a conservation law for \(\expected{T}\).
    
    There are many categories of symmetries worth considering.
    We list some important examples now.
    
    \begin{exm}{Poincar\'e Group}{}
        \begin{rmk}
            In this example we work in units where \(c = 1\).
        \end{rmk}
        The \defineindex{Lorentz group}, \(\orthogonal(1, 3)\), is the group preserving the metric\footnote{We use the metric sign convention \(({+}{-}{-}{-})\).} in special relativity, that is it preserves \(x^\mu x_\mu = t^2 - x^2 - y^2 - z^2\), by this we mean if \(\Lambda \in \orthogonal(1, 3)\) then the transformation \(x^\mu \to x'^\mu = \Lambda_{\mu\nu}x^\nu\) doesn't change \(x^\mu x_\mu\).
        The \defineindex{Poincar\'e group} is the Lorentz group and spacetime translations, it is given by the semi-direct product \(\orthogonal(1, 3) \ltimes \reals^{1,3}\).
        
        In the rest frame of some observer the Poincar\'e group reduces to the rotation group, \(\specialOrthogonal(3)\) (or \(\specialUnitary(2)\)), spatial translations, and temporal translations.
        These all have associated conservation laws.
        \begin{itemize}
            \item Symmetry under rotations, \(\specialOrthogonal(3)\), corresponds to conservation of angular momentum.
            \item Symmetry under spatial translations, \(\reals^3\), corresponds to conservation of momentum.
            \item Symmetry under temporal translations, \(\reals\), corresponds to conservation of energy.
        \end{itemize}
        Note that the last two can be combined in to symmetry under spacetime translations, \(\reals^{1,3}\), corresponding to conservation of four-momentum, \(p^\mu = (E, \vv{p})\).
        
        When we consider different inertial frames then we get additional symmetries given by boosting between frames.
        All boosts can be given as a combination of boosts along the three coordinate axes.
        Correspondingly \(\dim(\orthogonal(3)) = 3\) generalises to \(\dim(\orthogonal(1, 3)) = 6\).
        The Lie algebra associated with \(\orthogonal(1, 3)\) is rank 2, since the generators of rotations commute with the generators of boosts.
    \end{exm}
    
    \begin{exm}{CPT Symmetry}{}
        There are three fundamental symmetries in particle physics that one may wish to consider.
        These are
        \begin{itemize}
            \item \define{Parity symmetry}\index{parity symmetry}, \(\Psym(t, x, y, z) = (t, -x, -y, -z)\), that is \(\Psym \vv{r} = -\vv{r}\).
            The parity is conserved by any interactions not involving the weak force.
            \item \define{Time symmetry}\index{time symmetry}, \(\Tsym(t, x, y, z) = (-t, x, y, z)\), that is \(\Tsym t = -t\).
            This is violated slightly in weak interactions, which may well be responsible for the matter-antimatter asymmetry of the universe.
            \item \define{Charge symmetry}\index{charge symmetry}, \(\Csym p = \bar{p}\), where \(p\) is any particle and \(\bar{p}\) is its antiparticle.
            This is again conserved up to weak interactions.
        \end{itemize}
        We can combine these into one single symmetry, \({\Csym}{\Psym}{\Tsym}\), where we exchange all particles for their antiparticles, reflect everything through the origin and reverse time. 
        As far as we know \({\Csym}{\Psym}{\Tsym}\) is always conserved.
    \end{exm}
    
    \begin{exm}{Gauge Symmetries}{}
        Local symmetries of Lie groups lead to charge conservation.
        Here we are being generic with the word charge, allowing for electric charge, but also other properties like colour and isospin.
        At high energies the standard model exhibits a symmetry described by \(\specialUnitary(3)_{\mathrm{c}} \directproduct \specialUnitary(2)_{\mathrm{L}} \directproduct \unitary(1)_{\mathrm{Y}}\).
        The \(\mathrm{c}\) stands for colour charge, which is conserved under \(\specialUnitary(3)\) symmetry (3 because there are three colours).
        The \(\mathrm{L}\) stands for left-handed, and the \(\mathrm{Y}\) for hypercharge.
        At lower energies the Higgs mechanism reduces the symmetry to \(\specialUnitary(3)_{\mathrm{c}} \directproduct \unitary(1)_{\mathrm{em}}\).
        Here \(\unitary(1)_{\mathrm{em}}\) describes conservation of electric charge.
    \end{exm}

    \chapter{Physics of Angular Momentum}
    So far we have studied \(\specialUnitaryLie(2)\).
    We have seen how to construct representations and the eigenvalues of the \(J_3\) generator and \(J^2\) Casimir element.
    We've seen that the Casimir number, \(C_2(j) = j(j + 1)\), or the value of \(j\), can be used to label the eigenstates, of the mutual basis, in order to lift degeneracy we need a second label which we take to be \(m\), the projection of the spin on the \(z\)-axis.
    The value of \(j\) is representation dependent, but frame independent, since it is associated with a Casimir element which commutes with all generators.
    The value of \(m\) is frame dependent since \(J_3\) is not a Casimir operator.
    
    \section{Basics of Angular Momentum in Quantum Mechanics}
    
    Recall that the \defineindex{orbital angular momentum operator} is defined as
    \begin{equation}
        \vecoperator{L} \coloneqq \vecoperator{x} \times \vecoperator{p}, \qqor \operator{L}_a = (\vecoperator{x} \times \vecoperator{p})_a = \varepsilon_{abc}\operator{x}_a\operator{p}_c.
    \end{equation}
    As well as the orbital angular momentum we have the \define{intrinsic angular momentum}\index{intrinsic angular momentum|see{spin}}, or \defineindex{spin}, with the associated operator \(\vecoperator{S}\).
    Combined, these give the \defineindex{total angular momentum operator}, \(\vecoperator{J} = \vecoperator{L} + \vecoperator{S}\).
    If there is no spin or no orbital angular momentum it is common to simply talk about the total angular momentum, \(\vecoperator{J}\), in place of the non-zero angular momentum operator.
    
    Consider the commutation relation for two components of the orbital angular momentum operator.
    For this derivation we will need the canonical commutation relations,
    \begin{equation}
        \commutator{\operator{x}_i}{\operator{p}_j} = i\hbar\delta_{ij}, \qquad \commutator{\operator{x}_i}{\operator{x}_j} = 0, \qqand \commutator{\operator{p}_i}{\operator{p}_j} = 0,
    \end{equation}
    as well as the commutation identities
    \begin{equation}
        \commutator{AB}{C} = A\commutator{B}{C} + \commutator{A}{C}B, \qqand \commutator{A}{BC} = B\commutator{A}{C} + \commutator{A}{B}C,
    \end{equation}
    which can be proven by expanding the commutators.
    Finally we need the identity
    \begin{equation}
        \varepsilon_{ijk}\varepsilon_{klm} = \delta_{il}\delta_{jm} - \delta_{im}\delta_{jl}.
    \end{equation}
    Combing these we have
    \begin{align}
        \commutator{\operator{L}_a}{\operator{L}_b} &= \commutator{\varepsilon_{aij}\operator{x}_i\operator{p}_k}{\varepsilon_{bkl}\operator{x}_k\operator{p}_l}\\
        &= \varepsilon_{aij}\varepsilon_{bkl}\commutator{\operator{x}_i\operator{p}_j}{\operator{x}_k\operator{p}_l}\\
        &= \varepsilon_{aij}\varepsilon_{bkl}(\operator{x}_i\commutator{\operator{p}_j}{\operator{x}_k\operator{p}_l} + \commutator{\operator{x}_i}{\operator{x}_k\operator{p}_l}\operator{p}_j)\\
        &= \varepsilon_{aij}\varepsilon_{bkl}(\operator{x}_i\operator{x_j}\commutator{\operator{p}_j}{\operator{p}_l} + \operator{x}_i\commutator{\operator{p}_j}{\operator{x}_k}\operator{p}_l\notag\\
        &\qquad+ \operator{x}_k\commutator{\operator{x}_i}{\operator{p}_l}\operator{p}_k + \commutator{\operator{x}_i}{\operator{x}_k}\operator{p}_l\operator{p}_j)\\
        &= \varepsilon_{aij}\varepsilon_{bkl}(\operator{x}_i\commutator{\operator{p}_j}{\operator{x}_k}\operator{p}_l + \operator{x}_k\commutator{\operator{x}_i}{\operator{p}_l}\operator{p}_j)\\
        &= \varepsilon_{aij}\varepsilon_{bkl}(-i\hbar\delta_{jk}\operator{x}_i\operator{p}_l + i\hbar\delta_{il}\operator{x}_k\operator{p}_j)\\
        &= -i\hbar\varepsilon_{aij}\varepsilon_{jlb}\operator{x}_i\operator{p}_l + i\hbar\varepsilon_{jai}\varepsilon_{ibk}\operator{x}_k\operator{p}_j\\
        &= -i\hbar(\delta_{al}\delta_{ib} - \delta_{ab}\delta_{il})\operator{x}_i\operator{p}_l + i\hbar(\delta_{jb}\delta_{ak} - \delta_{jk}\delta_{ab})\operator{x}_k\operator{p}_j\\
        &= -i\hbar(\operator{x}_b\operator{p}_a - \delta_{ab}\operator{x}_i\operator{p}_i) + i\hbar(\operator{x}_a\operator{p}_b - \delta_{ab}\operator{x}_j\operator{p}_j)\\
        &= i\hbar(\operator{x}_a\operator{p}_b - \operator{x}_b\operator{p}_a).
    \end{align}
    Now consider \(\varepsilon_{abc}\operator{L}_c\):
    \begin{align}
        \varepsilon_{abc}\operator{L}_c &= \varepsilon_{abc}\varepsilon_{cij}\operator{x}_i\operator{p}_j\\
        &= (\delta_{ai}\delta_{bj} - \delta_{aj}\delta_{bi})\operator{x}_i\operator{p}_j\\
        &= \operator{x}_a\operator{p}_b - \operator{x}_b\operator{p}_a.
    \end{align}
    So we see that
    \begin{equation}
        \commutator{\operator{L}_a}{\operator{L}_b} = i\hbar\varepsilon_{abc}\operator{L}_c,
    \end{equation}
    which means that the angular momentum operators satisfy the \(\specialUnitaryLie(2)\) commutation relations, up to an unimportant factor of \(\hbar\).
    
    We can now ascribe a physical meaning to the value of \(j\).
    Consider the Wigner \(D\)-matrix \(D^j(\vartheta, 0, 0)_{mm'} = \e^{-im\vartheta}\delta_{mm'}\).
    We see that
    \begin{equation}
        D^j(2\pi, 0, 0)_{mm'} = \e^{-2\pi i m}\delta_{mm'} = (\e^{-i\pi})^{2m} = (-1)^{2m} = (-1)^{2j}
    \end{equation}
    where we've used the fact that \((-1)^{2m} = 1\) if \(m\) is an integer and \((-1)^{2m} = -1\) if \(m\) is a half-integer, and \(m\) is an integer or half-integer precisely when \(j\) is.
    From this we see that if a spin 1/2 particle is rotated \ang{360} its state picks up a minus sign, and another \ang{360} rotation is needed in order to return to the original state.
    This means that to obtain an invariant scalar we need an even number of fermions so that these minus signs cancel.
    
    For a system which is spherically symmetric the \emph{total} angular momentum must be conserved.
    For a system of a single particle this means \(\vv{J} = \vv{L} + \vv{S}\) is conserved.
    For a system of two particles with angular momenta \(\vv{J}^{(i)}\) this means \(\vv{J} = \vv{J}^{(1)} + \vv{J}^{(2)}\) is conserved.
    For a system of multiple particles their individual angular are compatible, in the sense that \(\commutator{\operator{J}_i^{(a)}}{\operator{J}_j^{(b)}} = 0\) for all \(a \ne b\).
    A simple example of a system with multiple sources of angular momentum is a particle with orbital angular momentum and spin 1/2, so the spin is represented by \(\ket{s = 1/2}\).
    Say this particle is in an \(l = 1\) state, so the position dependent part of the state is \(\ket{\Psi_{l=1}}\), then we can write its state as \(\ket{\Psi_{l=1}} \directproduct \ket{s = 1/2} = \ket{j = 1/2} \directsum \ket{j = 3/2}\), where we have used the Clebsch--Gordan series for \(\rho_{1} \directproduct \rho_{1/2} = \rho_{1/2} \directsum \rho_{3/2}\).
    Notice that, despite one of the sources having angular momentum 1, there is no way the total angular momentum can be 1, it can only be 1/2 or 3/2.
    
    \section{Wigner--Eckart Theorem}
    \begin{dfn}{Tensor Operator}{}
        Let \(G\) be a Lie group, \(\rho\) a representation of \(G\), and \(\rho_c\) an irreducible representation of \(G\).
        We say that \(T_{m_c}^c\) is a \define{tensor operator}\index{tensor!operator} in the irreducible representation, \(c\), if it transforms as
        \begin{equation}
            \rho(g)T_{m_c}^{c}\rho(g)^\hermit = \rho_c(g)_{m_cm_c'}T_{m_c'}^{c},
        \end{equation}
        with summation over \(m_c'\) implied.
    \end{dfn}
    
    An example of a tensor operator is the momentum operator, \(\operator{p}_i\), which transforms as \(\operator{p}_i \to R_{ij}\operator{p}_j\), where \(R_{ij}\) is in the fundamental representation of \(\specialOrthogonal(3)\) (that is the \(j = 1\) representation).
    We call objects that transform under the \(j = 1\) representation of \(\specialOrthogonal(3)\) vectors, and hence this is sometimes referred to as the \defineindex{vector representation}.
    Similarly the trivial representation is the scalar representation, since it leaves everything unchanged, which is how scalars transform.
    
    The direct product of irreducible representations will, in general, decompose into a direct sum of irreducible representations.
    This means that, given two irreducible representations, \(a\) and \(b\), we can decompose the tensor product as a sum over irreducible representations, \(c\).
    We will use the notation of \(\specialUnitaryLie(2)\) here, but this idea generalises to many other Lie algebras in a natural way.
    Suppose we have two states, \(\ket{a, m_a}\), and \(\ket{b, m_b}\).
    Consider their direct product, \(\ket{a, m_a} \directproduct \ket{b, m_b} = \ket{a, m_a \directproduct b, m_b}\).
    We can insert completeness giving
    \begin{align}
        \ket{a, m_a} \directproduct \ket{b, m_b} = \sum_{c, m_c, i} \ket{c, m_c}\bra{c, m_c}(\ket{a, m_a} \directproduct \ket{b, m_b}),
    \end{align}
    where \(i = 1, \dotsc, m_{ab}^c\) is the multiplicity of the representation \(c\) in the decomposition of \(a \directproduct b\).
    If \(m_{ab}^c = 0, 1\) then we call the group \defineindex{simply reducible}.
    In this case we may write
    \begin{equation}
        \ket{a, m_a} \directproduct \ket{b, m_b} = \sum_{c, m_c} \ket{c, m_c} \bra{c, m_c}(\ket{a, m_a} \directproduct \ket{b, m_b}),
    \end{equation}
    where representations with multiplicity 0 are implicitly left out of the sum.
    We will only consider simply reducible groups.
    In more terse notation we can write this as
    \begin{equation}
        \ket{a, m_a \directproduct b, m_b} = \sum_{c, m_c} \ket{c, m_c} \braket{c, m_c}{a, m_a \directproduct b, m_b}.
    \end{equation}
    We can identify the final bra-ket as simply a constant for fixed values of \(a\), \(b\), \(c\), \(m_a\), \(m_b\), and \(m_c\).
    This is useful since we can compute it once and then not worry about it again, giving rise to the following definition.
    
    \begin{dfn}{Clebsch--Gordan Coefficient}{}
        Let \(a\) and \(b\) be irreducible representations of some Lie group and let \(c\) index the irreducible representations in the decomposition of \(a \directproduct b\).
        Then we define the \define{Clebsch--Gordan coefficients}\index{Clebsch--Gordan!coefficient}, \(\clebschgordan{m_c}{m_a}{m_b}{c}{a}{b}\), are defined as the coefficients appearing in the decomposition of \(\ket{a, m_a} \directproduct \ket{b, m_b}\):
        \begin{equation}
            \clebschgordan{m_c}{m_a}{m_b}{c}{a}{b} \coloneqq \bra{c, m_c} (\ket{a, m_a} \directproduct \ket{b, m_b}) = \braket{c, m_c}{a, m_a \directproduct b, m_b}.
        \end{equation}
    \end{dfn}
    \begin{wrn}
        The Clebsch--Gordan coefficients are defined only up to a phase.
        Therefore there is a phase convention when we give their values.
        The common phase convention is the \defineindex{Condon--Shortley phase convention}, which is such that the Clebsch--Gordan coefficients are real for \(\specialUnitaryLie(2)\), this is what we will use.
    \end{wrn}
    
    \begin{thm}{Wigner--Eckart Theorem}{}
        For simply reducible groups the matrix elements of a tensor operator are proportional to a Clebsch--Gordan coefficient.
        That is,
        \begin{equation}
            \bra{c, m_c} T_{m_a}^a\ket{b, m_b} = \clebschgordan{m_c}{m_a}{m_b}{c}{a}{b} \bra{c} |T^a| \ket{b},
        \end{equation}
        where the constant of proportionality, \(\bra{c} |T^a| \ket{b}\), is called the \defineindex{reduced matrix element}, and is independent of \(m_a\), \(m_b\), and \(m_c\).
        
        \begin{proof}
            The state \(T^a_{m_a}\ket{b, m_b}\) transforms like a direct product of two states.
            We can see this by considering its transformation under some unitary representation, \(\rho\):
            \begin{align}
                \rho(g) T^a_{m_a} \ket{b, m_b} &= \rho(g) T^{a}_{m_a} \rho(g)^\hermit \rho(g) \ket{b, m_b}\\
                &= \rho_a(g)_{m_am_a'}T^a_{m_a'} \rho_b(g)_{m_b'}\ket{b, m_b'}.
            \end{align}
            Here we have used \(\rho(g)^\hermit\rho(g) = \ident\) for a unitary representation, as well as the definition of the tensor operator's transformation law, and that the state \(\ket{j, m}\) transforms under the representation \(\rho_j\).
            
            By the definition of Clebsch--Gordan coefficients we know that
            \begin{equation}
                \ket{j_1, m_1 \directproduct j_2, m_2} = \clebschgordan{M'}{m_1}{m_2}{J'}{j_1}{j_2}\ket{J', M'},
            \end{equation}
            with summation over \(J'\) and \(M'\) implied.
            We therefore have that
            \begin{equation}
                T_{m_a}^{a}\ket{b, m_b} \propto \clebschgordan{m_c'}{m_a}{m_b}{c'}{a}{b}\ket{c, m_c'}.
            \end{equation}
            Since this is the only way to have \(T_{m_a}^a\ket{b, m_b}\) transform like a product state.
            
            Now apply \(\bra{c, m_c}\) to this relation and we get
            \begin{align}
                \bra{c, m_c}T_{m_a}^{a}\ket{b, m_b} &\propto \clebschgordan{m_c'}{m_a}{m_b}{c'}{a}{b} \braket{c, m_c}{c, m_c'}\\
                &= \clebschgordan{m_c'}{m_a}{m_b}{c'}{a}{b} \delta_{cc'}\delta_{m_cm_c'}\\
                &= \clebschgordan{m_c}{m_a}{m_b}{c}{a}{b}.
            \end{align}
            The constant of proportionality in this equation is what we define as the reduced matrix element, \(\bra{c}|T^a|\ket{b}\).
        \end{proof}
    \end{thm}
    
    The Wigner--Eckart theorem is useful since it allows us to compute a single matrix element, \(\bra{c, m_c}T_{m_a}^a\ket{b, m_b}\), which we can use to compute \(\bra{c}|T^a|\ket{b}\), and then all other matrix elements are simply proportional to this, and the constants of proportionality are known.
    Things get even simpler if we only consider the ratio of matrix elements, since in this case we don't even need to compute the reduced matrix element.
    This is useful to calculate, for example, relative decay times of particles when the particles are part of a representation of some internal symmetry group.
    This occurs for the \(\specialUnitary(2)\) isospin symmetry where we exchange up-type quarks and down-type quarks in pions.
    
    Often the Clebsch--Gordan coefficients vanish.
    This provides \define{selection rules}\index{selection rule}, which restrict the possible states.
    In particular we must have \(\abs{j_1 + j_2} \le J \le \abs{j_1 - j_2}\) and \(M = m_1 + m_2\) for the Clebsch--Gordan coefficient to be non-zero.
    
    To summarise this section, the matrix element \(\bra{c, m_c}T_{m_a}^{a}\ket{b, m_b}\) is non-zero only if the irreducible representation appears in the Clebsch--Gordan series decomposition of \(a \directproduct b\) with non-zero multiplicity.
    The direction of the matrix element is determined completely by the Clebsch--Gordan coefficient.
    This is useful since Clebsch--Gordan coefficients have already been computed for most representations of interest and can be readily looked up.
    
    \subsection{Using Clebsch--Gordan Coefficients}
    Suppose we have an electron and a photon, that is a spin \(1/2\) particle and a spin \(1\) particle.
    The electron spin is then described by the representation \(\rho_{1/2}\), and the photon's spin by \(\rho_{1}\).
    The combined electron-photon system's is then described by the tensor product representation \(\rho_{1} \directproduct \rho_{1/2}\).
    By the addition of angular momentum theorem (\cref{thm:clebsch-gordan seris su(2)}) we know that this representation can be decomposed as a sum of \(1 - 1/2 = 1/2\) and \(1 + 1/2 = 3/2\) representations: \(\rho_{1/2} \directproduct \rho_{1} = \rho_{3/2} \directsum \rho_{1/2}\).
    Consider the state \(\ket{J, M}\) in this representation.
    We can decompose this as a sum of tensor products of states in the individual particle representations, with the Clebsch--Gordan coefficients as the weights in the sum.
    States in the \(j_1 = 11\) representation are \(\ket{j_1, m_1} = \ket{1, m_1}\) and states in the \(j_2 = 1/2\) representation are \(\ket{j_2, m_2} = \ket{1/2, m_2}\).
    As always we have that \(m_1 = -j_1, -j_1 + 1, \dotsc, j_1 - 1, j_1\), which means \(m_1 = -1, 0, 1\) and similarly we have \(m_2 = -1/2, 1/2\).
    The Clebsch--Gordan coefficients, \(\clebschgordan{M}{m_1}{m_2}{J}{j_1}{j_2}\), are non-zero only when \(m_1 + m_2 = M\).
    
    For example, consider the state \(\ket{J, M} = \ket{3/2, 1/2}\).
    One way to have \(m_1 + m_2 = 1/2\) we is \(m_1 = 1\) and \(m_2 = -1/2\).
    This corresponds to the states \(\ket{1, 1}\) and \(\ket{1/2, -1/2}\).
    The other way to have \(m_1 + m_2 = 1/2\) is to have \(m_1 = 0\) and \(m_2 = 1/2\), which corresponds to \(\ket{1, 0}\) and \(\ket{1/2, 1/2}\).
    We then find
    \begin{equation}
        \ket{\textcolor{highlight}{3/2}, \textcolor{highlight}{1/2}} = \textcolor{my yellow}{\clebschgordan{\textcolor{highlight}{\tfrac{1}{2}}}{\textcolor{my red}{0}}{\textcolor{my blue}{\tfrac{1}{2}}}{\textcolor{highlight}{\tfrac{3}{2}}}{\textcolor{my red}{1}}{\textcolor{my blue}{\tfrac{1}{2}}}} \ket{\textcolor{my red}{1}, \textcolor{my red}{0}} \directproduct \ket{\textcolor{my blue}{1/2}, \textcolor{my blue}{1/2}} + \textcolor{my green}{\clebschgordan{\textcolor{highlight}{\tfrac{1}{2}}}{\textcolor{my red}{1}}{\textcolor{my blue}{\bar{\tfrac{1}{2}}}}{\textcolor{highlight}{\tfrac{3}{2}}}{\textcolor{my red}{1}}{\textcolor{my blue}{\tfrac{1}{2}}}} \ket{\textcolor{my red}{1}, \textcolor{my red}{1}} \directproduct \ket{\textcolor{my blue}{1/2}, \textcolor{my blue}{-1/2}}.
    \end{equation}
    Note that we use a bar to denote a negative index, so \(\bar{\tfrac{1}{2}} = -1/2\).
    
    We can then look up the values of the Clebsch--Gordan coefficients.
    To do so we need to understand the standard layout of a table of Clebsch--Gordan coefficients.
    First we need to find the correct table, they are labelled by \(j_1\) and \(j_2\), usually as \(j_1 \times j_2\) or \(j_1 \directproduct j_2\).
    The tables are then of the form
    \begin{equation}
        \begin{array}{cc|ccc}
            && J & J & \cdots \\
            && M & M & \cdots \\ \hline
            m_1 & m_2 &&& \\
            m_1 & m_2 &&& \\
            \vdotswithin{m_1} & \vdotswithin{m_2} &&&
        \end{array}
    \end{equation}
    The entries into the table are then almost the Clebsch--Gordan coefficients.
    However, since most Clebsch--Gordan coefficients tend to be square roots there is an implicit square root.
    Choosing the Condon--Shortly phase coefficient the Clebsch--Gordan coefficients are also real, and so the square root misses any negatives.
    This means that, for example, the entry \(-1/2\) in the table is understood to mean \(-\sqrt{1/2}\).
    
    We are interested in the case of \(j_1 = 1/2\) and \(j_2 = 1\).
    The Clebsch--Gordan table for \(1 \directproduct 1/2\) is as follows:
    \begin{gather}
        \begin{array}{cc|c}
            && 3/2 \\
            && 3/2 \\ \hline
            1 & 1/2 & 1
        \end{array}
        \qquad
        \begin{array}{cc|cc}
            && \textcolor{highlight}{3/2} & 1/2 \\
            && \textcolor{highlight}{1/2} & 1/2 \\ \hline
            \textcolor{my red}{1} & \textcolor{my blue}{-1/2} & \textcolor{my green}{1/3} & 2/3\\
            \textcolor{my red}{0} & \textcolor{my blue}{1/2} & \textcolor{my yellow}{2/3} & -1/3
        \end{array}
        \\
        \begin{array}{cc|cc}
            && 3/2 & 1/2 \\
            && -1/2 & -1/2 \\ \hline
            0 & -1/2 & 2/3 & 1/3\\
            -1 & 1/2 & 1/3 & -2/3
        \end{array}
        \qquad
        \begin{array}{cc|c}
            && 3/2 \\
            && -3/2 \\ \hline
            -1 & -1/2 & 1
        \end{array}
    \end{gather}
    From this we can read off the Clebsch--Gordan coefficients as
    \begin{equation}
        \textcolor{my yellow}{\clebschgordan{\textcolor{highlight}{\tfrac{1}{2}}}{\textcolor{my red}{0}}{\textcolor{my blue}{\tfrac{1}{2}}}{\textcolor{highlight}{\tfrac{3}{2}}}{\textcolor{my red}{1}}{\textcolor{my blue}{\tfrac{1}{2}}}} = \textcolor{my yellow}{\sqrt{2/3}} \qqand \textcolor{my green}{\clebschgordan{\textcolor{highlight}{\tfrac{1}{2}}}{\textcolor{my red}{1}}{\textcolor{my blue}{\bar{\tfrac{1}{2}}}}{\textcolor{highlight}{\tfrac{3}{2}}}{\textcolor{my red}{1}}{\textcolor{my blue}{\tfrac{1}{2}}}} = \textcolor{my green}{\sqrt{1/3}}.
    \end{equation}
    Hence the state \(\ket{3/2, 1/2}\) is given by
    \begin{equation}
        \ket{\textcolor{highlight}{3/2}, \textcolor{highlight}{1/2}} = \textcolor{my yellow}{\sqrt{\frac{2}{3}}} \ket{\textcolor{my red}{1}, \textcolor{my red}{0}} \directproduct \ket{\textcolor{my blue}{1/2}, \textcolor{my blue}{1/2}} + \textcolor{my green}{\sqrt{\frac{1}{3}}} \ket{\textcolor{my red}{1}, \textcolor{my red}{1}} \directproduct \ket{\textcolor{my blue}{1/2}, \textcolor{my blue}{-1/2}}.
    \end{equation}
    
    As another example the state \(\ket{1/2, 1/2}\) is
    \begin{align}
        \ket{1/2, 1/2} & = \clebschgordan{\tfrac{1}{2}}{0}{\tfrac{1}{2}}{\tfrac{1}{2}}{1}{\tfrac{1}{2}} \ket{1, 0} \directproduct \ket{1/2, 1/2} + \clebschgordan{\tfrac{1}{2}}{1}{\bar{\tfrac{1}{2}}}{\tfrac{1}{2}}{1}{\tfrac{1}{2}} \ket{1, 1} \directproduct \ket{1/2, -1/2}\\
        &= -\sqrt{\frac{1}{3}} \ket{1, 0} \directproduct \ket{1/2, 1/2} + \sqrt{\frac{2}{3}} \ket{1, 1} \directproduct \ket{1/2, -1/2}.
    \end{align}
    
    Of course, now days the use of look up tables is quickly vanishing and many programs exist to compute/look up Clebsch--Gordan coefficients for you.
    \begin{cde}{Clebsch--Gordan}{}
        Looking up Clebsch--Gordan coefficients with \textit{Mathematica}.
        \begin{lstlisting}[gobble=12, language=mathematica, mathescape]
            (* ClebschGordan[{$\textcolor{codeCommentColor}{j_1}$, $\textcolor{codeCommentColor}{m_1}$}, {$\textcolor{codeCommentColor}{j_2}$, $\textcolor{codeCommentColor}{m_2}$}, {$\textcolor{codeCommentColor}{J}$, $\textcolor{codeCommentColor}{M}$}]
            gives $\textcolor{codeCommentColor}{\clebschgordan{M}{m_1}{m_2}{J}{j_1}{j_2}}$*)
            In[1]:= ClebschGordan[{1,0}, {1/2,1/2}, {3/2,1/2}]
            Out[1]= $\sqrt{\frac{2}{3}}$
            In[2]:= ClebschGordan[{1,1}, {1/2,-1/2}, {3/2,1/2}]
            Out[2]= $\frac{1}{\sqrt{3}}$
        \end{lstlisting}
    \end{cde}
    
    \subsection{Geometric Interpretation of the \texorpdfstring{\(j = 1\)}{j = 1} Representation}
    We now consider the geometric interpretation of the \(j = 1\) representation.
    Consider the definition of a tensor operator in the \(j = 1\) representation as something which transforms according to
    \begin{equation}
        \rho(g) T_{k}^1 \rho(g)^\hermit = \rho_c(g)_{kl}T_l^1.
    \end{equation}
    In infinitesmial form this is
    \begin{equation}
        \commutator{J_a}{T_k^1} = i\varepsilon_{akl}T_l^1.
    \end{equation}
    For \(\specialUnitaryLie(2)\) we can write \(\rho(g)\) as \(\exp[i\alpha^aJ_a]\) and \(\rho(g)^\hermit\) as \(\exp[i\alpha^bJ_b]\), with implied sums over \(a\) and \(b\).
    Expanding the transformation law to first order in \(\alpha\) we get
    \begin{align}
        (\ident + i\alpha^aJ_a + \order(\alpha^2))&T_k^1 (\ident - i\alpha^bJ_b + \order(\alpha^2))\\
        &= (T_k^1 + i\alpha^aJ_aT_k^1 + \order(\alpha^2))(\ident - i\alpha^bJ_b)\\
        &= T_k^1 + i\alpha^aJ_aT_k^1 - \alpha^bJ_bT_k^1J_b + \order(\alpha^2)\\
        &= T_k^1 + i\alpha^a\commutator{J_a}{T_k^1} + \order(\alpha^2)\\
        &= T_k^1 + i\alpha^a(i\varepsilon_{akl}T_l^1) + \order(\alpha^2)\\
        &= T_k^1 - \alpha^a\varepsilon_{akl}T_l^1 + \order(\alpha^2)\\
        &= T_k^1 + \alpha^a\varepsilon_{alk}T_l^1 + \order(\alpha^2).
    \end{align}
    Now, in a slight abuse of notation, we can define a \enquote{vector}, \(\vv{T} = (T_1^1, T_2^1, T_3^1)\), and a \enquote{vector} \(\vv{\alpha} = (\alpha^1, \alpha^2, \alpha^3)\).
    We can then identify
    \begin{equation}
        \varepsilon_{alk}\alpha^aT_l^1 = (\vv{\alpha} \times \vv{T})_k = \abs{\vv{\alpha}}\abs{\vv{T}}\sin(\gamma),
    \end{equation}
    where \(\gamma\) is the \enquote{angle} between \(\vv{T}\) and \(\vv{\alpha}\).
    We can then identify the first order term in the tensor transformation law as a small translation in the direction \enquote{perpendicular} to \(\vv{T}\) and \(\vv{\alpha}\), which is then added onto the original \(T_k^1\).
    This is simply the infinitesimal version of a rotation of \(\vv{T}\) about \(\vv{\alpha}\).
    This is shown in \cref{fig:infinitesimal rotation}.
    
    \begin{figure}
        \tikzsetnextfilename{infinitesimal-rotation}
        \begin{tikzpicture}
            \draw[very thick, <->, rounded corners=0.1]  (0, 3) coordinate (A) node[left] {\(\vv{\alpha}\)} -- (0, 0) coordinate (B) -- (140:2) coordinate (C) node[above right] {\(\vv{T}\)};
            \path pic[draw, "\(\gamma\)", angle eccentricity = 0.7] {angle};
            \node[left] at (C) {\(\vv{\alpha} \times \vv{T}\) \(\odot\)};
            \draw[very thick, ->] (0, 0) -- (147:1.8) node[below left] {\(\vv{\alpha} + \delta\vartheta \, \vv{\alpha} \times \vv{T}\)};
            \draw[dotted] (C) arc(180:189:2);
        \end{tikzpicture}
        \caption[Infinitesimal rotation.]{An infinitesimal rotation by \(\delta \vartheta\) of \(\vv{T}\) about \(\vv{\alpha}\) results in adding a term proportional to \(\vv{\alpha} \times \vv{T}\) to \(\vv{T}\).}
        \label{fig:infinitesimal rotation}
    \end{figure}
    
    \chapter{Further Applications}
    \section{Polynomial Invariants}
    \begin{rmk}
        This section is nonexaminable.
    \end{rmk}
    In physics we are very interested in invariant quantities, such as energy, momentum, mass, etc.
    These are all invariant under some particular symmetry and this fact can greatly simplify physics problems.
    One question that we might ask is given a number of irreducible representations how many invariant quantities can we construct?
    The simplest example of an invariant is the inner product.
    This is an invariant of a vector representation and its complex conjugate.
    
    Suppose we have an irreducible representation, which we denote by its dimension, \(\rep{X}\).
    Consider the representation \(\rep{X} \directproduct \overline{\rep{X}}\).
    We know by \cref{crl:trivial irrep in clebsch--gordan decomposition if second term is complex conjugate} that the trivial representation appears exactly once in the Clebsch--Gordan series, that is:
    \begin{equation}
        \rep{X} \directproduct \overline{\rep{X}} = 1\cdot \rep{1} \directsum \dotsb.
    \end{equation}
    The inner product corresponds to the trivial representation, \(\rep{1}\), in this decomposition.
    
    For a given group, \(G\), we can define an invariant, \(\mathcal{I}\), as an \(n\)-multilinear operator from the space of irreducible representations to \(\complex\), such that for a given group element, \(g \in G\), \(\mathcal{I}\) is unchanged if we act on each representation with itself.
    That is
    \begin{equation}
        \mathcal{I}_n = \mathcal{I}(\rep{X}_1, \dotsc, \rep{X}_n) = \mathcal{I}(\rho_1(g)\rep{X}_1, \dotsc, \rho_n(g)\rep{X}_n)
    \end{equation}
    for all \(g \in G\).
    
    The inner product then corresponds to \(\mathcal{I}_2\), it takes two values in and returns an invariant.
    More explicitly as long as \(\dim \rep{X}_a = \dim \rep{X}_b\) we define the inner product as the invariant
    \begin{equation}
        \mathcal{I}_2(\rep{X}_a, \rep{X}_b) \coloneqq \sum_{i = 1}^{\dim \rep{X}_a} (\rep{X}_a)^*_i(\rep{X}_b)_i.
    \end{equation}
    If \(\dim\rep{X}_a \ne \dim\rep{X}_b\) then no degree two invariant can be formed.
    Note that \((\rep{X}_a)_i\) here are the components of some element in the \(\rep{X}_a\) representation.
    The basis we choose isn't important since by definition of being an invariant this quantity is the same no matter what basis we choose.
    
    One way to find higher order invariants is to take Kronecker products and count how many times \(\rep{1}\) appears.
    Each occurrence leads to an invariant.
    The specific representation giving rise to \(\rep{1}\) corresponds to the degree of the associated invariant.
    
    As an example we will consider the group \(G = S_4\), and limit ourselves to irreducible representations of dimension 3 or fewer.
    The following Kronecker products can be computed:
    \begin{align}
        \rep{3}_1 \directproduct \rep{3}_1 &= \mathop{\mathrm{Sym}}(\rep{1} \directsum \rep{2} \directsum \rep{3}_1) \directsum \mathop{\mathrm{Asym}}(\rep{3}_2),\\
        \rep{3}_1 \directproduct \rep{2} &= \rep{3}_1 \directsum \rep{3}_2,\\
        \rep{3}_1 \directproduct \rep{3}_2 &= \rep{1}_1 \directsum \rep{2} \directsum \rep{3}_1 \directsum \rep{3}_2.
    \end{align}
    We restrict ourselves to considering invariants involving \(\rep{3}_1\).
    We can take \((x, y, z)\) as a generic element of this vector space associated with this representation.
    
    The antisymmetric part vanishes since it corresponds to \(\varepsilon_{ijk}x_ix_j = 0\).
    The invariants of degree at most three are
    \begin{equation}
        \mathcal{I}_2 = x^2 + y^2 + z^2, \qqand \mathcal{I}_3 = xyz.
    \end{equation}
    We have seen already that \(\mathcal{I}_2\) corresponds to the inner product.
    That this also appears for \(S_4\) is simply due to the fact that \(S_4\) is a subgroup of \(\orthogonal(3)\).
    The invariant of degree three comes from considering \(\rep{3}_1 \directproduct \rep{3}_1 \directproduct \rep{3}_1 = 1\cdot \rep{1} \directsum \dotsb\).
    
    Invariants are basis independent, as they must be to be useful.
    This follows since a similarity transformation on the representation will always affect the invariants in a definite way and we can construct them such that there is no change.
    
    A more systematic way of obtaining the degrees and interdependencies of invariants in a given representation uses the \defineindex{Molien function}, which is beyond the scope of this course.
    
    For a given degree we can construct an invariant by making a generic ansatz and averaging over the group by the \defineindex{Reynolds operator}.
    For example, consider the representation containing the vector \((x, y, z)\), again restricting ourselves to three dimensions.
    Now consider any polynomial function, \(f\), of \(x\), \(y\), and \(z\), that is \(f \in \complex[x, y, z]\).
    The following is then an invariant:
    \begin{equation}
        \mathcal{I}(x, y, z) = \frac{1}{\abs{G}} \sum_{g \in G} f(\rho(g) \circ x, \rho(g) \circ y, \rho(g) \circ z),
    \end{equation}
    where we understand \(\rho(g)\circ x\) as \(\rho(g)\ve{x}\), where \(\ve{x}\) is a unit vector in the \(x\)-direction.
    
    This operation corresponds to projection operators and will always give rise to an invariant.
    However, most of the time it will lead to 0, which is trivially invariant, unless the degree of \(f\) matches one of the degrees predicted by either the Kronecker product method or the Molien function.
    As an example consider \(f(x, y, z) = x^2\).
    From this we derive the invariant \(\mathcal{I}(x, y, z) \propto x^2 + y^2 + z^2 = \mathcal{I}_2\), with some unimportant constant of proportionality.
    
    One can think of the Clebsch--Gordan coefficient appearing in
    \begin{equation}
        \bra{c, m_c} T_{m_a}^a \ket{b, m_b} = \clebschgordan{m_c}{m_a}{m_b}{c}{a}{b} \bra{c} |T^a| \ket{b}
    \end{equation}
    as an invariant of degree three of type \(\mathcal{I}(\overline{\rep{c}}, \rep{a}, \rep{b})\).
    
    \section{Land\'e \texorpdfstring{\(g\)}{g}-Factor}
    \begin{wrn}
        In this section we choose units such that \(\hbar = 1\).
    \end{wrn}
    Consider a Hydrogen atom with a Coulomb potential.
    This system has the Hamiltonian \(H = H_0\), where \(H_0 = H_{\mathrm{Coulomb}} + H_{\text{spin-orbit}}\), where \(H_{\mathrm{Coulomb}}\) is the kinetic energy term and the Coulomb potential term, and \(H_{\text{spin-orbit}}\) is an extra term corresponding to spin-orbit coupling, the details of which are not important.
    This system exhibits spherical symmetry.
    We can break this symmetry by applying an external magnetic field.
    In this case we get a modified Hamiltonian \(H = H_0 + H'\), where \(H' = -\vv{\mu} \cdot \vv{B}\) where \(\vv{B}\) is the external magnetic field and \(\vv{\mu}\) is the dipole moment given by
    \begin{equation}
        \vv{\mu} = \frac{e}{2\pi m_{\mathrm{e}} c} (\vv{J} + \vv{S}) = \frac{e}{2\pi m_{\mathrm{e}} c} (\vv{L} + 2\vv{S}).
    \end{equation}
    Defining \(x = -eB/(2m_{\mathrm{e}} c)\) we can write the perturbing Hamiltonian as \(H' = x(L_3 + 2S_3) = x(J_3 + S_3)\), choosing the \(3\)-axis to be along the direction of the magnetic field.
    
    We assume that the energy eigenstates, \(\ket{n, l, j, m}\), of \(H_0\) are known and treat \(H'\) as a perturbation.
    The first order perturbation correction to the energy gives an extra term of the form
    \begin{equation}
        E' = \bra{n, l, j, m} H' \ket{n, l, j, m} = x(m + \expected{S_3}).
    \end{equation}
    Here we have used \(J_3\ket{n, l, j, m} = m\ket{n, l, j, m}\).
    
    Using the Wigner--Eckart theorem, and a condensed notation, \(\ket{m} = \ket{n, l, j, m}\), we have
    \begin{equation}
        \bra{m} S_i \ket{m'} = \clebschgordan{i}{m}{m'}{1}{j}{j'} \expected{\|S_1\|}, \qqand \bra{m} J_i \ket{m'} = \clebschgordan{i}{m}{m'}{1}{j}{j'} \expected{\|J_1\|}.
    \end{equation}
    Note that \(\expected{\|S_1\|}\) and \(\expected{\|J_1\|}\) are the reduced matrix elements in this condensed notation suppressing \(j\).
    What these relations tell us is that \(\bra{m}S_i\ket{m'} = a\bra{m}J_i\ket{m'}\) with
    \begin{equation}
        a \coloneqq \frac{\expected{\|S_1\|}}{\expected{\|J_1\|}}.
    \end{equation}
    Importantly \(a\) is independent of the values of \(m\) and \(m'\).
    
    We have
    \begin{equation}
        \bra{m} \vv{J}^2 \ket{m} = \sum_{m', i} \bra{m} J_i \ket{m'} \bra{m'} J_i \ket{m},
    \end{equation}
    where we have used \(\vv{J}^2 = \sum_i J_iJ_i\) and inserted the completeness relation,
    \begin{equation}
        \ident = \sum_{m'} \ket{m'}\bra{m'}.
    \end{equation}
    We similarly have
    \begin{align}
        \bra{m} \vv{J} \cdot \vv{S} \ket{m} &= \sum_{m', i} \bra{m} J_i \ket{m'}\bra{m'} S_i \ket{m}\\
        &= \sum_{m', i} \bra{m} J_i \ket{m'} a\ket{m'} J_i \ket{m}\\
        &= a\bra{m}\vv{J}^2\ket{m}\\
        &= a j(j + 1).
    \end{align}
    
    Alternatively, using \(\vv{L} = \vv{J} - \vv{S}\) we have
    \begin{equation}
        \vv{L}^2 = (\vv{J} - \vv{S})^2 = \vv{J}^2 + \vv{S}^2 - 2\vv{J}\cdot\vv{S}.
    \end{equation}
    Note that here we have used the fact that \(\vv{J}\) and \(\vv{S}\) are both from two different copies of the \(\specialUnitary(2)\) Lie algebra, and hence commute with each other, we can view both as elements of the universal enveloping algebra.
    Hence we find that
    \begin{equation}
        \bra{m} \vv{J} \cdot \vv{S} \ket{m} = -\frac{1}{2}[l(l + 1) - j(j + 1) - s(s + 1)].
    \end{equation}
    
    We can compare these two results and solve for \(a\).
    Doing so we find that
    \begin{equation}
        a = \frac{j(j + 1) + s(s + 1) - l(l + 1)}{2j(j + 1)},
    \end{equation}
    which is, as expected, independent of \(m\) and \(m'\).
    We find that
    \begin{equation}
        E'_{jlsm} = -\frac{emB}{2m_{\mathrm{e}}c} g
    \end{equation}
    where
    \begin{equation}
        g = 1 + a
    \end{equation}
    is the \define{Land\'e \(g\)-factor}\index{Lande g-factor@Land\'e \(g\)-factor} to first order in perturbation theory.
    This energy shift, \(E'\), can be added to the energy of the unperturbed Hamiltonian to lift the degeneracy.
    
    \section{Spherical Harmonics}
    The trace is linear, meaning \(\tr(A + B) = \tr(A) + \tr(B)\).
    For finite matrices we also have the cyclic property, which means in particular that \(\tr(AB) = \tr(BA)\).
    This implies that for finite matrices \(\tr(\commutator{A}{B}) = 0\).
    This cyclic property doesn't hold for operators on infinite dimensional spaces.
    The canonical commutation relation, \(\commutator{\operator{x}_i}{\operator{p}_j} = i\hbar \delta_{ij}\), has trace \(3i\hbar \ne 0\) in three spatial dimensions.
    This then implies that the position space representation is infinite dimensional.
    This shouldn't come as a surprise since the position basis is continuously labelled by \(x\).
    
    The question then is how come the representations associated with the angular momentum are finite dimensional?
    These representations are on vector spaces spanned by \(\ket{l, m}\) with \(l < n\) and \(m = -l, \dotsc, l\), making up a finite basis.
    The answer is that we can decompose the Hilbert space on the sphere, \(S^2\), into an infinite sum of finite dimensional Hilbert spaces.
    We will now investigate a basis for each of these finite dimensional Hilbert spaces.
    
    Consider \(S^2\) parametrised by the usual spherical coordinates \((\vartheta, \varphi)\).
    Note that we keep \(r = 1\) fixed and so this is a two-dimensional manifold embedded in 3-dimensional space.
    A state \(\ket{\vartheta, \varphi}\) obeys the completeness and orthogonality relations
    \begin{align}
        \ident_{S^2} &= \int \dl{\Omega} \, \ket{\vartheta, \varphi}\bra{\vartheta, \varphi},\\
        \braket{\vartheta, \varphi}{\vartheta', \varphi'} &= \delta(\varphi - \varphi')\delta(\cos\vartheta - \cos\vartheta').
    \end{align}
    Note that \(\dl{\Omega} = \sin\vartheta \dd{\varphi}\dd{\vartheta}\) is a solid angle element, so
    \begin{equation}
        \int_{S^2}\dd{\Omega} = \int_0^{2\pi} \dd{\varphi} \int_{0}^{\vartheta} \dd{\vartheta} \, \sin\vartheta = \int_0^{2\pi} \dd{\varphi} \int_{-1}^{1} \dl{(\cos \vartheta)}.
    \end{equation}

    We can decompose the Hilbert space \(\hilbert_{S^2} = \spn\{\ket{\vartheta, \varphi}\}\), as a direct sum of the Hilbert spaces \(\hilbert_{l} = \spn\{\ket{l, m} \mid m = -l, \dotsc, l\}\).
    Clearly
    \begin{equation}
        \dim\hilbert_l = \abs{\spn\{\ket{l, m} \mid m = -l, \dotsc, l\}} = \abs{\{-l, \dotsc, l\}} = 2l + 1.
    \end{equation}
    We then have
    \begin{equation}
        \hilbert_{S^2} = \bigoplus_{l = 0}^{\infty} \hilbert_l.
    \end{equation}
    We can rewrite the completeness and orthogonality relations for \(\hilbert_{S^2}\) in terms of its decomposition, giving
    \begin{align}
        \ident_{S^2} &= \sum_{l = 0}^{\infty}\sum_{m = -l}^{l} \ket{l, m}\bra{l, m},\\
        \braket{l, m}{l', m'} &= \delta_{ll'}\delta_{mm'}.
    \end{align}
    
    \begin{dfn}{Spherical Harmonic}{}
        We define the \define{spherical harmonics}\index{spherical harmonic}, \(Y_{lm}\colon S^2 \to \complex\), as the projection of the state \(\ket{l, m}\) onto \(\ket{\vartheta, \varphi}\):
        \begin{equation}
            Y_{lm}(\vartheta, \varphi) \coloneqq \braket{\vartheta, \varphi}{l, m}.
        \end{equation}
    \end{dfn}
    
    \begin{lma}{}{}
        The following orthogonality relations hold for the spherical harmonics:
        \begin{align}
            \int Y_{l'm'}^*(\vartheta, \varphi) Y_{lm}(\vartheta, \varphi) \dd{\Omega} &= \delta_{ll'}\delta_{mm'},\\
            \sum_{l = 0}^{\infty} \sum_{m = -l}^{l} Y_{lm}(\vartheta, \varphi)Y_{lm}^*(\vartheta', \varphi') &= \delta(\varphi - \varphi')\delta(\cos\vartheta - \cos\vartheta').
        \end{align}
        \begin{proof}
            We start with the orthogonality relation for \(\ket{l, m}\):
            \begin{equation}
                \braket{l, m}{l', m'} = \delta_{ll'}\delta_{mm'}.
            \end{equation}
            We then insert the completeness relation for \(\ket{\vartheta, \varphi}\):
            \begin{equation}
                \ident_{S^2} = \int \dl{\Omega} \, \ket{\vartheta, \varphi}\bra{\vartheta, \varphi}.
            \end{equation}
            This gives
            \begin{equation}
                \int\dl{\Omega} \braket{l, m}{\vartheta, \varphi}\braket{\vartheta, \varphi}{l', m'} = \delta_{ll'}\delta_{mm'}.
            \end{equation}
            Now we use \(\braket{l, m}{\vartheta, \varphi} = \braket{\vartheta, \varphi}{l, m}^*\) and the definition of the spherical harmonics as \(Y_{lm}(\vartheta, \varphi) \coloneqq \braket{\vartheta, \varphi}{l, m}\) to get
            \begin{equation}
                \int\dl{\Omega} Y_{lm}^*(\vartheta, \varphi)Y_{l'm'}(\vartheta, \varphi) = \delta_{ll'}\delta_{mm'}.
            \end{equation}
            Exchanging which symbols are primed we get the desired result.
            
            Next we start with the orthogonality relation for \(\ket{\vartheta, \varphi}\):
            \begin{equation}
                \braket{\vartheta, \varphi}{\vartheta', \varphi'} = \delta(\varphi - \varphi')\delta(\cos\vartheta - \cos\vartheta').
            \end{equation}
            We then insert the completeness relation for \(\ket{l, m}\):
            \begin{equation}
                \ident_{S_2} = \sum_{l = 0}^{\infty} \sum_{m = -l}^l \ket{l, m}\bra{l, m}.
            \end{equation}
            This gives
            \begin{equation}
                \sum_{l = 0}^{\infty} \sum_{m = -l}^{l} \braket{\vartheta, \varphi}{l, m}\braket{l, m}{\vartheta', \varphi'} = \delta(\varphi - \varphi')\delta(\cos\vartheta - \cos\vartheta').
            \end{equation}
            Again using \(\braket{l, m}{\vartheta', \varphi'} = \braket{\vartheta', \varphi'}{l, m}^*\) and the definition of the spherical harmonics as \(Y_{lm}(\vartheta, \varphi) \coloneqq \braket{\vartheta, \varphi}{l, m}\) we get
            \begin{equation}
                \sum_{l = 0}^{\infty} \sum_{m = -l}^{l} Y_{lm}(\vartheta, \varphi) Y_{lm}^*(\vartheta', \varphi') = \delta(\varphi - \varphi')\delta(\cos\vartheta - \cos\vartheta').
            \end{equation}
        \end{proof}
    \end{lma}
    
    For \(l = 0, 1\), using the Condon--Shortley phase convention, the spherical harmonics are given by
    \begin{alignat}{4}
        Y_{00}(\vartheta, \varphi) &= \frac{1}{2}\sqrt{\frac{1}{\pi}},\\
        Y_{10}(\vartheta, \varphi) &= \frac{1}{2}\sqrt{\frac{3}{\pi}} \cos\vartheta &&= \frac{1}{2}\sqrt{\frac{3}{\pi}} \frac{z}{r}, \label{eqn:Y10} \\
        Y_{1,{-1}}(\vartheta, \varphi) &= \frac{1}{2}\sqrt{\frac{3}{2\pi}}\e^{-i\varphi}\sin\vartheta &&= \frac{1}{2}\sqrt{\frac{3}{2\pi}} \frac{x - iy}{r},\\
        Y_{11}(\vartheta, \varphi) &= -\frac{1}{2}\sqrt{\frac{3}{2\pi}}\e^{i\varphi}\sin\vartheta &&= -\frac{1}{2}\sqrt{\frac{3}{2\pi}} \frac{x + iy}{r}.
    \end{alignat}
    In general the \(\varphi\) dependence of \(Y_{lm}\) only appears through a \(\e^{im\varphi}\) term and the \(\vartheta\) dependence appears through trigonometric terms \(\sin\vartheta\) and \(\cos\vartheta\), which occur with maximum power \(l\).
    This follows from the fact that we can write the spherical harmonics as
    \begin{equation}
        Y_{lm}(\vartheta, \varphi) = \sqrt{\frac{(2l + 1)}{4\pi}\frac{(l - m)!}{(l + m)!}} P_{lm}(\cos\vartheta)\e^{im\varphi}.
    \end{equation}
    Here \(P_{lm}\) are the \define{associated Legendre polynomials}\index{associated Legendre polynomial}, which are polynomials of order \(l\).
    A useful property of the spherical harmonics is that
    \begin{equation}
        Y_{l,{-m}} = (-1)^mY_{lm}^*.
    \end{equation}
    
    The quadratic Casimir operator, \(L^2\), on \(S^2\) is given by
    \begin{equation}
        \int \dl{\Omega} \bra{\vartheta', \varphi'} \left( -\frac{L^2}{\hbar^2} \right) \ket{\vartheta, \varphi} = \frac{1}{\sin^2\vartheta}\diffp[2]{}{\varphi} + \frac{1}{\sin \vartheta} \diffp{}{\vartheta}\left( \sin\vartheta \diffp{}{\vartheta} \right).
    \end{equation}
    This corresponds to the Laplacian, \(\laplacian[S^2]\), on \(S^2\).
    We therefore have that
    \begin{equation}
        \laplacian[S^2]Y_{lm} = -\frac{L^2}{\hbar^2}Y_{lm} = -l(l + 1)Y_{lm}.
    \end{equation}
    Which follows since we know that the action of the quadratic Casimir, \(L^2\), for \(\specialUnitary(2)\) is to multiply by a factor of \(l(l + 1)\).
    It makes sense that this should be the Casimir.
    First, the Laplacian is invariant under rotations, and it is of second order, as a quadratic Casimir must be.
    Solutions to Laplace's equation are called harmonic functions, which explains the name \enquote{spherical harmonics}, they are harmonic functions on the sphere.
    
    \subsection{Application to \texorpdfstring{\(\reals^3 \isomorphic \reals^+ \directproduct S^2\)}{R3 isomorphic R+ x S2}}
    We can decompose three-dimensional Euclidean space, \(\reals^3\), into a product of the non-negative real numbers, \(\reals^+ \coloneqq \{r \in \reals \mid r \ge 0\}\), and the sphere, \(S^2\):
    \begin{equation}
        \reals^3 \isomorphic \reals^+ \directproduct S^2.
    \end{equation}
    This is most obvious when working in spherical polar coordinates, where we associate the radial coordinate \(r\) with \(\reals^+\) and the angular components, \((\vartheta, \varphi)\), with \(S^2\).
    Since the spherical harmonics are a complete set of functions on \(S^2\) we can express any\footnote{Well, any function of interest in physics.} function, \(f \colon \reals^3 \to \complex\) as
    \begin{equation}
        f(x, y, z) = \sum_{n = 1}^{\infty} \sum_{l = 0}^{n - 1} \sum_{m = -l}^{l} u_{nlm}(r) Y_{lm}(\vartheta, \varphi)
    \end{equation}
    where \(u_{nlm} \colon \reals^+ \to \complex\) and \(Y_{lm}\colon S^2 \to \complex\) are the spherical harmonics.
    
    The Laplacian in spherical coordinates is
    \begin{equation}
        \laplacian[\reals^3] = \frac{1}{r^2} \diffp{}{r}\left( r^2 \diffp{}{r} \right) + \laplacian[S^2].
    \end{equation}
    This is used, for example, to express the Schrödinger equation in spherical coordinates.
    
    \begin{dfn}{Solid Harmonics}{}
        The \define{solid harmonics}\index{solid harmonic} are the functions \(\mathcal{Y}_{lm} \colon \reals^3 \to \complex\) defined by
        \begin{equation}
            \mathcal{Y}_{lm}(r, \vartheta, \varphi) \coloneqq r^{l} Y_{lm}(\vartheta, \varphi).
        \end{equation}
    \end{dfn}
    
    The solid harmonics are solutions to Laplace's equation in \(\reals^3\), that is
    \begin{equation}
        \laplacian[\reals^3] \mathcal{Y}_{lm} = 0.
    \end{equation}
    These are particularly useful when we have a spherically symmetric potential, which we can write as \(V(r)\), that is as a function of only the distance from the origin, with no angular dependence.
    In this case the solution doesn't depend on the magnetic quantum number, \(m\), since \(m\) singles out a direction in space, defining the \(L_3\) operator as the angular momentum along a particular axis.
    We therefore reduce \(u_{nlm}\) to \(u_{nl}\) and the Schrödinger equation in three dimensions reduces to a Schrödinger equation in one dimension for the radial component:
    \begin{equation}
        \left( -\frac{\hbar^2}{2m}\frac{1}{r^2}\diffp{}{r}(r^2\diffp{}{r}) + V_l^{\mathrm{eff}}(r) \right) u_{nl}(r) = E_{nl}u_{nl}.
    \end{equation}
    Here \(V_l^{\mathrm{eff}}\) is the effective potential
    \begin{equation}
        V_l^{\mathrm{eff}}(r) \coloneqq V(r) + \frac{\hbar^2}{2m}\frac{l(l + 1)}{r^2}.
    \end{equation}
    Note that we are now restricting \(u_{nl}\) to be eigenfunctions of the Schrödinger equation, rather than an arbitrary complete set of functions on \(\reals^+\) as they originally were.
    
    \subsection{Why \texorpdfstring{\(S^2\)}{S2}?}
    Why is it that \(S^2\) appears as the manifold of interest here.
    We are considering \(\specialOrthogonal(3)\) symmetry, and the manifold of \(\specialOrthogonal(3)\) is \(\mathbb{RP}^3\), that is \(S^3\) with antipodal points associated.
    So, why is this not the manifold that we are considering?
    
    Consider an arbitrary vector in \(\reals^3\).
    We may as well define a coordinate system such that this arbitrary vector is \(v = (1, 0, 0)\).
    We can act on this vector using \(\specialOrthogonal(2)\) to act on the \(y\) and \(z\) components, that is we act on it with \(\ident_1 \directproduct R\) with \(R \in \specialOrthogonal(2)\).
    So
    \begin{equation}
        \ident_1 \directproduct R = 
        \begin{pmatrix}
            1 & 0 & 0\\
            0 & \hphantom{-}\cos\vartheta & \sin\vartheta\\
            0 & -\sin\vartheta & \sin\vartheta
        \end{pmatrix}
        .
    \end{equation}
    The interpretation of this is of course as a rotation about \(v = (1, 0, 0)\).
    
    The fact that our arbitrary vector is invariant under this operation means we have some redundancy, and to get rid of it we mod out by \(\specialOrthogonal(2)\).
    Doing so the relevant manifold is
    \begin{equation}
        \specialOrthogonal(3) / \specialOrthogonal(2) \isomorphic S^2.
    \end{equation}
    Note that this is \emph{not} a group manifold since \(\specialOrthogonal(2)\) is \emph{not} normal in \(\specialOrthogonal(3)\).
    
    \chapter{Selection Rules}
    \section{Parity Selection}
    Recall that we defined \defineindex{parity symmetry} as
    \begin{equation}
        \Psym \circ (t, x, y, z) = (t, -x, -y, -z), \qquad \Psym \circ \vv{r} = -\vv{r}.
    \end{equation}
    Until 1956 it was believed that parity was strictly conserved in all interactions.
    In 1956 theorists posited that parity conservation may be violated in weak interactions.
    Experiments and further development of the theory found that parity is in fact maximally violated in weak interactions.
    The weak force ignores so called \enquote{right handed particles} and acts only on \enquote{left handed particles}.
    For this reason many attach an \(\mathrm{L}\) subscript to the symmetry group of the weak force, \(\specialUnitary(2)_{\mathrm{L}}\).
    In practice the weak force is, well, weak, and can be neglected in many strong and electromagnetic interactions, so the parity is still usually a good quantum number.
    
    By allowing parity violation we effectively extend the symmetry group from \(\specialOrthogonal(3)\) to \(\orthogonal(3)\), since we can write elements of \(\orthogonal(3)\) as a reflection times an element of \(\specialOrthogonal(3)\), with the reflection corresponding to swapping the parity.
    We implement the parity transformation as a unitary transformation, \(U(\Psym)\).
    Further, reversing parity and then reversing back cannot change the physics, and hence we must have that \(U(\Psym)^2 = \e^{i\eta}\ident\), so that upon doubly reversing the parity the only effect is a phase change.
    Often we are free to choose this phase factor, so in which case we usually choose \(\eta = 0\), corresponding to \(U(\Psym) = \pm\ident\).
    
    Given a state \(\ket{l, m}\) the parity transformation is
    \begin{equation}
        U(\Psym)\ket{l, m} = (-1)^l\ket{l, m}
    \end{equation}
    for \(l \in \naturals\).
    This can be inferred from the parity transformation properties of the spherical harmonics:
    \begin{equation}
        \Psym \circ Y_{lm}(\vartheta, \varphi) = Y_{lm}(\pi - \vartheta, \varphi + \pi) = (-1)^lY_{lm}(\vartheta, \varphi).
    \end{equation}
    This can be seen by either thinking about the coordinates \((r, \vartheta, \varphi)\) being inverted through the origin, or algebraically by changing \(x \to -x\), \(y \to -y\), and \(z \to -z\) in the equations \(\vartheta = \arctan(\sqrt{x^2 + y^2}/z)\) and \(\varphi = \arctan(y/x)\).
    
    Suppose we have  a transition governed by the operator \(X\).
    The parity of the operator is \(\eta_X\), which is given by \(U(\Psym) X U(\Psym)^\hermit = (-1)^{\eta_X}X\), or equivalently \(X = (-1)^{\eta_X}U(\Psym)^\hermit XU(\Psym)\).
    We then have the selection rule that
    \begin{equation}
        \bra{l, m} X \ket{l', m'} \ne 0
    \end{equation}
    only if \(l + l' + \eta_X\) is even, since we have
    \begin{align}
        \bra{l, m} X \ket{l', m'} &= \bra{l, m} (-1)^{\eta_X} U(\Psym)^\hermit XU(\Psym) \ket{l', m'}\\
        &= (-l)^{l + l' + \eta_X} \bra{l, m} X \ket{l', m'}.
    \end{align}
    Hence we have \((-1)^{l + l' + \eta_X} = 1\), which implies \(l + l' + \eta_X\) is even.
    Note that if \(X\) is a tensor operator we also have the selection rules that \(\abs{j_1 - j_2} \le J \le j_1 + j_2\), and \(M = m_1 + m_2\).
    
    Common examples of operators with definite parity are the position, \(\vv{x}\), momentum, \(\vv{p}\), angular momentum, \(\vv{L}\), electric field, \(\vv{E}\), and magnetic field, \(\vv{B}\).
    Of these \(\vv{x}\), \(\vv{p}\), \(\vv{E}\) are proper vectors, so have parity \(\eta_{\vv{x}} = \eta_{\vv{p}} = \eta_{\vv{E}} = 1\), whereas \(\vv{L}\) and \(\vv{B}\) are \define{pseudo-vectors}\index{pseudo-vector}, meaning they change by a sign under a parity transformation, and hence \(\eta_{\vv{L}} = \eta_{\vv{B}} = 0\).
    
    It is easy enough to show the pseudo-vector nature of \(\vv{L}\), since we know that \(\vv{L} = \vv{x} \times \vv{p}\), so
    \begin{equation}
        \Psym\circ \vv{L} = \Psym \circ (\vv{x} \times \vv{p}) = (\Psym\circ\vv{x})\times(\Psym\circ\vv{p}) = (-\vv{x}) \times (-\vv{p}) = \vv{x} \times \vv{p} = \vv{L}.
    \end{equation}
    In fact the cross product of two vectors is always a pseudo-vector.
    Similarly the cross product of two pseudo-vectors is a pseudo-vector, and the cross product of a pseudo-vector and vector is a vector.
    This is the logic that means \(\vv{B}\) is a pseudo vector since we have that \(\vv{F} = q\vv{E} + q\vv{v}\times\vv{B}\), and since \(\vv{F}\) is a proper vector and so is \(\vv{v}\) we must have that \(\vv{B}\) is a pseudo-vector.
    
    Each particle has an intrinsic parity.
    Bosons have the same intrinsic parity as their antiparticles, whereas fermions have the opposite parity to their antiparticles.
    This is a result of causality in QFT.
    The angular momentum of a particle is a quantum number, \(J\).
    It is also possible for the intrinsic parity to have this pseudo property.
    If we are allowing for this then we label the particles with the quantum number \(J^\pm\), with \(-\) for when \(\eta\) is odd and \(+\) when \(\eta\) is even.
    For example, the \(\uprho\)-meson (\(\mathrm{u}\bar{\mathrm{d}}\)) is a vector meson, meaning \(S = J = 1\), and the \(\mathrm{A}_1\)-meson (\(\bar{\mathrm{u}}\mathrm{d}\)) is also a vector meson.
    However, the \(\uprho\)-meson has negative parity, so is \(1^{-}\), whereas the \(\mathrm{A}_1\)-meson has positive parity, so is \(1^{+}\).
    We say that the \(\mathrm{A}_1\)-meson is a pseudo-vector particle.
    
    When we are only interested in computing transitions between like particles, for example, an electron going from one energy level to another, the parity cancels from both sides of the equation.
    For this reason we often ignore parity when we are doing quantum mechanics.
    When we are doing particle physics, such as QFT, however, we often consider cases where we generate new particles, potentially with different parities.
    A general example would be the decay \(A \to B + C\).
    In this case the parity of the final state is the sum of the intrinsic parities and the angular momentum, \(\eta_{\mathrm{tot}} = \eta_B + \eta_C + \eta_L\).
    If \(A\) has definite parity and the interaction conserves parity (i.e. it is not a weak interaction) then we must have that \((-1)^{\eta_A} = (-1)^{\eta_{\mathrm{tot}}}\).
    
    \section{Superselection}
    Superselection rules generalise selection rules.
    Selection rules refer to a particular Hamiltonian which is such that only certain transitions between states occur with non-zero probability.
    Superselection rules state that certain matrix elements vanish for \emph{all} observables.
    That is given the states \(\ket{\psi}\) and \(\ket{\varphi}\) ew say these states are separated by a \defineindex{superselection rule} if
    \begin{equation}
        \bra{\psi} O \ket{\varphi} = 0
    \end{equation}
    for all observables \(O\).
    
    This means that a relative phase between \(\ket{\psi}\) and \(\ket{\varphi}\) is unobservable, and hence there is no coherent state of the form \(a\ket{\psi} + b\ket{\varphi}\).
    
    Notice that we can always define an Hermitian operator \(X \coloneqq \ket{\psi}\bra{\varphi} + \ket{\varphi}\bra{\psi}\), and clearly
    \begin{equation}
        \bra{\psi}X\ket{\varphi} = \braket{\psi}{\psi}\braket{\varphi}{\varphi} + \braket{\psi}{\varphi}\braket{\varphi}{\psi} = 1 \ne 0.
    \end{equation}
    What this means is that the presence of superselection rules really means that not all Hermitian operators are valid observables.
    That is the space of observables is a strict subset of the space of Hermitian operators in the presence of superselection rules.
    
    An example of a superselection is the fermionic/bosonic nature of a particle.
    Suppose that \(\ket{1/2}\) is a fermionic state, and \(\ket{1}\) is a bosonic state.
    We could, at least mathematically in the Hilbert space, define a vector \(\ket{\psi} = a\ket{1/2} + b\ket{1}\), however, this doesn't correspond to a physically meaningful state.
    This is because upon a \(2\pi\) rotation, due to some operator, \(U(2\pi)\), a fermionic state will pick up a negative sign, but a bosonic state won't, so
    \begin{equation}
        U(2\pi)\ket{\psi} = -a\ket{1/2} + b\ket{1}.
    \end{equation}
    Clearly this doesn't make any physical sense, since the relative phase between \(a\) and \(b\) changes by \(\pi\) upon rotation.
    
    Another example is conserved charge.
    It doesn't make sense to write, for example, \(\ket{\psi} = \ket{\Pe} + \ket{\APe}\), since these transform under complex conjugates representations, and so pick up opposite phases when transforming.
    The only time that this isn't a problem is if the representation is real, but this corresponds to the particle being neutral.
    
    \section{Electric Dipole}
    Consider an extended charge distribution, \(\rho\).
    To first order in the multipole expansion we can write the Hamiltonian in an external field, \(\vv{E}\), as
    \begin{equation}
        H = H_0 + H'
    \end{equation}
    where \(H_0\) is a Coulomb term and \(H' = -\vv{d} \cdot \vv{E}\), where \(\vv{d}\) is the electric dipole moment, given by
    \begin{equation}
        \vv{d} \coloneqq \int \rho(\vv{x}) \vv{x} \dd{^3x}.
    \end{equation}
    Importantly \(\vv{d}\) transforms the same as \(\vv{x}\), due to the factor of \(\vv{x}\) in the definition.
    We know that \(\vv{x}\) is a \(j = 1\) tensor operator (that is, a vector) and hence the Wigner--Eckart theorem applies, as do the associated selection rules.
    
    For simplicity we consider the case of an external field aligned along the \(z\)-axis, so \(\vv{E} = E_z\ve{z}\).
    It follows from this that \(H' \propto z\) since
    \begin{equation}
        H' = -\int \rho(\vv{x}) \vv{x} \dd{^3x} \cdot \ve{E} = -E_z \int \rho(\vv{x}) \vv{x} \cdot \ve{z} \dd{^3x} = -E_z \int \rho(\vv{x}) z \dd{^3x}.
    \end{equation}
    So \(H'\) transforms like \(z\).
    Looking at the spherical harmonics in \cref{eqn:Y10} we see that
    \begin{equation}
        Y_{10} \propto \frac{z}{r}.
    \end{equation}
    Considering the definition of the solid harmonics as \(\mathcal{Y}_{lm} = r^lY_{lm}\) we see that
    \begin{equation}
        \mathcal{Y}_{10} \propto z.
    \end{equation}
    So \(H' \propto \mathcal{Y}_{10}\).
    
    Applying the Wigner--Eckart theorem we then have
    \begin{equation}
        \bra{l,m} H' \ket{l',m'} \propto \bra{l,m}\mathcal{Y}_{10}\ket{l',m'} = \clebschgordan{m}{0}{m'}{l}{1}{l'} \bra{l} |\mathcal{Y}_1 \ket{l'}.
    \end{equation}
    We know that the Clebsch--Gordan coefficient, \(\clebschgordan{M}{m_1}{m_2}{J}{j_1}{j_2}\), is non-zero only if \(M = m_1 + m_2\), so we must have \(m = m'\), and if \(\abs{j_1 - j_2} \le J \le j_1 + j_2\), which means that we must have \(\Delta l \coloneqq \abs{l - l'} \le 1\).
    Further \(\clebschgordan{m}{0}{m'}{0}{1}{0} = 0\) for all allowed values of \(m\) and \(m'\), which is just \(m = m' = 0\).
    This means we cannot have \(l = l' = 0\).
    
    Further considering parity selection rules since \(\eta_{\vv{x}} = 1\), and \(H'\) transforms like \(\vv{x}\), so has the same parity which means we must have \(l + l' + \eta_{\vv{x}} = l + l' + 1\) even.
    Hence we cannot have \(l - l' = 0\), since if this is the case then \(l + l' + 1 = 2l + 1\), which is odd for all integer \(l\).
    So \(\Delta l \ne 0\).
    Hence we have the selection rules
    \begin{equation}
        m = m', \qqand \Delta l = \abs{l - l'} = 1.
    \end{equation}
    
    \section{Pauli's Hydrogen Atom}
    \begin{wrn}
        In this section we use units such that \(c = \hbar = 1\).
    \end{wrn}
    Shortly after Schrödinger solved his equation for the hydrogen atom Pauli did so using group theory and a hidden \(\specialUnitary(2) \directproduct \specialUnitary(2)\) symmetry.
    The energy levels of the hydrogen atom are given by
    \begin{equation}
        E_n = -\frac{m_{\mathrm{e}}e^4}{2} \frac{1}{n^2}.
    \end{equation}
    Importantly there is no \(l\) dependence.
    This means that for each value of the principal quantum number, \(n\), we have \(l = 0, \dotsc, n - 1\), which means the total degeneracy is
    \begin{equation}
        \sum_{l = 0}^{n - 1} (2l + 1) = 2\sum_{l = 0}^{n - 1}l + \sum_{l = 0}^{n - 1} = 2\frac{n - 1}{2}n + n = n^2.
    \end{equation}
    Note that the \(2l + 1\) term is due to degeneracy in \(m\), which runs from \(-l\) to \(l\), so takes on \(2l + 1\) values for fixed \(l\).
    
    The degeneracy in \(m\) is expected, since we have a spherically symmetric potential, and \(m\) picks out a direction.
    The fact that we have no \(l\) dependence suggests that there is some further symmetry which we are missing.
    Pauli used the fact that there is a conserved vector, \(\vv{A}\), with components
    \begin{equation}
        A_i = \varepsilon_{ijk}p_jL_k - me^2\frac{x_i}{r},
    \end{equation}
    which is conserved in a spherically symmetric system in classical mechanics.
    This is known as the \defineindex{Laplace--Runge--Lenz vector}.
    In quantum mechanics there is a related quantity given by symmetrisation:
    \begin{equation}
        A_i = \varepsilon_{ijk}p_jL_k - me^2\frac{x_i}{r} - ip_i
    \end{equation}
    Note that this is an operator in quantum mechanics.
    It can be shown that the following hold:
    \begin{gather}
        \vv{L} \cdot \vv{A} = \vv{A} \cdot \vv{L} = 0,\\
        \commutator{L_i}{A_j} = i\varepsilon_{ijk}A_k,\\
        \commutator{H}{A_i} = 0, \qwhere H = \sum_i\frac{p_i^2}{2m} - \frac{e^2}{r}.\\
    \end{gather}
    The second of these means that \(A_i\) is a \(j = 1\) vector operator.
    The third means that \(\vv{A}\) is conserved.
    
    By Noether's theorem we can identify from this conservation law that there must be a hidden symmetry.
    To find this symmetry we make some new definitions and extract a Lie algebra.
    Start by defining
    \begin{equation}
        \tilde{A}_i \coloneqq \frac{A_i}{\sqrt{-2mH}}.
    \end{equation}
    Note that for a bound state the total energy is negative, so we take the minus sign in the root to keep \(\tilde{A}_i\) real.
    It can then be shown that
    \begin{equation}
        \liebracket{\tilde{A}_i}{\tilde{A}_j} = i\varepsilon_{ijk}\tilde{A}_k.
    \end{equation}
    This shows that \(\tilde{A}_i\) are the generators of a \(\specialUnitaryLie(2)\) Lie algebra.
    We then define \(X_i^{\pm} \coloneqq (L_i \pm \tilde{A}_i) / 2\).
    From this the commutation relation
    \begin{equation}
        \liebracket{X_i^{\pm}}{X_j^{\pm}} = i\varepsilon_{ijk}X_k^{\pm}
    \end{equation}
    follows, since both \(L_i\) and \(A_i\) individually satisfy the requirements of a \(\specialUnitaryLie(2)\) Lie algebra, and since \(\vv{L} \cdot \vv{A} = \vv{A} \cdot \vv{L} = 0\) cross terms vanish.
    
    The result is that we identify an overall \(\specialUnitary(2) \directproduct \specialUnitary(2)\) symmetry, corresponding to the two copies of the \(\specialUnitaryLie(2)\) Lie algebra, spanned by \(X_i^+\) and \(X_i^-\) respectively.
    We can think of one copy as resulting from rotational symmetry and the second from the symmetry associated with conservation of \(\vv{A}\).
    
    One question we may want to ask is what are the Casimir operators of these two Lie algebras are.
    The fact that \(\vv{L} \cdot \vv{A} = \vv{A} \cdot \vv{L} = 0\) means that the two Casimir operators are actually the same, in particular the quadratic Casimir is
    \begin{equation}
        C_2^+ = (X^+)^2 = (X^-)^2 = C_2^- = C_2.
    \end{equation}
    We can parametrise this Casimir by \(C_2 = x(x + 1)\) with \(x = 0, 1/2, 1, 3/2, \dotsc\).
    
    From this it is possible to show that
    \begin{equation}
        A_iA_i = 2mH(L_iL_i + 1) + m^2e^4.
    \end{equation}
    Dividing through by \(-2mH\) and solving for \(H\) we get
    \begin{equation}
        H = -\frac{me^4/2}{L_iL_i + \tilde{A}_i\tilde{A}_i + 1} = -\frac{me^4/2}{4C_2 + 1} = -\frac{me^4/2}{(2x + 1)^2}
    \end{equation}
    for \(x = 0, 1/2, 1, 3/2, \dotsc\).
    The second equality follows using \(L_i = X_i^+ + X_i^-\).
    Counting the degeneracy of the states corresponds to counting the states in the direct product representation \(\rho_{j = x} \directproduct \rho_{j = x}\), which has \((2x + 1)^2\) states.
    Comparing this with our initial work in this section we identify the principal quantum number \(n = 2x + 1\).
    
    Pauli's approach here was to write the Hamiltonian in terms of two Casimir operators (which just happen to be equal).
    That this is possible is not an accident.
    In the absence of spin we need three quantum numbers to describe a state, say \(\{x_1, x_2, x_3\}\), \(\{p_1, p_2, p_3\}\), or as is most common when talking about the hydrogen atom, \(\{n, l, m\}\).
    Spherical symmetry guarantees that \(m\) doesn't appear in the final result, and hence we are reduced to two quantum numbers.
    This means that a rank two symmetry, with two Casimirs, is sufficient to obtain solutions based solely on symmetry.
    In order to include the effects of spin, as well as other relativistic corrections, we still need to solve Schrödinger's equation.
    
    
    
    
    %   Appdendix
    \appendixpage
    \begin{appendices}
        \include{parts/math-prelim-appendix}
        \include{parts/groups-appendix}
        \include{parts/manifolds-appendix}
        \include{parts/algebras-appendix}
    \end{appendices}
    
    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}

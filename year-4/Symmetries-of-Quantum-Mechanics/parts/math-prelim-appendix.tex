\chapter{Mathematical Preliminaries}
\section{Basic Mathematics}
\subsection{Notation}
\begin{ntn}{Number Sets}{}
    The set of natural numbers is
    \begin{equation}
        \naturals \coloneqq \{0, 1, 2, \dotsc\}.
    \end{equation}
    Note that the inclusion of zero in \(\naturals\) is subject to debate.
    The set of integers is denoted
    \begin{equation}
        \integers \coloneqq \{\dotsc, -2, -1, 0, 1, 2, \dotsc\}.
    \end{equation}
    The set of positive integers is denoted
    \begin{equation}
        \positiveintegers \coloneqq \{1, 2, \dotsc\}.
    \end{equation}
    The set of rational numbers is denoted
    \begin{equation}
        \rationals \coloneqq \{p/q \mid p, q \in \integers \text{ and } q \ne 0\}.
    \end{equation}
    The set of real numbers is denoted \(\reals\), and the set of complex numbers \(\complex\).
    The set of \emph{all} quaternions (as opposed to the quaternion group of order 8) is denoted \(\quaternions\).
\end{ntn}

\begin{ntn}{Sphere}{}
    The unit sphere in \(n + 1\) dimensions is
    \begin{equation}
        S^n \coloneqq \{\vv{x} \in \reals^{n+1} \mid x_1^2 + \dotsb x_{n+1}^2 = 1\}.
    \end{equation}
    Note that \(S^n\) is an \(n\)-dimensional manifold, which we view as embedded in \((n + 1)\)-dimensional Euclidean space, \(\reals^{n+1}\).
    
    What we normally call the circle is \(S^1\) and what we normally call the sphere is \(S^2\).
\end{ntn}

\begin{ntn}{Sets of Matrices}{}
    We denote the set of \(m \times n\) matrices with entries in \(\field\) (which is usually a field and usually \(\reals\) or \(\complex\)) by \(\matrices[m]{n}{\field}\).
    
    We denote the set of square \(n \times n\) matrices with entries in \(\field\) by \(\matrices{n}{\field}\).
    
    We denote the set of invertible \(n\times n\) square matrices over \(\field\), called the general linear group, by
    \begin{equation}
        \generalLinear(n, \field) = \{A \in \matrices{n}{\field} \mid \det A \ne 0\}.
    \end{equation}
    If \(\field\) is evident from context we may simply write \(\generalLinear(n)\).
    If \(V\) is an \(n\)-dimensional vector space over \(\field\) then we may also write this set as \(\generalLinear(V)\).
\end{ntn}

\begin{ntn}{Einstein Summation Convention}{}
    When two identical indices appear in the same term then they are summed over, for example,
    \begin{equation}
        x_iy_i = \sum_{i} x_iy_i.
    \end{equation} 
\end{ntn}

\subsection{Definitions}
\begin{dfn}{Function Types}{}
    Let \(\varphi \colon A \to B\).
    Then \(\varphi\) is 
    \begin{itemize}
        \item \defineindex{injective} if for all \(a, a' \in A\) \(\varphi(a) = \varphi(a')\) implies \(a = a'\),
        \item \defineindex{surjective} if for all \(b \in B\) there exists \(a \in A\) such that \(\varphi(a) = b\), and
        \item \defineindex{bijective} if \(\varphi\) is both injective and surjective.
    \end{itemize}
    A function is invertible if and only if it is bijective.
\end{dfn}

\begin{figure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \tikzsetnextfilename{injective-function}
        \begin{tikzpicture}
            \foreach \y in {-0.5, 0, 0.5} {
                \fill (2, \y) circle [radius = 0.05cm];
            }
            \foreach \y in {-0.25, 0.25} {
                \fill (0, \y) circle [radius = 0.05cm];
            }
            \node [right] at (2, -0.5) {\(c\)};
            \node [right] at (2, 0) {\(b\)};
            \node [right] at (2, 0.5) {\(a\)};
            \node [left] at (0, -0.25) {\(2\)};
            \node [left] at (0, 0.25) {\(1\)};
            \draw (-0.1, 0) circle [x radius = 0.5, y radius = 0.75];
            \draw (2.1, 0) circle [x radius = 0.5, y radius = 1];
            
            \draw [very thick, highlight, ->] (0, 0.25) to[bend left, looseness=0.75] (2, 0.5);
            \draw [very thick, highlight, ->] (0, -0.25) to[bend right, looseness=0.75] (2, -0.5);
        \end{tikzpicture}
        \caption{An injective function, \(f \colon \{1, 2\} \to \{a, b, c\}\). Note that \(f(x) \ne b\) for any \(x \in \{1, 2\}\) and so the function fails to be surjective.}
    \end{subfigure}
    \hspace{0.05\textwidth}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \tikzsetnextfilename{surjective-function}
        \begin{tikzpicture}
            \foreach \y in {-0.5, 0, 0.5} {
                \fill (0, \y) circle [radius = 0.05cm];
            }
            \foreach \y in {-0.25, 0.25} {
                \fill (2, \y) circle [radius = 0.05cm];
            }
            \node [left] at (0, -0.5) {\(3\)};
            \node [left] at (0, 0) {\(2\)};
            \node [left] at (0, 0.5) {\(1\)};
            \node [right] at (2, -0.25) {\(b\)};
            \node [right] at (2, 0.25) {\(a\)};
            \draw (-0.1, 0) circle [x radius = 0.5, y radius = 1];
            \draw (2.1, 0) circle [x radius = 0.5, y radius = 0.75];
            
            \draw [very thick, highlight, ->] (0, 0.5) to[bend left, looseness=0.75] (2, 0.25);
            \draw [very thick, highlight, ->] (0, 0) to[bend right, looseness=0.75] (2, 0.25);
            \draw [very thick, highlight, ->] (0, -0.5) to[bend right, looseness=0.75] (2, -0.25);
        \end{tikzpicture}
        \caption{A surjective function, \(g \colon \{1, 2, 3\} \to \{a, b\}\). Note that \(g(1) = g(2)\) but \(1 \ne 2\) and so the function fails to be injective.}
    \end{subfigure}
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \tikzsetnextfilename{bijective-function}
        \begin{tikzpicture}
            \foreach \y in {-0.5, 0, 0.5} {
                \fill (0, \y) circle [radius = 0.05cm];
            }
            \foreach \y in {-0.5, 0, 0.5} {
                \fill (2, \y) circle [radius = 0.05cm];
            }
            \node [left] at (0, -0.5) {\(3\)};
            \node [left] at (0, 0) {\(2\)};
            \node [left] at (0, 0.5) {\(1\)};
            \node [right] at (2, 0.5) {\(a\)};
            \node [right] at (2, 0) {\(b\)};
            \node [right] at (2, -0.5) {\(c\)};
            \draw (-0.1, 0) circle [x radius = 0.5, y radius = 1];
            \draw (2.1, 0) circle [x radius = 0.5, y radius = 1];
            
            \draw [very thick, highlight, ->] (0, 0.5) to[bend left, looseness=0.75] (2, 0.5);
            \draw [very thick, highlight, ->] (0, 0) -- (2, 0);
            \draw [very thick, highlight, ->] (0, -0.5) to[bend right, looseness=0.75] (2, -0.5);
        \end{tikzpicture}
        \caption{A bijective function, \(h \colon \{1, 2, 3\} \to \{a, b, c\}\).}
    \end{subfigure}
    \caption{Injective, surjective, and bijective functions.}
\end{figure}

\begin{dfn}{Kernel}{}
    Given a map \(\varphi \colon A \to B\) the \defineindex{kernel} is defined as the set of elements of \(A\) which map to the trivial element of \(B\), which is the zero vector, \(\vv{0}\), if \(B\) is a vector space, or the identity if \(B\) is a group:
    \begin{equation}
        \ker\varphi \coloneqq \{a \in A \mid \varphi(a) \text{ is the trivial element of \(B\)}\} \subseteq A.
    \end{equation}
\end{dfn}

\begin{dfn}{Image}{}
    Given a map \(\varphi \colon A \to B\) the \defineindex{image} is set of \(b \in B\) for which there exists some \(a \in A\) such that \(\varphi(a) = b\):
    \begin{equation}
        \image\varphi = \varphi(A) \coloneqq \{b \in B \mid \exists \, a \in A \text{ such that } \varphi(a) = b\} \subseteq B.
    \end{equation}
\end{dfn}

\begin{dfn}{Empty Set}{}
    The \defineindex{empty set}, \(\emptyset\), is the set containing no elements.
\end{dfn}

\begin{dfn}{Kronecker Delta}{}
    The \defineindex{Kronecker delta}, \(\delta_{ij}\), is defined as
    \begin{equation}
        \delta_{ij} \coloneqq
        \begin{cases}
            1, & \text{if } i = j,\\
            0, & \text{if } i \ne j.
        \end{cases}
    \end{equation}
    Note that \(\delta_{ij}\) are the elements of the identity matrix.
\end{dfn}

\begin{dfn}{Levi-Civita Symbol}{}
    The \defineindex{Levi-Civita Symbol} in \(n\)-indices is the completely asymmetric (pseudo)tensor which is defined so that \(\varepsilon_{123\dotso n} \coloneqq 1\).
    Antisymmetry then means that \(\varepsilon_{1\dotsm i\dotsm j \dotsm n} = -\varepsilon_{1\dotsm j\dotsm i\dotsm n}\), for example \(\varepsilon_{213\dotsm n} = -1\).
    Antisymmetry also means that Levi--Civita symbol vanishes if it has repeated indices.
    
    Most commonly \(n = 3\) and
    \begin{equation}
        \varepsilon_{ijk} \coloneqq 
        \begin{cases}
            1, & \text{if } (i, j, k) = (1, 2, 3), (2, 3, 1), (3, 1, 2),\\
            -1, & \text{if } (i, j, k) = (1, 3, 2), (2, 1, 3), (3, 2, 1),\\
            0, & \text{if any index is repeated}.
        \end{cases}
    \end{equation}
\end{dfn}

\begin{dfn}{Equivalence Relations}{}
    Given two sets, \(A\) and \(B\), a \defineindex{relation}, \(R\), is a subset of \(A \times B \coloneqq \{(a, b) \mid a \in A \text{ and } b \in B\} \supseteq R\).
    We say that \(a \in A\) is related to \(b \in B\), which we denote with infix notation, \(a\mathbin{R}b\), if \((a, b) \in R\).
    
    If \(A = B\) in the above definition then we call \(R\) a \defineindex{binary!relation} on \(A\).
    
    A relation, \(\sim\), on a set \(A\) is a binary relation on \(A\) such that the following axioms hold for all \(a, b, c \in A\)
    \begin{itemize}
        \item \(\sim\) is \defineindex{reflexive}, so \(a \sim a\).
        \item \(\sim\) is \define{symmetric}\index{symmetric!relation}, so if \(a \sim b\) then \(b \sim a\).
        \item \(\sim\) is \defineindex{transitive}, so if \(a \sim b\) and \(b \sim c\) then \(a \sim c\).
    \end{itemize}
    
    An \defineindex{equivalence class} of an element \(a \in A\) under some equivalence relation, \(\sim\), is the set
    \begin{equation}
        [a] \coloneqq \{x \mid a \sim x\}.
    \end{equation}
    We call elements of \([a]\) representatives of the equivalence class.
    We denote the set of all equivalence classes by \(A/\sim\).
\end{dfn}

\begin{exm}{Equivalence Relations}{}
    \(=\) is the prototypical equivalence relation.
    
    Congruence modulo \(m \in \positiveintegers\) is an equivalence relation on \(\reals\).
    
    \(\sim\) defined by \(z\sim w\) if \(\abs{z} = \abs{w}\) is an equivalence relation on \(\complex\).
    
    \(\sim\) defined by \(\vv{v} \sim \vv{u}\) if \(\vv{u}\) and \(\vv{v}\) are parallel is an equivalence relation on \(\reals^n\).
\end{exm}

\begin{exm}{Isomorphism}{exm:isomorphism is equivalence relation}
    Isomorphisms, as defined in the text, are equivalence relations:
    \begin{itemize}
        \item Let \(A\) be a group, then the identity function, \(\mathrm{id}_A\colon A \to A\) defined by \(\mathrm{id}_A(a) = a\) for all \(a \in A\) is an isomorphism since \(\mathrm{id}_A(aa') = aa' = \mathrm{id}_A(a)\mathrm{id}_A(a')\) and clearly \(\mathrm{id}_A\) is invertible, and is its own inverse.
        \item Let \(A\) and \(B\) be isomorphic groups.
        Then there exists some bijection \(\varphi \colon A \to B\) such that \(\varphi(aa') = \varphi(a)\varphi(a')\).
        Since \(\varphi\) is a bijection \(\varphi^{-1}\colon B \to A\) exists and is also a bijection.
        Applying the inverse to both sides of the defining relation we have \(\varphi^{-1}(\varphi(aa')) = \varphi^{-1}(\varphi(a)\varphi(a'))\).
        Since \(\varphi\) is surjective any element of \(B\) can be written in the form \(b = \varphi(a)\) for some \(a \in A\) and so it follows that \(\varphi^{-1}(\varphi(aa')) = \varphi^{-1}(b)\varphi(b')\) where \(b, b' \in B\) are arbitrary, and we choose \(a, a' \in A\) to be such that \(b = \varphi(a)\) and \(b' = \varphi(a')\).
        From the defining relation for \(\varphi\) we know that \(\varphi(aa') = \varphi(a)\varphi(a') = bb'\).
        It follows that \(\varphi^{-1}(\varphi(aa')) = \varphi^{-1}(bb') = \varphi^{-1}(b)\varphi^{-1}(b')\), which means that \(B \isomorphic A\).
        \item Let \(A\), \(B\), and \(C\) be groups such that \(A \isomorphic B\) and \(B \isomorphic C\).
        Then there exists isomorphisms \(\varphi \colon A \to B\) and \(\psi\colon B \to C\).
        We claim that \(\psi\circ \varphi \colon A \to C\) is an isomorphism.
        Clearly \(\psi\circ \varphi\) is bijective, since \(\varphi^{-1}\circ \psi^{-1}\) is its inverse, as can be seen by considering \((\varphi^{-1}\circ \psi^{-1})((\psi\circ \varphi)(a)) = \varphi^{-1}(\psi^{-1}(\psi(\varphi(a)))) = \varphi^{-1}(\varphi(a)) = a\) for all \(a \in A\).
        
        It remains to show that \(\psi \circ \varphi\) is a homomorphism.
        To do so consider \((\psi \circ \varphi)(aa') = \psi(\varphi(aa')) = \psi(\varphi(a)\varphi(a'))\), which follows since \(\varphi\) is an isomorphism.
        Now write \(\varphi(a) = b\) and \(\varphi(a') = b'\), where \(b, b' \in B\).
        We then have \((\psi \circ \varphi)(aa') = \psi(bb') = \psi(b)\psi(b')\), which follows since \(\psi\) is an isomorphism.
        We then have \((\psi \circ \varphi)(aa') = \psi(b)\psi(b') = \psi(\varphi(b))\psi\varphi(b') = (\psi \circ \varphi)(b)(\psi \circ \varphi)(b')\), and so \(\psi\circ\varphi\) is a bijective homomorphism and hence an isomorphism, meaning \(A \isomorphic C\).
    \end{itemize}
\end{exm}

\section{Linear Algebra}
\subsection{Vectors}
\begin{dfn}{Vector Space}{}
    A vector space, \(V\), over a field, \(\field\), is a set of vectors, \(V\), with two operations, \(\cdot\colon \field \times V \to V\), known as scalar multiplication, and \(+\colon V\times V \to V\), known as vector addition, which are defined such that the following hold for all \(\vv{u}, \vv{v}, \vv{w} \in V\) and \(k, k' \in \field\):
    \begin{enumerate}
        \item \define{Associativity}\index{accociative}: \(\vv{u} + (\vv{v} + \vv{w}) = (\vv{u} + \vv{v}) + \vv{w}\),
        \item There exists a \define{zero vector}, \(\vv{0} \in V\), such that \(\vv{u} + \vv{0} = \vv{u}\).
        \item There exists \(-\vv{u} \in V\) such that \(\vv{u} + (-\vv{u}) = \vv{0}\).
        We write this as \(\vv{u} - \vv{u}\) for short.
        \item \define{Commutativity}\index{commutative}: \(\vv{u} + \vv{v} = \vv{v} + \vv{u}\).
        \item Distributivity of scalar multiplication over vector addition \(k(\vv{u} + \vv{v}) = k\vv{u} + k\vv{v}\).
        \item Distributivity of scalar multiplication over field addition \((k + k')\vv{u} = k\vv{u} + k'\vv{u}\).
        \item Compatibility of field and scalar multiplication \((kk')\vv{u} = k(k'\vv{u})\).
        \item \(1\vv{u} = \vv{u}\) where \(1\) is the multiplicative identity of \(\field\).
    \end{enumerate}
    Note that the first three axioms make \((V, +)\) a group and the fourth makes it Abelian.
\end{dfn}

\begin{dfn}{Hilbert Space}{}
    A \defineindex{Hilbert space}, \(\hilbert\), is a vector space over either \(\reals\) or \(\complex\), equipped with an inner product that induces a complete metric.
    We shall assume a complex Hilbert space, for a real Hilbert space simply ignore any complex conjugates and replace \(\complex\) with \(\reals\).
    
    An \defineindex{inner product}\index{\(\innerprod{-}{-}\), inner product} is a function \(\innerprod{-}{-} \colon \hilbert \times \hilbert \to \complex\) such that for all \(\vv{u}, \vv{v}, \vv{w} \in \hilbert\) and \(k, k' \in \complex\)
    \begin{enumerate}
        \item \(\innerprod{-}{-}\) is linear in its second argument, that is
        \begin{equation}
            \innerprod{\vv{u}}{k\vv{v} + k'\vv{w}} = k\innerprod{\vv{u}}{\vv{v}} + k'\innerprod{\vv{u}}{\vv{w}}.
        \end{equation}
        \item \(\innerprod{-}{-}\) is conjugate symmetric:
        \begin{equation}
            \innerprod{\vv{u}}{\vv{v}} = \innerprod{\vv{v}}{\vv{u}}^*.
        \end{equation}
        \item \(\innerprod{-}{-}\) is positive definite, that is \(\innerprod{\vv{u}}{\vv{u}} \ge 0\) with equality only if \(\vv{u} = \vv{0}\).
    \end{enumerate}
    \begin{wrn}
        Mathematicians often define an inner product to be linear in its first argument, so
        \begin{equation}
            \innerprod{k\vv{u} + k'\vv{v}}{\vv{w}} = k\innerprod{\vv{u}}{\vv{w}} + k'\innerprod{\vv{v}}{\vv{w}}.
        \end{equation}
    \end{wrn}
    The first two axioms are sometimes combined to give an extra axiom that \(\innerprod{-}{-}\) is conjugate linear in its first argument (or second if we follow the other convention).
    That is
    \begin{equation}
        \innerprod{k\vv{u} + k'\vv{v}}{\vv{w}} = k^*\innerprod{\vv{u}}{\vv{w}} + k'^*\innerprod{\vv{v}}{\vv{w}}.
    \end{equation}
    We can then define a \defineindex{norm}\index{\(\norm{-}\), norm} on \(\hilbert\) by \(\norm{\vv{u}} \coloneqq \sqrt{\innerprod{\vv{u}}{\vv{u}}}\).
    
    The final condition for \(\hilbert\) to be a Hilbert space is completeness.
    Namely, that if the series \(\sum_{n=0}^{\infty} \vv{u_n}\) converges absolutely, so that \(\sum_{n=0}^{\infty} \norm{\vv{u_n}}\) converges to a finite value then the original series, \(\sum_{n=0}^{\infty} \vv{u_n}\), converges to some vector in \(\hilbert\).
\end{dfn}

\begin{exm}{}{}
    The set of \(n\)-tuples of complex numbers, \(\complex^n\), is a Hilbert space over \(\complex\) with the inner product
    \begin{equation}
        \innerprod{\vv{u}}{\vv{v}} = \innerprod{(u_1, \dotsc, u_n)}{(v_1, \dotsc, v_n)} \coloneqq \sum_{i=1}^n u_i^*v_i = u_i^*v_i
    \end{equation}
    where in the last term we use the Einstein summation convention.
\end{exm}

\begin{exm}{Functions}{}
    The space of square integrable functions on \(X \subseteq \reals^n\) forms a Hilbert space, denoted \(L^2(X)\).
    A function, \(f\colon X \to \complex\), is square integrable if\footnote{For this space to be complete (a requirement for Hilbert spaces) this must be a Lebesgue integral but in physics functions are usually nice enough that we can use the standard Riemann integral, which agrees with the Lebesgue integral when both exists.}
    \begin{equation}
        \int_{X} \abs{f(x)}^2 \dd{x}
    \end{equation}
    exists and is finite.
    For example, the function defined by \(f(x) = e^{-x^2}\) is an element of \(L^2(\reals)\).
    
    Given \(f, g \in L^2(X)\) we define the inner product in this space to be
    \begin{equation}
        \innerprod{f}{g} \coloneqq \int_{X} f^*(x)g(x) \dd{x}.
    \end{equation}
    \begin{rmk}
        Another subtly that arises here is that we actually need to consider elements of \(L^2(X)\) to be equivalence classes of functions which are equal almost everywhere (meaning that the measure of the set of points where they are not equal is zero).
        Otherwise, we may have some functions such that \(\innerprod{f}{g} = 0\) but \(f \ne g\) since \(f\) and \(g\) disagree on some set of points with a vanishing measure.
        We say that we are considering the functions mod the equivalence relation of being equal almost everywhere.
    \end{rmk}
    
    This is an important example since we can often identify \enquote{square integrable functions} with \enquote{possible wave functions}, since square-integrability is a requirement for us to be able to normalise a wave function, which we do so by the procedure
    \begin{equation}
        \psi \to \frac{\psi}{\norm{\psi}} = \frac{\psi}{\sqrt{\innerprod{\psi}{\psi}}} = \left( \int \abs{\psi(x)}^2 \dd{x} \right)^{-1/2} \psi.
    \end{equation}
    This only makes sense if \(\int \abs{\psi(x)}^2 \dd{x}\) is finite (and nonzero).
\end{exm}

\begin{ntn}{Bra-Ket Notation}{}
    In physics, particularly in quantum mechanics, we often use \defineindex{bra-ket notation}, developed by Dirac.
    We identify vectors, \(\vv{u}\), with \define{kets}\index{ket}\index{\(\ket{-}\), ket}, \(\ket{u}\), and dual vectors, \(\vv{v}\), with \define{bras}\index{bra}\index{\(\bra{-}\), bra}, \(\bra{v}\).
    The inner product \(\innerprod{\vv{u}}{\vv{v}}\) is then written \(\braket{v}{u}\)\index{\(\braket{-}{-}\), inner product}.
    This is the notation we will use for most of this course.
\end{ntn}

\begin{dfn}{Linear Operator}{}
    Given two vector spaces, \(V\) and \(W\), over some field, \(\field\), a function, \(f \colon V \to W\), is said to be a linear operator if for \(\vv{u}, \vv{v} \in V\) and \(k \in \field\) we have
    \begin{equation}
        f(\vv{u} + \vv{v}) = f(\vv{u}) + f(\vv{v}), \qqand f(k\vv{u}) = kf(\vv{u}).
    \end{equation}
    
    Instead of the function notation \(f(\vv{u})\) we typically use a multiplicative notation, \(A\vv{u}\), which is due to the fact that if \(V\) is finite dimensional then we can choose a basis and represent a linear map by a matrix.
    Using bra-ket notation also we have
    \begin{equation}
        A(\ket{u} + \ket{v}) = A\ket{u} + A\ket{v}, \qqand A(k\ket{u}) = kA\ket{u}.
    \end{equation}
    
    An operator is \defineindex{antilinear} if
    \begin{equation}
        A(\ket{u} + \ket{v}) = A\ket{u} + A\ket{v}, \qqand A(k\ket{u}) = k^*A\ket{u}.
    \end{equation}
    An example of such an operator is the time reversal operator, \(T\), which takes \(t \to -t\).
\end{dfn}

For simplicity from now on we will consider the complex vector space \(V = \complex^N\).
This is \(N\)-dimensional (\(\dim V = N\)), which we take to be finite, although many of these ideas work, possibly with slight modification, for infinite dimensional vector spaces.
Most of the time we will also consider linear operators from \(V\) to \(V\), since this is far more common in practice that linear operators from \(V\) to some different vector space, \(W\).

\begin{dfn}{Basis}{}
    Given a vector space, \(V\), we say that \(\{\ket{e_i}\}\) is a \defineindex{linearly independent} set if the only solution to
    \begin{equation}
        \lambda_i\ket{e_i} = \ket{0},
    \end{equation}
    where \(\ket{0}\) is the zero vector, is \(\lambda_i = 0\) for all \(i\).
    
    We say that \(\{\ket{e_i}\}\) is a \defineindex{basis} for \(V\) if \(\{\ket{e_i}\}\) is a linearly independent set and spans \(V\).
    That is given some \(\ket{u} \in V\) we can write
    \begin{equation}
        \ket{u} = u_i\ket{e_i}
    \end{equation}
    for some \(u_i \in \complex\).
    
    The number of vectors in a basis is the \defineindex{dimension} of the vector space, denoted \(\dim V\)\index{dim@\(\dim\), dimension}.
    
    We say that two vectors, \(\ket{u}, \ket{v} \in V\), are \define{orthogonal}\index{orthogonal!vectors} (with respect to some inner product) if \(\braket{u}{v} = 0\).
    
    We say that a vector, \(\ket{u} \in V\), is normalised if \(\norm{u} = \sqrt{\braket{u}{u}} = 1\).
    
    We say that \(\{\ket{e_i}\}\) is an orthonormal basis for \(V\) if it is a basis for \(V\), \(\ket{e_i}\) is normalised for all \(i\) and all of the basis vectors are mutually orthogonal.
    These last two conditions are summarised by requiring that
    \begin{equation}
        \braket{e_i}{e_j} = \delta_{ij}.
    \end{equation}
\end{dfn}

\begin{dfn}{Completeness Relation}{}
    Given a vector space, \(V\), and an orthonormal basis, \(\{\ket{e_i}\}\), we can write the identity operator, \(\ident\), as
    \begin{equation}
        \ident = \sum_{i=1}^{N} \ket{e_i}\bra{e_i}.
    \end{equation}
    Recall that the identity operator is defined such that\index{\(\ident\), identity matrix}
    \begin{equation}
        \ident\ket{u} = \ket{u}
    \end{equation}
    for all \(\ket{u} \in V\).
\end{dfn}

\begin{dfn}{Partition of the Identity}{}
    A \defineindex{projection operator}, \(P_i\), is an operator satisfying
    \begin{equation}
        P_iP_j = \delta_{ij}P_i, \qqand P_i^\hermit = P_i.
    \end{equation}
    A \defineindex{partition of the identity} is a collection of projection operators, \(\{P_j\}\), such that
    \begin{equation}
        \ident = \sum_{j=1}^{N_P} P_j
    \end{equation}
    where \(N_P = \abs{\{P_j\}}\) is the number of projection operators in the partition.
    We can write each partition operator as
    \begin{equation}
        P_j = \sum_{i=1}^{N_j} \ket{e_i}\bra{e_i}
    \end{equation}
    where \(N_j\) are such that
    \begin{equation}
        \dim V = N = \sum_{j=1}^{N_P}N_j.
    \end{equation}
\end{dfn}

\begin{dfn}{Matrix Element}{}
    Given a linear operator, \(A \colon V \to V\), and an orthonormal basis, \(\{\ket{e_i}\}\), we define the \define{matrix elements}\index{matrix element} to be
    \begin{equation}
        A_{ij} \coloneqq \braket{e_i}{Ae_j} = \bra{e_i}A\ket{e_j}
    \end{equation}
    where \(A\) is understood to act on the right and \(\ket{Ae_j} \coloneqq A\ket{e_j}\).
    
    If we know the matrix elements of \(A\) we can reconstruct \(A\) using
    \begin{equation}
        A = \sum_{i=1}^{N}\sum_{j=1}^{N} A_{ij}\ket{e_i}\bra{e_j}.
    \end{equation}
\end{dfn}

\begin{dfn}{Eigenvalues and Eigenvectors}{}
    Given a linear operator \(A\), we call \(\ket{v_i}\) an \defineindex{eigenvector} and \(\lambda_i\in\complex\) an \defineindex{eigenvalue} if
    \begin{equation}
        A\ket{v_i} = \lambda_i\ket{v_i}.
    \end{equation}
    There are \(N\) solutions to this, which follows from the \define{characteristic polynomial}\index{characteristic!polynomial}, \(\det(A - \lambda\ident) = 0\), having \(N\) solutions, which in turn follows from the fundamental theorem of algebra.
\end{dfn}

\subsection{Matrices}
\begin{dfn}{Transpose and Hermitian Conjugate}{}
    Given a matrix, \(A\), with matrix elements \(A_{ij}\), the \defineindex{transpose}\index{\(-^\trans\), transpose} matrix, \(A^\trans\), has matrix elements \(A^\trans_{ij} = A_{ji}\).
    
    A matrix is \defineindex{symmetric} if \(A^\trans = A\), or \defineindex{antisymmetric} if \(A^\trans = -A\).
    
    Given a matrix, \(A\), with matrix elements \(A_{ij}\), the \defineindex{Hermitian conjugate}\index{\(-^\hermit\), Hermitian conjugate}, \(A^\hermit\), has matrix elements \(A^\hermit_{ij} = A_{ji}^*\).
    Here \(^*\) denotes the \defineindex{complex conjugate}\index{\(-^*\), complex conjugate}, so \((x + iy)^* = x - iy\) and \((r\e^{i\vartheta})^* = r\e^{-\vartheta}\) for \(x, y, r, \vartheta \in \reals\).
    That is the Hermitian conjugate is the complex conjugate of the transpose.
    
    A matrix is \defineindex{Hermitian} if \(A^\hermit = A\), or \defineindex{anti-Hermitian} if \(A^\hermit = -A\).
\end{dfn}

\begin{lma}{}{}
    The eigenvalues of a Hermitian matrix are real.
    \begin{proof}
        Let \(A\) be a Hermitian matrix and \(\vv{v}\) an eigenvector with nonzero eigenvalue \(\lambda\).
        Note that this means \(\vv{v}\) is nonzero.
        If \(0\) is an eigenvalue of \(A\) then this is real, so we need not consider this case further.
        By definition \(A\vv{v} = \lambda\vv{v}\).
        Taking the Hermitian conjugate of both sides we get \(\vv{v}^\hermit A^\hermit = \lambda^*\vv{v}^\hermit\), where we have used \((XY)^\hermit = Y^\hermit X^\hermit\).
        Multiplying both sides on the right by \(\vv{v}\) we get \(\vv{v}^\hermit A \vv{v} = \lambda^*\vv{v}^\hermit \vv{v}\).
        Identifying \(A\vv{v} = \lambda\vv{v}\) on the left-hand side this becomes \(\vv{v}^\hermit \lambda \vv{v} = \lambda\vv{v}^\hermit \vv{v} = \lambda^*\vv{v}^\hermit \vv{v}\).
        It follows that we must have \(\lambda = \lambda^*\), which means we must have \(\lambda \in \reals\).
    \end{proof}
\end{lma}

We can choose the eigenvalues of a Hermitian matrix to be orthonormal, and hence they form a basis for the vector space.
In this basis the matrix will be diagonal and the values on the diagonal are simply the eigenvalues.

Given a Hermitian matrix, \(A\), with eigenvalues \(\lambda_i\) and corresponding eigenvectors \(\ket{v_i}\) we can write this matrix as
\begin{equation}
    A = \sum_{i=1}^{N} \lambda_i\ket{v_i}\bra{v_i}.
\end{equation}
This is diagonalised by the transformation \(V^\hermit A V\) where
\begin{equation}
    V = \sum_{i=1}^{N} \ket{v_i}\bra{e_i}
\end{equation}
where \(\ket{e_i}\) are the basis vectors in the original basis.
It is easy to see that this transform gives the desired result:
\begin{align}
    V^\hermit AV &= \underbrace{\ket{e_i}\bra{v_i}}_{=V^\hermit}  \underbrace{(\lambda_j \ket{v_j}\bra{v_j})}_{=A}\underbrace{\ket{v_k}\bra{e_k}}_{=V}\\
    &= \lambda_j \ket{e_i}\braket{v_i}{v_j}\braket{v_j}{v_k}\bra{e_k}\\
    &= \lambda_j \delta_{ij}\delta_{jk} \ket{e_i}\bra{e_k}\\
    &= \lambda_i \ket{e_i}\bra{e_i}
\end{align}
This last term is just a diagonal matrix with the eigenvalues, \(\lambda_i\), as the diagonal elements, which is exactly what we wanted.

For non-Hermitian matrices it is possible that the eigenvalues aren't linearly independent.
In this case the best we can do is Jordan normal form where the eigenvalues are on the diagonal and all other entries are either zero or one for elements in the subspace of degenerate eigenvalues.

\begin{dfn}{Inverse}{}
    The \defineindex{inverse}\index{\(-^{-1}\), inverse} of a matrix, \(A\), is the matrix \(A^{-1}\) such that \(A^{-1}A = AA^{-1} = \ident\).
    Such a matrix exists only if the determinant is non-zero.
\end{dfn}

An equivalent requirement for \(A^{-1}\) to exist is for \(A\) to have no zero eigenvalues.
For a Hermitian matrix the inverse in the eigenbasis is simply \(A^{-1} = \diag(1/\lambda_1, \dotsc, 1/\lambda_N)\).

\begin{dfn}{Orthogonal and Unitary}{}
    A matrix, \(O\), is \define{orthogonal}\index{orthogonal!matrix} if \(O^\trans O = \ident\), that is \(O^\trans = O^{-1}\).
    
    A matrix, \(U\), is \defineindex{unitary} if \(U^\hermit U = \ident\), that is \(U^\hermit = U^{-1}\).
\end{dfn}

The following holds:
\begin{equation}
    \braket{u}{Av} = \bra{u}A\ket{v} = \braket{A^\hermit u}{v}.
\end{equation}
For a unitary matrix, \(U\), this implies
\begin{equation}
    \braket{Uu}{Uv} = \bra{u}U^\hermit U\ket{v} = \bra{u}\ident \ket{v} = \braket{u}{v}.
\end{equation}
We say that unitary transforms preserve the inner product, or that the inner product is invariant under unitary transforms.

\begin{dfn}{Trace}{}
    The \defineindex{trace}\index{tr@\(\tr\), trace} of a matrix, \(A\) is
    \begin{equation}
        \tr A \coloneqq \sum_{i} \bra{e_i} A_i \ket{i} = A_{ii}
    \end{equation}
    where in the last term we are using the Einstein summation convention to sum over \(i\).
\end{dfn}

The trace of a matrix is simply the sum of its eigenvalues, this doesn't just hold for Hermitian matrices.

The trace is cyclic, meaning \(\tr(AB) = \tr(BA)\), \(\tr(ABC) = \tr(BCA) = \tr(CAB)\), etc.

The trace is linear, meaning \(\tr(k A) = k\tr(A)\) for scalar \(k\).

\(\innerprod{A}{B} \coloneqq \tr(A^\hermit B)\) is an inner product on the vector space of matrices.
This is called the \defineindex{Gram--Schmidt inner product}.

\begin{dfn}{Determinant}{}
    The \defineindex{determinant}\index{det@\(\det\), determinant}\index{\(\abs{-}\), determinant} of a matrix, \(A\), is 
    \begin{equation}
        \det A = \abs{A} = \coloneqq \varepsilon_{i_1\dotso i_N} A_{1i_1}\dotsm A_{Ni_N}
    \end{equation}
    with summation over indices implied.
\end{dfn}

The determinant of a matrix is the product of its eigenvalues.

The determinant of a product is the product of the determinants:
\begin{equation}
    \det(AB) = \det(A)\det(B) = \det(B)\det(A) = \det(BA).
\end{equation}

\begin{dfn}{Diagonal}{}
    A matrix, \(A\), is \defineindex{diagonal} if \(A_{ij} = 0\) for \(i \ne j\).
    
    A matrix, \(A\), is \defineindex{block diagonal} if it can be written in the form
    \begin{equation}
        A = 
        \begin{pmatrix}
            A_1 & 0 & 0 & \dots & 0\\
            0 & A_2 & 0 & \dots & 0\\
            \vdots & \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & 0 & 0 & A_n
        \end{pmatrix}
    \end{equation}
    where \(A_i\) are square matrices and the \(0\)s represent matrices where all elements are zero.
\end{dfn}    

\subsection{Combining Vector Spaces}
\begin{dfn}{Direct Sum}{dfn:direct sum}
    Given vector spaces \(V\) and \(W\) we call \(V\directsum W\)\index{\(\directsum\), direct sum} the \defineindex{direct sum}.
    It is defined by associating with each pair of vectors, \(\ket{v_i} \in V\) and \(\ket{w_a} \in W\), a vector \(\ket{v_i} \directsum \ket{w_i} = \ket{v_i \directsum w_a} \in V \directsum W\) and extending the inner product to
    \begin{equation}
        \braket{v_i \directsum w_a}{v_j \directsum w_b}_{V\directsum W} \coloneqq \braket{v_i}{v_j}_{V} + \braket{w_a}{w_b}_{W}
    \end{equation}
    where the subscripts denote which vector space the inner product is in.
    Note that the notation \(\ket{v \directsum w}\) is non-standard.
    
    The dimension of \(V \directsum W\) is
    \begin{equation}
        \dim(V \directsum W) = \dim V + \dim W.
    \end{equation}
    
    Given \(A \in \generalLinear(V)\) and \(B \in \generalLinear(W)\) the direct sum, \(A \directsum B\), acts on \(\ket{v} \directsum \ket{w} \in V \directsum W\) as
    \begin{equation}
        (A \directsum B)(\ket{v} \directsum \ket{w}) \coloneqq (A\ket{v}) \directsum (B\ket{w}).
    \end{equation}
    This shows we can think of \(V \directsum W\) as a \((\dim V + \dim W)\)-dimensional vector space with operators represented by \((v + w) \times (v + w)\) block diagonal matrices:
    \begin{equation}
        A \directsum B = 
        \begin{pmatrix}
            A & 0\\
            0 & B
        \end{pmatrix}
        .
    \end{equation}
    We then think of \(\ket{v} \directsum \ket{w} \in V \directsum W\) as \((v_1, \dotsc, v_{\dim V}, w_1, \dotsc, w_{\dim W})\).
\end{dfn}

An important question is can a given vector space be written as a direct sum of vector spaces, this occurs when considering the irreducibility of representations.

\begin{dfn}{Direct Product}{}
    Given vector spaces \(V\) and \(W\) we call \(V \directproduct W\)\index{\(\directproduct\), direct product} the \defineindex{direct product}.
    It is defined by associating with each pair of vectors, \(\ket{v_i} \in V\) and \(\ket{w_a} \in W\), a vector \(\ket{v_i} \directproduct \ket{w_a} = \ket{v_i \directproduct w_a} \in V \directproduct W\) and extending the inner product to
    \begin{equation}
        \braket{v_i \directproduct w_a}{v_j \directproduct w_b}_{V\directproduct W} \coloneqq \braket{v_i}{v_j}_{V}\braket{w_a}{w_b}
    \end{equation}
    where the subscripts denote which vector space the inner product is in.
    Note that the notation \(\ket{v\directproduct w}\) is non-standard.
    
    The dimension of \(V\directproduct W\) is
    \begin{equation}
        \dim(V\directproduct W) = \dim(V) \dim(W).
    \end{equation}
    
    Given \(A \in \generalLinear(V)\) and \(B \in \generalLinear(W)\) the direct product, \(A \directproduct B\), acts on \(\ket{v}\directproduct \ket{w} \in V\directproduct W\) as
    \begin{equation}
        (A\directproduct B)(\ket{v}\directproduct \ket{w}) = (A\ket{v})\directproduct(B\ket{w}).
    \end{equation}
\end{dfn}

\begin{app}{}{}
    In quantum mechanics we can combine states from different Hilbert spaces representing different properties with direct products.
    For example, given an electron wave function with a spatial component and a spin component the direct product of these gives the state of the electron.
\end{app}

The direct product plays a role in representation theory in terms of what we will call Kronecker products.
These can be used to obtain all representations from the fundamental representations.
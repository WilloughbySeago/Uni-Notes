\documentclass[fleqn]{NotesClass}

\strictpagecheck  % check what the page is before typesetting marginpars, guarantees they will appear on the correct side

% Hyphenation
\hyphenation{Schr\"o-ding-er}

%% Packages
\usepackage{csquotes}
\usepackage{cancel}
\usepackage{siunitx}
\usepackage{tensor}
\usepackage{slashed}

% Tikz stuff
\usepackage{tikz}
\tikzset{>=latex}
% external
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
%\tikzexternaldisable
% other libraries
\usetikzlibrary{decorations.markings, decorations.pathreplacing}
\usetikzlibrary{angles, quotes}

% Tikz Feynman
\usepackage[compat=1.1.0]{tikz-feynman}
\tikzfeynmanset{warn luatex=false}

% References, should be last things loaded
\usepackage{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{mathtools}
\usepackage{NotesBoxes}
\usepackage{NotesMaths}


% Title page info
\title{Quantum Theory}
\author{Willoughby Seago}
\date{September 20, 2021}
% \subtitle{}
% \subsubtitle{}

% Highlight colour
\definecolor{highlight}{HTML}{990000}
\definecolor{darker}{HTML}{370000}
\definecolor{highlightpurple}{HTML}{990085}
\definecolor{complementary}{HTML}{009999}

% Commands
% Maths
\newcommand*{\hilbert}{\mathcal{H}}
\undef{\Re}
\undef{\Im}
\DeclareMathOperator{\Re}{Re}
\DeclareMathOperator{\Im}{Im}
\newcommand*{\e}{\mathrm{e}}
\newcommand*{\idop}{\hat{1}}
\def\OLDvector\vector
\undef{\vector}
\renewenvironment*{vector}{\begin{pmatrix}}{\end{pmatrix}}
\newenvironment*{mat}{\begin{pmatrix}}{\end{pmatrix}}
\newcommand*{\hermit}{\dagger}
\newcommand*{\trans}{\top}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1, #2}
\newcommand{\ident}{I}
\let\OLDdd\dd
\RenewDocumentCommand{\dd}{ o m }{
    \IfValueTF{#1}{
        \OLDdd{^{#1} #2}
    }{
        \OLDdd{#2}
    }
}
\NewDocumentCommand{\DL}{ o m }{
    \IfValueTF{#1}{
        \mathcal{D}^{#1}{#2}
    }{
        \mathcal{D}{#2}
    }
}
\NewDocumentCommand{\DD}{ o m }{
    \IfValueTF{#1}{
        \mukern2mu\mathcal{D}^{#1}{#2}
    }{
        \mkern2mu\mathcal{D}{#2}
    }
}
\newcommand*{\order}{\mathop{\mathcal{O}}}
\newcommand*{\lagrangian}{L}
\newcommand*{\hamiltonian}{H}
\newcommand*{\cl}{\mathrm{cl}}
\DeclareMathOperator{\sinc}{sinc}
\newcommand*{\timeorder}{T}
\newcommand*{\schrodingerPicture}{\mathrm{S}}
\newcommand*{\heisenbergPicture}{\mathrm{H}}
\DeclareMathOperator{\SE}{SE}
\DeclareMathOperator{\Res}{Res}
\newcommand*{\boltzmann}{k_{\mathrm{B}}}
\DeclareMathOperator{\tr}{tr}
\newcommand*{\SIunits}{\mathrm{SI}}
\newcommand*{\natunitpic}{\tikzexternaldisable\tikz[baseline=(a.base)]{\node[draw=darker, fill=highlight!30, thick, text=darker, rounded corners] (a) {\(c = \hbar = 1\)}}\tikzexternalenable}
\newcommand*{\natunit}{\tag*{\natunitpic\hspace{1em}(\refstepcounter{equation}\theequation)}}
\newcommand*{\dalembertian}{\partial^2}
\DeclareMathOperator{\sech}{sech}
\DeclarePairedDelimiterX{\anticommutator}[2]{\{}{\}}{#1 , #2}
\newcommand*{\rightgrad}{\overset{\smash{\scalebox{0.5}{\raisebox{-0.5ex}{\(\rightarrow\)}}}}{\nabla}}
\newcommand*{\leftgrad}{\overset{\smash{\scalebox{0.5}{\raisebox{-0.5ex}{\(\leftarrow\)}}}}{\nabla}}

% Include
\includeonly{parts/appendix-identities, parts/appendix-formal-definitions, parts/appendix-integrals, parts/appendix-combinatorial-factor}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/double-slit-result.pdf} 
    \tableofcontents
    \mainmatter
    
    \chapter{Introduction}
    This course builds on quantum mechanics knowledge from previous courses.
    In particular we assume knowledge of the contents of \enquote{principles of quantum mechanics}, notes for this course can be found here: \url{https://github.com/WilloughbySeago/Uni-Notes}.
    As well as this knowledge of the basics of Lagrangian dynamics and complex analysis is assumed.
    
    The goal of this course is to develop the quantum mechanics formalism with the goal of later studying quantum field theory (QFT)\glossary[acronym]{QFT}{quantum field theory}.
    We won't spend much time thinking about \enquote{real world problems}, whatever that means in quantum mechanics.
    
    We will take the approach of theoretical physicists being only as rigorous as necessary for the problem at hand but we also won't worry ourselves too much about real world details.
    In doing so we will probably annoy both experimental physicists and mathematicians in equal parts, which hopefully cancels out to keep everyone happy.
    
    \section{A Brief History of Quantum Mechanics}
    In 1925 Werner Heisenberg, Max Born, and Pascual Jordan developed the key ideas of matrix mechanics, the first logically consistent formalism of quantum mechanics.
    Shortly after this, still in 1925 Erwin Schr\"odinger developed wave mechanics.
    Then in 1926 Schr\"odinger and Paul Dirac went on to prove that matrix mechanics and wave mechanics are equivalent.
    Both of these formalisms are based on Hamiltonian mechanics.
    
    Later on, in 1942, Richard Feynman, developing earlier work of Dirac's, came up with a third formalism called the path integral formalism.
    In this formalism problems are treated by summing (or integrating) over all possible outcomes, called a sum over histories.
    Unlike matrix and wave mechanics the path integral formalism is based on Lagrangian mechanics.
    
    The path integral formalism lends itself to relativistic problems, such as scattering, and eventually to QFT.
    However, the mathematics is significantly more complicated than wave mechanics when it comes to bound states, for example, the quantum harmonic oscillator.
    
    In this course we will focus on developing the path integral formalism for the most part before switching to wave mechanics to develop some basic ideas of relativistic quantum mechanics, such as the Klein-Gordon equation, Dirac equation, antimatter, and more.
    We will end with a brief introduction to QFT.
    
    \part{The Basics}
    \chapter{Quantum Kinematics}
    \section{The Two-Slit Experiment}
    We start with the classic starting point for any quantum mechanics course, the two slit experiment.
    In this experiment a particle source is set up such that it puts out particles incident on a grating with two slits.
    On the other side of the grating is a screen or some sort of detector allowing us to measure the distribution of particles that make it through the slits.
    
    \begin{figure}
        \tikzsetnextfilename{double-slit-result}
        \begin{tikzpicture}
            \pgfmathdeclarefunction{sinc}{1}{%
                \pgfmathparse{abs(#1)<0.01 ? int(1) : int(0)}%
                \ifnum\pgfmathresult>0 \pgfmathparse{1}\else\pgfmathparse{sin(#1 r)/#1}\fi%
            }
            \draw[very thick] (0, 0) -- (0, 5);
            \draw[highlight, thick, domain=0.01:5,
            samples=100] plot ({sinc(5*(\x-2.3))^2}, \x);
            \draw[complementary, thick, domain=0.01:5, samples=100] plot ({sinc(5*(\x-2.7))^2}, \x);
            \begin{scope}[xshift=3cm]
                \draw[very thick] (0, 0) -- (0, 5);
                \draw[highlightpurple, thick, domain=0.01:5,
                samples=100] plot ({sinc(5*(\x-2.3))^2+sinc(5*(\x-2.7))^2}, \x);
            \end{scope}
            \begin{scope}[xshift=6cm]
                \draw[very thick] (0, 0) -- (0, 5);
                \draw[highlightpurple, thick, domain=0.01:5,
                samples=600] plot ({1.5*cos(20*deg(\x))^2 * sinc(5*(\x - 2.5))^2}, \x);
            \end{scope}
        \end{tikzpicture}
        \caption{The two peaks that form with just one slit open (left), the sum of the two peaks (middle), which is the pattern we would expect classically, and the actual pattern we find (right)}
        \label{fig:two slit results}
    \end{figure}
    
    We typically measure the intensity of the particles on the screen, however we can treat this as proportional to the probability that a particle reaches that part of the screen.
    \Cref{fig:two slit results} shows three different plots.
    The first is the two curves that form if only one slit is open (ignoring single slit diffraction).
    These curves have peaks that align with the open slit.
    The second is the sum of these two curves, which is classically what we would expect if both slits were open.
    The third is what we actually measure.
    As you can see it is very different from what we expect.
    
    The intensity is given by the square of the amplitude.
    Hence if the amplitude for going through slit 1 is \(\varphi_1\), and the amplitude for going through slit 2 is \(\varphi_2\) then the two single slit curves are \(P_1 = \abs{\varphi_1}^2\) and \(P_2 = \abs{\varphi_2}^2\).
    The curve for both slits being open is classically given by the sum of the other two curves, that is \(P_{1,2}^\mathrm{C} = \abs{\varphi_1}^2 + \abs{\varphi_2}^2\), notice that this is non-negative.
    However, the quantum mechanics prediction, which agrees with experiment, has us sum the amplitudes, and \emph{then} square them, giving instead \(P_{1,2}^\mathrm{Q} = \abs{\varphi_1 + \varphi_2}^2\).
    The difference between the classical and quantum predictions is the interference term, which we will show later is \(2\Re(\varphi_1\varphi_2^*)\), which is real but can take any sign, hence why there are points with zero intensity.

    \section{Mathematical Framework}
    We pivot now to introducing the mathematical framework, consisting of definitions and notation, which the rest of the course will use.
    These concepts should be familiar and we won't give full definitions, just point out the useful features.
    
    \subsection{States and Linearity}
    The state of the system is a vector in a Hilbert space.
    We will use the \enquote{bra-ket} notation\footnote{joke due to Dirac} where vectors are represented by \define{kets}\index{ket}.
    These are denoted \(\ket{\cdot}\) where we can use anything to label the ket, for example the position, \(\ket{x}\), some variable, \(\ket{\psi}\), or even a list of properties, \(\ket{\ell, m_\ell, s, m_s}\).
    
    A \defineindex{Hilbert space} is, for our purposes, a complex vector space with a conjugate symmetric inner product, more on this later.
    We will typically denote the state space for a given system \(\hilbert\).
    
    The normalisation of a given state isn't physically important since the states \(\ket{\psi}\) and \(c\ket{\psi}\) have the same eigenvalues and eigenvectors for all \(c\in\complex\), and these are the only measurable aspects of the state.
    For this reason we often assume that all states are normalised.
    
    Since \(\hilbert\) is a complex vector space this means that for all \(\ket{\psi_1},\ket{\psi_2}\in\hilbert\) and \(c_1, c_2\in\complex\) we have
    \begin{equation}
        \ket{\psi_3} = c_1\ket{\psi_1} + c_2\ket{\psi_2}
    \end{equation}
    is another state in \(\hilbert\).
    We say that \(\ket{\psi_3}\) is a \defineindex{linear superposition}\index{superposition} of \(\ket{\psi_1}\) and \(\ket{\psi_2}\).
    
    \subsection{Dual Space}
    We define the inner product between two states to be the function
    \begin{equation}
        \braket{\cdot}{\cdot} \colon \hilbert^*\times\hilbert \to \complex
    \end{equation}
    where \(\hilbert^*\) is the \defineindex{dual space} of \(\hilbert\).
    For every \(\ket{\psi}\in\hilbert\) there exists a dual state, \(\bra{\psi}\in\hilbert^*\).
    In Dirac notation we refer to this as a \defineindex{bra}.
    Bras are the \defineindex{dual}, \defineindex{adjoint}, or \defineindex{Hermitian conjugate} of kets and vice versa.
    In practice we often don't distinguish between a space and its dual.
    
    The inner product satisfies \(\braket{\varphi}{\psi} = \braket{\psi}{\varphi}^* \in \complex\) for all \(\ket{\psi}, \ket{\varphi} \in\hilbert\).
    This is called being conjugate symmetric.
    The inner product is also positive definite meaning that \(\braket{\psi}{\psi} \ge 0\) for all \(\ket{\psi}\in\hilbert\) with equality only if \(\ket{\psi}\) is the zero vector.
    
    The inner product is also linear, meaning that
    \begin{equation}
        \bra{\varphi}(c_1\ket{\psi_1} + c_2\ket{\psi_2}) = c_1\braket{\varphi}{\psi_1} + c_2\braket{\varphi}{\psi_2}.
    \end{equation}
    Taking the complex conjugate of this we get
    \begin{align}
        c_1^*\braket{\varphi}{\psi_1}^* + c_2^*\braket{\varphi}{\psi_2} &= c_1^*\braket{\psi_1}{\varphi} + c_2^*\braket{\psi_2}{\varphi}\\
        &= [c_1^*\bra{\psi_1} + c_2^*\bra{\psi_2}]\ket{\varphi}.
    \end{align}
    This implies that \(\ket{\psi} = \bra{\psi}^*\).
    
    From the inner product we can define a norm on the space:
    \begin{equation}
        \norm{\ket{\psi}}^2 \coloneqq \braket{\psi}{\psi}.
    \end{equation}
    We work mostly with normalised states where \(\norm{\ket{\psi}} = 1\).
    
    \subsection{Probability}
    If a system starts in the state \(\ket{\psi}\) then the probability that it ends up in the state \(\ket{\varphi}\) is
    \begin{equation}
        P(\psi \to \varphi) \coloneqq \abs{\braket{\varphi}{\psi}}^2.
    \end{equation}
    We call \(\braket{\varphi}{\psi}\) the \defineindex{probability amplitude} since it squares to the probability (in the same way that the normal amplitude squares to the intensity).
    
    If we consider a system changing states, from \(\ket{\psi}\) to \(\ket{\chi}\), via some intermediate state, \(\ket{\varphi}\), then the amplitude for this is very simply the product of the amplitude for the two changes that need to happen, \(\psi \to \chi\) then \(\chi\to\varphi\).
    The probability is then
    \begin{equation}
        P(\psi \to \varphi \to \chi) = \abs{\braket{\psi}{\varphi}\braket{\varphi}{\psi}}^2 = P(\psi\to\varphi)P(\varphi\to\chi).
    \end{equation}
    This is also what we would expect classically.
    
    If instead we have a system that starts in the state \(\ket{\psi}\) and we want to know what is the probability that it ends up in either the state \(\ket{\varphi}\) or \(\ket{\chi}\) then this is given by
    \begin{align}
        P(\psi \to \varphi \text{ or }\psi \to \chi) &= \abs{\braket{\chi}{\psi} + \braket{\varphi}{\psi}}\\
        &= \abs{\braket{\chi}{\psi}}^2 + \abs{\braket{\varphi}{\psi}}^2 + 2\Re(\braket{\chi}{\psi}\braket{\varphi}{\psi}^*)\\
        &\ne P(\psi \to \varphi) + P(\psi \to \chi).
    \end{align}
    Here we have used an identity expanded upon in \cref{app:useful identities}.
    
    
    We can now put this formalism together to derive the interference term for the two slit experiment.
    \begin{exm}{Two Slit Experiment}{}
        In the quantum mechanical treatment of the two slit experiment we consider a two-dimensional state space, \(\hilbert\).
        In particular we work with a basis \(\{\ket{1}, \ket{2}\}\), where \(\ket{i}\) represents the state \enquote{the particle travelled through slit \(i\)}.
        The final state of a general particle is some linear combination of these two states:
        \begin{equation}
            \ket{f} = c_1\ket{1} + c_2\ket{2}.
        \end{equation}
        
        Suppose that the system starts in the state \(\ket{i}\).
        The amplitude that it passes through the state \(\ket{1}\), i.e. that the particle goes through slit 1, and then ends up in state \(\ket{f}\) is \(\braket{f}{1}\braket{1}{i}\), and similarly for slit 2.
        Since the particle must go through one of the slits (we only consider particles which make it to the screen) the amplitude is the sum of the amplitudes for going through the two slits.
        Then the probability that it ends up in the state \(\ket{f}\) going via either slit is
        \begin{align}
            \abs{\braket{f}{i}}^2 &= \abs{\braket{f}{1}\braket{1}{i} + \braket{f}{2}\braket{2}{i}}^2\\
            &= \abs{\braket{f}{1}\braket{1}{i}}^2 + \abs{\braket{f}{2}\braket{2}{i}}^2 + 2\Re[\braket{f}{1}\braket{1}{i}(\braket{f}{2}\braket{2}{f})^*].
        \end{align}
        Hence we have derived the form of the interference term.
    \end{exm}

    \chapter{Bases and Observables}
    \section{Basis States and Completeness}
    Hilbert spaces are by definition \defineindex{complete}.
    While this has a technical mathematical meaning we won't worry ourselves with this.
    We take complete to mean \enquote{contains all states}, that is there aren't any \enquote{holes} in the space, a hole being a state we can approach, but which is not actually in the Hilbert space.
    
    The most important application of this for our purposes comes when we introduce a basis, \(\{\ket{n}\}\).
    This allows us to write some arbitrary \(\ket{\psi}\in\hilbert\) as
    \begin{equation}
        \ket{\psi} = \sum_n \psi_n\ket{n}.
    \end{equation}
    Without loss of generality we will assume that bases are always orthonormal, meaning \(\braket{m}{n} = \delta_{mn}\).
    Using this we have
    \begin{equation}
        \braket{m}{\psi} = \sum_n \psi_n\braket{m}{n} = \sum_n \psi_n\delta_{mn} = \psi_m.
    \end{equation}
    This gives us a way to compute the coefficients, \(\psi_n\).
    
    We can recognise now \(\braket{m}{\psi}\) as the amplitude for the state \(\ket{\psi}\) to go to the basis state \(\ket{m}\).
    
    We can use this definition of \(\psi_n\) to write
    \begin{equation}
        \ket{\psi} = \sum_n \psi_n\ket{n} = \sum_n \ket{n}\braket{n}{\psi}
    \end{equation}
    where we have used the fact that \(\braket{n}{\psi}\) is simply a complex number so we can move it past \(\ket{n}\).
    Since this relationship must hold for \(\ket{\psi}\in\hilbert\) we have that
    \begin{equation}
        \sum_n \ket{n}\bra{n} = \idop
    \end{equation}
    where \(\idop\) is the identity operator defined by
    \begin{equation}
        \idop\ket{\psi} \coloneqq \ket{\psi}
    \end{equation}
    for all \(\ket{\psi}\in\hilbert\).
    The relation
    \begin{equation}
        \sum_n\ket{n}\bra{n} = \idop
    \end{equation}
    is called the \defineindex{completeness relation}.
    It is a result of the fact that the total probability must be 1:
    \begin{align}
        \sum_n P(\psi \to n) &= \sum_n \abs{\braket{n}{\psi}}^2\\
        &= \sum \braket{n}{\psi}\braket{n}{\psi}^*\\
        &= \sum \braket{\psi}{n}\braket{n}{\psi}\\
        &= \braket{\psi}{\psi}\\
        &= 1.
    \end{align}
    
    \begin{exm}{Two State System}{}
        Consider some two state system, \(\hilbert\), with basis \(\{\ket{1}, \ket{2}\}\).
        We may represent these basis vectors as column vectors:
        \begin{equation}
            \ket{1} =
            \begin{vector}
                1\\ 0
            \end{vector}
            , \qqand
            \ket{2} = 
            \begin{vector}
                0\\ 1
            \end{vector}
            .
        \end{equation}
        Note that the dual vectors are then the transpose, that is
        \begin{equation}
            \bra{1} =
            \begin{vector}
                1 & 0
            \end{vector}
            , \qqand
            \bra{2} = 
            \begin{vector}
                0 & 1
            \end{vector}
            .
        \end{equation}
        We can check that this is indeed an orthonormal basis:
        \begin{align}
            \braket{1}{1} &= 
            \begin{vector}
                1 & 0
            \end{vector}
            \begin{vector}
                1\\ 0
            \end{vector}
            =
            1,\\
            \braket{2}{2} &= 
            \begin{vector}
                0 & 1
            \end{vector}
            \begin{vector}
                0\\ 1
            \end{vector}
            =
            1,\\
            \braket{1}{2} &= 
            \begin{vector}
                1 & 0
            \end{vector}
            \begin{vector}
                0\\ 1
            \end{vector}
            =
            0,\\
        \end{align}
        and hence \(\braket{n}{m} = \delta_{nm}\) as required.
        
        This basis also satisfies the completeness relation:
        \begin{align}
            \sum_n \bra{n}\ket{n} &= \bra{1}\ket{1} + \bra{2}\ket{2}\\
            &= 
            \begin{vector}
                1\\ 0
            \end{vector}
            \begin{vector}
                1 & 0
            \end{vector}
            +
            \begin{vector}
                0\\ 1
            \end{vector}
            \begin{vector}
                0 & 1
            \end{vector}
            \\
            &= 
            \begin{mat}
                1 & 0\\
                0 & 1
            \end{mat}
            +
            \begin{mat}
                0 & 0\\
                0 & 1
            \end{mat}
            \\
            &=
            \begin{mat}
                1 & 0\\
                0 & 1
            \end{mat}
            \\
            &= \idop.
        \end{align}
    \end{exm}

    \section{Observables and Operators}
    An \defineindex{observable} corresponds to a property of a system that we can measure and determine the state of at a given time.
    We will not worry ourselves with how these measurements can be performed in reality, we are interested only in the results of making a measurement.
    Examples of observables are energy, position, and momentum.
    
    Observables must take real values as these are the only values we can measure.
    Suppose the set of possible observables is \(\{\xi_n\}\) and we work in some basis \(\{\ket{n}\}\).
    We can define an operator, for our purposes, to be
    \begin{equation}
        \operator{\xi} \coloneqq \sum_n \xi_n \ket{n}\bra{n}.
    \end{equation}
    This generalises the completeness relation which is the special case of \(\xi_n = 1\) for all \(n\).
    We call this the \defineindex{spectral representation} of the operator.
    
    Operators are \defineindex{linear}, meaning
    \begin{equation}
        \operator{\xi} (c_1\ket{\psi_1} + c_2\ket{\psi_2}) = c_1\operator{\xi}\ket{\psi_1} + c_2\operator{\xi}\ket{\psi_2}
    \end{equation}
    for all \(c_1, c_2\in\complex\) and \(\ket{\psi_1}, \ket{\psi_2}\in\hilbert\).
    This is clearly true when we define an operator by its spectral representation.
    
    Operators corresponding to observables must be Hermitian, that is equal to their own Hermitian conjugate where the Hermitian conjugate is defined as follows.
    \begin{dfn}{Hermitian Conjugate}{}
        Given an operator, \(\operator{\xi}\), we define the \defineindex{Hermitian conjugate}, \(\operator{\xi}^\hermit\), to be such that
        \begin{equation}
            \bra{\psi} \operator{\xi}^\hermit \ket{\varphi} \coloneqq \bra{\varphi} \operator{\xi} \ket{\psi}^*
        \end{equation}
        for all states \(\ket{\psi}, \ket{\varphi} \in \hilbert\).
        
        If \(\operator{\xi} = \operator{\xi}^\hermit\) we say that \(\operator{\xi}\) is \defineindex{Hermitian}.
    \end{dfn}
    
    We can easily show that an operator with a spectra decomposition is Hermitian:
    \begin{align}
        \bra{\psi} \operator{\xi} \ket{\varphi} &= \bra{\varphi} \operator{\xi} \ket{\psi}^*\\
        &= \bra{\varphi} \left( \sum_n \xi_n\ket{n}\bra{n} \right) \ket{\psi}^*\\
        &= \sum_n \xi_n^*\braket{\varphi}{n}^*\braket{n}{\psi}^*\\
        &= \sum_n \xi_n \braket{n}{\varphi}\braket{\psi}{n}\\
        &= \sum_n \xi_n \braket{\psi}{n}\braket{n}{\varphi}\\
        &= \bra{\psi}\left( \sum_n \xi_n \ket{n}\bra{n} \right)\ket{\varphi}\\
        &= \bra{\psi} \operator{\xi} \ket{\varphi}.
    \end{align}
    Since this relationship holds for all \(\ket{\psi}, \ket{\varphi} \in \hilbert\) we have \(\operator{\xi} = \operator{\xi}^{\hermit}\) and so \(\operator{\xi}\) is Hermitian.
    
    The states \(\ket{n}\) are the eigenstates of \(\operator{\xi}\) with eigenvalues \(\xi_n\):
    \begin{equation}
        \operator{\xi} \ket{n} = \sum_m \xi_m \ket{m}\braket{m}{n} = \sum_m \xi_m \ket{m}\delta_{mn} = \xi_n\ket{n}.
    \end{equation}
    Similarly this implies that \(\bra{n}\operator{\xi} = \xi_n\bra{n}\) using the Hermitian nature of \(\operator{\xi}\).
    
    If we have some second observable, \(\operator{\eta}\), with eigenvalues \(\eta_n\) such that \(\operator{\eta}\) and \(\operator{\xi}\) both share \(\{\ket{n}\}\) as a basis then \(\operator{\eta}\) and \(\operator{\xi}\) commute:
    \begin{equation}
        \operator{\xi}\operator{\eta} \ket{n} = \eta_n\operator{\xi}\ket{n} = \eta_n\xi_n\ket{n} = \xi_n\eta_n\ket{n} = \xi_n\operator{\eta}\ket{n} = \operator{\eta}\operator{\xi}\ket{n}
    \end{equation}
    which holds for all \(\ket{n}\), and by linearity holds for all \(\ket{\psi}\in\hilbert\), and hence \(\operator{\xi}\operator{\eta} = \operator{\eta}\operator{\xi}\).
    \begin{dfn}{Commutator}{}
        Given two operators, \(\operator{\xi}\) and \(\operator{\eta}\) we define the \defineindex{commutator}:
        \begin{equation}
            \commutator{\operator{\xi}}{\operator{\eta}} \coloneqq \operator{\xi}\operator{\eta} - \operator{\eta}\operator{\xi}.
        \end{equation}
        If \(\commutator{\operator{\xi}}{\operator{\eta}} = 0\) then we say that \(\operator{\xi}\) and \(\operator{\eta}\) \defineindex{commute}.
    \end{dfn}
    
    \subsection{Measurement}
    When we make a measurement according to the operator \(\operator{\xi}\) we will achieve as a result one of the eigenvalues of \(\operator{\xi}\),
    After the measurement the state, initially \(\ket{\psi}\), collapses into the corresponding eigenstate, \(\ket{n}\).
    The probability of measuring \(\xi_n\) is therefore
    \begin{equation}
        P(\xi = \xi_n) = \abs{\braket{n}{\psi}}^2.
    \end{equation}
    
    Using the definition of the mean of the probability distribution, \(P\), as
    \begin{equation}
        \mean{x} = \sum_n xP(x)
    \end{equation}
    we can work out the mean, or \defineindex{expected value}, of \(\operator{\xi}\), which is formally the average result of an infinite number of measurements of \(\operator{\xi}\) all on the same identical initial state \(\ket{\psi}\).
    \begin{align}
        \expected{\operator{\xi}} &= \sum_n \xi_n \abs{\braket{n}{\psi}}^2\\
        &= \sum_n \xi_n \braket{\psi}{n}\braket{n}{\psi}\\
        &= \bra{\psi}\operator{\xi}\ket{\psi}.
    \end{align}
    
    We define the \defineindex{projection operator} onto the basis state \(\ket{n}\) to be
    \begin{equation}
        \operator{P}_n \coloneqq \ket{n}\bra{n}.
    \end{equation}
    We can easily check that this has the mathematical properties required to be a projection operator, namely that \(\operator{P}_n\operator{P}_m = \delta_{mn}\operator{P}_n\) and \(\sum_n \operator{P}_n = \idop\):
    \begin{align}
        \operator{P}_n\operator{P}_m &= \ket{n}\braket{n}{m}\bra{m}\\
        &= \ket{n}\bra{m}\delta_{mn}
        &= \operator{P}_n\delta_{mn}.
    \end{align}
    Here we have used the fact that if \(m\ne n\) then the third expression is zero and so it doesn't matter whether we include a factor of \(\ket{n}\bra{m}\) or \(\operator{P}_n\), and if \(m = n\) then \(\ket{n}\bra{m} = \ket{n}\bra{n} = \operator{P}_n\).
    \begin{align}
        \sum_n \operator{P}_n = \sum_n \bra{n}\ket{n} = \idop.
    \end{align}
    
    After a measurement yielding the value \(\xi_n\) the state of the system will be \(\operator{P}_n\ket{\psi}\).
    We can compute the probability that the system ends up in this state for some given \(n\):
    \begin{align}
        \norm{\operator{P}_n\ket{\psi}}^2 &= \bra{\psi}\operator{P}_n^\hermit \operator{P}_n\ket{\psi}\\
        &= \braket{\psi}{n}\braket{n}{n}\braket{n}{\psi}\\
        &= \abs{\braket{n}{\psi}}^2,
    \end{align}
    which is, as we already saw, the probability of measuring \(\xi_n\).
    
    \subsection{Degeneracy}
    It is possible that more than one state will have the same eigenvalue for the same observable.
    This is called \defineindex{degeneracy}.
    We get around it by assigning multiple labels to states to distinguish them.
    Conceptually degeneracy doesn't add very much, it usually just requires us to sum over degenerate states where we would have a single state otherwise.
    For this reason we often assume that states are non-degenerate, just to avoid having to write out lots of sums.
    
    \chapter{Basis Changes and Continuous Variables}
    \section{Change of Basis}
    Suppose \(\{\ket{n}\}\) and \(\{\ket{\bar{n}}\}\) are two orthonormal bases for \(\hilbert\).
    Since basis vectors are just states themselves we can define the basis vectors \(\ket{\bar{n}}\) in terms of \(\{\ket{\bar{n}}\}\):
    \begin{equation}
        \ket{\bar{n}} = \sum_m \ket{m}\braket{m}{\bar{n}} = \sum_m\ket{m}U_{mn}
    \end{equation}
    where we define \(U_{mn} \coloneqq \braket{m}{\bar{n}}\).
    We follow Dirac's lead and call \(U_{mn}\) transformation coefficients.
    It is no coincidence that we use matrix notation here as this is simply a linear (unitary) transformation.
    
    Orthonormality of the bases gives us
    \begin{equation}
        \delta_{mn} = \braket{\bar{m}}{\bar{n}} = \sum_l \braket{\bar{m}}{l}\braket{l}{\bra{n}} = \sum_l U_{ml}^\hermit U_{ln}
    \end{equation}
    where we define \(U_{ml}^\hermit = U_{lm}^*\), i.e. the conjugate transpose.
    Recognising this sum as the definition of matrix multiplication we identify \((U^\hermit U)_{mn} = \delta_{mn}\) and hence \(U^\hermit U = \ident\).
    
    We can define a corresponding operator, \(\operator{U}\), such that
    \begin{equation}
        \ket{\bar{n}} = \operator{U}\ket{n}.
    \end{equation}
    This means that
    \begin{equation}
        U_{mn} \bra{m}\operator{U}\ket{n}.
    \end{equation}
    Hence \(U_{mn}\) are the matrix elements of some unitary operator, \(\operator{U}\).
    We can show easily that \(\operator{U}\) is unitary:
    \begin{align}
        \delta_{mn} &= \sum_{l}U_{lm}^*U_{ln}\\
        &= \sum_l \bra{l}\operator{U}\ket{m}^*\bra{l}\operator{U}\ket{n}\\
        &= \sum_l \bra{m}\operator{U}^\hermit\ket{l}\bra{l}\operator{U}\ket{n}\\
        &= \bra{m}\operator{U}^\hermit \operator{U} \ket{n}.
    \end{align}
    Hence the matrix elements of \(\operator{U}^\hermit \operator{U}\) are \(\delta_{mn}\) and so \(\operator{U}^\hermit \operator{U} = \idop\) meaning \(\operator{U}\) is a unitary operator.
    
    At this point it should be noted that most unitary operators are \emph{not} observables since there is no requirement that unitary operators have real eigenvalues.
    The only restriction on their eigenvalues is that they have unit modulus.
    
    \section{Continuous Variables}
    So far we have assumed discrete variables.
    This means that the dimension of the Hilbert space is finite, or countable.
    We can also have Hilbert spaces of uncountable dimension, and these have a real use in physics.
    We can define them formally in a limiting process from countable dimension Hilbert spaces but we won't bother here.
    Instead we will assume that, after sufficient substitutions, the rules are exactly the same.
    
    Consider some uncountable basis, \(\{\ket{x}\}\), where the index \(x\) is now a real number, as opposed to \(\{\ket{n}\}\), which is indexed by integers.
    Everything we have said so far holds if we make the following substitutions:
    \begin{equation}
         \ket{n} \to \ket{x}, \qquad \sum_n \to\int\dl{x}, \qqand \delta_{mn} \to \delta(x - x').
    \end{equation}
    Here \(\delta\) is the \defineindex{delta distribution}.
    
    \subsection{Position Basis}
    The most common continuous variable is the position of some particle.
    In this section we will recap many of our results for the discrete case but for the position basis, \(\{\ket{x}\}\).
    
    A generic state, \(\ket{\psi} \in \hilbert\) can be written as
    \begin{equation}
        \ket{\psi} = \int \psi(x) \ket{x} \dd{x},
    \end{equation}
    where \(\psi(x)\) is the continuous analogue of \(\psi_n\).
    Note that when we write an integral with no limits in quantum mechanics what we usually mean is a definite integral over the entire domain.
    The orthonormality condition becomes
    \begin{equation}
        \braket{x'}{x} = \delta(x - x').
    \end{equation}
    From this we have
    \begin{align}
        \braket{x}{\psi} &= \int \psi(x')\braket{x'}{x}\dd{x'}\\
        &= \int\psi(x')\delta(x - x')\dd{x'}\\
        &= \psi(x).
    \end{align}
    Note the importance of having a dummy variable, \(x'\), for the expansion of \(\ket{\psi}\).
    We have used here the defining property of the delta distribution:
    \begin{equation}
        \int f(x')\delta(x - x') \dd{x'} = f(x)
    \end{equation}
    so long as \(x\) is in the range of integration, otherwise the integral is zero.
    Also the integral of just the delta distribution over all space is 1.
    From this we see that
    \begin{equation}
        \psi(x) = \braket{x}{psi}, \qqand \psi^*(x) = \braket{\psi}{x}.
    \end{equation}
    
    Substituting this into our expansion for \(\ket{\psi}\) we have
    \begin{equation}
        \ket{\psi} = \int \ket{x}\braket{x}{\psi}\dd{x}
    \end{equation}
    which gives us the continuous completeness relation:
    \begin{equation}
        \int \ket{x}\bra{x} = \idop.
    \end{equation}
    Again, this is a result of the total probability being 1.
    We use this to show
    \begin{equation}
        1 = \braket{\psi}{\psi} = \int \braket{\psi}{x}\braket{x}{\psi} = \int \psi^*(x)\psi(x)\dd{x} = \int\abs{\psi(x)}^2 \dd{x}.
    \end{equation}
    This means we can interpret \(\abs{\psi(x)}^2\) as a probability density.
    That is, \(\abs{\psi(x)}^2 \dl{x}\) is the probability of finding the particle in the range \([x, x + \dl{x}]\).
    This means that \(\psi(x)\) is the \defineindex{wave function} that we are familiar with from previous courses.
    
    We can define an operator in an analogous way to the discrete case.
    In particular the \defineindex{position operator} is defined to be
    \begin{equation}
        \operator{x} \coloneqq \int x\ket{x}\bra{x}\dd{x}.
    \end{equation}
    This is a spectral representation and so \(\operator{x}\) is trivially Hermitian.
    We can check that \(\ket{x}\) are indeed eigenvalues of this operator>
    \begin{equation}\label{eqn:eigenvalues of position basis are x}
        \operator{x}\ket{x} = \int x'\ket{x'}\braket{x'}{x} \dd{x'} = \int x'\ket{x'}\delta(x' - x) \dd{x'} = x\ket{x}.
    \end{equation}
    Similarly we have
    \begin{equation}
        \bra{x}\operator{x} = \int x'\braket{x}{x'}\bra{x} \dd{x'} = \int x'\bra{x'}\delta(x' - x) \dd{x'} = \bra{x}x.
    \end{equation}
    
    The expectation value of \(\operator{x}\) in the state \(\ket{\psi}\) is
    \begin{align}
        \expected{x} &= \bra{\psi}\operator{x}\ket{\psi}\\
        &= \int \bra{\psi}\operator{x}\ket{x}\braket{x}{\psi} \dd{x}\\
        &= \int \braket{\psi}{x}x\braket{x}{\psi} \dd{x}\\
        &= \int x\psi^*(x)\psi(x) \dd{x}\\
        &= \int x\abs{\psi(x)}^{2}\dd{x}.
    \end{align}
    Note that this fits the normal definition of the mean of a continuous probability density function, \(\abs{\psi(x)}^2\).
    
    \subsection{Fourier Transform Basis}
    An alternative basis to the position basis of the previous section is the Fourier transform basis, \(\{k\}\), defined by
    \begin{equation}
        \ket{k} \coloneqq \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \e^{ikx}\ket{x}\dd{x}.
    \end{equation}
    We call \(k\) the wave number.
    We will soon see that this basis is related to the momentum basis by a factor of \(\hbar\), and it is the momentum basis with which we tend to work over the \(\{\ket{k}\}\) basis.
    
    The completeness relation for \(\{\ket{x}\}\) gives us
    \begin{equation}
        \ket{k} = \int_{-\infty}^{\infty} \ket{x}\braket{x}{k}\dd{x}.
    \end{equation}
    Combining this with the definition of \(\ket{k}\) we have
    \begin{equation}
        \braket{x}{k} = \frac{1}{\sqrt{2\pi}} \e^{ikx}, \qqand \braket{k}{x} = \frac{1}{\sqrt{2\pi}} \e^{-ixk}.
    \end{equation}
    
    From this we get
    \begin{equation}
        \braket{k'}{k} = \int_{-\infty}^{\infty} \braket{k'}{x}\braket{x}{k}\dd{x} = \frac{1}{2\pi}\int_{-\infty}^{\infty} \e^{i(k - k')x} = \delta(k - k')
    \end{equation}
    where we have used the well known Fourier transform of the delta distribution.
    This shows that \(\{\ket{k}\}\) is orthonormal, and hence the transformation is unitary since it transforms orthonormal bases into orthonormal bases.
    
    We can now compute the action of the position operator on this basis:
    \begin{align}
        \operator{x}\ket{k} &= \int x\ket{x}\braket{x}{k}\dd{x}\\
        &= \frac{1}{\sqrt{2\pi}} x\e^{ikx}\ket{x}\dd{x}\\
        &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} -i\diffp*{}{k} \e^{ikx}\ket{x}\dd{x}\\
        &= -i\diffp*{}{k} \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \e^{ikx}\ket{x}\dd{x}\\
        &= -i\diffp*{}{k} \int_{-\infty}^\infty \ket{x}\braket{x}{k}\dd{x}\\
        &= -i\diffp*{}{k}\ket{k}.
    \end{align}
    Similarly
    \begin{equation}
        \bra{k}\hat{x} = i\diffp*{}{k}\bra{k}.
    \end{equation}
    
    We can now construct a Hermitian operator, \(\operator{k}\):
    \begin{equation}
        \operator{k} \coloneqq \int k\ket{k}\bra{k}\dd{k}.
    \end{equation}
    We have \(\operator{k}\ket{k} = k\ket{k}\) and \(\bra{k}\operator{k} = \bra{k}k\) by the same logic as \cref{eqn:eigenvalues of position basis are x}.
    
    We can now compute the action of \(\operator{k}\) on \(\ket{x}\):
    \begin{align}
        \operator{k}\ket{x} &= \int_{-\infty}^{\infty} k\ket{k}\braket{k}{x}\dd{k}\\
        &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} k\ket{k}\e^{-ikx}\dd{k}\\
        &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} i\diffp*{}{x} \e^{-ikx} \ket{k}\dd{k}\\
        &= i\diffp*{}{x} \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \e^{-ikx}\ket{k}\dd{k}\\
        &= i\diffp*{}{x} \int_{-\infty}^{\infty} \ket{k}\braket{k}{x}\\
        &= i\diffp*{}{x} \ket{x}.
    \end{align}
    Similarly
    \begin{equation}
        \bra{x}\operator{k} = -i\diffp*{}{x}\bra{x}.
    \end{equation}
    
    Using this we have
    \begin{equation}
        \bra{x}\operator{k}\ket{\psi} = -i\diffp*{}{x}\braket{x}{\psi} = -i\diffp*{\psi(x)}{x}.
    \end{equation}
    Hence,
    \begin{equation}
        \bra{\varphi}\operator{k}\ket{\psi} = \int_{-\infty}^{\infty} \braket{\varphi}{x}\bra{x}\operator{k}\ket{\psi}\dd{x} = \int_{-\infty}^{\infty} \varphi^*(x)\left( -i\diffp*{}{x} \right) \psi(x) \dd{x}.
    \end{equation}
    
    We can define
    \begin{align}
        \tilde{\psi}(k) &\coloneqq \braket{k}{\psi}\\
        &= \int_{-\infty}^{\infty} \braket{k}{x}\braket{x}{\psi}\dd{x}\\
        &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \e^{-ikx}\psi(x)\dd{x}\\
        &= \fourierTransform\{\psi(x)\}.
    \end{align}
    This justifies us calling \(\{\ket{k}\}\) the Fourier transform basis.
    
    Similarly we can show that the following hold:
    \begin{align}
        \bra{k}\operator{k}\ket{\psi} &= k\tilde{\psi}(k),\\
        \bra{\varphi}\operator{k}\ket{\psi} &= \int\dd{k}\tilde{\varphi}^*(k)\tilde{\psi}(k)\dd{k},\\
        \bra{k}\operator{x}\ket{\psi} &= i\diffp*{}{k} \tilde{\psi}(k),\\
        \bra{\varphi}\operator{x}\ket{\psi} &= \int \tilde{\varphi}^*(k)\left( i\diffp*{}{k} \right)\tilde{\psi}(k) \dd{k}.
    \end{align}
    
    We can also derive arguably the most important relation in quantum mechanics,
    \begin{equation}
        \commutator{\operator{x}}{\operator{k}} = i
    \end{equation}
    or, as we will more often use it after identifying \(\operator{p} = \hbar\operator{k}\)
    \begin{equation}
        \commutator{\operator{x}}{\operator{p}} = i\hbar.
    \end{equation}
    This follows by introducing some test state, \(\ket{\psi}\):
    \begin{align}
        \bra{x}\commutator{\operator{x}}{\operator{p}}\ket{\psi} &= \bra{x}(\operator{x}\operator{k} - \operator{k}\operator{x})\ket{\psi}\\
        &= \bra{x}\operator{x}\operator{k}\ket{\psi} - \bra{x}\operator{k}\operator{x}\ket{\psi}\\
        &= x\left( -i\diffp{}{x} \right)\braket{x}{\psi} - \left( -i\diffp{}{x} \right)x\braket{x}{\psi}\\
        &= -ix\diffp{\psi}{x} + i\diffp{\psi}{x}x + i\psi(x).
    \end{align}
    Since this holds for all \(\ket{\psi} \in \hilbert\) the commutation relation follows.
    
    From this follows the Heisenberg uncertainty principle
    \begin{equation}
        \Delta x \Delta k \ge \frac{1}{2}, \qqor \Delta x \Delta p \ge \frac{\hbar}{2}
    \end{equation}
    where \(\Delta\xi = \expected{(\operator{\xi} - \expected{\operator{\xi}})^2} = \expected{\operator{\xi}^2} - \expected{\operator{\xi}}^2\) for some operator \(\operator{\xi}\).
    
    \subsection{Three Dimensions}
    So far we have worked in one dimension.
    The change to three dimensions is fairly trivial and consists mostly of replacing \(x\) and \(k\) with \(\vv{r}\) and \(\vv{k}\) and single integrals with triple integrals.
    In particular we have the position basis \(\{\ket{\vv{r}}\}\), and Fourier transform basis, \(\{\ket{\vv{k}}\}\), which we assume are orthonormal, meaning
    \begin{equation}
        \braket{\vv{r'}}{\vv{r}} = \delta^{(3)}(\vv{r} - \vv{r'}), \qqand \braket{\vv{k'}}{\vv{k}} = \delta^{(3)}(\vv{k} - \vv{k'})
    \end{equation}
    where \(\delta^{(3)}\) is a three-dimensional delta distribution, which can be given by
    \begin{equation}
        \delta^{(3)}(\vv{r'} - \vv{r}) = \delta(x' - x)\delta(y' - y)\delta(z' - z)
    \end{equation}
    where \(\vv{r} = (x, y, z)\) and \(\vv{r'} = (x', y', z')\).
    We will often be lazy with the notation and drop the superscript \((3)\).
    
    The completeness relation in three dimensions is
    \begin{equation}
        \int \ket{\vv{r}}\bra{\vv{r}} \dd[3]{r} = \int \ket{\vv{k}}\bra{\vv{k}} \dd[3]{k} = \idop.
    \end{equation}
    
    The wave function in Fourier space is given again by the Fourier transform, now of a three-dimensional function:
    \begin{equation}
        \tilde{\psi}(\vv{k}) = \frac{1}{(2\pi)^{3/2}}\int \e^{-i\vv{k} \cdot \vv{r}} \psi(\vv{r}) \dd[3]{k}.
    \end{equation}
    Note that there is a factor of \(1/\sqrt{2\pi}\) for each dimension.
    
    \chapter{Time as a Continuum}
    \section{Infinite Gratings}
    We started with two slits.
    We then imagined the number of slits tending to infinity to give us a continuous position.
    We still only have a single point in time.
    To get around this we imagine many gratings placed one after the other.
    We can consider passing through each grating to be a moment of time passing.
    
    Suppose we have \(N\) gratings, labelled \(1\) through \(N\). We take to have an infinite number of slits, and so position is continuous.
    Place these gratings one after the other.
    We are interested in the amplitude, \(\braket{f}{i}\), that a system initially in state \(\ket{i}\) ends up in state \(\ket{f}\).
    
    We use the notation \(\ket{x_n, t_n}\) to mean that the \(n\)th grating was passed through at time \(t_n\) and position \(x_n\).
    At any fixed time, \(t_i\), this set up is identical to a single grating and so we have a completeness relation in \(x_i\).
    In particular at time \(t_1\) we have
    \begin{equation}
        \braket{f}{i} = \int \dl{x_1} \braket{f}{x_1, t_1}\braket{x_1, t_1}{i}.
    \end{equation}
    
    The same logic then applies at \(t_2\) giving us
    \begin{equation}
        \braket{f}{i} = \int \dl{x_2} \int \dl{x_2} \braket{f}{x_2, t_2} \braket{x_2, t_2}{x_1, t_1}\braket{x_1, t_1}{i}.
    \end{equation}
    And continuing on we have
    \begin{equation}\label{eqn:N gratings}
        \braket{f}{i} = \int\dl{x_N} \dotsi \int\dl{x_2}\int\dl{x_1} \braket{f}{x_N, t_N} \dotsm \braket{x_2, t_2}{x_1, t_1}\braket{x_1, t_1}{i}.
    \end{equation}
    
    For consistency of notation we change our notation to \(\ket{i} = \ket{x_a, t_a}\), and \(\ket{f} = \ket{x_b, t_b}\).
    We take the gratings to be equally spaced such that the times satisfy
    \begin{equation}
        t_n = t_a + n\varepsilon \qqwhere \varepsilon = \frac{t_b - t_a}{N + 1}.
    \end{equation}
    In particular notice that this means \(t_0 = t_a\) and \(t_{N+1} = t_b\).
    
    Using this we can write \cref{eqn:N gratings} as
    \begin{equation}\label{eqn:product of integrals}
        \braket{x_b, t_b}{x_a, t_a} = \left( \prod_{n=1}^{N} \int \dl{x_n} \right) \left( \prod_{n=0}^{N} \braket{x_{n+1}, t_{n+1}}{x_n, t_n} \right).
    \end{equation}
    
    In the limit\footnote{This limit is at best mathematically dubious and at worst nonsense, however, path integrals give us accurate predictions so we move on with them anyway.} of \(\varepsilon \to 0\), which is to say \(N \to \infty\), we have
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \int_{x_a}^{x_b} \DL{x} \braket{x_b, t_b}{x_a, t_a}\big\vert_{x(t)}.
    \end{equation}
    Here we have introduced new notation where this integral represents an integral over all paths, \(x(t)\), which begin at \(x(t_a) = x_a\), and end at \(x(t_b) = x_b\), and the amplitude \(\braket{x_b, t_b}{x_a, t_a}\big\vert_{x(t)}\) is the amplitude for the particle travelling along the continuous path \(x(t)\).
    That is, the integral and measure denotes the first bracket of the continuous limit of \cref{eqn:product of integrals}, and inner product for \(x(t)\) denotes the second bracket.
    
    \subsection{Intuitive Rewriting}\label{sec:intuitive rewriting}
    We expect that
    \begin{equation}
        \braket{x_{n+1}, t_{n+1}}{x_n, t_n} \approx \e^{i\varepsilon\varphi}
    \end{equation}
    where \(\varepsilon = t_{n+1} - t_n\) and \(\varphi\) is a real function of \(x_n\), \(x_{n+1}\), \(t_n\), and \(t_{n+1}\).
    We call this the transition amplitude along the path \(x(t)\).
    As \(\varepsilon \to 0\) we see that the amplitude becomes constant.
    Since \(\varphi\) only depends on the \(n\)th and \((n + 1)\)th positions and times this is a local quantity.
    
    We can regard the transition \(\ket{x_n, t_n} \to \ket{x_{n+1}, t_{n+1}}\) as a change of basis, which is given by some operator \(\operator{U}\):
    \begin{equation}
        \ket{x_{n+1}, t_{n+1}} = \operator{U}^\hermit \ket{x_n, t_n}.
    \end{equation}
    The Hermitian conjugate here is just a matter of convention.
    Probably because someone originally defined \(\operator{U}\) using
    \begin{equation}
        \ket{x_n, t_n} = \operator{U}\ket{x_{n+1}, t_{n+1}}.
    \end{equation}
    
    We can write \(\operator{U}\) as
    \begin{equation}
        \operator{U} = \idop + i\varepsilon\operator{\varphi} + \order(\varepsilon^2) \approx \e^{i\varepsilon\operator{\varphi}}.
    \end{equation}
    We can expand any unitary operator in this way.
    The fact that \(\operator{U}\) is unitary means that \(\operator{\varphi}\) must be Hermitian.
    Hence \(\operator{\varphi}\) has real eigenvalues, \(\varphi\), satisfying
    \begin{equation}
        \operator{\varphi} \ket{x_n, t_n} \approx \varphi \ket{x_n, t_n}.
    \end{equation}
    We therefore have
    \begin{equation}
        \ket{x_n, t_n} = \operator{U}\ket{x_{n+1}, t_{n+1}} \approx \e^{i\varepsilon\operator{\varphi}}\ket{x_{n+1}, t_{n+1}} \approx \e^{i\varepsilon\varphi}\ket{x_{n+1}, t_{n+1}}.
    \end{equation}
    
    Hence
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &\approx \left( \prod_{n=1}^{N} \int \dl{x_n} \right) \left( \prod_{n=0}^{N} \e^{i\varepsilon\varphi(x_n, x_{n+1}, t_n, t_{n+1})} \right)\\
        &= \left( \prod_{n=1}^{N} \int\dl{x_n} \right)\exp\left[ i\sum_{n=0}^{N} \varepsilon\varphi(x_n, x_{n+1}, t_n, t_{n+1}) \right]\\
        &= \left( \prod_{n=1}^{N} \int\dl{x_n} \right)\exp\left[ i\sum_{n=0}^{N} (t_{n+1} - t_n) \varphi(x_n, x_{n+1}, t_n, t_{n+1}) \right].
    \end{align}
    In the limit of \(N \to \infty\) we have \(\Delta t = t_{n+1} - t_n \to 0\) and our sum becomes an integral giving us
    \begin{equation}\label{eqn:integral over paths}
        \braket{x_b, t_b}{x_a, t_a} \approx \int_{x_a}^{x_b} \DL{x} \exp\left[ i\int_{t_a}^{t_b} \dl{t} \varphi(x(t), \dot{x}(t), t) \right]
    \end{equation}
    where we introduce \(\dot{x}(t)\) as the continuous analogue of being able to find absolute differences \(x_{n+1} - x_n\).
    This is called a \defineindex{path integral} as we integrate over all possible paths.
    Path integrals will be the object of our study for much of the course.
    
    \part{Quantum Dynamics}
    \chapter{Classical Mechanics}
    \epigraph{This is not a course where we solve interesting problems. It's a course where we solve the harmonic oscillator in increasingly complicated ways.}{Roger Horsley}
    \section{Euler--Lagrange Equation}
    \begin{dfn}{Action}{}
        Given a path, \(x(t)\), we define the \defineindex{action} from time \(t_a\) to \(t_b\) to be the functional
        \begin{equation}
            S[x(t)] \coloneqq \int_{t_a}^{t_b} \lagrangian(x, \dot{x}, t) \dd{t}.
        \end{equation}
        Here \(\lagrangian\) is the \defineindex{Lagrangian} of the system which, for a non-relativistic point particle in a potential, \(V\), in the absence of changing electromagnetic fields is
        \begin{equation}
            \lagrangian(x, \dot{x}, t) \coloneqq T - V = \frac{1}{2}m\dot{x}^2 - V(x, t).
        \end{equation}
    \end{dfn}
    
    This definition turns out to be very powerful when combined with the \defineindex{principle of least action}, which states that the classical path, \(\bar{x}(t)\), is such that the action is an extrema.
    We usually assume that the action is minimised.
    As a result of this
    \begin{equation}
        \diffd{S}{x}[x=\bar{x}] = 0
    \end{equation}
    where \(\diffd{S}/{x}\) is the functional derivative of \(S\) with respect to \(x\).
    For our purposes this is not that different from a normal derivative.
    We can compute it by considering the result of a small change in the path \(x(t)\).
    Specifically consider a path given by \(x(t) + \delta x(t)\) where \(\delta x(t)\) is taken to be small so that terms of order \(\delta x(t)^2\) are negligible.
    We also assume that \(\delta x(t_a) = \delta x(t_b) = 0\) so that the endpoints of the path are fixed.
    
    The change in \(S\) that follows from this change in the path is
    \begin{equation}
        \delta S = S[x + \delta x] - S[x] = \int_{t_a}^{t_b} \left[ \lagrangian(x + \delta x, \dot{x} + \dot{\delta x}, t) - \lagrangian(x, \dot{x}, t) \right] \dd{t}.
    \end{equation}
    Taylor expanding the first Lagrangian about \(x\) we have
    \begin{equation}
        \delta S = \int_{t_a}^{t_b} \left[ \diffp{\lagrangian}{x}\delta x + \diffp{\lagrangian}{\dot{x}}\dot{\delta x} + \order(\delta x^2) \right] \dd{t}.
    \end{equation}
    We now consider just the second term of this integral:
    \begin{equation}
        \int_{t_a}^{t_b} \diffp{\lagrangian}{\dot{x}}\dot{\delta x} \dd{t}.
    \end{equation}
    We can integrate this by parts, using
    \begin{align}
        u &= \diffp{\lagrangian}{\dot{x}}, & v &= \delta x,\\
        u' &= \diff*{\left( \diffp{\lagrangian}{\dot{x}} \right)}{t}, & v' = \dot{\delta x},
    \end{align}
    which gives us
    \begin{equation}\label{eqn:integtral of dL/dxdot delta x}
        \left[ \diffp{\lagrangian}{\dot{x}}\delta x \right]_{t_a}^{t_b} - \int_{t_a}^{t_b} \diff*{\left( \diffp{\lagrangian}{\dot{x}} \right)}{t} \delta x \dd{t}.
    \end{equation}
    Taking the end points to be fixed means that the first term vanishes.
    Bringing back the other term we have
    \begin{equation}
        \delta S = \int_{t_a}^{t_b} \left[ \diffp{\lagrangian}{x} - \diff*{\left( \diffp{\lagrangian}{\dot{x}} \right)}{t} \right]\delta x \dd{t}
    \end{equation}
    where we have dropped terms of order \(\delta x^2\) or higher.
    
    In order for the derivative to disappear, as we are looking for extrema, we must have that \(\delta S = 0\).
    Since this also has to apply to all time intervals it follows that the integrand must be zero, that is
    \begin{equation}
        \diff*{\left( \diffp{\lagrangian}{\dot{x}} \right)}{t} - \diffp{\lagrangian}{x} = 0.
    \end{equation}
    This is the famous \defineindex{Euler--Lagrange equation}.
    
    We define the action on the classical path to be \(S_{\cl} \coloneqq S[\bar{x}(t)]\).
    This is a function of the end points, \(x_a\) and \(x_b\), as well as the times \(t_a\) and \(t_b\), which appear through the limits on the integral and implicitly in \(\bar{x}\).
    
    \section{Hamilton's Principle Function}
    We now investigate what happens if we vary slightly the final spatial point from \((x_b, t_b)\) to \((x_b + \delta x_b, t_b + \delta t_b)\).
    First suppose we keep \(t_b\) fixed.
    Then, with \(x_b = x(t_b)\), we have \(\delta x_b = \delta x_b(t_b)\).
    Considering the first term in \cref{eqn:integtral of dL/dxdot delta x} we now have that the variation in the path is
    \begin{equation}\label{eqn:delta Scl}
        \delta S_{\cl} = \diffp{\lagrangian}{\dot{x}} \delta x\bigg\vert^{t_b}.
    \end{equation}
    Recall that the \defineindex{canonical momentum} is defined as
    \begin{equation}
        p \coloneqq \diffp{\lagrangian}{\dot{x}}.
    \end{equation}
    Dividing \cref{eqn:delta Scl} through by \(\delta x\) we get a functional derivative.
    We are evaluating this for a particular path so it becomes a standard derivative and we have
    \begin{equation}\label{eqn:dS = p dx}
        p_b = \diffp{\lagrangian}{\dot{x}}\bigg|^{t_b} = \diffp{S_{\cl}}{x_b} \implies \delta S_{\cl} = p_b\delta x_b
    \end{equation}
    where \(p_b\) is the classical momentum at the endpoint.
    
    Suppose instead that we vary \(t_b\) and keep \(x_b\) fixed.
    Now \(x_b = x(t_b + \delta t_b)\), Taylor expanding this gives us
    \begin{equation}
        \delta x(t_b + \delta t_b) = \delta x_b(t_b) + \dot{x}(t_b)\delta t_b + \order(\delta t_b^2).
    \end{equation}
    Recognising that keeping \(x_b\) fixed means the left hand side of this is zero we must have
    \begin{equation}
        \delta x(t_b) = -\dot{x}_b(t_b) \delta t_b.
    \end{equation}
    
    We have seen now how varying \(x_b\) effects the classical action and how varying \(t_b\) effects \(x_b\), and hence the classical action, it follows that the change in the action is
    \begin{equation}\label{eqn:dS = -E dt}
        \delta S_{\cl} = \lagrangian_b\delta t_b + p_b\delta x(t_b) = (\lagrangian_b - p_b\dot{x}_b)\delta t_b = -E_b \delta t_b
    \end{equation}
    where \(E_b = \hamiltonian(x_b, \dot{x}_b, t_b)\) is the value of the Hamiltonian evaluated at the endpoint.
    This follows from the definition of the \defineindex{Hamiltonian} as the Legendre transform of the Lagrangian, in one dimension that is
    \begin{equation}
        \hamiltonian \coloneqq \lagrangian - p\dot{x}.
    \end{equation}
    Dividing through by \(\delta t_b\) we have
    \begin{equation}
        E_b = -\diffp{S_{\cl}}{t_b}
    \end{equation}
    which can be rewritten taking \(E_b \) to be the value of some function \(E\) evaluated at the endpoint:
    \begin{equation}
        E(x_b, \diffp{S_{\cl}}/{x_b}, t_b) + \diffp{S_{\cl}}{x_b} = 0.
    \end{equation}
    This is the \defineindex{Hamilton--Jacobi equation} (HJ)\glossary[acronym]{HJ}{Hamilton--Jacobi}.
    Since \(t_b\) is arbitrary we usually drop the index and write this as
    \begin{equation}
        H(x, \diffp{S_{\cl}}/{t}, t) + \diffp{S_{\cl}}{t} = 0.
    \end{equation}
    \defineindex{Hamilton's principle function}, here denoted \(S_{\cl}\), is a solution of the Hamilton--Jacobi equation and is equal to the action up to a constant.
    
    This equation is useful for finding the action as it is a first order differential equation which is relatively easy to solve.
    The alternative being solving the second order Euler--Lagrange equation and then integrating.
    We don't usually need to find the action in classical mechanics so we aren't that interested in this function.
    
    \begin{exm}{Free Particle}{}
        Consider a free particle.
        The corresponding Lagrangian is
        \begin{equation}
            \lagrangian = \frac{1}{2}m\dot{x}^2.
        \end{equation}
        From this it follows that \(\ddot{\bar{x}} = 0\), which is obvious as there are no forces.
        We must then have
        \begin{equation}
            \bar{x}(t) = x_a + v(t - t_a)
        \end{equation}
        where
        \begin{equation}
            v = \frac{x_b - x_a}{t_b - t_a} = \dot{\bar{x}}
        \end{equation}
        so \(v\) is the constant velocity of the particle.
        Then,
        \begin{equation}\label{eqn:classical action free particle}
            S_{\cl} = S[\bar{x}] = \int_{t_a}^{t_b} \frac{1}{2}m\dot{x}^2 \dd{t} = \frac{1}{2}m \frac{(x_b - x_a)^2}{t_b - t_a}.
        \end{equation}
        This then gives us
        \begin{equation}
            p_b = \diffp{S_{\cl}}{x_b} = mv, \qqand E_b = - \diffp{S_{\cl}}{t_b} = \frac{1}{2}mv^2.
        \end{equation}
        That is the classical momentum of a free particle is \(mv\) and the energy is \(mv^2/2\).
        These are just the standard results that we are familiar with which are taken as definitions in Newtonian mechanics.
    \end{exm}
    
    \begin{exm}{Simple Harmonic Oscillator}{}
        The Lagrangian for a simple harmonic oscillator is
        \begin{equation}
            \lagrangian = \frac{1}{2}m(\dot{x}^2 - \omega^2x^2).
        \end{equation}
        From this it is trivial to derive the equation of motion
        \begin{equation}
            \ddot{\bar{x}} + \omega^2\bar{x} = 0.
        \end{equation}
        Taking \(T = t_b - t_a\), \(\bar{x}(t_a) = x_a\), and \(\bar{x}(t_b) = x_b\) we have
        \begin{equation}
            \bar{x}(t) = x_b\frac{\sin[\omega(t - t_a)]}{\sin(\omega T)} + x_a\frac{\sin[\omega(t_b - t_a)]}{\sin(\omega T)}.
        \end{equation}
        It should be noted that this solution is slightly more complicated than the standard solution since it considers boundary conditions at two points, \(x_a\) and \(x_b\), whereas the standard solution considers boundary and initial conditions at \(x_a\) and \(t = 0\).
        The action is then
        \begin{equation}
            S_{\cl} = S[\bar{x}] = \frac{m\omega}{2\sin(\omega T)} [(x_a^2 + x_b^2)\cos(\omega T) - 2x_ax_b].
        \end{equation}
        Notice that when \(\omega \to 0\) we recover the free particle limit since \(\cos(0) = 0\) and
        \begin{equation}
            \lim_{\omega \to 0} \frac{\omega}{\sin(\omega T)} = \lim_{\omega\to 0} \frac{\omega T}{T\sin(\omega T)} = \frac{1}{T}\lim_{\omega\to 0} \frac{1}{\sinc(\omega T)} = \frac{1}{T}.
        \end{equation}
        The Hamilton--Jacobi equations then give
        \begin{equation}
            p_b = m\dot{\bar{x}}\big\vert_{t=t_b}, \qqand E_b = \frac{m}{2}(\dot{\bar{x}}^2 + \omega^2\bar{x}^2)\bigg\vert_{t=t_b}.
        \end{equation}
    \end{exm}

    \chapter{Path Integrals}
    \section{Amplitude for a Path}
    Recall from \cref{eqn:integral over paths} we can write the amplitude for some path \(x(t)\) as
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a}\big\vert_{x(t)} \sim \exp\left[ i \int_{t_a}^{t_b} \varphi(x(t), \dot{x}(t), t) \right].
    \end{equation}
    To determine \(\varphi\) we go to the classical case since we should always be able to recover known classical results in the classical limit of the quantum result.
    
    The classical action is the Lagrangian integrated over a path.
    This suggests that the phase, \(\varphi\), should be proportional to the Lagrangian, \(\lagrangian\).
    We will find that this recovers the expected results.
    However, for dimensionality reasons we need to include an extra factor since
    \begin{equation}
        [S] = [\text{time}][\text{energy}] = [\text{length}][\text{momentum}] = [\text{angular momentum}]
    \end{equation}
    and the argument of the exponential must be dimensionless.
    We call this constant extra factor \(\hbar\) with \([\hbar] = [S]\) and write
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a}\big\vert_{x(t)} = \e^{iS[x(t)]/\hbar}.
    \end{equation}
    
    At this point there are a few things to note.
    First we have an overall normalisation constant which we simply absorb into our definition of \(\DL{x}\).
    Second, if the action is modified by a constant, \(S \to S + c\), then the amplitudes are unchanged since this just gives us a global phase, \(\e^{ic/\hbar}\), which has no physical consequence.
    Finally, the size at which quantum effects become important is set by the size at which variations in the exponent are \(\order(1)\), that is when \(\delta S = \order(\hbar)\).
    
    So far we have been attempting to motivate this definition.
    From now on we take it as a postulate and see what we can derive from it.
    
    \section{The Feynman Path Integral}
    Integrating the result of the previous section over all paths gives us the transition amplitude
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \int_{x_a}^{x_b} \DL{x} \e^{iS[x(t)]/\hbar}.
    \end{equation}
    In order to evaluate this we use the limiting process from \cref{sec:intuitive rewriting}, now including a normalisation factor, \(A_N\):
    \begin{equation}
        \int_{x_a}^{x_b}\DL{x} = \lim_{N\to\infty} A_N \prod_{n=1}^{N}\int_{-\infty}^{\infty} \dl{x_n}.
    \end{equation}
    We take the normalisation factor to be
    \begin{equation}
        A_n = \nu(\varepsilon)^{N+1}
    \end{equation}
    where \(\nu(\varepsilon)\) is the normalisation factor for each discrete interval.
    
    It should be noted that this notation of \(\int \DL{x}\) is hard, if not impossible, to make mathematically precise.
    This is one of those times where we use the maths just because it works, not because there is a rigorous reason behind it.
    The limits on this integral shouldn't be thought of like the limits of a normal integral.
    They are just a way of keeping track of which section of the path we are considering.
    
    We can insert a complete set of states in the transition amplitude giving
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &= \int \dl{x_c} \braket{x_b, t_b}{x_c, t_c}\braket{x_c, t_c}{x_a, t_a}\\
        &= \int\dl{x_c} \int_{x_c}^{x_b} \DL{x} \int_{x_a}^{x_c} \DL{x'} \exp\left[ \frac{i}{\hbar}\left( \int_{t_c}^{t_b} \lagrangian \dd{t} + \int_{t_a}^{t+c} \lagrangian \dd{t}\right) \right].
    \end{align}
    We can see this as splitting the path into two sections and then integrating over all paths that go through the chosen mid point.
    We then integrate over all possible mid points and hence over all possible paths.
    This will be useful when it comes to evaluating the normalisation factors, \(\nu\).
    
    \begin{figure}
        \tikzsetnextfilename{split-path}
        \begin{tikzpicture}
            \tikzset{mid arrow/.style={postaction={decorate,decoration={
                            markings,
                            mark=at position .5 with {\arrow{>}}
            }}}}
            \foreach \a in {-60, -40, ..., 60} {
                \draw [thick, mid arrow] (0, 0) to[out=\a, in=180-\a, looseness=1.2] (2, 1.5);
                \draw [thick, mid arrow, looseness=0.8] (2, 1.5) to[out=\a, in=180-\a] (5, 2);
            }
            \fill[highlight] (0, 0) circle [radius = 0.1cm];
            \fill[highlightpurple] (5, 2) circle [radius = 0.1cm];
            \fill[complementary] (2, 1.5) circle [radius = 0.1cm];
            \node at (2.5, -1) {\(\displaystyle\braket{\textcolor{highlight}{x_b}, \textcolor{highlight}{t_b}}{\textcolor{highlightpurple}{x_a}, \textcolor{highlightpurple}{t_a}} = \int \dl{\textcolor{complementary}{x_c}} \int_{\textcolor{complementary}{x_c}}^{\textcolor{highlight}{x_b}} \DL{x} \int_{\textcolor{highlightpurple}{x_a}}^{\textcolor{complementary}{x_c}} \DL{x'} \exp\left[ \frac{i}{\hbar} \left( \int_{\textcolor{complementary}{t_c}}^{\textcolor{highlight}{t_b}} \lagrangian \dd{t} + \int_{\textcolor{highlightpurple}{t_a}}^{\textcolor{complementary}{t_c}} \lagrangian \dd{t} \right) \right]\)};
        \end{tikzpicture}
        \caption{Splitting a path from \(\textcolor{highlightpurple}{x_a}\) to \(\textcolor{highlight}{x_b}\) into two paths from \(\textcolor{highlightpurple}{x_a}\) to \(\textcolor{complementary}{x_c}\) and then \(\textcolor{complementary}{x_c}\) to \(\textcolor{highlight}{x_b}\).}
    \end{figure}

    \section{Connection to Classical Limit}
    In a quantum situation we typically consider \(t_a\) and \(t_b\) to be close, and also \(x_a\) and \(x_b\) to be close.
    We take the action, \(S[x(t)]\), to be on the order of \(\hbar\).
    Hence the magnitude of the exponent is on the order of 1 and therefore paths far from the classical path may well have non-negligible contributions.
    
    On the other hand, in the classical process the time and space intervals are typically much greater and hence the action is larger.
    This means that the exponent is typically much larger than \(\hbar\).
    Formally the classical limit is found by taking the limit of \(\hbar \to 0\).
    
    Consider paths which are a small perturbation, \(\delta x\), from a given path, \(x(t)\).
    Given that \(S\) is large we expect \(\delta S\) to be large, even if \(\delta x\) is small.
    The result is that the exponential oscillates rapidly and hence most of the paths cancel out, apart from those near to the classical path where \(S[x(t)]\) is minimised\footnote{see the section on asymptotic integral expansions in the methods of mathematical physics course where this is discussed in more detail}, and hence the terms in \(\delta S\) of order \(\delta x\) are zero and \(\delta S\) is \(\order(\delta x^2)\), which will be considerably smaller for small \(\delta x\).
    Paths near to the classical path therefore add constructively.
    In the limit of \(\hbar \to 0\) the classical path gives the dominant contribution and we arrive at the principle of least action.
    We then have\footnote{this is a proper asymptotic relation, again, see methods of mathematical physics.}
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} \sim \e^{i S_{\cl}/\hbar}
    \end{equation}
    
    \section{Momentum and Energy}
    \subsection{Momentum}
    Consider a small change in the endpoint of the path, \(x_b \to x_b + \delta x_b\), keeping \(t_b\) fixed.
    The change in the integrand, \(\delta S_{\cl}/\hbar\), is defined to be \(k_b\delta x_b\), where \(k_b\) is the wavenumber.
    We also know from \cref{eqn:dS = p dx} that \(\delta S_{\cl} = p_b\delta x_b\), and so we finally identify \(p_b = \hbar k_b\), or, since the endpoint is arbitrary,
    \begin{equation}
        p = \hbar k.
    \end{equation}
    This then naturally converts to operators as \(\operator{p} = \hbar \operator{k}\).
    
    We can then define the \defineindex{momentum basis} as the states \(\ket{p} = \hbar\ket{p}\) and from this follows the expected relations, such as
    \begin{equation}
        \commutator{\operator{x}}{\operator{p}} = i\hbar
    \end{equation}
    and
    \begin{equation}
        \braket{x}{p} = \frac{1}{\sqrt{2\pi\hbar}} \e^{ipx/\hbar}, \qqand \braket{p}{x} = \frac{1}{\sqrt{2\pi\hbar}} \e^{-ipx/\hbar}.
    \end{equation}
    Notice the change in the normalisation factor to include \(1/\sqrt{\hbar}\) to account for the extra factor of \(1/\hbar\) in the exponent.
    
    \subsection{Energy}
    Now suppose we vary \(t_b\) and keep \(x_b\) fixed.
    The change in the exponent \(\delta S_{\cl}/\hbar\), is defined to be \(-\omega_b\delta t_b\), where \(\omega_b\) is the (angular) frequency.
    \Cref{eqn:dS = -E dt} tells us that \(\delta S_{\cl} = -E_b\delta t_b\), and hence \(E_b = \hbar\omega_b\), or, since the endpoint is arbitrary,
    \begin{equation}
        E = \hbar\omega.
    \end{equation}
    
    \chapter{Path Integrals In Use}
    \epigraph{Very rough mathematically, but never mind.}{Roger Horsley}
    In this chapter we will give examples of how path integrals can be used to find the amplitude for some simple systems, including a free particle and a harmonic oscillator.
    
    \section{Free Particle}
    The Lagrangian for a free particle is
    \begin{equation}
        \lagrangian = T = \frac{1}{2}m\dot{x}^2.
    \end{equation}
    The simplest method for computing the amplitude is to consider the discrete case and then take the limit.
    In this case we have a discrete Lagrangian where the rate of change is turned into a ratio of finite differences:
    \begin{equation}
        \lagrangian \approx \frac{1}{2}m\left( \frac{x_{n+1} - x_n}{\varepsilon} \right)^2.
    \end{equation}
    Recall that \(\varepsilon = t_{n+1} - t_n\).
    We then have
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &= \int_{x_a}^{x_b} \DL{x} \exp\left[ \frac{i}{\hbar} \int_{t_a}^{t_b} \lagrangian \dd{t} \right]\\
        &= \lim_{N\to\infty} A_N \left( \prod_{n=1}^{N} \int\dl{x_n} \right) \exp\left[ \frac{i\varepsilon m}{2\hbar} \sum_{n=0}^{N} \left( \frac{x_{n+1} - x_n}{\varepsilon} \right)^2 \right].\label{eqn:free particle path integral discrete}
    \end{align}
    In this the integral
    \begin{equation}
        \int \lagrangian \dd{t}
    \end{equation}
    becomes the discrete sum
    \begin{equation}
        \sum_{n=0}^{N} \lagrangian \varepsilon.
    \end{equation}

    We will determine the normalisation factor, \(A_N\), later.
    For now we state that we expect it to be of the form \(\nu(\varepsilon)^{N+1}\) for some function \(\nu\).
    
    \subsection{Double Gaussian}
    The integrals above are a series of nested Gaussian integrals.
    In order to compute this we consider a single double Gaussian integral:
    \begin{equation}
        I = \int_{-\infty}^{\infty} \exp\left[ \frac{i}{a}(x - u)^2 + \frac{i}{b}(u - y)^2 \right] \dd{u}.
    \end{equation}
    This integral is translation invariant, which is a fancy way of saying that if we replace \(u\) with \(u + y\) then nothing changes.
    We can also view this as a change of variables to \(u' = u - y\).
    \begin{equation}
        I = \int_{-\infty}^{\infty} \exp\left[ \frac{i}{a}(x - y - u)^2 + \frac{i}{b}u^2 \right] \dd{u}.
    \end{equation}
    Another way of viewing this is that this integral depends only on \(x - y\), not the actual values of \(x\) and \(y\).
    
    Completing the square in the exponent we have
    \begin{equation}
        \frac{1}{a}(x - y - u)^2 + \frac{1}{b}u^2 = \left( \frac{1}{a} + \frac{1}{b} \right)\left( u - \frac{x - y}{a\left( \frac{1}{a} + \frac{1}{b} \right)} \right)^2 + \frac{(x - y)^2}{a _ b}.
    \end{equation}
    Again shifting \(u\), this time to
    \begin{equation}
        u - \frac{x - y}{a\left( \frac{1}{a} + \frac{1}{b} \right)}
    \end{equation}
    we end up with
    \begin{equation}
        I = \int_{-\infty}^{\infty} \exp\left[ i(\frac{1}{a} + \frac{1}{b})u^2 + i\frac{(x - y)^2}{a + b} \right] \dd{u}.
    \end{equation}
    This is simply a Gaussian integral in \(u\) and so, using the results in \cref{app:gaussian integral}, we get
    \begin{equation}
        I = \sqrt{\frac{i\pi ab}{a + b}} \exp\left[ i\frac{(x - y)^2}{a + b} \right].
    \end{equation}
    
    \subsection{Nested Gaussian Integrals}
    Now consider the nested integrals in \cref{eqn:free particle path integral discrete}.
    Writing \(c = 2\hbar \varepsilon/m\) and \(x_n' = x_n/\sqrt{c}\) for brevity we need to evaluate
    \begin{equation}
        I_N = c^{N/2} \int \dl{x_1'} \dotsm \dl{x_N'} \exp[i(x_1' - x_0')^2 + i(x_2' - x_1')^2 + \dotsb ]
    \end{equation}
    Computing the \(x_1'\) integral we get
    \begin{equation}
        I_N = \sqrt{\frac{i \pi 1\cdot 1}{1 + 1}} \int \dl{x_2'} \dotsm \dl{x_N'} \exp[i(x_2' - x_0')/2 + (x_3' - x_2')^2 + \dotsm].
    \end{equation}
    Continuing on and computing all integrals we have
    \begin{align}
        I_N &= c^{N/2} \sqrt{\frac{i\pi 1\cdot1}{\cancel{1 + 1}}} \sqrt{\frac{i\pi \cancel{2\cdot 1}}{\cancel{2 + 1}}} \dotsm \sqrt{\frac{i\pi \cancel{N}}{N + 1}} \exp[i(x'_{N+1} - x_0')^2/(N+1)]\\
        &= \left( \frac{2\pi i\varepsilon \hbar}{m} \right)^{N/2} \frac{1}{\sqrt{N + 1}} \exp\left[ \frac{i}{\hbar}\frac{m}{2\varepsilon m\hbar(N+1)} (x_b - x_a)^2 \right].
    \end{align}
    
    \subsection{Evaluating the Free Particle Amplitude}
    Putting in the result from the previous section we have
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \lim_{N\to\infty} A_N \left( \frac{2\pi i \varepsilon \hbar}{m} \right)^{(N+1)/2} \sqrt{\frac{m}{2\pi i \hbar(N + 1)\varepsilon}} \exp\left[ \frac{i}{\hbar} \frac{m(x_b - x_a)^2}{2\varepsilon(N + 1)} \right].
    \end{equation}
    Notice that \((N + 1)\varepsilon = t_b - t_a\) since \(t_b = t_{N+1}\) and \(t_a = t_0\) and \(t_n = t_0 + n\varepsilon\).
    We will often call the time interval \(t_b - t_a\) \(T\) and unless stated otherwise assume that this is what \(T\) means.
    
    We need to normalise this quantity.
    As it stands we simply need the quantity to be finite and we look for a normalisation factor that makes this so.
    If we have \(A_N = \nu(\varepsilon)^{N + 1}\) then clearly by setting
    \begin{equation}
        \nu(\varepsilon) = \sqrt{\frac{m}{2\pi i\hbar T}}
    \end{equation}
    not only do we remove the factor of \(1/\sqrt{\varepsilon}\), which diverges in the limit, but we also remove the constant to the power of \((N + 1)/2\), which either explodes to infinity or becomes zero depending on the constant.
    Neither of these behaviours is desirable so this seems to be a good choice of normalisation.
    
    The result of this normalisation is
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \sqrt{\frac{m}{2\pi i\hbar T}} \exp\left[ \frac{i}{\hbar} \frac{m}{2}\frac{(x_b - x_a)^2}{T} \right].
    \end{equation}

    \subsection{Rewriting in Terms of the Classical Action}
    We can recognise from \cref{eqn:classical action free particle} that the exponent is proportional to the classical action for a free particle:
    \begin{equation}
        S_{\cl} = \frac{m}{2}\frac{(x_b - x_a)^2}{T}.
    \end{equation}
    This means that we have
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = F_0(T) \e^{i S_{\cl}/\hbar}
    \end{equation}
    where
    \begin{equation}
        F_0(T) = \sqrt{\frac{m}{2\pi i\hbar T}}.
    \end{equation}
    Notice that the prefactor is independent of both \(x_a\) and \(x_b\).
    
    \subsection{Notes}
    \subsubsection{Plane Waves}
    If we consider a variation in \(x_b\) and \(t_b\) then for fixed \(t_b\) we have \(\delta S_{\cl} = p_b\delta x_b\) and for fixed \(x_b\) we have \(\delta S_{\cl} = -E_b\delta t_b\).
    We then have
    \begin{align}
        \braket{x_b + \delta x_b, t_b + \delta t_b}{x_a, t_a} &= F_0(T + \delta t_b) \exp\left[ \frac{i}{\hbar}(S_{\cl} + \delta S_{\cl}) \right]\\
        &= F_0(T + \delta t_b) \e^{iS_{\cl}/\hbar} \exp\left[ \frac{i}{\hbar} (p_b\delta x_b - E_b\delta t_b) \right]\\
        &\approx \braket{x_b, t_b}{x_a, t_a} \exp\left[ \frac{i}{\hbar} (p_b\delta x_b - E_b\delta t_b) \right].
    \end{align}
    This represents a plane wave with amplitude \(\braket{x_b, t_b}{x_a, t_a}\), momentum \(p_b\), and energy \(E_b = p_b^2/2m\).
    This justifies us calling the wave function a plane wave when discussing the double slit experiment.
    
    \subsubsection{Green's Function}\label{sec:green's function}
    This note is mostly on terminology.
    We call the free particle amplitude the \defineindex{free particle Green's function}, which we write as
    \begin{equation}
        G_0(x_b - x_a, t_b - t_a) \coloneqq \braket{x_b, t_b}{x_a, t_a}
    \end{equation}
    for \(t_b > t_a\).
    This makes clear the translation invariance since the Green's function depends only on the difference between the endpoints.
    
    We can take the Fourier transform of the Green's function:
    \begin{align}
        \tilde{G}_0(p, t) &= \int_{-\infty}^{\infty} G_0(x, t) \e^{ipx/\hbar} \dd{x}\\
        &= \exp\left[ -\frac{i}{\hbar} \frac{p^2}{2m}t \right]\\
        &= \exp\left[ -\frac{i}{\hbar} Et \right]
    \end{align}
    where \(E = p^2/2m\).
    We see that we get a plane wave of classical energy \(E\), which is what we would expect.
    \begin{wrn}
        Notice that we are using a different Fourier transform convention here, this is simply for convenience.
    \end{wrn}
    
    \subsubsection{Normalisation}
    \epigraph{Just don't worry about it.}{Roger Horsley}
    The normalisation constant, \(\nu(\varepsilon)\), diverges as \(\varepsilon \to 0\).
    This means that the amplitude diverges for infinitesimal time intervals.
    This is all fine since for finite time intervals all of the diverging cancels out and we are left with a finite quantity proportional to
    \begin{equation}
        \lim_{N\to \infty} \nu(\varepsilon)\sqrt{\frac{2\pi i\hbar \varepsilon}{m}}.
    \end{equation}
    For this to be finite we have some freedom in \(\nu(\varepsilon)\), we simply need
    \begin{equation}\label{eqn:single amplitude freedom in normalisation}
        \nu(\varepsilon) \propto \frac{1}{\sqrt{\varepsilon}}(1 + \order(\varepsilon))
    \end{equation}
    since this will always be finite in the limit of \(\varepsilon \to 0\).
    We use this freedom to cancel out a bunch of constants and to keep things simple we just don't include any \(\order(\varepsilon)\) terms.
    
    \subsubsection{Orthogonality}
    Since \(\braket{x_b, t_b}{x_a, t_a}\) are Gaussians in \(x_b - x_a\) with width \(\sigma = \sqrt{\hbar T/m}\) and unit area (another advantage of the choice of normalisation) we can recover the orthonormality condition that as \(t_b \to t_a\)
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} \to \delta(x_b - x_a).
    \end{equation}
    Now we can make sense of this as if \(x_b\) and \(x_a\) are distinct points then in zero time there is no way to get from \(x_b\) to \(x_a\).
    This follows from identifying one common definition of the delta distribution as the limit of the function sequence \((\delta_\varepsilon(x))\) where
    \begin{equation}
        \delta_{\varepsilon} = \sqrt{\frac{a}{\pi \varepsilon}} \e^{-ax^2/\varepsilon}.
    \end{equation}
    
    \subsubsection{Completeness}
    A final reason to suggest that we have made a good choice of normalisation, which is really a result of the orthogonality suggested in the previous note, is that the completeness relation
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \int_{-\infty}^{\infty} \braket{x_b, t_b}{x, t} \braket{x, t}{x_a, t_a} \dd{x}
    \end{equation}
    for all \(t \in [t_a, t_b]\).
    This follows by splitting the path in two.
    
    \subsection{Free Particle: The Action Strikes Back}
    Now that we have done this full derivation we discuss a shorter method.
    This shorter method applies to many simple cases, but cannot be applied to more complicated systems for which we need to use the discrete method from above.
        
    This alternative derivation follows from the observation that we ended up with the classical action at the end of the longer derivation.
    Consider quantum fluctuations about the classical path.
    In particular consider the path
    \begin{equation}
        x(t) = \bar{x}(t) + \eta(t)
    \end{equation}
    where \(\eta\colon[t_a, t_b] \to \reals\) is some arbitrary function with the boundary conditions \(\eta(t_a) = \eta(t_b) = 0\), which keeps the endpoints of \(x\) fixed as \(x_a\) and \(x_b\).
    The action associated with this path is
    \begin{align}
        S[x] &= \int_{t_a}^{t_b} \frac{1}{2}m\left( \diff*{(\bar{x} + \eta)}{t} \right)^2 \dd{t}\\
        &= \frac{m}{2}\int_{t_a}^{t_b} (\dot{\bar{x}} + \dot{\eta})^2 \dd{t}\\
        &= \frac{m}{2} \int_{t_a}^{t_b} \dot{\bar{x}}^2 + \dot{\eta})^2 + 2\dot{\bar{x}}\dot{\eta} \dd{t}\\
        &= \textcolor{highlight}{\frac{m}{2} \int_{t_a}^{t_b} \dot{\bar{x}}^2 \dd{t}} + \frac{m}{2} \textcolor{complementary}{\int_{t_a}^{t_b} \dot{\eta}^2\dd{t}} + \textcolor{highlightpurple}{\int_{t_a}^{t_b} \dot{\bar{x}}\dot{\eta}\dd{t}}\\
        &= \textcolor{highlight}{S[\bar{x}]} + \frac{m}{2}\textcolor{complementary}{[\eta]_{t_a}^{t_b}} + \textcolor{highlightpurple}{[\dot{\bar{x}}\eta]_{t_a}^{t_b}} \textcolor{highlightpurple}{- \int_{t_a}^{t_b} \ddot{\bar{x}}\eta \dd{t}}\\
        &= S[\bar{x}].
    \end{align}
    The second and third terms vanishes as \(\eta(t_b) = \eta(t_a) = 0\).
    The last term vanishes as the equations of motion for a free particle give us \(\ddot{\bar{x}} = 0\) (i.e. a free particle doesn't accelerate).
    
    Since \(\bar{x}\) is a fixed path we have \(\dl{x_n} = \dl{\eta_n}\) for all \(n\).
    This gives us
    \begin{equation}
        \int_{x_a}^{x_b} \DL{x} = \int_0^0 \DL{\eta}.
    \end{equation}
    \begin{rmk}
        The limits look a bit weird here because they are just a reminder of the boundary conditions, in this case the zeros emphasise that the result is independent of \(x_a\) and \(x_b\).
    \end{rmk}
    It follows that
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \int_{x_a}^{x_b} \DL{x} \e^{iS[x(t)]/\hbar} = \e^{iS_{\cl}/\hbar}\int_0^0 \DL{\eta} \e^{iS[\eta]/\hbar}.
    \end{equation}
    This integral is independent of \(x_a\) and \(x_b\) so is a function only of \(t_a\) and \(t_b\).
    Further, the situation is translation invariant and so is a function only of \(T = t_b - t_a\).
    That is
    \begin{equation}
        \int_0^0 \DL{\eta} \e^{iS[\eta]/\hbar} \eqqcolon F_0(T) = \braket{0, t_b}{0, t_a} = \braket{0, T}{0, 0}.
    \end{equation}
    Here we have used the translational invariance to choose \(t_b = T\) and hence \(t_a = 0\).
    
    We can explicitly compute this integral, but we don't have to.
    Instead notice that inserting completeness we have
    \begin{align}
        F_0(T) &= \braket{0, T}{0, 0}\\
        &= \int_{-\infty}^{\infty} \braket{0, T}{x, t}\braket{x, t}{0, 0} \dd{x}\\
        &= \int_{-\infty}^{\infty} F_0(T - t)\exp\left[ \frac{imx^2}{2\hbar(T - t)} \right] F_0(t)\exp\left[ \frac{imx^2}{2\hbar t} \right]  \dd{x}\\
        &= \sqrt{\frac{2\pi i \hbar (T - t) t}{mT}} F_0(T - t)F_0(T).
    \end{align}
    We can now see by inspection that we must have \(F_0(t) \propto 1/\sqrt{t}\).
    Alternatively we can take the limit that \(T \gg t\), and hence \(T - t \approx T\).
    Either way a little bit of algebra gives us
    \begin{equation}
        F_0(t) = \sqrt{\frac{m}{2\pi i\hbar t}}.
    \end{equation}

    We have now arrived at the same point as the previous method but we didn't have to do any of the nasty maths like treating the discrete path integral. 
    
    \section{Harmonic Oscillator}
    The Lagrangian for the harmonic oscillator is
    \begin{equation}
        \lagrangian = \frac{1}{2}m\dot{x}^2 - \frac{1}{2}m\omega^2x^2.
    \end{equation}
    Bolstered by our success with the free particle we write \(x = \bar{x} + \eta\) where \(\bar{x}\) is the classical path satisfying \(\bar{x}(t_a) = x_a\) and \(\bar{x}(t_b) = x_b\), and \(\eta\) is the deviation from this path satisfying \(\eta(t_a) = \eta(t_b) = 0\).
    Using this we find
    \begin{align}
        S[x] &= \frac{m}{2}\int_{t_a}^{t_b} (\dot{\bar{x}} + \dot{\eta})^2 - \omega^2(\bar{x} + \eta)^2\dd{t}\\
        &= \frac{m}{2}\int_{t_a}^{t_b} \dot{\bar{x}}^2 - \omega^2 \bar{x}^2 \dd{t} + \frac{m}{2}\int_{t_a}^{t_b} \dot{\eta}^2- \omega^2\eta^2 \dd{t}\\
        &\qquad+ m\int_{t_a}^{t_b} (\dot{\bar{x}}\dot{\eta} - \omega^2\bar{x}\eta) \dd{t}\\
        &= S[\bar{x}] + S[\eta] + m[\eta\dot{\bar{x}}]_{t_a}^{t_b} - m\int_{t_a}^{t_b} \varepsilon(\ddot{\bar{x}} + \omega^2 x) \dd{t}\\
        &= S_{\cl} + S[\eta].
    \end{align}
    Note that \([\eta\dot{\bar{x}}]_{t_a}^{t_b}\) term is zero as \(\eta\) vanishes on the boundary and the final integral vanishes as we can use the equation of motion for a simple harmonic oscillator and replace \(\ddot{\bar{x}}\) in the integrand with \(-\omega^2\bar{x}\), and hence the integrand vanishes.
    
    It can be shown more generally that this result holds for any Lagrangian quadratic in \(x\), not just a simple harmonic oscillator.
    
    As for the free particle we have \(\DL{x} = \DL{\eta}\) and hence
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \e^{iS_{\cl}/\hbar} \int_0^0 \DL{\eta} \e^{iS[\eta]/\hbar} \eqqcolon F_{\omega}(T)\e^{iS_{\cl}/\hbar}.
    \end{equation}
    Again, since the \(\eta\) path integral has no \(x_a\) or \(x_b\) dependence the integral depends only on \(T\).
    
    We can evaluate \(F_{\omega}(T)\) directly by discretising the path integral or by expanding \(\eta(t)\) in a Fourier series.
    However, there is an easier way to do this using the completeness relation.
    We have
    \begin{equation}
        F_{\omega}(T) = \braket{0, T}{0, 0} = \int_{-\infty}^{\infty} \braket{0, T}{0, 0} \braket{x, t}{0, 0} \dd{x}.
    \end{equation}
    It can be shown\footnote{see problem sheet 2} that the classical action for a harmonic oscillator is
    \begin{equation}
        S_{\cl} = \frac{m\omega}{2\sin(\omega T)} [(x_a^2 + x_b^2)\cos(\omega T) - 2x_ax_b].
    \end{equation}
    From this it follows that
    \begin{align}
        F_{\omega}(T) &= \int_{-\infty}^{\infty} \bigg\{ F_{\omega}(T - t) \exp\left[ \frac{im\omega x^2}{2\hbar}\frac{\cos[\omega(T - t)]}{\sin[\omega(T - t)]} \right]\\
        &\hspace{5em}F_{\omega}(t) \exp\left[ \frac{im\omega x^2}{2\hbar} \frac{\cos(\omega t)}{\sin(\omega t)} \right]\bigg\} \dd{x}\\
        &= F_\omega(T - t)F_\omega(t) \sqrt{\frac{2\pi i\hbar \sin[\omega(T - t)\sin(\omega t)]}{m \omega \sin(\omega T)}}.
    \end{align}
    Noticing that this implies \(F_\omega(t) \propto 1/\sqrt{\sin(\omega t)}\), or taking the limit \(T \gg t\), we can show that
    \begin{equation}
        F_{\omega}(t) = \sqrt{\frac{m \omega}{2\pi i\hbar \sin (\omega t)}}.
    \end{equation}
    Notice that in the limit \(\omega \to 0\) approximating \(\sin(\omega t) \approx \omega t\) we recover \(F_0(t)\).
    That is to say that the free particle is a special case of the harmonic oscillator with zero spring constant.
    This is exactly what we would expect classically.
    
    \section{Forced Harmonic Oscillator}\label{sec:forced harmonic oscillator}
    The forced harmonic oscillator includes an additional term linear in \(x\):
    \begin{equation}
        \lagrangian = \frac{1}{2}m(\dot{x}^2 - \omega^2x^2) + J(t)x
    \end{equation}
    where \(J\colon[t_a, t_b]\to\reals\) is some arbitrary function of time.
    The equation of motion for the classical path is still quite simple:
    \begin{equation}
        \ddot{\bar{x}} = \omega^2\bar{x} = \frac{J}{m}.
    \end{equation}
    We can split the action into two parts as usual by considering the path \(x = \bar{x} + \eta\) where \(\eta\) satisfies \(\eta(t_a) = \eta(t_b) = 0\).
    We then have
    \begin{align}
        S[\bar{x} + \eta, J] &= \frac{m}{2}\int\left[ (\dot{\bar{x}} + \dot{\eta})^2 - \omega^2(\bar{x} + \eta)^2 + \frac{2J}{m}(\bar{x} + \eta) \right] \dd{t}\\
        &= \frac{m}{2} \int_{t_a}^{t_b} (\dot{\bar{x}} + \dot{\eta})^2 - \omega^2(\bar{x} + \eta)^2 + \frac{2J}{m}(\bar{x} + \eta) \dd{t}\\
        &= \textcolor{highlight}{\frac{m}{2} \int_{t_a}^{t_b}  \dot{\bar{x}}^2 - \omega^2\bar{x}^2 + \frac{2J}{m}\bar{x} \dd{t}} + \textcolor{complementary}{\frac{m}{2} \int_{t_a}^{t_b} \dot{\eta}^2 - \omega^2\eta^2  \textcolor{highlightpurple}{{}+ \frac{2J}{m}\eta} \dd{t}}\\
        &\qquad+ m \int_{t_a}^{t_b}  \dot{\bar{x}}\dot{\eta} - \omega^2\bar{x}\eta \dd{t}\\
        &= \textcolor{highlight}{S[\bar{x}, J]} + \textcolor{complementary}{S[\eta, 0]} + m \int_{t_a}^{t_b} \dot{\bar{x}}\dot{\eta} - \omega^2\bar{x}\eta  \textcolor{highlightpurple}{{}+\frac{J}{m}\eta} \dd{t}\\
        &= S[\bar{x}, J] + S[\eta, 0] + m\underbrace{[\dot{\bar{x}}\eta]_{t_a}^{t_b}}_{=0} + m \int_{t_a}^{t_b} -\ddot{\bar{x}} \underbrace{- \omega^2\bar{x} + \frac{J}{m}}_{=\ddot{\bar{x}}} \dd{t}\\
        &= S[\bar{x}, J] + S[\eta, 0].
    \end{align}
    Here we integrated the \(\dot{\bar{x}}\dot{\eta}\) term by parts to get the final result.
    The boundary terms vanish since \(\eta(t_a) = \eta(t_b) = 0\) and substituting the classical equation of motion in for \(\ddot{x}\) shows that the remaining integral also vanishes.
    
    The main result then follows the same way as for the harmonic oscillator:
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = F_{\omega}(T) \e^{iS[\bar{x}, J]/\hbar}.
    \end{equation}
    This is the same prefactor, \(F_{\omega}(T)\), as for the un-driven case, the only difference is in the action.
    The classical action for a driven harmonic oscillator is horrible:
    \begin{align}
        S_{\cl} &= \frac{m\omega}{2\sin(\omega T)} \bigg[ (x_a^2 + x_b^2)\cos(\omega T) - 2x_ax_b\\
        &\qquad+ \frac{2x_b}{m\omega} \int_{t_a}^{t_b} \sin[\omega(t - t_a)]J(t) \dd{t}\\
        &\qquad+ \frac{2x_a}{m\omega} \int_{t_a}^{t_b} \sin[\omega(t_b - t)]J(t) \dd{t}\\
        &\qquad- \frac{2}{(m\omega)^2}\mkern-2mu \int_{t_a}^{t_b}\mkern-10mu\int_{t_a}^{t} \mkern-8mu\sin[\omega(t_b - t)]\sin[\omega(t' - t_a)] J(t)J(t') \dd{t'}\dd{t} \bigg]
    \end{align}
    
    \section{Arbitrary Potentials}
    We now consider the case of an arbitrary potential, \(V(x, t)\).
    We restrict ourselves to velocity independent potentials, so, for example, this doesn't work for electromagnetism, we will see the velocity dependent case in a tutorial.
    The Lagrangian with an arbitrary potential is
    \begin{equation}
        \lagrangian = \frac{1}{2}m\dot{x}^2 - V(x, t).
    \end{equation}
    For an arbitrary potential we turn to the definition of the path integral:
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &=\\
        &\mkern-90mu\lim_{N\to\infty} \int\!\dl{x_1}\dotsm\dl{x_N} \braket{x_b, t_b}{x_N, t_N} \braket{x_N, t_N}{x_{N-1}, t_{N-1}} \dotsm \braket{x_1, t_1}{x_a, t_a}\\
        &= \lim_{N\to\infty} \left( \prod_{n=1}^{N} \int\dl{x_n} \right) \left( \prod_{n=0}^{N} \braket{x_{n+1}, t_{n+1}}{x_n, t_n} \right).
    \end{align}
    
    Consider the infinitesimal amplitude between two time steps:
    \begin{equation}
        \braket{x_{n+1}, t_{n+1}}{x_n, t_n} = \sqrt{\frac{m}{2\pi i\hbar\varepsilon}} \exp\left[ \frac{i}{\hbar}\varepsilon\left[ \frac{m}{2}\left( \frac{x_{n+1} - x_n}{\varepsilon} \right)^2 - V(x_n, t_n) \right] \right].
    \end{equation}
    Recall we define \(\varepsilon = t_{n+1} - t_n\).
    
    We have taken here the free particle normalisation.
    This is justified by the potential being of higher order in \(\varepsilon\) and we were free to choose \(\nu(\varepsilon) = \sqrt{m/(2mi\hbar\varepsilon)}(1 + \order(\varepsilon))\) (see \cref{eqn:single amplitude freedom in normalisation}) for our normalisation.
    We can further justify this by showing it works for the simple harmonic case (and therefore for all potentials up to second order in \(\varepsilon\)):
    \begin{align}
        F_{\omega}(\varepsilon) &= \sqrt{\frac{m\omega}{2\pi i\hbar\sin(\omega \varepsilon)}}\\
        &= \sqrt{\frac{m}{2\pi i\hbar\varepsilon}}\left( 1 + \frac{1}{12}\omega^2\varepsilon^2 + \dotsb \right)\\
        &= F_0(\varepsilon) + \order(\varepsilon^{3/2}).
    \end{align}
    Here we have used a series expansion for \([\sin x]^{-1/2}\).
    Another justification which we will show in a tutorial is that when we consider the limiting behaviour of
    \begin{equation}
        \psi(x, t + \varepsilon) = \int \braket{x, t + \varepsilon}{y, t} \braket{y, t}{\psi} \dd{y}
    \end{equation}
    we get the Schr\"odinger equation, which we assume is true and hence justifies our choice.
    
    \subsection{Phase Space Path Integral}
    It can be shown that the following identity holds:
    \begin{multline}
        \exp\left[ \frac{im\varepsilon}{2\hbar} \left( \frac{x_{n+1} - x_n}{\varepsilon} \right)^2 \right]\\
        = \sqrt{\frac{2\pi\hbar i\varepsilon}{m}} \int_{-\infty}^{\infty} \frac{1}{\pi\hbar}\exp[-\frac{i\varepsilon}{2m\hbar}p_n^2 + \frac{i}{\hbar}p_n(x_{n+1} - x_n)] \dd{p_n}.
    \end{multline}
    From this we find that the infinitesimal amplitude is
    \begin{multline}\label{eqn:infinitesimal amplitude}
        \braket{x_{n+1}, t_{n+1}}{x_n, t_n}\\
        = \int_{-\infty}^{\infty} \frac{1}{2\pi\hbar} \exp\left[ \frac{ip_n}{\hbar}(x_{n+1} - x_n) - \frac{i\varepsilon}{2m\hbar}p_n^2 - \frac{i\varepsilon}{2m\hbar}V(x_n, t_n) \right] \dd{p_n}.
    \end{multline}
    Inserting this into the definition of the path integral we have
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &= \lim_{N\to\infty} \left( \prod_{n=1}^{N} \int\dl{x_n} \right) \left( \prod_{n=0}^{N} \braket{x_{n+1}, t_{n+1}}{x_n, t_n} \right)\\
        &= \lim_{N\to\infty} \left( \prod_{n=1}^{N} \int\dl{x_n} \right) \left( \prod_{n=0}^{N} \int \frac{\dl{p_n}}{2\pi\hbar} \right)\notag\\
        &\qquad \times \exp\left[ \frac{i\varepsilon}{\hbar} \sum_{n=0}^{N} \left[ p_n \left( \frac{x_{n+1} - x_n}{\varepsilon} \right) - \frac{p_n^2}{2m} - V(x_n, t_n) \right] \right]\\
        &\eqqcolon \int\DL{x} \int\DL{p} \exp\left[ \frac{i}{\hbar} \int_{t_a}^{t_b} p\dot{x} - \hamiltonian(x, p, t) \right]
    \end{align}
    where we define
    \begin{equation}
        \int\DL{p} \coloneqq \prod_{n=0}^{\infty} \int \frac{\dl{p_n}}{2\pi\hbar},
    \end{equation}
    and 
    \begin{equation}
        \hamiltonian(x, p, t) \coloneqq \frac{p^2}{2m} + V(x, t)
    \end{equation}
    as the \defineindex{Hamiltonian}, which requires us to identify \(p\) with the momentum.
    Note that we have used \((x_{n+1} - x_n)/\varepsilon \to \dot{x}\).
    We can then identify
    \begin{equation}
        p\dot{x} - \hamiltonian = \lagrangian
    \end{equation}
    as the inverse of the Legendre transformation that defines the Hamiltonian.
    
    \section{Free Particle: Return of the Phase Space}
    Using the phase space path integral we can derive the amplitude for a free particle for a third time.
    We start with
    \begin{multline}
        \braket{x_b, t_b}{x_a, t_a} = \lim_{N\to\infty} \left( \prod_{n=1}^{N} \dl{x_n} \right) \left( \prod_{n=0}^{N} \int\frac{\dl{p_n}}{2\pi\hbar} \right)\\
        \times \exp\left[ \frac{i}{\hbar} \sum_{n=0}^{N} \left[ p_n(x_{n+1} - x_n) - \frac{p_n^2}{2m}\varepsilon \right] \right].
    \end{multline}
    We can split the exponential into two:
    \begin{equation}
        \exp\left[ \frac{i}{\hbar} \sum_{n=0}^{N} p_n(x_{n+1} - x_n) \right]\exp \left[ - \frac{i}{\hbar} \sum_{n=0}^{N} \frac{p_n^2}{2m}\varepsilon \right].
    \end{equation}
    Consider the sum in the first of these:
    \begin{align}
        \sum_{n=0}^{N} p_n(x_{n+1} - x_n) &\\
        &\mkern-50mu=p_0x_1 - p_0x_0 + p_1x_2 - p_1x_1 + p_2x_3 - p_2x_2 + p_3x_4 - p_3x_3\\
        +&\dotsb + p_{N-1}x_N - p_{N-1}x_{N-1} + p_Nx_{N+1} - p_Nx_N\\
        &\mkern-50mu= x_0p_0 + x_1(p_0 - p_1) + x_2(p_1 - p_2) + x_3(p_2 - p_3)\\
        &+ \dotsb + x_{N-1}(p_{N-1} - p_N) + x_{N+1}p_N\\
        &\mkern-50mu= x_{N+1}p_N - x_0p_0 + \sum_{n=1} x_n(p_{n-1} - p_n)\\
        &\mkern-50mu= x_bp_N - x_ap_0 + \sum_{n=1} x_n(p_{n-1} - p_n)\\
    \end{align}
    We then have
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &= \lim_{N\to\infty} \left( \prod_{n=1}^{N} \int\dl{x_n} \right) \left( \prod_{n=0}^{N} \int\frac{\dl{p_n}}{2\pi\hbar} \right) \exp \left[ - \frac{i}{\hbar} \sum_{n=0}^{N} \frac{p_n^2}{2m}\varepsilon \right]\notag\\
        & \qquad \times \exp\left[ \frac{i}{\hbar} \left( x_bp_N - x_ap_0 + \sum_{n=1} x_n(p_{n-1} - p_n) \right) \right]\\
        &= \lim_{N\to\infty} \left( \prod_{n=0}^{N} \int \frac{\dl{p_n}}{2\pi\hbar} \right) \left( \prod_{n=1}^{N} \delta(p_n - p_{n-1}) \right)\notag\\
        &\qquad \times \exp\left[ \frac{i}{\hbar} \left( p_Nx_b - p_0x_a - \frac{\varepsilon}{2m} \sum_{n=0}^{N} p_n^2 \right) \right].
    \end{align}
    Here\footnote{is it valid to swap the integrals here? Who knows, this whole thing is mathematically dubious and we get the same free particle result at the end, so its probably fine.} we have identified the integral representation of the Dirac delta:
    \begin{equation}
        \int \e^{ix(p - p')} \dd{x} = \delta(p - p').
    \end{equation}
    We can then use the delta distribution to greatly reduce the number of integrals to one since for this answer to be nonzero we must have \(p_n = p_{n-1}\) for all \(n\).
    We then have
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &= \lim_{N\to \infty} \int_{-\infty}^{\infty} \frac{\dl{p_0}}{2\pi\hbar} \exp\left[ \frac{i}{\hbar}p_0(x_b - x_a) - \frac{i}{2m\hbar}(N + 2)\varepsilon p_0^2 \right]\\
        &= \sqrt{\frac{m}{2\pi \iota\hbar T}} \exp\left[ \frac{i}{\hbar}\frac{m}{2}\frac{(x_b - x_a)^2}{T} \right]
    \end{align}
    where we have identified \((N + 1)\varepsilon = T = t_b - t_a\).
    
    \chapter{Connection to Standard Quantum Mechanics}
    Consider again \cref{eqn:infinitesimal amplitude}, which we can write as
    \begin{multline}
        \braket{x_{n+1}, t_{n+1}}{x_n, t_n} = \int_{-\infty}^{\infty} \frac{\dl{p_n}}{2\pi\hbar} \exp\left[ \frac{i}{\hbar}p_n(x_{n+1} - x_n) \right]\\
        \times \exp\left[ -\frac{i}{\hbar}\frac{p_n^2}{2m}\varepsilon \right] \exp\left[ -\frac{i}{\hbar}\varepsilon V(x_n, t_n) \right].
    \end{multline}
    We introduce a basis of time independent momentum and position eigenstates satisfying
    \begin{equation}
        \operator{p}\ket{p_n} = p_n\ket{p_n}, \qqand \operator{x}\ket{x_n} = x_n\ket{x_n}.
    \end{equation}
    We can then write the infinitesimal amplitude above as
    \begin{equation}
        \int_{-\infty}^{\infty} \dl{p_n} \braket{x_{n+1}}{p_n}\braket{p_n}{x_n} \exp\left[ -\frac{i}{\hbar}\frac{p_n^2}{2m}\varepsilon \right] \exp[-\frac{i}{\hbar}\varepsilon V(x_n, t_n)].
    \end{equation}
    Here we have used
    \begin{equation}
        \braket{x}{p} = \frac{1}{\sqrt{2\pi\hbar}}\e^{ipx/\hbar}.
    \end{equation}
    We can write this as
    \begin{equation}
        \int_{-\infty}^{\infty}  \dl{p_n} \bra{x_{n+1}} \exp\left[ -\frac{i}{\hbar}\frac{p_n^2}{2m}\varepsilon \right] \ket{p_n}\bra{p_n} \exp\left[ -\frac{i}{\hbar} \varepsilon V(x_n, t_n) \right] \ket{x_n}.
    \end{equation}
    Noticing that we have something of the form \(\e^{ap_n}\ket{p_n} = \e^{a\operator{p}}\ket{p_n}\) we can write this as
    \begin{equation}
        \int_{-\infty}^{\infty}  \dl{p_n} \bra{x_{n+1}} \exp\left[ -\frac{i}{\hbar}\frac{\operator{p}^2}{2m}\varepsilon \right] \ket{p_n}\bra{p_n} \exp\left[ -\frac{i}{\hbar} \varepsilon V(\operator{x}, t_n) \right] \ket{x_n}.
    \end{equation}
    We have also identified here \(V(x_n, t_n)\) with \(V(\operator{x}, t_n)\).
    Strictly this requires that \(V\) is analytic allowing us to Taylor expand and then replace \(x_n\) with \(\operator{x}\).
    This is generally not an issue with physically reasonable potentials.
    We can identify here the factor of \(\int \dl{p_n} \ket{p_n}\bra{p_n} = \idop\) and hence we have
    \begin{equation}
        \bra{x_{n+1}} \exp\left[ -\frac{i}{\hbar}\frac{\operator{p}^2}{2m}\varepsilon \right] \exp\left[ -\frac{i}{\hbar} \varepsilon V(\operator{x}, t_n) \right] \ket{x_n}.
    \end{equation}
    
    The two exponentials do not commute, however, we do have
    \begin{align}
        \e^{\operator{A}}\e^{\operator{B}} &= (\idop + \operator{A} + \dotsb)(\idop + \operator{B} + \dotsb)\\
        &= \idop + \operator{A} + \operator{B} + \dotsb\\
        &= \e^{\operator{A} + \operator{B} + \dotsb}
    \end{align}
    This is a truncated form of the more general \defineindex{Baker--Campbell--Hausdorff formula}:
    \begin{equation}
        \e^{\operator{A}}\e^{\operator{B}} = \exp\left[ \operator{A} + \operator{B} + \frac{1}{2}\commutator{\operator{A}}{\operator{B}} + \frac{1}{12}\commutator{\operator{A}}{ \commutator{\operator{A}}{\operator{B}}} - \frac{1}{12}\commutator{\operator{B}}{\commutator{\operator{A}}{\operator{B}}} + \dotsb \right].
    \end{equation}
    Which reduces to 
    \begin{equation}
        e^{\operator{A}}\e^{\operator{B}} = \exp\left[ \operator{A} + \operator{B} + \frac{1}{2}\commutator{\operator{A}}{\operator{B}} \right]
    \end{equation}
    in the case where \(\operator{A}\) and \(\operator{B}\) commute with their commutator, \(\commutator{\operator{A}}{\operator{B}}\), and further reduces to
    \begin{equation}
        \e^{\operator{A}}\e^{\operator{B}} = \e^{\operator{A} + \operator{B}}
    \end{equation}
    in the case where \(\operator{A}\) and \(\operator{B}\) commute.
    
    Ignoring terms of second order in \(\varepsilon\) we then have
    \begin{align}
        \braket{x_{n+1}, t_{n+1}}{x_n, t_n} &= \bra{x_{n+1}} \exp[-\frac{i}{\hbar}\varepsilon \left( \frac{\operator{p}^2}{2m} + V(\operator{x}, t_n) \right)]\ket{x_n}\\
        &= \bra{x_{n+1}} \exp\left[ -\frac{i}{\hbar}\varepsilon H(\operator{x}, \operator{p}, t_n) \right]\ket{x_n}.
    \end{align}
    Here we have identified the \define{Hamiltonian operator}\index{Hamiltonian!operator}
    \begin{equation}\label{eqn:hamiltonian operator}
        \operator{\hamiltonian}(t) = H(\operator{x}, \operator{p}, t) \coloneqq \frac{\operator{p}^2}{2m} + V(\operator{x}, t).
    \end{equation}
    
    Now consider some time, \(t_0\), which is infinitesimally close to both \(t_n\) and \(t_{n+1}\).
    We have \(\varepsilon = t_{n+1} - t_n = (t_{n+1} - t_0) - (t_n - t_0)\).
    Hence
    \begin{align}
        \braket{x_{n+1}, t_{n+1}}{x_n, t_n} &\approx \bra{x_{n+1}} \exp\left[ -\frac{i}{\hbar}[(t_{n+1} - t_0) - (t_n - t_0)]\operator{\hamiltonian}(t_0) \right]\\
        &\approx \bra{x_{n+1}} \exp\left[ -\frac{i}{\hbar}(t_{n+1} - t_0)\operator{\hamiltonian}(t_0) \right]\notag\\
        &\qquad\qquad\times\exp\left[ -\frac{i}{\hbar}(t_n - t_0)\operator{\hamiltonian}(t_0) \right] \ket{x_n}.
    \end{align}
    The first of these holds up to second order in \(\varepsilon\) and the latter to second order in \((t_{n+1} - t_0)\) and \((t_n - t_0)\).
    We also approximate \(\operator{\hamiltonian}(t_n) = \operator{\hamiltonian}(t_0)\), which is valid for sufficiently smooth \(H\).
    
    Now define a time dependent state \(\ket{x, t}\) for \(t - t_0 = \order(\varepsilon)\) by
    \begin{equation}
        \ket{x, t} = \exp\left[ \frac{i}{\hbar}(t - t_0) \right]\ket{x} = \operator{U}^\hermit(t, t_0)\ket{x}
    \end{equation}
    where we define the \defineindex{time evolution operator}, \(\operator{U}\), such that
    \begin{equation}
        \operator{U}^\hermit(t, t_0) = \exp\left[ \frac{i}{\hbar}(t - t_0) \right]\operator{\hamiltonian}(t_0)
    \end{equation}
    for infinitesimal \(t - t_0\).
    
    The first thing to notice is that at \(t = t_0\) we have \(\ket{x, t_0} = \ket{x}\), this means that the basis, \(\{\ket{x, t_0}\}\) is independent of when it is inserted.
    However, it is often useful to consider \(\ket{x, t_0}\) with the \(t_0\) written out.
    
    \section{Properties of the Time Evolution Operator}
    The infinitesimal time evolution operator as defined above has several properties.
    Firstly it is unitary, since we can view the action of changing \(\ket{x}\) to \(\ket{x, t}\) as a basis change.
    This means
    \begin{equation}
        \operator{U}^\hermit(t, t_0) = \operator{U}^{-1}(t, t_0).
    \end{equation}
    We have the boundary condition on \(\operator{U}\) that
    \begin{equation}
        \operator{U}(t_0, t_0) = \idop.
    \end{equation}
    The product rule for time evolution operators is
    \begin{equation}
        \operator{U}(t, t')\operator{U}(t', t_0) = \operator{U}(t, t_0).
    \end{equation}
    This is simple to show with the exponential form and holds up to second order in \(t - t'\) and \(t' - t_0\).
    We can use this process to build up a time evolution operator for a finite time interval as a product of many time evolution operators for infinitesimal time intervals.
    Considering the special case for the product rule of \(t = t_0\) we have
    \begin{equation}
        \operator{U}(t_0, t')\operator{U}(t', t_0) = \operator{U}(t_0, t_0) = \idop.
    \end{equation}
    Hence,
    \begin{equation}
        \operator{U}^{-1}(t, t_0) = \operator{U}(t_0, t).
    \end{equation}
    This allows us to compute the state in the past.
    
    \subsection{A Differential Equation for the Time Evolution Operator}
    Consider the following Taylor expansion:
    \begin{align}
        \operator{U}(t + \delta t, t_0) &= \operator{U}(t + \delta t, t)\operator{U}(t, t_0)\\
        &= \left( \operator{U}(t, t) + \diff*{\operator{U}(t, t)}{t} + \dotsb \right)\operator{U}(t, t_0)\\
        &= \operator{U}(t, t_0) + \delta t \diffp*{\operator{U}(t, t_0)}{t} + \dotsb.
    \end{align}
    We can also Taylor expand the definition of the infinitesimal time evolution operator:
    \begin{align}
        \operator{U}(t + \delta t, t_0) &= \operator{U}(t + \delta t, t)\operator{U}(t, t_0)\\
        &= \exp\left[ -\frac{i}{\hbar}\delta t\operator{\hamiltonian}(t) \right] \operator{U}(t, t_0)\\
        &= \left( 1 - \frac{i}{\hbar}\delta t \operator{\hamiltonian}(t) + \dotsb \right) \operator{U}(t, t_0).
    \end{align}
    Comparing these two expansions we identify
    \begin{equation}\label{eqn:ih dU/dt = HU}
        \diffp*{\operator{U}(t, t_0)}{t} = -\frac{i}{\hbar} \operator{\hamiltonian}(t) \operator{U}(t, t_0) \implies i\hbar\diffp{\operator{U}(t, t_0)}{t} = \operator{\hamiltonian}(t)\operator{U}(t, t_0)
    \end{equation}
    
    Now suppose that we have a conservative system, that is \(\operator{\hamiltonian}\) is independent of time, this is usually the case.
    We then have time translation invariance\footnote{conservation of energy implies time translation invariance due to Noether's theorem}, which means
    \begin{equation}
        \operator{U}(t, t_0) = \operator{U}(t - t_0),
    \end{equation}
    that is the time evolution operator depends only on the length of the time interval, not the end points.
    When this is the case we then have
    \begin{equation}
        \operator{U}(t, t_0) = \exp\left[ -\frac{i}{\hbar}(t - t_0)\operator{\hamiltonian} \right],
    \end{equation}
    which we can show by either considering an infinite number of infinitesimal time steps or by solving \cref{eqn:ih dU/dt = HU}.
    
    If instead our Hamiltonian has explicit time dependence then it can be shown that the solution to \cref{eqn:ih dU/dt = HU} is
    \begin{equation}
        \operator{U}(t, t_0) = \timeorder \exp\left[ -\frac{i}{\hbar} \int_{t_0}^t \operator{\hamiltonian}(t') \dd{t'} \right].
    \end{equation}
    Here \(\timeorder\) represents time ordering, a concept we will see later.
    We will return to this case once we have a better understanding of time ordering.
    
    We'll derive one final useful result relating to the time evolution operator.
    We have now shown that for finite \(t - t_0\) we have
    \begin{equation}
        \ket{x, t} = \operator{U}^\hermit(t, t_0)\ket{x}, \qqand \bra{x, t} = \bra{x}\operator{U}(t, t_0).
    \end{equation}
    We also have \(\ket{x, t_0} = \ket{x}\), by setting \(t = t_0\).
    It follows that
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &= \bra{x_b} \operator{U}(t_b, t_0) \operator{U}^\hermit(t_a, t_0)\ket{x_a}\\
        &= \bra{x_b} \operator{U}(t_b, t_0) \operator{U}(t_0, t_a)\ket{x_a}\\
        &= \bra{x_b} \operator{U}(t_b, t_a) \ket{x_a}.
    \end{align}
    
    \section{The \texorpdfstring{Schr\"odinger}{Schrodinger} Equation}
    Consider the wave function, \(\psi\), associated with the state \(\ket{\psi}\) at time \(t\), this is given by
    \begin{equation}
        \psi(x, t) = \braket{x, t}{\psi} = \bra{x} \operator{U}(t, t_0) \ket{\psi}.
    \end{equation}
    Differentiating with respect to \(t\) and multiplying through by \(i\hbar\) we get
    \begin{equation}
        i\hbar\diffp{\psi(x, t)}{t} = \bra{x} i\hbar\diffp{\operator{U}(t, t_0)}{t}\ket{\psi} = \bra{x} H(\operator{x}, \operator{p}, t) \operator{U}(t, t_0) \ket{\psi}.
    \end{equation}
    Here we have used the fact that the states \(\ket{x}\) and \(\ket{\psi}\) are time independent and the result of \cref{eqn:ih dU/dt = HU} for the time derivative of \(\operator{U}(t, t_0)\).
    
    Recall that \(\bra{x}\operator{x} = x\operator{x}\) and \(\bra{x}\operator{p} = -i\hbar\diffp{\bra{x}}/{x}\) and so
    \begin{align}
        i\hbar\diffp{\psi(x, t)}{t} &= H\left( x, -i\hbar\diff{}{x}, t \right) \bra{x}\operator{U}(t, t_0) \ket{\psi}\\
        &=  H\left( x, -i\hbar\diff{}{x}, t \right)\psi(x, t).
    \end{align}
    Substituting in the functional form of the Hamiltonian from \cref{eqn:hamiltonian operator} we have
    \begin{equation}
        i\hbar\diffp{\psi}{t} = \left[ -\frac{\hbar^2}{2m}\diff[2]{}{x} + V(x, t) \right] \psi(x, t).
    \end{equation}
    
    This is the famous \defineindex{Schr\"odinger equation}.
    We have derived it as a consequence of the path integral formalism.
    The reverse can also be done, path integrals can be derived from the Schr\"odinger equation.
    This shows that the two are equivalent and also justifies are claim that the path integral formulation should apply to the dynamics of a quantum system, assuming of course that we are willing to accept the efficacy of the Schr\"odinger equation.
    
    \section{\texorpdfstring{Schr\"odinger}{Schrodinger} and Heisenberg Pictures}
    There are two common pictures of basic quantum mechanics (as well as the path integral formulation and Dirac's picture which we will see later).
    These are the Schr\"odinger picture and Heisenberg picture.
    The Schr\"odinger picture is the most familiar for basic quantum mechanics.
    
    \subsection{\texorpdfstring{Schr\"odinger}{Schrodinger} Picture}
    In the Schr\"odinger picture we can define a time dependent state vector
    \begin{equation}
        \ket{\psi, t} = \operator{U}(t, t_0)\ket{\psi}.
    \end{equation}
    When \(t = t_0\) the states \(\ket{\psi, t_0}\) and \(\ket{\psi}\) coincide.
    This state automatically satisfies the Schr\"odinger equation since substituting \(\ket{\psi, t}\) into the Schr\"odinger equation reduces it to \cref{eqn:ih dU/dt = HU}.
    
    In the \defineindex{Schr\"odinger picture} the state of the system, \(\ket{\psi, t}\) is a time dependent state, and the position eigenstate, \(\ket{x}\), is time independent.
    Operators, which are defined through their spectra decompositions,
    \begin{equation}
        \operator{\xi} \coloneqq \int x\ket{\xi}\bra{\xi} \dd{x},
    \end{equation}
    are time independent.
    To make it clear that we are treating an object in the Schr\"odinger picture we will sometimes attach a subscript \(\schrodingerPicture\).
    So we could call the operator above \(\operator{\xi}_{\schrodingerPicture}\).
    
    \subsection{Heisenberg Picture}
    In the \defineindex{Heisenberg picture} the state vector, \(\ket{\psi}\), is time independent.
    Instead the position eigenstate, \(\ket{x, t}\), is time dependent.
    This then means that operators, defined through their spectral decomposition,
    \begin{equation}
        \operator{\xi}(t) \coloneqq \int \xi \ket{\xi, t}\bra{\xi, t} \dd{\xi},
    \end{equation}
    are time dependent.
    It follows then that
    \begin{align}
        \operator{\xi}(t) &= \int \xi \ket{\xi, t}\bra{\xi, t} \dd{\xi}\\
        &= \int \xi \operator{U}^\hermit(t, t_0) \ket{\xi}\bra{\xi} \operator{U}(t, t_0)\\
        &= \operator{U}^\hermit(t, t_0) \left( \int \xi \ket{\xi}\bra{\xi} \right) \operator{U}(t, t_0)\\
        &= \operator{U}^\hermit(t, t_0) \operator{\xi_{\schrodingerPicture}} \operator{U}(t, t_0).
    \end{align}
    This gives us the time evolution of an operator in the Heisenberg picture.
    
    To make it clear that we are treating something in the Heisenberg picture we will sometimes attach a subscript \(\heisenbergPicture\).
    So we could call the operator above \(\operator{\xi}_{\heisenbergPicture}(t)\).
    Often we will just explicitly write the time dependence and assume the Heisenberg picture.
    
    \subsection{Working Between Pictures}
    At time \(t = t_0\) all states and operators in both pictures coincide.
    The wave function is the same in either picture since
    \begin{equation}
        \psi(x, t) = \braket{x, t}{\psi} = \bra{x}\operator{U}(t, t_0)\ket{\psi} = \braket{x}{\psi, t}.
    \end{equation}
    Similarly expectation values are unchanged, to show this recall that the time evolution operator is unitary:
    \begin{align}
        \expected{\operator{O}}_{\heisenbergPicture} &= \bra{\psi} \operator{O}(t) \ket{\psi}\\
        &= \bra{\psi, t} \operator{U}(t, t_0)\operator{O}(t) \operator{U}^\hermit(t, t_0) \ket{\psi, t}\\
        &= \bra{\psi, t} \operator{U}(t, t_0)\operator{U}^\hermit(t, t_0) \operator{O}_{\schrodingerPicture} \operator{U}(t, t_0)\operator{U}^\hermit(t, t_0) \ket{\psi, t}\\
        &= \bra{\psi, t} \operator{O}_{\schrodingerPicture} \ket{\psi, t}\\
        &= \expected{\operator{O}}_{\schrodingerPicture}.
    \end{align}
    Here we have used
    \begin{gather}
        \ket{\psi} = \idop\ket{\psi} =  \operator{U}^\hermit\operator{U}\ket{\psi} = \operator{U}\ket{\psi, t}.\\
        \bra{\psi} = \bra{\psi}\idop = \bra{\psi}\operator{U}^\hermit\operator{U} = \bra{\psi, t}\operator{U},\\
        \operator{O}(t) = \operator{U}^\hermit \operator{O}_{\schrodingerPicture}\operator{U}.
    \end{gather}
    
    We have defined the time evolution of operators in the Heisenberg picture.
    We can now derive an equation of motion for operators
    \begin{align}
        i\hbar \diffp*{\operator{O}(t)}{t} &= i\hbar \diffp*{(\operator{U}^\hermit \operator{O} \operator{U})}{t}\\
        &= \left( i\hbar \diffp{\operator{U}^\hermit}{t} \right) \operator{O} \operator{U} + \operator{U}^\hermit \operator{O} \left( i\hbar \diffp{\operator{U}}{t} \right)\\
        &= -\operator{U}^\hermit \operator{\hamiltonian} \operator{O} \operator{U} + \operator{U}^\hermit \operator{O} \operator{\hamiltonian} \operator{U}.
    \end{align}
    Here we have used both \cref{eqn:ih dU/dt = HU} and its Hermitian conjugate,
    \begin{equation}
        -i\hbar\diff{\operator{U}}{t} = \operator{U}^\hermit \operator{\hamiltonian},
    \end{equation}
    note that \(\operator{\hamiltonian}\) is Hermitian.
    Continuing on we can insert the identity, in the form \(\idop = \operator{U}\operator{U}^\hermit\) to get
    \begin{align}
        i\hbar \diffp*{\operator{O}(t)}{t} &= -\operator{U}^\hermit \operator{\hamiltonian} \operator{U}\operator{U}^\hermit \operator{O} \operator{U} + \operator{U}^\hermit \operator{O} \operator{U}\operator{U}^\hermit \operator{\hamiltonian} \operator{U}\\
        &= -\operator{\hamiltonian}(t) \operator{O}(t) + \operator{O}(t) \operator{\hamiltonian}(t)\\
        &= \commutator{\operator{O}(t), \operator{\hamiltonian}_{\heisenbergPicture}(t)}.
    \end{align}
    Here
    \begin{equation}
        \operator{\hamiltonian}_{\heisenbergPicture}(t) = \operator{U}^\hermit(t, t_0)H(\operator{x}, \operator{p}, t)\operator{U}(t, t_0) = H(\operator{x}(t), \operator{p}(t), t).
    \end{equation}
    Care must be taken to distinguish between the Hamiltonian of a conservative system in the Heisenberg picture and the Hamiltonian of a system with an explicit time dependence in the Schr\"odinger picture.
    
    For the case of a time independent Hamiltonian we have \(\operator{\hamiltonian}_{\heisenbergPicture}(t) = H(\operator{x}, \operator{p}) = \operator{\hamiltonian}\) and then
    \begin{equation}
        i\hbar\diffp*{\operator{O}(t)}{t} = \commutator{\operator{O}(t)}{\operator{\hamiltonian}}.
    \end{equation}
    This is the \defineindex{Heisenberg equation of motion} for the time evolution of the position operator in the Heisenberg picture.
    
    If \(\diffp{\operator{O}(t)}/{t} = 0\), that is \(\commutator{\operator{O}(t)}{\operator{\hamiltonian}} = 0\), then we say that \(\operator{O}\) is \defineindex{conserved}.
    For example, momentum is conserved if \(\commutator{\operator{p}, \operator{\hamiltonian}} = 0\), this is the case, for example, for the free particle, but not for the Harmonic oscillator where
    \begin{align}
        \commutator{\operator{p}}{\operator{\hamiltonian}} &= \omega^2\commutator{\operator{p}}{\operator{x}^2}\\
        &= \omega^2(\commutator{\operator{p}}{\operator{x}}\operator{x} + \operator{x}\commutator{\operator{p}}{\operator{x}})\\
        &= -2i\hbar\omega^2\operator{x}.
    \end{align}
    
    In the rest of this course we will largely adopt the Heisenberg picture.
    
    \chapter{Transition Amplitude as a Green's Function}\label{sec:transition amplitude as a green's function}
    In \cref{sec:green's function} we commented that the transition amplitude, \(\braket{x_b, t_b}{x_a, t_a}\), is a Green's function.
    Now we show that it is a Green's function for the Schr\"odinger equation.
    Consider the wave function
    \begin{equation}
        \psi(x, t) = \braket{x, t}{\psi} = \int \braket{x, t}{x', t'} \braket{x', t'}{\psi} \dd{x'}.
    \end{equation}
    Define the function
    \begin{equation}
        G(x, x'; t, t') \coloneqq 
        \begin{cases}
            \braket{x, t}{x', t'}, & t > t',\\
            0 & t < t'
        \end{cases}
    \end{equation}
    We then have
    \begin{equation}\label{eqn:wave func times heaviside}
        \psi(x, t)\theta(t - t') = \int G(x, x'; t, t') \psi(x, t) \dd{x'}
    \end{equation}
    where \(\theta\) is the Heaviside step function defined by
    \begin{equation}\label{eqn:heaviside step function}
        \theta(t - t') \coloneqq
        \begin{cases}
            1, & t - t_0 > 0,\\
            1/2, & t - t_0 = 0,\\
            0, & t - t_0 < 0.
        \end{cases}
    \end{equation}
    
    Since \(\psi\) is a solution to the Schr\"odinger equation it follows that
    \begin{align}
        \left( -\frac{\hbar^2}{2m} \diffp[2]{}{x} + V(x, t) - i\hbar\diffp{}{t} \right) \psi(x, t)\theta(t - t') &\\
        &\hspace{-6cm}= -\frac{\hbar^2}{2m}\diff[2]{\psi}{x}\theta(t - t') + V(x, t)\psi(x, t)\theta(t - t')\\
        &\hspace{-5cm}- i\hbar\diffp{\psi}{t}\theta(t - t') - i\hbar\diffp{\theta}{t}\psi(x, t)\\
        &\hspace{-6cm}= \underbrace{\left( -\frac{\hbar^2}{2m}\diffp[2]{\psi}{x} + V(x, t)\psi(x, t) - i\hbar\diffp{\psi}{t} \right)}_{=\SE\psi = (\operator{\hamiltonian} - i\hbar\partial_t)\psi = 0}\theta(t - t') - i\hbar\diffp{\theta}{t}\psi(x, t)\\
        &\hspace{-6cm}= -i\hbar\diffp{\theta}{t}\psi(x, t)\\
        &\hspace{-6cm}= -i\hbar\delta(t - t')\psi(x, t)
    \end{align}
    where \(\delta\) is the Dirac delta, which is related to the Heaviside step function by\footnote{see the chapter on the Dirac delta in the methods of mathematical physics course.}
    \begin{equation}
        \diff*{\theta(t - t')}{t} = \delta(t - t').
    \end{equation}
    
    It follows from \cref{eqn:wave func times heaviside} that
    \begin{multline}
        \int \left( -\frac{\hbar^2}{2m} \diffp[2]{}{x} + V(x, t) - i\hbar\diffp{}{t} \right) G(x, x'; t, t')\psi(x', t') \dl{x'}\\
        = -i\hbar\delta(t - t')\psi(x, t).
    \end{multline}
    This must hold for all \(\psi\) and hence
    \begin{equation}
        \left( -\frac{\hbar^2}{2m}\diffp[2]{}{x} + V(x, t) - i\hbar\diffp{}{t} \right)G(x, x'; t, t') = -i\hbar\delta(t - t')\delta(x - x').
    \end{equation}
    This shows that \(G\) is Green's function for the Schr\"odinger equation.
    
    \section{Energy Eigenstate Representation}
    Consider a time independent Hamiltonian, \(\operator{\hamiltonian}\).
    For simplicity we assume a discrete spectrum, so this applies, for example, to the harmonic oscillator, but not the free particle.
    Let \(\{\ket{n}\}\) be the energy eigenbasis, that is
    \begin{equation}
        \operator{\hamiltonian}\ket{n} = E_n\ket{n}
    \end{equation}
    where we take \(n = 0, 1, \dotsc\).
    We also define the energy eigenfunctions
    \begin{equation}
        u_n(x) \coloneqq \braket{x}{n}.
    \end{equation}
    
    For \(t > 0\) we have
    \begin{align}
        G(x, y; t) &\coloneqq \braket{x, t}{y, 0}\\
        &\hphantom{:}= \bra{x} \e^{-it\operator{\hamiltonian}/\hbar}\ket{y}\\
        &\hphantom{:}= \sum_n \bra{x} \e^{-it\operator{\hamiltonian}/\hbar}\ket{n}\braket{n}{y}\\
        &\hphantom{:}= \sum_n \e^{-itE_n/\hbar}\braket{x}{n}\braket{n}{y}\\
        &\hphantom{:}= \sum_n \e^{-itE_n/\hbar} u_n(x)u_n^*(y).
    \end{align}
    Here we have used the relation
    \begin{equation}
        \e^{-it\operator{\hamiltonian}/\hbar}\ket{n} = \e^{-itE_n/\hbar}\ket{n}
    \end{equation}
    with the second exponential being simply a complex number and so commuting with the ket.
    
    From this we see that \(G\) contains all of the information about the wave functions and energy eigenstates.
    This explains why the path integral formulation is typically more complicated than solving the Schr\"odinger equation for a single wave function at a time.
    
    \section{Fourier Transform of the Amplitude}\label{sec:fourier transform of the amplitude}
    It is useful to define the Fourier transform of \(G\).
    In particular the Fourier transform from time to energy space, which is analogous to moving from position to momentum space.
    However, we need a slightly modified form of the Fourier transform, which includes an exponential damping factor \(\e^{it(i\varepsilon)/\hbar} = \e^{-t\varepsilon/\hbar}\) where \(\varepsilon > 0\) is arbitrarily small.
    This is to ensure convergence of the transform.
    We then define the Fourier transform as
    \begin{equation}
        \tilde{G}(x, y; E) \coloneqq \int_{-\infty}^{\infty} G(x, y; t)\e^{itE/\hbar}\e^{it(i\varepsilon)/\hbar} \dd{t}.
    \end{equation}
    
    Substituting in the energy eigenstate representation, and remembering that for \(t < 0\) we have \(G(x, y; t) = 0\), we get
    \begin{align}
        \tilde{G}(x, y; E) &= \sum_n \int_{0}^{\infty} \e^{-itE_n/\hbar}u_n(x)u_n^*(x) \e^{itE/\hbar} \e^{it(i\varepsilon)/\hbar} \dd{t}\\
        &= \sum_n u_n(x)u_n^*(y) \int_{0}^{\infty} \e^{it(E - E_n + i\varepsilon)}\dd{t}\\
        &= \sum_n u_n(x)u_n^*(y) \left[ i\hbar \frac{\e^{it(E - E_n + i\varepsilon)}}{E - E_n + i\varepsilon} \right]_{0}^{\infty}\\
        &= i\hbar \sum_n \frac{u_n(x)u_n^*(y)}{E - E_n + \varepsilon}
    \end{align}
    
    The bound states correspond to the poles of \(\tilde{G}\), which lie just below the real axis.
    The choice of \(\varepsilon > 0\) ensures that when we perform the inverse Fourier transform we recover \(G\):
    \begin{equation}
        G(x, y; t) = \frac{1}{2\pi\hbar} \int_{-\infty}^{\infty} \tilde{G}(x, y; E) \e^{-iEt/\hbar} \dd{E}.
    \end{equation}
    If \(t < 0\) then we can close a contour\footnote{See methods of theoretical/mathematical physics notes for more details on complex analysis} in the upper half plane with a semicircle.
    The contour integral gives zero as it contains no poles and the integral over the semicircle vanishes by Jordan's lemma, so we have \(G(x, y; t) = 0\).
    For \(t > 0\) we can close the contour in the lower half plane with another semicircle.
    The integral over the semicircle vanishes again due to Jordan's lemma.
    The contour now poles corresponding to each energy eigenstate and so by the residue theorem
    \begin{align}
        G(x, y; t) &= \frac{1}{2\pi\hbar} \int_{-\infty}^{\infty} \tilde{G}(x, y; E) \e^{-iEt/\hbar} \dd{E}\\
        &= \sum_n u_n(x) u_n^*(y) \frac{1}{2\pi \hbar} i\hbar \oint \frac{1}{E - E_n + i\varepsilon} \dd{E}\\
        &= \sum_n u_n(x) u_n^*(y) \frac{1}{2\pi \hbar} (i\hbar) (2\pi i) \Res\left( \frac{1}{E - E_n + i\varepsilon}, E = E_n - i\varepsilon \right)\\
        &= \sum_n u_n(x) u_n^*(y)\e^{-it(E_n - i\varepsilon)/\hbar}.
    \end{align}
    So taking \(\varepsilon \to 0\) we recover the causal Green's function.
    
    This \enquote{\(i\varepsilon\) prescription} is fairly useful to make things converge.
    For example if we consider the simple harmonic oscillator then we change \(E_n \to E_n - i\varepsilon\) so \(\omega \to \omega - i\varepsilon/\hbar = \omega - i\varepsilon'\), and then \(\omega^2 \to \omega^2 - i\varepsilon' \omega = \omega^2 - i\varepsilon''\), where we take \(\varepsilon\), \(\varepsilon'\), and \(\varepsilon''\) to be infinitesimal, we will often drop the primes and write these all as \(\varepsilon\) and think of it as absorbing unimportant prefactors.
    We then have that the action changes from
    \begin{equation}
        S[x] \to S[x] + i\varepsilon \int x^2 \dd{t}
    \end{equation}
    so the integrand of the path integral is damped by a factor of
    \begin{equation}
        \exp\left[ -\varepsilon\int x^2 \dd{t} \right].
    \end{equation}
    
    \section{Trace}
    Consider what happens if we take \(x = y\) and integrate over \(x\), that is we take the trace of \(\idop\):
    \begin{align}
        \int_{-\infty}^{\infty} \braket{x, t}{x, 0} \dd{x} &= \int G(x, x; t) \dd{x}\\
        &= \sum_{n} \e^{-itE_n/\hbar} \int\abs{u_n(x)}^2 \dd{x}\\
        &= \sum_{n} \e^{-itE_n/\hbar}\label{eqn:trace}
    \end{align}
    where we use the orthonormality of the states in the last step.
    
    If we know \(G(x, y; t)\) we can use this to deduce the energy eigenvalues.
    For example, consider the harmonic oscillator.
    We have
    \begin{equation}
        \int_{-\infty}^{\infty} \braket{x, t}{x, 0} \dd{x} = \sqrt{\frac{m\omega}{2\pi i\hbar \sin(\omega t)}} \int_{-\infty}^{\infty} \exp\left[ \frac{i}{\hbar} \frac{m}{2} \frac{\omega}{\sin(\omega t)} 2x^2(\cos\omega t - 1) \right] \dd{x}.
    \end{equation}
    While this looks hideous its actually just a Gaussian integral, the result of which, after applying some trig identities to simplify, is
    \begin{align}
        \int_{-\infty}^{\infty} \braket{x, t}{x, 0} \dd{x} &= \frac{1}{2i} \frac{1}{\sin(\omega t/2)}\\
        &= \frac{1}{\e^{i\omega t/2} - \e^{-i\omega t/2}}\\
        &= \frac{\e^{-i\omega t/2}}{1 - \e^{-i\omega t}}\\
        &= \e^{-i\omega t/2}\sum_{n=0}^{\infty} \e^{in\omega t}
    \end{align}
    where we recognise a geometric series in \(\e^{-i\omega t}\).
    Comparing the result here to \cref{eqn:trace} we can identify
    \begin{equation}
        \e^{-itE_n/\hbar} = \e^{-i\omega t/2} \e^{-in\omega t} \implies E_n = \left( n + \frac{1}{2} \right)\hbar \omega
    \end{equation}
    so we get the expected energy eigenvalues for the simple harmonic oscillator.
    It is also possible to deduce the eigenfunctions in a similar way.
    
    \section{Statistical Mechanics}\label{sec:statistical mechanics}
    The form of 
    \begin{equation}
        \sum_n \e^{-itE_n/\hbar}
    \end{equation}
    may be familiar from the partition function in statistical mechanics\footnote{see the second half of the thermodynamics course}.
    
    Recall that the \defineindex{Helmholtz free energy} is defined as
    \begin{equation}
        F \coloneqq U - TS
    \end{equation}
    where \(U\) is the internal energy, \(T\) is the temperature, and \(S\) is the entropy.
    Combining this with the central equation of statistical mechanics,
    \begin{equation}
        \dl{U} = T\dd{S} - p\dd{V},
    \end{equation}
    where \(p\) is the pressure and \(V\) is the volume, we get
    \begin{equation}
        S = -\diffp{F}{T}[V], \qqand p = -\diffp{F}{V}[T].
    \end{equation}
    So clearly the free energy is a useful quantity.
    Unfortunately it's not easy to work with in this form.
    Fortunately we can also use the form
    \begin{equation}
        F = -\boltzmann T\ln Z
    \end{equation}
    where \(Z\) is the \defineindex{partition function}, defined by
    \begin{equation}
        Z \coloneqq \sum_n \e^{-\beta E_n}
    \end{equation}
    where \(\beta \coloneqq 1/(\boltzmann T)\).
    With an orthonormal basis \(\{\ket{n}\}\) we have
    \begin{equation}
        Z = \sum_n \e^{-\beta E_n}\braket{n}{n} = \sum_n \bra{n}\e^{-\beta E_n}\ket{n} = \sum_n \bra{n}\e^{-\beta\operator{\hamiltonian}}\ket{n} = \tr(\e^{-\beta\operator{\hamiltonian}}).
    \end{equation}
    
    To complete the example we consider a system of one degree of freedom and we take the classical limit, \(\hbar \to 0\).
    To do so we use the fact that the trace is independent of the basis we choose to calculate it in and so can calculate it in the position basis \(\{\ket{x}\}\):
    \begin{align}
        Z &= \tr(\e^{-\beta\operator{\hamiltonian}})\\
        &= \int \bra{x} \e^{-\beta\operator{\hamiltonian}}\ket{x} \dd{x}\\
        &= \int \bra{x} \e^{-\beta \operator{p}^2/2m - \beta V(\operator{x})}\ket{x} \dd{x}\\
        &= \int \bra{x} \e^{-\beta \operator{p}^2/(2m)}\e^{-\beta V(\operator{x})}\ket{x} \dd{x} + \order(\hbar)\\
        &= \int\mkern-7mu\dl{x} \int\mkern-7mu\dl{p} \bra{x} \e^{-\beta\operator{p}^2/(2m)}\ket{p}\bra{p} \e^{-\beta V(\operator{x})}\ket{x} + \order(\hbar)\\
        &= \int\mkern-7mu\dl{x} \int\mkern-7mu\dl{p} \braket{x}{p}\braket{p}{x} \e^{-\beta H(x, p)}  + \order(\hbar)\\
        &= \frac{1}{2\pi \hbar} \int\mkern-7mu\dd{x} \int\mkern-7mu\dd{p}\, \e^{-\beta H(x, p)} + \order(\hbar).
    \end{align}
    Here we split the exponential which gives us a term proportional to \(\commutator{\operator{p}}{V(\operator{x})}\), the exact form of which depends on the potential, but will in general be at least \(\order(\hbar)\).
    
    Comparing this result to \cref{eqn:trace} we see that
    \begin{equation}
        Z = \int_{-\infty}^{\infty} \braket{x, -i\beta \hbar}{x, 0} \dd{x}.
    \end{equation}
    This rather remarkable result says that a path integral (usually called a \define{propagator}\index{propagator|see{path integral}}) at negative imaginary time is related to partition functions.
    
    We considered a system of one degree of freedom, in statistical mechanics typical systems have Avagadro's number of degrees of freedom, that is \(10^{23}\).
    In this case we replace \(x\) with \(\vv{x} \in \reals^{10^{23}}\).
    Clearly it is not possible to treat these all individually and we usually need to apply statistical methods.
    One case we can manage fairly easily is when there is no interaction between the particles since this means that \(Z = [Z(1)]^N\) where \(Z(1)\) is the partition function for a single particle.
    
    To evaluate the path integral we can set \(t = -i\tau\) for \(\tau > 0\).
    This can be viewed as an analytic continuation of the path integral.
    This process of moving to imaginary time is often called a \defineindex{Wick rotation}, since multiplication by \(i\) corresponds to a rotation by \(\pi/2\).
    We then have a slightly modified Lagrangian
    \begin{equation}
        \lagrangian = \frac{1}{2}m\left( \diff{x}{t} \right)^2 - V(x) = -\left[ \frac{1}{2}\left( \diff{x}{\tau} \right)^2 + V(x) \right] = - \lagrangian_{\mathrm{E}}
    \end{equation}
    where \(\lagrangian_{\mathrm{E}}\) is the Euclidean Lagrangian, that is the normal Lagrangian with real time, but an inverted potential.
    We also take the upper integration limit as \(\tau = \hbar\beta\).
    This means the exponent in the path integral changes from
    \begin{equation}
        \frac{i}{\hbar} \int_{0}^{t} \lagrangian \dd{t} \to \frac{i}{\hbar} \int_{0}^{\hbar \beta} (-\lagrangian_{\mathrm{E}}) (-i\dd{\tau}) = \frac{i}{\hbar} \int_{0}^{\hbar \beta} \lagrangian_{\mathrm{E}} \dd{\tau}.
    \end{equation}
    Hence the partition function is
    \begin{align}
        Z &= \int_{-\infty}^{\infty} \braket{x, -i\beta\hbar}{x, 0} \dd{x}\\
        &= \int\mkern-7mu \dl{x} \int_x^x \DL{x(\tau)} \, \exp\left[ -\frac{1}{\hbar} \int_{0}^{\hbar\beta} \lagrangian_{\mathrm{E}} \dd{\tau} \right].
    \end{align}
    The value of \(\hbar \beta\) controls how temperature and quantum effects interact.
    
    If we don't take the trace then we have the \defineindex{density matrix}, given by
    \begin{equation}
        \rho(x, y) = G(x, y; -i\beta\hbar) = \sum_n \e^{-\beta E_n}u_n(x)u_n^*(y).
    \end{equation}
    Since \(G\) contains all knowledge of the system the density matrix does too.
    
    Returning to the example of a simple harmonic oscillator we have
    \begin{align}
        Z &= \int_{-\infty}^{\infty} \braket{x, -i\hbar\beta}{x, 0} \dd{x}\\
        &= \frac{1}{2i} \frac{1}{\sin(-\omega i\hbar\beta/2)}\\
        &= \frac{1}{2\sinh(\hbar\omega\beta/2)}.
    \end{align}
    Expanding this as before we can recover \(E_n = (n + 1/2)\hbar \omega\).
    
    \chapter{Single Particle in an Electric Field}
    \section{Units}
    In this course we will use \defineindex{Heaviside--Lorentz} units for electromagnetism.
    There are two advantages to this unit system, first they get the dimensions right in equations in terms of position and time, if we have \(x\) then we'll have \(ct\), so this is dimensionally consistent.
    SI units fail here, for example in SI units the electric field is defined as
    \begin{equation}
        \vv{E} = -\grad\varphi - \diffp{\vv{A}}{t}
    \end{equation}
    which has spatial derivatives and then a time derivative, it would be better to have a factor of \(1/c\) with the time derivative so that all derivatives are with respect to a length, equivalently this would mean that both \(\varphi\) and \(\vv{A}\) have the same units, which is useful when it comes to combing them in a four-potential for relativistic calculations.
    
    Second, the constants, such as \(4\pi\), \(\varepsilon_0\), and \(\mu_0\), appear in the solutions to the field equations rather than in the fields themselves.
    This is preferable as there is really no justification for these quantities to be in the fields apart from as a historical accident where we basically found the solutions, defined our units, then derived the underlying equations.
    If we want to take the modern approach of starting with the underlying equations then the appearance of these factors is annoying.
    SI units gets part way there by getting the factors of \(4\pi\) to appear in the solutions but still has factors of \(\varepsilon_0\) and \(\mu_0\).
    Gaussian units are the opposite and have factors of \(4\pi\) in the field equations but \(\varepsilon_0\) and \(\mu_0\) are absorbed into the definitions of the fields.
    
    So, what are Heaviside--Lorentz units?
    They are easiest to define in relation to the more familiar SI units, with subscript \(\SIunits\) to denote quantities in SI units the electric and magnetic fields in Heaviside--Lorentz units are
    \begin{equation}
        \vv{E} = \sqrt{\varepsilon_0}\vv{E_{\SIunits}}, \qqand \vv{B} = c\sqrt{\varepsilon_0}\vv{B_{\SIunits}}.
    \end{equation}
    The charge density, current density, and charge are given by
    \begin{equation}
        \rho = \frac{1}{\sqrt{\varepsilon_0}}\rho_{\SIunits}, \qquad \vv{j} = \frac{1}{\sqrt{\varepsilon_0}}\vv{j}_{\SIunits}, \qqand q = \frac{1}{\sqrt{\varepsilon_0}}q_{\SIunits}.
    \end{equation}
    The scalar and vector potentials in these units are
    \begin{equation}
        \varphi = \sqrt{\varepsilon_0}\varphi_{\SIunits}, \qqand \vv{A} = c\sqrt{\varepsilon_0}\vv{A_{\SIunits}}.
    \end{equation}
    From this we see that the electric and magnetic fields relate to the potentials via
    \begin{equation}
        \vv{E} = -\grad\varphi - \frac{1}{c}\diffp{\vv{A}}{t}, \qqand \vv{B} = \curl \vv{A}.
    \end{equation}
    
    Maxwell's equations in these units are
    \begin{align}
        \div \vv{E} &= \rho,\\
        \div \vv{B} &= 0,\\
        \curl\vv{E} &= -\frac{1}{c}\diffp{\vv{B}}{t},\\
        \curl\vv{B} &= \frac{1}{c}\vv{j} + \frac{1}{c}\diffp{\vv{E}}{t}.
    \end{align}
    In these units the Lorentz force is
    \begin{equation}
        \vv{F} = q\left( \vv{E} + \frac{1}{c}\vv{v} \times \vv{B} \right).
    \end{equation}
    Note that \(\vv{F}\) and \(\vv{v}\) are in the standard SI units of newtons and metres per second.
    Coulomb's law, and the resulting electric field for a single point charge, are given in these units by
    \begin{equation}
        \vv{F} = \frac{1}{4\pi} \frac{q_1q_2}{r^2}\vh{r} \implies \vv{E} = \frac{1}{4\pi}\frac{q}{r^2}\vh{r}
    \end{equation}
    where \(\vv{r}\) is measured in the expected SI unit of metres.
    Notice that the force and electric field are related in these units by \(\vv{F} = q\sqrt{\varepsilon_0}\vv{E}\).
    
    It is possible to view the Heaviside--Lorentz units as setting \(\varepsilon_0 = \mu_0 = 1\) and redefining Maxwell's equations to have extra factors of \(c\).
    
    \section{The Lagrangian}
    The standard Lagrangian, \(\lagrangian = T - V\), doesn't work for electromagnetism since the potential is not velocity independent.
    It has been found that the correct Lagrangian for a single particle of mass \(m\) and charge \(e\) in an electromagnetic field due to potentials \(\varphi\) and \(\vv{A}\) is
    \begin{equation}
        \lagrangian(\vv{r}, \dot{\vv{r}}, t) = \frac{1}{2}m\dot{\vv{r}}^2 - e\varphi + \frac{e}{c}\dot{\vv{r}}\cdot\vv{A}.
    \end{equation}
    This is justified by giving the correct equation of motion for a single charged particle in an electromagnetic field, namely the Lorentz force equation.
    To see this we simply go through the usual motions of plugging the Lagrangian into the Euler--Lagrange equations.
    When doing so we note that \(\varphi\) and \(\vv{A}\) depend on \(\vv{r}\), but not \(\dot{\vv{r}}\), for full generality we assume both are time dependent.
    In order to make this easier we write the Lagrangian in index notation, with the Einstein summation convention, which we shall assume throughout:
    \begin{equation}
        \lagrangian(\{x\}, \{\dot{x}\}, t) = \frac{1}{2}m\dot{x}_j\dot{x}_j - e\varphi(\{x\}, t) + \frac{e}{c}\dot{x}_jA_j(\{x\}, t)
    \end{equation}
    Hence, we have
    \begin{align}
        \diffp{\lagrangian}{\dot{x}_i} &= m\dot{x}_i + \frac{e}{c}A_i\\ \diff*{\left( \diffp{\lagrangian}{\dot{x}_i} \right)}{t} &= m\ddot{x}_i + \frac{e}{c}\diff{A_i}{t}\\
        &= m\ddot{x}_i + \frac{e}{c}\left( \diffp{A_i}{x_j}\dot{x}_j + \diffp{A_i}{t} \right),
    \end{align}
    and
    \begin{align}
        \diffp{\lagrangian}{x_i} = e\diffp{\varphi}{x_i} + \frac{e}{c}\dot{x_j}\diffp{A_j}{x_i}.
    \end{align}
    Combining these we have
    \begin{equation}
        m\ddot{x}_i + \frac{e}{c}\left( \diffp{A_i}{x_j}\dot{x_j} + \diffp{A_i}{t} \right) - e\diffp{\varphi}{x_i} - \frac{e}{c}\diffp{A_j}{x_i}\dot{x}_j = 0.
    \end{equation}
    Rearranging this we get
    \begin{equation}\label{eqn:lorentz force derivation}
        m\ddot{x}_i = e\left[ -\diffp{\varphi}{x_i} - \frac{1}{c}\diffp{A_i}{t} + \frac{\dot{x}_j}{c}\diffp{A_i}{x_i} - \frac{\dot{x_j}}{c}\diffp{A_i}{x_j} \right].
    \end{equation}
    At this point we need an identity which we easily derive:
    \begin{align}
        (\dot{\vv{r}}\times \vv{B})_i &= [\dot{\vv{r}}\times(\curl\vv{A})]_i\\
        &= \varepsilon_{ijk} \dot{x}_j(\curl\vv{A})_{k}\\
        &= \varepsilon_{ijk} \dot{x_j} \varepsilon_{klm}\partial_lA_m\\
        &= (\delta_{il}\delta_{jm} - \delta_{im}\delta_{jl})\dot{x}_j\partial_lA_m\\
        &= \dot{x}_j\partial_iA_j - \dot{x}_j\partial_jA_i.
    \end{align}
    We can recognise this last term as appearing as the last two terms of \cref{eqn:lorentz force derivation}.
    We can also recognise \(-\diffp{\varphi}/{x_i} - \diffp{A_i}/{t} = E_i\) as the first two terms and so we have
    \begin{equation}
        m\ddot{\vv{x}} = e\left[ \vv{E} + \frac{1}{c}\dot{\vv{r}} \times \vv{B} \right].
    \end{equation}
    This is exactly the Lorentz force law justifying our choice of Lagrangian.
    
    \subsection{Hamiltonian}\label{sec:hamiltonian}
    Recall that we can define the generalised momentum as
    \begin{equation}
        p_i = \diffp{\lagrangian}{\dot{x}_i} = m\dot{x}_i + \frac{e}{c}A_i.
    \end{equation}
    
    The Hamiltonian can be found from the Legendre transform:
    \begin{align}
        \hamiltonian &= p_i\dot{x}_i - \lagrangian\\
        &= \left[ m\dot{x}_i + \frac{e}{c}A_i \right]\dot{x}_i - \left[ \frac{1}{2}m\dot{x}_i\dot{x}_i - e\varphi + \frac{e}{c}\dot{x}_iA_i \right]\\
        &= \frac{1}{2}m\dot{x}_i\dot{x}_i + e\varphi,
    \end{align}
    this is almost there but the Hamiltonian should be written in terms of potion and generalised momentum, not velocity.
    We can easily invert the generalised momentum to find
    \begin{equation}
        \dot{x}_i = \frac{1}{m}\left( p_i - \frac{e}{c}A_i \right)
    \end{equation}
    which we can substitute into the Hamiltonian to get
    \begin{equation}
        \hamiltonian = \frac{1}{2m}\left( \vv{p} - \frac{e}{c}\vv{A} \right)^2 + e\varphi.
    \end{equation}
    
    \section{Gauge Invariance}
    Classically the electromagnetic fields are unchanged by a \defineindex{gauge transformation}, which is defined as a transformation of the form
    \begin{equation}
        \varphi \to \varphi' = \varphi - \frac{1}{c}\diffp{\chi}{t}, \qqand \vv{A'} = \vv{A} \to \vv{A} + \grad\chi
    \end{equation}
    for some twice differentiable\footnote{the first differentiability is required so \(\diffp{\chi}/{x}\) etc.\@ exist, the second so that partial derivatives commute} scalar field, \(\chi\).
    We can show this by computing the electric and magnetic fields after the transformation, first the electric field:
    \begin{align}
        \vv{E'} &= -\grad\left( \varphi - \frac{1}{c}\diffp{\chi}{t} \right) - \frac{1}{c}\diffp{}{t}( \vv{A} + \grad\chi )\\
        &= \underbrace{-\grad{\varphi} - \frac{1}{c}\diff{\vv{A}}{t}}_{=\vv{E}} + \frac{1}{c}\grad\diffp{\chi}{t} - \frac{1}{c}\diffp{}{t}\grad{\chi}\\
        &= \vv{E}
    \end{align}
    and then the magnetic field:
    \begin{align}
        \vv{B'} &= \curl \left( \vv{A} + \grad\chi \right)\\
        &= \underbrace{\curl\vv{A}\vphantom{\chi}}_{=\vv{B}} + \underbrace{\curl\grad\chi}_{=\vv{0}}\\
        &= \vv{B}.
    \end{align}
    So the electric and magnetic fields are unchanged.
    
    The Lagrangian, on the other hand, is changed:
    \begin{align}
        \lagrangian &\to \frac{1}{2}m\dot{\vv{r}}^2 - e\left( \varphi - \frac{1}{c}\diffp{\chi}{t} \right) + \frac{e}{c}\dot{\vv{r}} \cdot\left( \vv{A} + \grad\chi \right)\\
        &= \lagrangian + \frac{e}{c}\left( \diffp{\chi}{t} + \dot{\vv{r}}\cdot\grad\chi \right)\\
        &= \lagrangian + \frac{e}{c}\diff{\chi}{t}.
    \end{align}
    So the result is adding a total derivative to the Lagrangian, which doesn't change the Lagrange equations of motion since
    \begin{align}
        \diff{}{t}\diffp{}{\dot{x}_i}\diff{\chi}{t} - \diffp{}{x_i}\diff{\chi}{t} &= \diff{}{t}\diffp{}{\dot{x}_i}\left( \diffp{\chi}{x_j}\dot{x}_j + \diffp{\chi}{t} \right) - \diffp{}{x_i}\left( \diff{\chi}{t} \right)\\
        &= \diff{}{t}\left( \diffp{\chi}{x_i} + 0  \right) - \diff{}{t}\diffp{\chi}{x_i}\\
        &= 0.
    \end{align}
    Hence there is no contribution to Lagrange's equations.
    So classical physics doesn't change under a gauge transformation, we say it is \defineindex{gauge invariant}.
    
    The action is changed by the addition of boundary terms:
    \begin{align}
        S &= \int_{t_a}^{t_b} \lagrangian \dd{t}\\
        &\to \int_{t_a}^{t_b} \lagrangian + \frac{e}{c}\diff{\chi}{t} \dd{t}\\
        &= S + \frac{e}{c}[\chi(\vv{r_b}, t_b) - \chi(\vv{r_a}, t_a)].
    \end{align}
    Another way to view the gauge invariance of classical physics is that these extra terms don't effect the location of the minimum of \(S\), since \(\delta\chi(\vv{r_a}, t_a) = \delta\chi(\vv{r_b}, t_b) = 0\) and so \(\delta S = 0\) for the same classical path as before.
    
    However, the transition amplitude does change, in particular it becomes
    \begin{equation}
        \braket{\vv{r_b}, t_b}{\vv{r_a}, t_a} \to \exp\left[ \frac{i}{\hbar} \frac{e}{c} [\chi(\vv{r_b}, t_a) - \chi(\vv{r_a}, t_a)] \right] \int_{\vv{r_a}}^{\vv{r_b}} \DL{\vv{r}} \, \e^{iS/\hbar}.
    \end{equation}
    The resulting change can be written as
    \begin{equation}
        \ket{\vv{r}, t} \to \exp\left[ -\frac{i}{\hbar}\frac{e}{c}\chi(\vv{r}, t) \right]\ket{\vv{r}, t}.
    \end{equation}
    So, the result is an independent phase change locally at every point in space and time.
    The transition probability, \(\abs{\braket{\vv{r_b}, t_b}{\vv{r_a}, t_a}}^2\), is unchanged.
    This symmetry of the theory, which results in an extra phase factor, that is a complex number of unit modulus, is known as a \(\unitary(1)\) local \defineindex{gauge symmetry}.
    
    It should be noted at this point that to evaluate a path integral with a velocity dependent potential we need to use the \enquote{\defineindex{midpoint rule}} when discretising the integrand, meaning that instead of evaluating at \(x_n\) we evaluate at \((x_n + x_{n+1})/2\).
    
    \section{Aharonov--Bohm Effect}
    The \defineindex{Aharonov--Bohm effect} is a quantum effect where the effect of a magnetic field extends into a region where the field is zero.
    It was first observed in 1960.
    The setup is a double slit experiment with a magnetic field in the centre, for example, a field created by a long, thin solenoid.
    We assume that the magnetic field is perfectly shielded so that particles outside of the region experience no magnetic field.
    See \cref{fig:aharonov-bohm effect}.
    The corresponding vector potential to create the field forms circles around the field as shown in the image.
    
    \begin{figure}
        \tikzsetnextfilename{aharonov-bohm-effect}
        \begin{tikzpicture}
            \draw[very thick] (0, 1) -- (0, -1);
            \draw[very thick] (0, 1.2) -- (0, 2);
            \draw[very thick] (0, -1.2) -- (0, -2);
            \fill[highlight, opacity=0.5] (0, 0) circle [radius = 0.5cm];
            \node[highlight] at (0.2, 0) {\(\vv{B}\)};
            \foreach \i in {0.6, 0.75, 0.9} {
                \draw[highlightpurple, very thick] (0, 0) circle [radius = \i cm];
                \draw[highlightpurple, ->, very thick] (\i-0.01, 0) -- (\i-0.01, 0.2);
            }
            \node[right, highlightpurple] at (0.9, 0) {\(\vv{A}\)};
            \draw[thick] (-4, 0) -- (0, 1.1) -- (4, 0);
            \draw[thick] (-4, 0) -- (0, -1.1) -- (4, 0);
            \node[left] at (-4, 0) {Source};
            \node[right] at (4, 0) {Detector};
            \draw[thick, <-] (-2, 0.55) -- (-2.1, 0.53) node[above] {1};
            \draw[thick, <-] (-2, -0.55) -- (-2.1, -0.53) node[below] {2};
            \draw[thick, <-] (2, 0.55) -- (1.9, 0.57) node[above] {1};
            \draw[thick, <-] (2, -0.55) -- (1.9, -0.57) node[below] {2};
        \end{tikzpicture}
        \caption{The setup for the Aharonov--Bohm effect.}
        \label{fig:aharonov-bohm effect}
    \end{figure}
    
    If there is no magnetic field then we add the amplitudes for the two slits as usual and we get
    \begin{equation}
        \e^{iS[1]/\hbar} + \e^{iS[2]/\hbar} = \e^{iS[1]/\hbar} \left( 1 + \e^{i(S[2] - S[1])} \right).
    \end{equation}
    We get interference according to the relative phase, \(\delta\varphi = (S[2] - S[1])/\hbar\).
    
    If we now include a time independent magnetic field then the Lagrangian becomes
    \begin{equation}
        \lagrangian \to \lagrangian + \frac{e}{c}\dot{\vv{r}} \cdot \vv{A}.
    \end{equation}
    The action becomes
    \begin{equation}
        S \to S + \frac{e}{c} \int_{t_a}^{t_b} \diff{\vv{r}}{t} \cdot \vv{A} \dd{t} = S + \frac{e}{c} \int_{\vv{r_a}}^{\vv{r_b}} \vv{A}\cdot\dl{\vv{r}}.
    \end{equation}
    Where the line integral is along either path 1 or 2.
    
    The action changes, and the change depends on the path, so the phase changes:
    \begin{align}
        \delta\varphi &= \frac{e}{\hbar c} \left[ \int_{C_2} \vv{A}\cdot\dl{r} - \int_{C_1} \vv{A}\cdot\dl{\vv{r}} \right]\\
        &= \frac{e}{\hbar c} \oint_C \vv{A}\cdot\dl{\vv{r}}\\
        &= \frac{e}{\hbar c}\Phi
    \end{align}
    where \(C_1\)  and \(C_2\) are paths 1 and 2, \(C\) is the path along 2 and then back along 1, and \(\Phi\) is the flux of the magnetic field, which follows by applying Stokes' theorem:
    \begin{equation}
        \Phi = \oint_C \vv{A}\cdot\dd{\vv{r}} = \int_S (\curl\vv{A}) \cdot \dl{\vv{S}} = \int_S \vv{B}\cdot\dl{\vv{S}}.
    \end{equation}
    This is valid for any surface, \(S\), bounded by \(C\).
    
    The interference pattern therefore shifts by a relative phase shift \(e\Phi/(\hbar c)\) when the field is turned on, even though the particle doesn't pass through any region of non-zero magnetic field, and therefore doesn't \enquote{feel} any electromagnetic forces.
    It is as if the particle can feel the potential.
    This suggests that perhaps potentials are fundamental, rather than just being a mathematical trick.
    
    The phase change is gauge invariant since the flux is gauge invariant:
    \begin{equation}
        \Phi \to \Phi + \oint_C \grad\chi \cdot \dl{\vv{r}} = \Phi + \oint_C \dl{\chi} = \Phi + 0 = \Phi.
    \end{equation}
    The effect is also periodic, in particular there is no effect if \(\delta\varphi = 2\pi n\), which corresponds to
    \begin{equation}
        \Phi = 2\pi n\frac{\hbar c}{e} = n\frac{hc}{e}, \qquad\text{where } n \in \integers.
    \end{equation}

    \chapter{Transition Elements}
    So far we have considered transition amplitudes of the form
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \int_{x_a}^{x_b} \DL{x} \e^{iS[x]/\hbar}.
    \end{equation}
    This has gotten us quite far but we should also consider what happens if we put other quantities in the path integral.
    The most basic being a factor of \(x(t)\):
    \begin{align}
        \expected{x(t)} &\coloneqq \int_{x_a}^{x_b} \!\! \DL{x} \, x(t) \e^{iS[x]/\hbar}\\
        &= \int_{-\infty}^{\infty} \!\!\! \dl{x} \! \int_x^{x_b} \!\!\! \DL{x''} \! \int_{x_a}^x \!\!\! \DL{x'} \exp\left[ \frac{i}{\hbar} \int_{t}^{t_b} \!\! \lagrangian'' \dd{t''} \right] \exp\left[ \frac{i}{\hbar} \int_{t_a}^{t} \!\! \lagrangian' \dd{t'} \right]\\
        &= \int \!\! \dl{x} \braket{x_b, t_b}{x, t} x \braket{x, t}{x_a, t_a}\\
        &= \bra{x_b, t_b} \operator{x}(t) \ket{x_a, t_a}.
    \end{align}
    This gives us the matrix element of the operator \(\operator{x}(t)\) between the states \(\ket{x_a, t_a}\) and \(\ket{x_b, t_b}\) in the Heisenberg picture.
    
    We can use this result, and other derived results, to find a connection between operators and path integrals.
    
    A similar derivation to above words for any (analytic) function, \(f\):
    \begin{equation}
        \bra{x_b, t_b}f(\operator{x}(t))\ket{x_a, t_a} = \int_{x_a}^{x_b} \!\! \DL{x} \, f(x(t)) \e^{iS[x]/\hbar}.
    \end{equation}

    \section{Time Ordered Products}
    Consider now the matrix elements of the operator \(\operator{x}(t)\operator{x}(t')\), importantly in the Heisenberg picture operators at different times do not, in general, commute.
    If \(t > t'\) then we have
    \begin{align}
        &\expected{x(t)x(t')}\int_{x_a}^{x_b} \!\! \DL{x} \, x(t) x(t') \e^{iS/\hbar}\\
        &\qquad= \int \!\! \dl{x} \int \!\! \dl{x'} \braket{x_b, t_b}{x, t} x \braket{x, t}{x', t'} x' \braket{x', t'}{x_a, t_a}\notag\\
        &\qquad= \bra{x_b, t_b} \operator{x}(t) \operator{x}(t') \ket{x_a, t_a}.
    \end{align}
    We can interpret the second set of integrals as the state transitioning from \(\ket{x_a, t_a}\) to \(\ket{x', t'}\), then to \(\ket{x, t}\) and \(\ket{x_b, t_b}\).
    This is why the order of the operators is what it is.
    
    If \(t < t'\) then the same relation holds but with \(t\) and \(t'\) swapped:
    \begin{equation}
        \expected{x(t)x(t')} = \bra{x_b, t_b} x(t') x(t) \ket{x_a, t_a}.
    \end{equation}
    
    We can combine these two results writing
    \begin{equation}
        \expected{x(t)x(t')} = \bra{x_b, t_b} \timeorder (\operator{x}(t)\operator{x}(t')) \ket{x_a, t_a}
    \end{equation}
    where we define the \defineindex{time-ordered product} as
    \begin{equation}
        \timeorder (\operator{x}(t)\operator{x}(t')) = \theta(t - t')\operator{x}(t)\operator{x}(t') + \theta(t' - t)\operator{x}(t')\operator{x}(t).
    \end{equation}
    Here \(\theta\) is the Heaviside step function (see \cref{eqn:heaviside step function}).
    The effect here is to kill the term that appears in the wrong order.
    
    We can generalise this idea to arbitrarily many functions, \(f_i\), of \(x\):
    \begin{align}
        \expected{f_1(x(t_1)) \dotsm f_n(x(t_n))} &= \int_{x_a}^{x_b} \!\! \DL{x} \, f_1(x(t_1)) \dotsm f_n(x(t_n)) \e^{iS/\hbar}\\
        \hspace{4cm}&= \bra{x_b, t_b} \timeorder (f_1(\operator{x}(t_1)) \dotsm f_n(\operator{x}(t_n)))\ket{x_a, t_a}.
    \end{align}
    The time ordered product generalises so that the result is a product of functions of operators such that all terms to the left of a given operator occur at a later time, this means that the product of operators acts on a state on the right in time order.
    This can be written with products of multiple Heaviside step functions, but its not particularly enlightening.
    The difference between the first expression and the last expression above is that the first is a product of commuting functions and the later a product of non-commuting operators.
    In this way we can include the non-commuting weirdness of quantum operators in the path integral formulation.
    
    \subsection{Time Ordered Product Between Arbitrary States}
    Consider the time ordered product between the states \(\ket{\psi}\) and \(\ket{\varphi}\):
    \begin{align}
        &\bra{\psi} \timeorder (\operator{x}(t_1) \dotsm \operator{x}(t)) \ket{\varphi}\\
        &\qquad= \int \!\! \dl{x_a} \int \!\! \dl{x_b} \braket{\psi}{x_b, t_b}\bra{x_b, t_b} \timeorder (\operator{x}(t_1) \dotsm \operator{x}(t_n)) \ket{x_a, t_a} \braket{x_a, t_a}{\varphi}\notag\\
        &\qquad= \int \!\! \dl{x_a} \int \!\! \dl{x_b} \psi^*(x_b, t_b) \varphi(x_a, t_a) \int_{x_a}^{x_b} \!\! \DL{x} \, x(t_1) \dotsm x(t_n) \e^{iS/\hbar}\\
        &\qquad= \int \!\! \dl{x_a} \int \!\! \dl{x_b} \psi^*(x_b, t_b) \varphi(x_a, t_a) \expected{x(t_1) \dotsm x(t_n)}.
    \end{align}
    This also holds more generally if we replace \(x\) with a function of \(x\).
    
    \section{Including Time Derivatives}
    We will need to consider transition elements including time derivatives of \(x\).
    We start our analysis by returning to the definition of the path integral:
    \begin{multline}\label{eqn:definition of path integral}
        \braket{x_b, t_b}{x_a, t_a}\\
        = \lim_{N\to\infty} A_N \left( \prod_{n=1}^{N} \int\!\!\dl{x_n} \right) \exp\left[ \frac{i\varepsilon}{\hbar} \sum_{n=0}^{N} \left[ \frac{m}{2}\left( \frac{x_{n+1} - x_n}{\varepsilon} \right)^2 - V(x_n, t_n) \right] \right].
    \end{multline}

    We will use the result
    \begin{equation}
        \int_{-\infty}^{\infty} \diffp{f}{x} \dd{x} = 0
    \end{equation}
    which holds for any function \(f\) which approaches zero sufficiently quickly as \(\abs{x} \to \infty\).
    We then have that for any \(k = 1, \dotsc, N\) and function \(F\)
    \begin{align}
        0 &= \lim_{N\to\infty} A_N \left( \prod_{n=1}^{N} \int \!\! \dl{x_n} \right)\notag\\
        &\qquad\qquad\times\diffp{}{x_k} \left[ F(x_k) \exp\left[ \frac{i\varepsilon}{\hbar} \sum_{n=0}^{N} \left\{ \frac{x_{n+1} - x_n}{\varepsilon} \right\}^2 - V(x_n, t_n) \right] \right]\\
        &= \lim_{N\to\infty} A_N \left( \prod_{n=1}^{N} \int \!\! \dl{x_n} \right)\notag\\
        &\qquad\qquad\times \left[ \diffp{F}{x_k} - \frac{i\varepsilon}{\hbar}F(x_k)\left[ \frac{m}{\varepsilon^2}(x_{k+1} - 2x_k + x_{k-1}) + \diffp{V}{x_k} \right] \right]\notag\\
        &\qquad\qquad\times \exp\left[ \frac{i\varepsilon}{\hbar} \sum_{n=0}^{N} \left\{ \frac{x_{n+1} - x_n}{\varepsilon} \right\}^2 - V(x_n, t_n) \right].
    \end{align}
    Using this, and noting that \(N \to \infty\) is equivalent to \(\varepsilon \to 0\) we can write
    \begin{equation}\label{eqn:step in derivation of ehrenfest}
        0 = \lim_{\varepsilon\to 0} \expected*{ \diffp{F}{x_k} - \frac{i\varepsilon}{\hbar} F(x_k) \left[ \frac{m}{\varepsilon}\left( \frac{x_{k+1} - x_k}{\varepsilon} - \frac{x_k - x_{k-1}}{\varepsilon} \right) + \diffp{V}{x_k} \right] }.
    \end{equation}
    Here, we use \(\expected{-}\) as a notation to mean
    \begin{equation}
        \expected{O} = \int_{x_a}^{x_b} \!\! \DL{x} \, O \e^{iS/\hbar}.
    \end{equation}
    It is possible that we will need to include a damping factor also to ensure convergence.
    
    We can also use \(\expected{x_k} \to \expected{x(t)}\) as \(\varepsilon \to 0\) and so
    \begin{equation}
        \expected*{\frac{x_{k+1} - x_k}{\varepsilon}} \to \expected{\dot{x}(t + \varepsilon/2)} \to \expected{\dot{x}(t)}.
    \end{equation}
    It then follows that
    \begin{equation}
        \expected*{\frac{1}{\varepsilon}\left( \frac{x_{k+1} - x_{k}}{\varepsilon} - \frac{x_{k} - x_{k-1}}{\varepsilon} \right)} \to \expected*{\frac{\dot{x}(t + \varepsilon/2) - \dot{x}(t - \varepsilon/2)}{\varepsilon}} \to \expected{\ddot{x}(t)}.
    \end{equation}
    
    \subsection{Ehrenfest's Theorem}
    Consider \cref{eqn:step in derivation of ehrenfest} with \(F(x_k) = 1\).
    The term in square brackets must therefore vanish.
    Using the limiting behaviour defined above we see that
    \begin{equation}
        0 = \expected*{m\ddot{x} + \diffp{V}{x_k}}.
    \end{equation}
    We then have
    \begin{equation}
        m\expected{\ddot{x}} = -\expected*{\diffp{V}{x}}.
    \end{equation}
    This is \defineindex{Ehrenfest's theorem}, interpreting \(\expected{-}\) as the expected value this is a quantum version of Newton's second law with a potential, \(V\).
    
    \subsection{Commutation Relations}
    Now take \(F(x_k) = x_k\).
    \Cref{eqn:step in derivation of ehrenfest} then becomes
    \begin{align}\label{eqn:step in derivation of commutation relations}
        0 &= \lim_{\varepsilon\to 0} \expected*{1 - \frac{i}{\hbar}\left[ mx_k \left( \frac{x_{k+1} - x_k}{\varepsilon} - \frac{x_k - x_{k-1}}{\varepsilon} \right) + \varepsilon x_k \diffp{V}{x_k} \right]}\\
        &= \expected*{1 - \frac{i}{\hbar}x(t)m[\dot{x}(t + \varepsilon/2) - \dot{x}(t - \varepsilon/2)]}
    \end{align}
    Setting \(p = m\dot{x}\) we get
    \begin{equation}
        0 = \expected*{1 - \frac{i}{\hbar}x(t)[p(t + \varepsilon/2) - p(t - \varepsilon/2)]}.
    \end{equation}
    Therefore
    \begin{align}
        0 = \bra{x_b, t_b} \left[ 1 - \frac{i}{\hbar}[\operator{p}(t)\operator{x}(t) - \operator{x}(t)\operator{p}(t)] \right]\ket{x_a, t_a}.
    \end{align}
    The order of the operators here is important.
    We have \(\operator{p}(t)\operator{x}(t)\) first since we had \(\expected{x(t)p(x + \varepsilon/2)}\), which is time ordered to have \(\operator{p}(t + \varepsilon/2)\) first, with \(\varepsilon/2\) vanishing in the limit.
    Similarly the for the second term we have \(\expected{x(t)p(t - \varepsilon/2)}\), which is time ordered to give \(\operator{x}(t)\operator{p}(t)\).
    
    Rearranging the equation, and since \(\ket{x_a, t_a}\) and \(\ket{x_b, t_b}\) are arbitrary, we have
    \begin{equation}
        -\frac{i}{\hbar}[\operator{p}(t)\operator{x}(t) - \operator{x}(t)\operator{p}(t)] = -\frac{i}{\hbar} \commutator{\operator{x}(t)}{\operator{p}(t)} = 1 \implies \commutator{\operator{x}(t)}{\operator{p}(t)} = i\hbar.
    \end{equation}
    So we have derived the canonical commutation relation in the Heisenberg picture, considering this at \(t = 0\) gives us the same result in the Schr\"odinger picture.
    
    \subsection{Zitterbewegung}
    We have
    \begin{equation}
        \expected*{x_k\left( \frac{x_k - x_{k-1}}{\varepsilon} \right)} = \expected*{x_{k+1} \left( \frac{x_{k+1} - x_k}{\varepsilon} \right)} + \order(\varepsilon)
    \end{equation}
    since we have just shifted everything in time by \(\varepsilon\).
    We can therefore write \cref{eqn:step in derivation of commutation relations} as
    \begin{align}
        0 &= \lim_{\varepsilon\to 0} \expected*{1 - \frac{i}{\hbar}m\left[ x_k\frac{x_{k+1} - x_k}{\varepsilon} - x_{k+1}\frac{x_{k+1} - x_k}{\varepsilon} \right]} + \order(\varepsilon)\\
        &= \lim_{\varepsilon\to 0} \expected*{1 - \frac{i}{\hbar}m\varepsilon \left[ x_k\frac{x_{k+1} - x_k}{\varepsilon} \right]^2} + \order(\varepsilon).
    \end{align}
    Rearranging this we get
    \begin{equation}
        \lim_{\varepsilon\to 0}\expected*{m(\frac{x_{k+1} - x_k}{\varepsilon})^2} = \expected{\dot{x}(t)^2} = \expected{p(t)^2} = \lim_{\varepsilon\to 0} -\frac{\hbar}{i\varepsilon}\expected{1} + \order(1) \to \infty.
    \end{equation}
    This means that \(\expected{\dot{x}(t)^2}\) is infinite.
    However, the action is finite.
    How is this possible?
    Considering \cref{eqn:definition of path integral} we see that we must have
    \begin{equation}
        \lim_{\varepsilon \to 0} \varepsilon\left( \frac{x_{k+1} - x_{k}}{\varepsilon} \right) = \lim_{\varepsilon \to 0} \varepsilon \left( \frac{\delta x}{\varepsilon} \right) < \infty.
    \end{equation}
    From this we see that \(\delta x \sim \sqrt{\varepsilon}\), from which it follows that
    \begin{equation}
        \delta\dot{x} \sim \frac{\delta x}{\varepsilon} \sim \frac{1}{\sqrt{\varepsilon}}
    \end{equation}
    This means that the paths we consider are continuous, since \(\delta x \to 0\), but nowhere differentiable, since \(\delta \dot{x} \to \infty\).
    This phenomenon is called \textit{\defineindex{Zitterbewegung}}, which is German for \enquote{trembling motion}.
    
    \subsection{Defining Kinetic Energy}
    The standard definitions of kinetic energy, as \(m\dot{x}^2/2\) or \(p^2/(2m)\), lead to problems now as we see that these will be infinite.
    To get around this we consider the two factors of \(p\) at slightly different times, and then take the limit of these times coming together:
    \begin{align}
        T &\coloneqq \lim_{\varepsilon \to 0} \frac{m}{2} \expected*{\left( \frac{x_{k+1} - x_k}{\varepsilon} \right)\left( \frac{x_{k} - x_{k-1}}{\varepsilon} \right)}\\
        &\hphantom{:}= \lim_{\varepsilon \to 0} \frac{1}{2m} \expected{p(t + \varepsilon/2)p(t - \varepsilon/2)}.
    \end{align}
    We can show this is finite by considering \cref{eqn:step in derivation of ehrenfest} with \(F(x_k) = x_{k+1} - x_k\) giving
    \begin{equation}
        0 = \lim_{\varepsilon \to 0} \expected*{1-\frac{i\varepsilon}{\hbar} \left[ m\left( \frac{x_{k+1} - x_k}{\varepsilon} \right)\left( \frac{x_{k+1} - x_k}{\varepsilon} - \frac{x_{k} - x_{k-1}}{\varepsilon} \right) + (x_{k+1} - x_k) \diffp{V}{x_k} \right]}.
    \end{equation}
    Since \(x_{k+1} - x_k = \order(\varepsilon)\) we have
    \begin{equation}
        0 = \lim_{\varepsilon\to 0} \expected*{\frac{i\varepsilon}{\hbar} m\left( \frac{x_{k+1} - x_k}{\varepsilon} \right)^2} - \lim_{\varepsilon\to 0} \expected*{\frac{i\varepsilon}{\hbar}m\left( \frac{x_{k+1} - x_k}{\varepsilon} \right)\left( \frac{x_{k} - x_{k-1}}{\varepsilon} \right)} + \order(\varepsilon).
    \end{equation}
    Identifying the second term as proportional to our new definition of the kinetic energy moving it to the other side and dividing by \(i\varepsilon/\hbar\) we get
    \begin{align}
        &\lim_{\varepsilon\to 0} \frac{1}{2m}\expected{p(t + \varepsilon/2)p(t - \varepsilon/2)}\notag\\
        &\qquad= \lim_{\varepsilon\to 0}\frac{m}{2}\expected{\left( \frac{x_{k+1} - x_k}{\varepsilon} \right)} + \order(1) + \frac{\hbar}{2i\varepsilon}\order(1) + \order(\varepsilon)\\
        &\qquad= -\frac{\hbar}{2i\varepsilon}\expected{1} + \order(1) + \frac{\hbar}{2i\varepsilon}\expected{1} + \order(\varepsilon)\\
        &\qquad= \order(1) + \order(\varepsilon)\\
        &\qquad< \infty.
    \end{align}
    
    This process, of removing infinities through processes like this, is called \defineindex{renormalisation}.
    The method used here, splitting \(p^2\) into two factors evaluated at slightly different times, is called \defineindex{point splitting regularisation}.
    
    An alternative, which leads to the same results, is to use the classical mechanics \defineindex{virial theorem}, which states
    \begin{equation}
        \expected*{p\diffp{H}{p}} = \expected*{x\diffp{H}{x}}
    \end{equation}
    with \(H = p^2/(2m) + V(x)\), giving
    \begin{equation}
        \frac{\expected{p^2}}{2m} = \frac{1}{2}\expected{xV'(x)}.
    \end{equation}
    We can then define the kinetic energy using the right hand side which avoids the problem with infinity.
    
    \part{Perturbation Theory}
    \chapter{Perturbation Theory from Path Integrals}
    \section{Why Perturbation Theory?}
    Most dynamical systems don't have an analytic solution.
    However, often we can separate the action into part with an analytical solution, and a small term that we can treat with a series expansion.
    We call this second part a \defineindex{perturbation} on the solvable system.
    We will assume that we have an action, \(S[x(t)]\), which can be split into a solvable part, \(S_0[x(t)]\), and perturbation, \(S_1[x(t)]\), such that
    \begin{equation}
        S[x(t)] = S_0[x(t)] + S_1[x(t)].
    \end{equation}
    
    For example, in a slowly varying potential one could write
    \begin{equation}
        S_0[x(t)] = \int_{t_a}^{t_b} \frac{1}{2}m\dot{x}\dd{x}, \qqand S_1[x(t)] = -\int_{t_a}^{t_b} V(x(t), t) \dd{t}.
    \end{equation}
    Remember that \(S\) is the integral of the Lagrangian, \(\lagrangian = T - V\), explaining the minus sign in the \(S_1\) term.
    
    Alternatively we could split the potential into \(V(x(t), t) = U(x) + \tilde{V}(x(t), t)\) writing
    \begin{equation}\label{eqn:perturbation example}
        S_0[x(t)] = \int_{t_a}^{t_b} \frac{1}{2}m\dot{x}^2 - U(x) \dd{t}, \qqand S_1[x(t)] = -\int_{t_a}^{t_b} \tilde{V}(x, t) \dd{t}.
    \end{equation}
    Where we chose \(U\) such that there is an exact solution for the \(S_0\) action.
    A common choice is \(U = m\omega^2x^2/2\), meaning \(S_0\) corresponds to a harmonic oscillator.
    
    \section{Perturbation Theory Framework}
    \subsection{Transition Amplitude}
    The transition amplitude is
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \int_{x_a}^{x_b} \!\! \DL{x} \, \exp\left[ \frac{i}{\hbar} (S_0[x(t)] + S_1[x(t)]) \right].
    \end{equation}
    Splitting the exponential in two and Taylor expanding assuming \(S_1\) is small we get
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \int_{x_a}^{x_b} \!\! \DL{x} \, \e^{iS_0[x(t)]} \sum_{n=0}^{\infty} \frac{1}{n!} \left( \frac{i}{\hbar} S_1[x(t)] \right)^n.
    \end{equation}
    Separating out the first term of the sum we have
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &= \int_{x_a}^{x_b} \!\!\! \DL{x} \, \e^{iS_0[x(t)]}\notag\\
        &\qquad\qquad+ \int_{x_a}^{x_b} \!\!\! \DL{x} \, \e^{iS_0[x(t)]} \sum_{n=1}^{\infty} \frac{1}{n!} \left( \frac{i}{\hbar} S_1[x(t)] \right)^n\\
        &= \braket{x_b, t_b}{x_a, t_a}_0 + \int_{x_a}^{x_b} \!\! \DL{x} \, \e^{iS_0[x(t)]} \sum_{n=1}^{\infty} \frac{1}{n!} \left( \frac{i}{\hbar} S_1[x(t)] \right)^n\label{eqn:perturbation series}
    \end{align}
    where \(\braket{x_b, t_b}{x_a, t_a}_0\) denotes the transition amplitude for the unperturbed system, in general we will use a subscript 0 to denote quantities referring to the unperturbed system.
    
    Now, we have
    \begin{equation}
        S_1[x(t)] = -\int_{t_a}^{t_b} V(x(t), t) \dd{t}
    \end{equation}
    where \(V\) is a potential corresponding to the perturbation, note that \(V\) may be what we referred to as \(\tilde{V}\) in \cref{eqn:perturbation example}.
    Inserting this definition, making sure to use a different dummy variable for each integral, we get
    \begin{align}
        &\braket{x_b, t_b}{x_a, t_a} = \braket{x_b, t_b}{x_a, t_a}_0 \notag\\
        &\quad\qquad+ \sum_{n=1}^{\infty} \frac{1}{n!} \left( -\frac{i}{\hbar} \right)^n \int_{t_a}^{t_b} \!\! \dl{t_1} \dotsm \int_{t_a}^{t_b} \!\! \dl{t_n} \textcolor{highlight}{\int_{x_a}^{x_b} \!\! \DL{x} \, V_1\dotsm V_n \e^{iS_0[x(t)]}}\label{eqn:series for perturbed transition amplitude}
    \end{align}
    where \(V_i \coloneqq V(x(t_i), t_i)\).
    We can identify the path integral here as the time-ordered product
    \begin{equation}
        \textcolor{highlight}{\tensor[_0]{\bra{x_b, t_b}}{} \timeorder (V_1\dotsm V_n) \ket{x_a, t_a}_{0}}.
    \end{equation}
    Taking \(t_a \le t_1 \le \dotsb \le t_n \le t_b\) we then may write
    \begin{multline}
        \int_{x_a}^{x_b} \!\! \DL{x} \, V_1\dotsm V_n \e^{iS_0[x(t)]/\hbar} = \int \!\! \dl{x_1} \dotsm \int \!\! \dl{x_n}\\
        \braket{x_b, t_b}{x_n, t_n}_0 V_n \braket{x_n, t_n}{x_{n-1}, t_{n-1}}_0 \dotsm V_1 \braket{x_1, t_1}{x_a, t_a}_{0}.
    \end{multline}
    
    From this we see that we can interpret the sum in \cref{eqn:perturbation series} as a sum of partial amplitudes, with the first term corresponding to no scattering (as there is no \(V_i\)), the second to a single scattering (as there is \(V_1\) and no other potentials), the third to a double scattering (as there are \(V_1\) and \(V_2\)), and so on.
    It can be useful to represent this diagrammatically.
    We denote the transition amplitude \(\braket{x_k, t_k}{x_{k-1}, t_{k-1}}_0\) by straight lines, and insertions of the potential by wavy lines.
    Hence our sum is
    \begin{equation}
        \tikzsetnextfilename{perturbation-first-term}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (tb) at (1, 1) {\(t_b\)};
                \diagram {
                    (ta) -- (tb);
                };
            \end{feynman}
        \end{tikzpicture}
        + \tikzsetnextfilename{perturbation-second-term}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (t1) at (1, -0.5);
                \vertex (tb) at (2, 0) {\(t_b\)};
                \vertex (t1 in) at (1, -1.5);
                \node[above] at (t1) {\(t_1\)};
                \diagram {
                    (ta) -- (t1) -- (tb);
                    (t1 in) -- [photon] (t1);
                };
            \end{feynman}
        \end{tikzpicture}
        + \tikzsetnextfilename{perturbation-third-term}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (t1) at (1, 0);
                \vertex (t2) at (2, 0.5);
                \vertex (tb) at (3, 0.5) {\(t_b\).};
                \vertex (t1 in) at (1, -1);
                \vertex (t2 in) at (2, 1.5);
                \node[above] at (t1) {\(t_1\)};
                \node[below] at (t2) {\(t_2\)};
                \diagram {
                    (ta) -- (t1) -- (t2) -- (tb);
                    (t1 in) -- [photon] (t1);
                    (t2 in) -- [photon] (t2);
                };
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Where
    \begin{align}
        \tikzsetnextfilename{perturbation-first-term-again}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (tb) at (1, 1) {\(t_b\)};
                \diagram {
                    (ta) -- (tb);
                };
            \end{feynman}
        \end{tikzpicture}
        &= \braket{x_a, t_a}{x_b, t_b}_0,\\
        \tikzsetnextfilename{perturbation-second-term-again}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (t1) at (1, -0.5);
                \vertex (tb) at (2, 0) {\(t_b\)};
                \vertex (t1 in) at (1, -1.5);
                \node[above] at (t1) {\(t_1\)};
                \diagram {
                    (ta) -- (t1) -- (tb);
                    (t1 in) -- [photon] (t1);
                };
            \end{feynman}
        \end{tikzpicture}
        &= -\frac{i}{\hbar} \int_{t_a}^{t_b} \!\! \dl{t_1} \int_{x_a}^{x_b} \!\! \dl{x_1} \braket{x_b, t_b}{x_1, t_1}_0 V_1 \braket{x_1, t_1}{x_a, t_a}_0, \\
        \tikzsetnextfilename{perturbation-third-term-again}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (t1) at (1, 0);
                \vertex (t2) at (2, 0.5);
                \vertex (tb) at (3, 0.5) {\(t_b\)};
                \vertex (t1 in) at (1, -1);
                \vertex (t2 in) at (2, 1.5);
                \node[above] at (t1) {\(t_1\)};
                \node[below] at (t2) {\(t_2\)};
                \diagram {
                    (ta) -- (t1) -- (t2) -- (tb);
                    (t1 in) -- [photon] (t1);
                    (t2 in) -- [photon] (t2);
                };
            \end{feynman}
        \end{tikzpicture}
        &= -\frac{1}{\hbar^2} \int_{t_a}^{t_b} \!\! \dl{t_1} \int_{t_a}^{t_b} \!\! \dl{t_2} \int_{x_a}^{x_b} \!\! \dl{x_1} \int_{x_a}^{x_b} \!\! \dl{x_2}\\[-2.5em]
        &\qquad \times \braket{x_b, t_b}{x_2, t_2}_0 V_2 \braket{x_2, t_2}{x_1, t_1}_0 V_1 \braket{x_1, t_1}{x_a, t_a}_0.
    \end{align}

    \subsection{An Identity}\label{sec:an identity}
    In this section we prove a special case of an identity that will help us simplify further.
    The identity in question is
    \begin{multline}\label{eqn:an identity}
        \frac{1}{n!} \int_{t_a}^{t_b} \!\! \dl{t_1} \int_{t_a}^{t_b} \!\! \dl{t_2} \dotsm \int_{t_a}^{t_b} \!\! \dl{t_n} \, V_1 V_2 \dotsm V_n\\
        = \int_{t_a}^{t_b} \!\! \dl{t_n} \int_{t_a}^{t_n} \!\! \dl{t_{n-1}} \dotsm \int_{t_a}^{t_2} \!\! \dl{t_1} \, V_1 V_2 \dotsm V_n.
    \end{multline}
    We will demonstrate this for the case of \(n = 2\), the general case then follows inductively.
    
    Start with the right hand side of the identity for \(n = 2\), we have
    \begin{equation}
        I = \int_{t_a}^{t_b} \!\! \dl{t_2} \int_{t_a}^{t_b} \!\! \dl{t_1} \, V_1V_2.
    \end{equation}
    We can split the integral over \(t_1\) into two parts, one running from \(t_a\) to \(t_2\) and the other from \(t_2\) to \(t_b\):
    \begin{equation}
        I = \int_{t_a}^{t_b} \!\! \dl{t_2} \left[ \int_{t_a}^{t_2} V_1V_2 \dd{t_1} + \int_{t_2}^{t_b} V_1V_2 \dd{t_1} \right].
    \end{equation}
    Consider the second term,
    \begin{equation}
        \int_{t_a}^{t_b} \!\! \dl{t_2} \int_{t_2}^{t_b} \!\! \dl{t_1} \, V_1V_2.
    \end{equation}
    This is a double integral, so it corresponds to integrating \(V_1V_2\) over an area in the plane.
    In particular we integrate from \(t_2\) (which lies on the line \(t_1 = t_2\)) to \(t_b\) along \(t_1\), and then along \(t_a\) to \(t_b\) along \(t_2\).
    This means that \(t_2 \le t_1 \le t_b\).
    This corresponds to a triangle in the plane, shown in \cref{fig:region of integration}
    
    \begin{figure}
        \tikzsetnextfilename{triangle-in-plane}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 4) node[left] {\(t_2\)} -- (0, 0) -- (4, 0) node[below] {\(t_1\)};
            \draw[highlight, very thick] (0, 0) -- (4, 4);
            \coordinate (t1=t2) at (3.53, 3.8);
            \node[rotate around={45:(t1=t2)}] at (t1=t2) {\(t_1 = t_2\)};
            \node[below] at (1, 0) {\(t_a\)};
            \node[left] at (0, 1) {\(t_a\)};
            \node[below] at (3, 0) {\(t_b\)};
            \node[left] at (0, 3) {\(t_b\)};
            \draw[highlight] (1, 0) -- ++ (0, 1);
            \draw[highlight] (3, 0) -- ++ (0, 3);
            \draw[highlight] (0, 1) -- ++ (3, 0);
            \draw[highlight] (0, 3) -- ++ (3, 0);
            \draw[very thick, highlight, fill=highlight!50] (1, 1) -- (3, 3) -- (3, 1) -- cycle;
        \end{tikzpicture}
        \caption[Region of integration.]{The region of integration used in \cref{sec:an identity}.}
        \label{fig:region of integration}
    \end{figure}
    
    We can integrate over the same region in a different way.
    We instead integrate along \(t_2\) from \(t_a\) to \(t_1\) and then along \(t_1\) from \(t_a\) to \(t_b\).
    This corresponds to the integral
    \begin{equation}
        \int_{t_a}^{t_b} \!\! \dl{t_1} \int_{t_a}^{t_1} \!\! \dl{t_2} V_1V_2.
    \end{equation}
    However, since \(t_1\) and \(t_2\) are dummy variables, and the integrand is symmetric in \(t_1\) and \(t_2\), we are free to rename them, in particular we can swap them, \(t_1 \leftrightarrow t_2\), giving
    \begin{equation}
        \int_{t_a}^{t_b} \!\! \dl{t_2} \int_{t_a}^{t_2} \!\! \dl{t_1} V_1V_2.
    \end{equation}
    Combing this with the first term again we have
    \begin{align}
        I &= \int_{t_a}^{t_b} \!\! \dl{t_2} \left[ \int_{t_a}^{t_2} V_1V_2 \dd{t_1} +  \int_{t_a}^{t_2} V_1V_2 \dd{t_1} \right]\\
        &= 2\int_{t_a}^{t_b} \!\! \dl{t_2} \int_{t_a}^{t_b} \!\! \dl{t_1} \, V_1V_2.
    \end{align}	
    Noticing that \(2! = 2\) and dividing through by this quantity we have proven \cref{eqn:an identity} for the case of \(n = 2\).
    
    To prove the general case apply induction, in particular we can use the induction hypothesis to swap \(n - 1\) integrals, and then a similar argument about regions of integration to swap the final integral.
    
    \subsection{Series for Transition Amplitudes}
    Using \cref{eqn:an identity} in \cref{eqn:series for perturbed transition amplitude} we have
    \begin{align}\label{eqn:transition amplitude scattering series}
        &\braket{x_b, t_b}{x_a, t_a} = \braket{x_b, t_b}{x_a, t_a}_0 \notag\\
        &\quad\qquad {} + \sum_{n=1}^{\infty} \left( -\frac{i}{\hbar} \right)^n \int \!\! \dl{x_1} \dotsm \int \!\! \dl{x_n} \int_{t_a}^{t_b} \!\! \dl{t_n} \int_{t_a}^{t_n} \!\! \dl{t_{n-1}} \dotsm \int_{t_a}^{t_2} \!\! \dl{t_1} \notag\\
        &\quad\qquad \times \braket{x_b, t_b}{x_n, t_n}_0 V_n \braket{x_n, t_n}{x_{n-1}, t_{n-1}}_0 \dotsm V_1 \braket{x_1, t_1}{x_a, t_a}_0.
    \end{align}
    We have cancelled the \(1/n!\) with the factor of \(n!\) appearing from \cref{eqn:an identity}.
    The result is that we are left with a geometric series instead of an exponential series.
    Writing out the first couple of terms will allow us to see how to simplify this.
    \begin{align}
        &\braket{x_b, t_b}{x_a, t_a} = \braket{x_b, t_b}{x_a, t_a}_0\notag\\
        &\quad\qquad {} - \frac{i}{\hbar} \int_{-\infty}^{\infty} \!\! \dl{x_1} \int_{t_a}^{t_b} \!\! \dl{t_1} \braket{x_b, t_b}{x_1, t_1}_0 V_1 \braket{x_1, t_1}{x_a, t_a}_0\notag\\
        &\quad\qquad {} + \left( -\frac{i}{\hbar} \right)^2 \int_{-\infty}^{\infty} \!\! \dl{x_2} \int_{t_a}^{t_b} \!\! \dl{t_2} \int_{-\infty}^{\infty} \!\! \dl{x_1} \int_{t_a}^{t_2} \!\! \dl{t_1}\notag\\
        &\quad\qquad \times \braket{x_b, t_b}{x_2, t_2}_0 V_2 \braket{x_2, t_2}{x_1, t_1} V_1 \braket{x_1, t_1}{x_a, t_a}_0 + \dotsb.
    \end{align}
    Again, \(x_i\) and \(t_i\) (for \(i = 1, \dotsc, n\), not for \(i = a, b\)) are just dummy variables, and so we can rename them.
    For the second order term we rename \(x_1 \leftrightarrow x_2\) and \(t_1 \leftrightarrow t_2\), and similarly for higher orders.
    This allows us to factorise the integrals in the second, and subsequent terms, to give
    \begin{align}
        &\braket{x_b, t_b}{x_a, t_a} = \braket{x_b, t_b}{x_a, t_a}_0\notag\\
        &\qquad\quad{} - \frac{i}{\hbar} \int_{-\infty}^{\infty} \!\! \dl{x_1} \int_{t_a}^{t_b} \!\! \dl{t_a} \braket{x_b, t_b}{x_1, t_1}_0 V_1 \bigg[\braket{x_1, t_1}{x_a, t_a}\notag\\
        &\qquad\quad {} - \frac{i}{\hbar} \int_{-\infty}^{\infty} \!\! \dl{x_2} \int_{t_a}^{t_b} \!\! \dl{t_2} \braket{x_1, t_1}{x_2, t_2}_0 V_2 \braket{x_2, t_2}{x_a, t_a}_0 + \dotsb \bigg].\label{eqn:transition amplitude 2}
    \end{align}
    
    We can now identify the term in square brackets as \(\braket{x_1, t_1}{x_a, t_a}\).
    Relabelling \(x_1\) to \(x\) and \(t_1\) to \(t\) we then have
    \begin{multline}\label{eqn:transition amplitude}
        \braket{x_b, t_b}{x_a, t_a} =\\
        \braket{x_b, t_b}{x_a, t_a}_0 - \frac{i}{\hbar} \int_{-\infty}^{\infty} \!\! \dl{x} \int_{t_a}^{t_b} \!\! \dl{t} \braket{x_b, t_b}{x, t}_0 V(x, t) \braket{x, t}{x_a, t_a}.
    \end{multline}
    
    The interpretation of this is that the perturbed transition amplitude is the sum of the unperturbed transition amplitude and a term that corresponds to scattering before time \(t\) and no scattering after, which we then integrate over \(t\) to account for all scattering processes.
    We write this pictorially as
    \begin{equation}
        \tikzsetnextfilename{all-scattering}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (tb) at (1, 1) {\(t_b\)};
                \diagram { (ta) -- [double] (tb)  };
            \end{feynman}
        \end{tikzpicture}
        =
        \tikzsetnextfilename{no-scattering}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (tb) at (1, 1) {\(t_b\)};
                \diagram { (ta) -- (tb)  };
            \end{feynman}
        \end{tikzpicture}
        +
        \tikzsetnextfilename{half-scattering}
        \begin{tikzpicture}[baseline=(current bounding box.west)]
            \begin{feynman}
                \vertex (ta) {\(t_a\)};
                \vertex (t) at (1, 1);
                \vertex (tb) at (2, 0) {\(t_b\).};
                \vertex (t in) at (1, 2);
                \node[below] at ($(t) + (0, -0.05)$) {\(t\)};
                \diagram {
                    (ta) -- [double] (t) -- (tb);
                    (t) -- [photon] (t in);
                };
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Here a double line corresponds to the full amplitude for a transition with all scattering, and the single line to the unperturbed amplitude.
    The final term therefore contains scattering up to time \(t\), then an insertion of the potential at time \(t\), and then no scattering.
    
    \section{Wave Function}
    Now that we have developed perturbation theory for transition amplitudes we can develop a similar equation for wave functions.
    Are starting point is
    \begin{align}
        \braket{x, t}{\psi} &= \int_{-\infty}^{\infty} \!\! \dl{x''} \braket{x, t}{x'', t_0} \braket{x'', t_0}{\psi}\\
        &= \braket{x, t}{\psi}_{0} - \frac{i}{\hbar} \int_{-\infty}^{\infty} \!\! \dl{x'} \int_{t_0}^{t} \!\! \dl{t'} \braket{x, t}{x', t'}_0 V(x', t') \braket{x', t'}{\psi}.
    \end{align}
    The first term here corresponds to the system evolving from an undisturbed state at time \(t_0\), under the action \(S_0\).
    At time \(t_0\) the perturbed states and unperturbed states coincide, since the perturbation hasn't yet taken effect.
    Using this we have
    \begin{equation}\label{eqn:perturbation SE}
        \psi(x, t) = \psi_0(x, t) - \frac{i}{\hbar} \int_{-\infty}^{\infty} \!\! \dl{x'} \int_{t_0}^{t} \!\! \dl{t'} \braket{x, t}{x', t'}_0 V(x', t') \psi(x', t').
    \end{equation}
    Here \(\psi_0\) is the unperturbed wave function, which is a solution to the unperturbed Schr\"odinger equation,
    \begin{equation}
        \left( i\hbar\diffp{}{t} - \operator{\hamiltonian}_0(x, t) \right) \psi_0(x, t) = 0.
    \end{equation}
    Here \(\operator{\hamiltonian}_0\) is the Hamiltonian operator corresponding to the unperturbed system.
    
    Considering \cref{eqn:perturbation SE} we see that we can insert a factor of \(\theta(t - t')\) inside the integral, since \(t' \le t\) in the integral and so this amounts to a factor of 1.
    Further this lets us extend the region of integration to \([t_0, \infty)\), since the integrand will vanish outside of this region with the Heaviside step function.
    Doing so we have
    \begin{equation}
        \psi(x, t) = \psi_0(x, t) - \frac{i}{\hbar} \int_{-\infty}^{\infty} \!\! \dl{x'} \int_{t_0}^{\infty} \theta(t - t') \braket{x, t}{x', t'}_0 V(x', t') \psi(x', t').
    \end{equation}
    
    Now recall that we defined
    \begin{equation}
        G_0(x, x'; t, t') = \theta(t - t') \braket{x, t}{x', t'}
    \end{equation}
    in \cref{sec:transition amplitude as a green's function} we showed that this satisfies
    \begin{equation}
        \left( i\hbar\diffp{}{t} - \operator{\hamiltonian}_0 \right)G_0(x, x'; t, t') = i\hbar\delta(t - t')\delta(x - x').
    \end{equation}
    We can then use this, and the fact that the first term vanishes as a solution to the unperturbed Schr\"odinger equation, to write
    \begin{align}
        &\left( i\hbar\diffp{}{t} - \operator{\hamiltonian}_0 \right)\psi(x, t)\notag\\
        &\qquad\qquad= -\frac{i}{\hbar} \int_{-\infty}^{\infty} \!\! \dl{x'} \int_{t_0}^{\infty} \!\! \dl{t'} \, i\hbar \delta(t - t')\delta(x - x') V(x', t')\psi(x', t')\\
        &\qquad\qquad= V(x, t)\psi(x, t).
    \end{align}
    Rearranging this we get
    \begin{equation}
        i\hbar\diffp{\psi}{t} = (\operator{\hamiltonian}_0 + V)\psi = \operator{\hamiltonian}\psi.
    \end{equation}
    Which is exactly the Schr\"odinger equation for the perturbed system, which has the Hamiltonian \(\operator{\hamiltonian} \coloneqq \operator{\hamiltonian}_0 + V\).
    We see, therefore, that \cref{eqn:perturbation SE} is an integral equation for \(\psi\) which is equivalent to the Schr\"odinger equation.
    
    \chapter{Fixed Target Scattering}
    \section{The \texorpdfstring{\(S\)}{S}-Matrix}
    Consider elastic scattering of a particle of mass \(m\) in a potential, \(V(\vv{r}, t)\).
    We can write
    \begin{equation}
        \braket{\vv{r_b}, t_b}{\vv{r_a}, t_a} = \bra{\vv{r_a}} \operator{U}(t_b, t_a) \ket{\vv{r_a}}.
    \end{equation}
    We then define the \defineindex{scattering operator} or the \define{\(S\)-matrix}\index{S-matrix@\(S\)-matrix} as the time evolution operator taking us from infinitely before the scattering to infinitely after, that is
    \begin{equation}
        \operator{S} \coloneqq \lim_{\substack{t_a\to-\infty\\t_b\to\infty}} \operator{U}(t_b, t_a)
    \end{equation}
    so
    \begin{equation}
        \lim_{\substack{t_a\to-\infty\\t_b\to\infty}} \braket{\vv{r_b}, t_b}{\vv{r_a}, t_a} = \bra{\vv{r_b}} \operator{S} \ket{\vv{r_a}}.
    \end{equation}
    
    Since \(\operator{U}\) is unitary \(\operator{S}\) is too so \(\operator{S}^\hermit \operator{S} = \operator{S}\operator{S}^\hermit = \idop\).
    
    We call \(\ket{\vv{r_b}, \infty}\) the \defineindex{out state}, it represents a free particle in the far future, long after the effects of the potential.
    Similarly we call \(\ket{\vv{r_a}, -\infty}\) the \defineindex{in state}, it represents a free particle in the far past, long before the first effects of the potential.
    This all assumes that the potential, \(V(\vv{r}, t)\), is short ranged so that it vanishes at infinity.
    
    \subsection{Lowest Order Scattering}
    Suppose that the potential is time independent.
    Then the system will be invariant under time translation so defining \(T = t_b - t_a\) we have
    \begin{equation}
        \braket{\vv{r_b}, t_b}{\vv{r_a}, t_a} = \braket{\vv{r_b}, T/2}{\vv{r_a}, -T/2} = \braket{\vv{r_b}, T}{\vv{r_a}, 0}.
    \end{equation}
    
    \begin{figure}
        \tikzsetnextfilename{fixed-target-scattering}
        \begin{tikzpicture}
            \path (40:3) coordinate (C) -- (0, 0) coordinate (B) -- (3, 0) coordinate (A) pic [draw, "\(\vartheta\)", angle eccentricity = 0.75, angle radius = 0.7cm] {angle};
            \draw[very thick] (-3, 0) -- (3, 0);
            \draw[very thick] (0, 0) -- (40:3);
            \draw[very thick, highlight] (-3, 0) -- (110:0.5) node[midway, above, black] {\(R_a\)} -- (40:3) node[midway, above left, black] {\(R_b\)};
            \fill[complementary] (-3, 0) circle [radius = 0.1cm];
            \node[below] at (-3, 0) {\((\vv{r_a}, 0)\)};
            \fill[complementary] (40:3) circle [radius = 0.1cm];
            \node[above] at (40:3) {\((\vv{r_b}, T)\)};
            \fill[complementary] (3, 0) circle [radius = 0.1cm];
            \node[below] at (3, 0) {\((\vv{r_c}, T)\)};
            \draw[decorate, decoration={snake, amplitude=1pt, segment length=5pt}] (0, 0) -- (110:0.5);
            \node at (-0.3, 0.25) {\(V\)};
            \node at (0.075, 0.3) {\(r\)};
            \draw[highlightpurple, ->, very thick] (-2, -0.15) -- (-1, -0.15) node[midway, below, black] {\(\vv{p_a}\)};
            \draw[highlightpurple, ->, xshift=0.2cm, very thick] (40:1) -- (40:2) node[midway, below right, black] {\(\vv{p_b}\)};
            \node[above left] at (95:0.4) {\((\vv{r}, t)\)};
            \fill[highlightpurple] (0, 0) circle [radius = 0.1cm];
            \node[below] at (0, 0) {Target};
        \end{tikzpicture}
        \caption[Fixed target scattering]{A particle enters at \(\vv{r_a}\) at time \(t = 0\) with momentum \(\vv{p_a}\) and is scattered off of a fixed target to the point \(\vv{r_b}\) at time \(T\) by a potential \(V\), which we think of as being applied at point \(\vv{r}\) at time \(t\). The scattering angle is \(\vartheta\). If there were no potential then the particle would finish at \((\vv{r_c})\) at time \(T\) instead.}
        \label{fig:scattering experiment}
    \end{figure}
    
    This gives us the classic scattering setup from a fixed target, as shown in \cref{fig:scattering experiment}.
    \Cref{eqn:transition amplitude scattering series} then gives us
    \begin{multline}
        \braket{\vv{r_b}, T}{\vv{r_a}, 0} = \braket{\vv{r_b}, T}{\vv{r_a}, 0}_0 \\
        - \frac{i}{\hbar} \int_{\reals^3}\dl{^3r} \int_{0}^{T} \dl{t} \braket{\vv{r_b}, T}{\vv{r}, t} V(\vv{r})\braket{\vv{r}, t}{\vv{r_a}, 0}_0 + \order(V^2).
    \end{multline}
    The first term corresponds to no scattering with a free particle.
    We are then treating \(V\) as a perturbation.
    The unperturbed Lagrangian for the free particle is simply the kinetic energy,
    \begin{equation}
        \lagrangian_0(\vv{r}) = \frac{m}{2}(\dot{x}^2 + \dot{y}^2 + \dot{z}^2).
    \end{equation}
    In Cartesian coordinates the path integral for the transition amplitude of a free particle factorises into three one-dimensional path integrals:
    \begin{equation}
        \braket{\vv{r'}, t'}{\vv{r}, t}_0 = \prod_{i=1}^{3} \braket{x_i', t'}{x_i, t}_0.
    \end{equation}
    
    To first order in perturbation theory (that is ignoring terms \(\order(V^2)\) or smaller) the transition amplitude, \(A_1\), is given by inserting the transition amplitudes for the free particles giving
    \begin{align}
        A_1 &\coloneqq - \frac{i}{\hbar} \int_{\reals^3}\dl{^3r} \int_{0}^{T} \dl{t} \braket{\vv{r_b}, T}{\vv{r}, t} V(\vv{r})\braket{\vv{r}, t}{\vv{r_a}, 0}_0\\
        &\hphantom{:}= -\frac{i}{\hbar} \int_{\reals^3} \!\! \dl{^3r}\int_0^T \!\! \dl{t} \left( \frac{m}{2\pi i\hbar(T - t)} \right)^{3/2} \exp\left[ \frac{i}{\hbar}\frac{m}{2}\frac{\abs{\vv{r_b} - \vv{r}}^2}{T - t} \right]\\
        &\qquad \times V(\vv{r}) \left( \frac{m}{2\pi i\hbar t} \right)^{3/2} \exp\left[ \frac{i}{\hbar} \frac{m}{2} \frac{\abs{\vv{r} - \vv{r_a}}^2}{t} \right].
    \end{align}
    This corresponds to the particle travelling to \(\vv{r}\) at time \(t\), experiencing an impulse, and exiting at \(\vv{r_b}\) at time \(T\).
    
    We then use the result
    \begin{multline}
        \int_0^T \frac{1}{[t(T - t)]^{3/2}} \exp\left[ -\frac{\alpha}{T - t} - \frac{\beta}{t} \right]\\
        = \frac{1}{T} \sqrt{\frac{\pi}{T}} \left( \frac{1}{\sqrt{\alpha}} + \frac{1}{\sqrt{\beta}} \right) \exp\left[ -\frac{1}{T}(\sqrt{\alpha} + \sqrt{\beta})^2 \right].
    \end{multline}
    This is derived in \cref{sec:feynman's scattering integral}.
    
    Setting
    \begin{equation}
        R_a = \abs{\vv{r_a} - \vv{r}}, \qqand R_b = \abs{\vv{r_b} - \vv{r}}
    \end{equation}
    as well as
    \begin{equation}
        \alpha = -\frac{i}{\hbar} \frac{m}{2} R_a^2, \qqand \beta = -\frac{i}{\hbar} \frac{m}{2} R_b^2
    \end{equation}
    we get
    \begin{equation}
        \frac{1}{\sqrt{\alpha}} + \frac{1}{\sqrt{\beta}} = \sqrt{\frac{2i\hbar}{m}} \left( \frac{1}{R_a} + \frac{1}{R_b} \right),
    \end{equation}
    and
    \begin{equation}
        (\sqrt{\alpha} + \sqrt{\beta})^2 = -\frac{im}{2\hbar}(R_a + R_b)^2.
    \end{equation}
    Hence, the transition amplitude for the first order scattering is
    \begin{multline}
        A_1 = -\frac{i}{\hbar} \left( \frac{m}{2\pi i\hbar} \right)^{3} \frac{1}{T}\sqrt{\frac{\pi}{T}} \int_{\reals^3} \!\! \dl{^3r} \sqrt{\frac{2i\hbar}{m}} \left( \frac{1}{R_a} + \frac{1}{R_b} \right)\\
        \times V(\vv{r}) \exp\left[ \frac{i}{\hbar} \frac{m}{2} \frac{1}{T} (R_a + R_b)^2 \right].
    \end{multline}
    Collecting prefactors we have
    \begin{equation}
        A_1 = -\frac{i}{\hbar} \left( \frac{m}{2\pi i\hbar} \right)^{5/2} \frac{1}{T^{3/2}} \int_{\reals^3} \left( \frac{1}{R_a} + \frac{1}{R_b} \right) V(\vv{r}) \exp\left[ \frac{im}{2\hbar T}(R_a + R_b)^2 \right].
    \end{equation}
    
    Since we take the potential to be short range the values of \(\vv{r}\) for which the contribution to the integral is non-negligible will have \(r \ll r_a, r_b\) and so we can expand \(R_a\) and \(R_b\) as
    \begin{align}
        R_a &= \abs{\vv{r_a} - \vv{r}}\\
        &= \abs*{r_a\left(\vh{n}_{\vv{a}} - \frac{\vv{r}}{r_a}\right)}\\
        &= \sqrt{r_a^2\left(\vh{n}_{\vv{a}} - \frac{\vv{r}}{r_a}\right)^2}\\
        &= r_a\left( 1 - 2\vh{n}_{\vv{a}} \cdot \vv{r} + \frac{r^2}{r_a^2} \right)^{1/2}\\
        &= r_a - \vh{n}_{\vv{a}} \cdot \vv{r} + \order(1/r_a)
    \end{align}
    where \(\vh{n}_{\vv{a}} = \vv{r_a}/r_a\) is the unit vector in the direction of \(\vv{r_a}\).
    Similarly \(R_b = r_b - \vh{n}_{\vv{b}} \cdot \vv{r} + \order(1/r_b)\).
    We can also expand other terms, such as
    \begin{align}
        \frac{1}{R_a} + \frac{1}{R_b} &= \frac{1}{r_a} + \frac{1}{r_b} + \order(1/r_a^2, 1/r_b^2),\\
        (R_a + R_b)^2 &= (r_a + r_b)^2 - 2(r_a + r_b)(\vh{n}_{\vv{a}} + \vh{n}_{\vv{b}}) \cdot \vv{r} + \order(1).
    \end{align}
    
    We then have, to leading order
    \begin{multline}
        A_1 = -\frac{i}{\hbar} \left( \frac{m}{2\pi i \hbar} \right)^{5/2} \frac{1}{T^{3/2}} \left[ \frac{1}{r_1} + \frac{1}{r_b} \right] \exp\left[ \frac{im}{2\hbar T}(r_a + r_b)^2 \right]\\
        \times \int_{\reals^3} \!\! \dl{^3r} V(\vv{r}) \exp\left[ -\frac{im}{\hbar T} (r_a + r_b)(\vh{n}_{\vv{a}} + \vh{n}_{\vv{b}}) \cdot \vv{r} \right].
    \end{multline}
    
    As the potential is short ranged most of the time the particle behaves as a particle.
    We therefore define quantities, which are approximately the momentum and energy of the particle using the distance travelled, which is approximately \(r_a + r_b\), since the particle starts at \(\vv{r_a}\), travels close to the origin, and then is scattered to \(\vv{r_b}\).
    The average speed is then approximately \((r_a + r_b)/T\), giving approximate momentum and energy
    \begin{equation}
        p = m\left( \frac{r_a + r_b}{T} \right), \qqand E = \frac{1}{2}m\left( \frac{r_a + r_b}{T} \right)^2.
    \end{equation}
    
    Using this we have
    \begin{equation}
        \vv{p_a} = -p\vh{n}_{\vv{a}}, \qqand \vv{p_b} = p\vh{n}_{\vv{b}}
    \end{equation}
    as approximations of the particles incoming and outgoing momenta.
    The negative sign is because initially the particle heads toward the origin from \(\vh{r_a}\), and so \(\vh{n}_{\vv{a}}\) points away from the origin.
    We are assuming momentum is conserved so \(\abs{\vv{p_a}} = \abs{\vv{p_b}} = p\).
    
    From this we can compute the approximate momentum transfer
    \begin{equation}
        \vv{p_a} - \vv{p_b} = -m\frac{r_a + r_b}{T}(\vh{n}_{\vv{a}} + \vh{n}_{\vv{b}}) \eqqcolon \hbar\vh{k}
    \end{equation}
    where we define \(\vv{k}\) as the wave number transfer.
    This quantity is sometimes denoted \(\vv{q}\), \(\vv{K}\), or \(\vv{Q}\) as well.
    
    It follows that
    \begin{align}
        A_1 &= -\frac{i}{\hbar} \left( \frac{m}{2\pi i\hbar} \right)^{5/2} \frac{1}{T^{3/2}} \underbrace{ \frac{r_a + r_b}{r_ar_b} }_{=\frac{1}{r_a} + \frac{1}{r_b}} \e^{iET/\hbar} \int_{\reals^3} V(\vv{r}) \e^{i\vv{k} \cdot \vv{r}} \dd[3]{r}\\
        &= -\frac{i}{\hbar} \left( \frac{m}{2\pi i\hbar} \right)^{5/2} \frac{1}{T^{3/2}} \frac{r_a + r_b}{r_ar_b} + \e^{iET/\hbar} \tilde{V}(\vv{k})
    \end{align}
    where
    \begin{equation}
        \tilde{V}(\vv{k}) = \fourierTransform\{V(\vv{r})\} = \int_{\reals^3} V(\vv{r}) \e^{i\vv{k} \cdot \vv{r}} \dd[3]{r}
    \end{equation}
    is the Fourier transform of the scattering potential.
    
    The transition probability (per unit volume) is
    \begin{equation}
        P(a \to b) = \abs{A_1}^2 = \frac{1}{\hbar^2} \left( \frac{m}{2\pi\hbar} \right)^5 \frac{1}{T^3} \left( \frac{r_a + r_b}{r_ar_b} \right)^2 \abs{\tilde{V}(\vv{k})}^2.
    \end{equation}
    
    For no scattering, i.e. for a free particle, the particle ends up at \(\vv{r_c}\) (see \cref{fig:scattering experiment}) and the transition amplitude for this is
    \begin{equation}
        A_0 = \braket{\vv{r_c}, T}{\vv{r_a}, 0}_0 = \left( \frac{m}{2\pi i\hbar T} \right)^{3/2} \exp\left[ \frac{i}{\hbar} \frac{m}{2} \frac{\abs{\vv{r_c} - \vv{r_a}}^2}{T} \right].
    \end{equation}
    The probability (per unit volume) of this is
    \begin{equation}
        P(a \to c) = \abs{A_0}^2 = \left( \frac{m}{2\pi\hbar T} \right)^3.
    \end{equation}
    
    The ratio of scattering to no-scattering probabilities is then
    \begin{equation}\label{eqn:scattering probability ratio}
        \frac{P(a \to b)}{P(a \to c)} = \left( \frac{m}{2\pi\hbar^2} \right)^{2} \left( \frac{r_a + r_b}{r_ar_b} \right)^2 \abs{\tilde{V}(\vv{k})}^2.
    \end{equation}
    
    \section{Cross Sections}
    Scattering processes are described using the effective target area, called the scattering cross section.
    Recall that we define the solid angle element of the area element, \(\dl{\vv{S}}\), as \(\dl{\Omega} = \vh{n} \cdot \dl{\vv{S}}/r^2\) where \(\vv{r}\) is the vector from the origin to the area element and \(\vh{n} = \vh{r} = \ve{r}\) is a unit vector in this direction.
    For an element of the sphere in spherical polar coordinates the solid angle is \(\dl{\Omega} = \sin\vartheta \dd{\vartheta} \dd{\varphi}\).
    Integrating over the sphere gives a total solid angle of \(4\pi\).
    
    \begin{figure}
        \tikzsetnextfilename{scattering-cross-sections}
        \begin{tikzpicture}
            \draw[very thick, highlight, fill=highlight!10] (0, 0) -- (-2:8) arc(-28:28:{16*tan(2)}) -- cycle;
            \draw[very thick, highlight] (2:8) arc(180-28:180+28:{16*tan(2)});
            \draw[very thick, highlight, fill=highlight!10] (2:4) -- ++ (46:4)  arc(73:17:{16*tan(2)}) coordinate (A) -- (-2:4);
            \draw[very thick, highlight] (A) arc(73+180:17+180:{16*tan(2)});
            \draw[very thick, highlight] (-2:4) arc(-28:28:{8*tan(2)});
            \draw[very thick, highlight] (2:4) arc(180-28:180+28:{8*tan(2)});
            
            \draw[|-|, very thick] (0, -1) -- (4.02, -1) node[midway, below] {\(r_a\)};
            \draw[|-|, very thick] (3.98, -1) -- (8, -1) node[midway, below] {\(r_b\)};
            \draw[|-|, very thick] (3.7, 0.3) -- (6.6, 3.15) node[midway, above left] {\(r_b\)};
            \node[below] at (-2:4) {\(\dl{\sigma}\) \(O\)};
            \node[below] at (0, 0) {\(a\)};
            \node[below] at (-2:8) {\(c\)};
            \node at (7.25, 2.5) {\(b\)};
            \node[right] at (8, 0) {\(\displaystyle \frac{(r_a + r_b)^2}{r_a^2}\dl{\sigma}\)};
            \node at (7.3, 3.1) {\(r_b^2\dd{\sigma}\)};
        \end{tikzpicture}
        \caption{Cross sections in a scattering process.}
        \label{fig:scattering cross sections}
    \end{figure}

    Consider \cref{fig:scattering cross sections}.
    At a distance \(r_a\) the effect of the potential is \(\dl{\sigma}\).
    If there was no potential then paths through this area would reach the area at \(c\), which has area
    \begin{equation}
        A_c = \frac{(r_a + r_b)^2}{r_b^2}\dl{\sigma}.
    \end{equation}
    This follows by rearranging the definition of solid angle, which is the same both at \(O\), a distance \(r_a\) from the initial point and \(c\), a distance \(r_a + r_b\) from the initial point, hence
    \begin{equation}
        \frac{\dl{\sigma}}{r_a^2} = \frac{A_c}{(r_a + r_b)^2}.
    \end{equation}
    
    Instead, with the potential, the paths are scattered into the small area of solid angle \(\dl{\Omega}\), which corresponds to the angle \(r_b^2\dd{\Omega}\).
    Therefore we find that
    \begin{equation}
        P(a \to b) r_b^2\dd{\Omega} = P(a \to c) \frac{(r_a + r_b)^2}{r_a^2} \dd{\sigma}.
    \end{equation}
    This is simply conservation of the number of particle crossing the scattering region and being scattered.
    
    Rearranging this result we have
    \begin{equation}
        \frac{P(a \to b)}{P(a \to c)} = \left[ \frac{r_a + r_b}{r_ar_b} \right]^2 \diff{\sigma}{\Omega}.
    \end{equation}
    Comparing this to \cref{eqn:scattering probability ratio} we see that
    \begin{equation}
        \diff{\sigma}{\Omega} = \left( \frac{m}{2\pi\hbar^2} \right)^2 \abs{\tilde{V}(\vv{k})}^2.
    \end{equation}
    This is the differential cross section in the \define{Born approximation}\index{Born!approximation}.
    Notice that it is independent of \(T\), \(r_a\), and \(r_b\), so we are free to take all of these to infinity, essentially demanding that the potential has no effect at the start or end of the process.
    
    We only went to first order in \(V\), if we went to higher orders then the result is called the \define{Born series}\index{Born!series}.
    
    The total cross section is given by
    \begin{equation}
        \sigma = \int \diff{\sigma}{\Omega} \dd{\Omega} = \int_{0}^{2\pi} \dl{\varphi} \int_{0}^{\pi} \dl{\vartheta} \, \sin(\vartheta) \diff{\sigma}{\Omega}.
    \end{equation}
    Here \(\vartheta\) is the scattering angle, and \(\varphi\) is the angle about the beam axis, which is the line from \(a\) to the origin, to \(c\).
    
    It is possible to derive these results using time independent perturbation theory with plane wave states.
    Here we took the approach of treating the initial and final states as particles.
    This is a case of wave particle duality in quantum mechanics.
    
    \section{Central Potential}
    Suppose we have a central potential, \(V(r)\), we then find that
    \begin{align}
        \tilde{V}(\vv{k}) &= \int_{\reals^3} \dl{^3r'} V(r') \e^{i\vv{k\cdot}\vv{r}'}\\
        &= \int_{0}^{2\pi} \dl{\varphi} \int_{0}^{\pi} \dl{\vartheta'} \int_{0}^{\infty} \dl{r'} \, {r'}^2\sin(\vartheta') V(r') \e^{ikr'\cos\vartheta'}\\
        &= 2\pi \int_{0}^{\infty} \dl{r'} \int_{-1}^{1} \dl{(\cos\vartheta)} {r'}^2V(r')\e^{-kr'\cos\vartheta'}\\
        &= \frac{2\pi}{ik} \int_{0}^{\infty} \dl{r'} \, {r'}^2V(r')\left[ \frac{1}{r'}  \e^{ikr'\cos\vartheta'} \right]_{\cos\vartheta'=-1}^{\cos\vartheta'=1}\\
        &= \frac{2\pi}{ik} \int_{0}^{\infty} \dl{r'} \, r' V(r')[ \e^{ikr'} - \e^{ikr'} ]\\
        &= \frac{4\pi}{k} \int_{0}^{\infty} \dl{r'} \, r'V(r') \sin(kr').
    \end{align}
    We then find that
    \begin{equation}
        \diff{\sigma}{\Omega} = \frac{4m^2}{\hbar^4k^2} \abs*{ \int_{0}^{\infty} rV(r)\sin(kr) \dd{r} }^2.
    \end{equation}
    From the definitions before we have \(p_a^2 = p^2 = p_b^2\), and \(\vv{p_a} \cdot \vv{p_b} = p^2\cos\vartheta\), and so
    \begin{equation}
        k = \frac{1}{\hbar} \abs{\vv{p_a} - \vv{p_b}} = \frac{2p}{\hbar} \sin \frac{\vartheta}{2}.
    \end{equation}
    Remember that we restrict \(\vartheta \in [0, \pi]\).
    
    % TODO: fill out details of calculation
    \begin{exm}{Coulomb Potential}{}
        The most important central potential is the Coulomb potential,
        \begin{equation}
            V(r) = -\frac{e^2}{4\pi\textcolor{gray}{\varepsilon_0}r}.
        \end{equation}
        Where we may or may not set \(\varepsilon_0 = 1\), depending on the choice of units, so we write \(\textcolor{gray}{\varepsilon_0}\).
        For the integral to converge we multiply \(V(r)\) by \(\e^{-\mu r}\) and then take the limit as \(\mu \to 0\).
        Writing \(\sin(kr)\) in terms of exponentials the integral isn't too difficult and we find that
        \begin{equation}
            \int_{0}^{\infty} rV(r)\sin(kr) \dd{r} = -\frac{e^2}{4\pi\textcolor{gray}{\varepsilon_0}k},
        \end{equation}
        and so the differential cross section is
        \begin{equation}
            \diff{\sigma}{\Omega} = \left[ \frac{1}{4\pi\textcolor{gray}{\varepsilon_0}} \right]^2 \frac{e^2}{16E^2} \frac{1}{\sin^4(\vartheta/2)}
        \end{equation}
        where \(E = p^2/(2m)\).
        This is the quantum mechanical \defineindex{Rutherford cross section}, which just happens to be equal to the classical Rutherford cross section.
        
        \begin{rmk}
            For details on the integral see the principles of quantum mechanics notes.
            For details on the classical derivation of the Rutherford cross section see the dynamics part of the dynamics and vector calculus course, or the relativity part of the relativity, nuclear, and particle physics course.
        \end{rmk}
    \end{exm}
    
    \chapter{Colliding Beams}
    We now consider two particles, of masses \(m_1\) and \(m_2\), and positions \(\vv{r_1}\) and \(\vv{r_2}\) respectively, which interact through a mutual potential, depending only on their relative position, \(\vv{r_1} - \vv{r_2}\).
    The Lagrangian is then
    \begin{equation}
        \lagrangian = \frac{1}{2}m\dot{r}_1^2 + \frac{1}{2}m\dot{r}_2^2 - V(\vv{r_1} - \vv{r_2}).
    \end{equation}

    We can write this in terms of the centre of mass motion, where the centre of mass is
    \begin{equation}
        \vv{R} = \frac{m_1\vv{r_1} + m_2\vv{r_2}}{m_1 + m_2},
    \end{equation}
    the total mass, \(M = m_1 + m_2\), the relative motion position,
    \begin{equation}
        \vv{r} = \vv{r_1} - \vv{r_2},
    \end{equation}
    and the reduced mass,
    \begin{equation}
        \mu = \frac{m_1m_2}{m_1 + m_2}.
    \end{equation}
    
    We then find that
    \begin{equation}
        \lagrangian = \frac{1}{2}MR^2 + \frac{1}{2}\mu r^2 - V(\vv{r}).
    \end{equation}
    If we work in the centre of mass frame, in which \(\vv{R} = \vv{0}\), the first term vanishes and the Lagrangian is simply
    \begin{equation}
        \lagrangian = \frac{1}{2}\mu r^2 - V(\vv{r}).
    \end{equation}
    This is simply the Lagrangian for a single particle at \(\vv{r}\) of mass \(\mu\) in a potential \(V(\vv{r})\).
    
    The differential cross section is therefore the same as for a single particle:
    \begin{equation}
        \diff{\sigma}{\Omega} = \left[ \frac{\mu}{2\pi\hbar^2} \right]^{2} \abs{\tilde{V}(\vv{k})}^2 \eqqcolon \abs{f(\vartheta, \varphi)}^2.
    \end{equation}
    Here \(\vv{k} = (\vv{p_a} - \vv{p_b})/\hbar\) and this defines the \defineindex{scattering amplitude}, \(f(\vartheta, \varphi)\).
    For a central potential we have seen that \(f\) doesn't depend on \(\varphi\), just on \(\vartheta\).
    Considering the limit of \(m_2 \to \infty\) we recover the fixed target result, this is called the Born--Oppenheimer approximation.
    
    For non-identical particles of equal mass, \(m_1 = m_2 = m\), we have \(\mu = m/2\) and so
    \begin{equation}
        \diff{\sigma}{\Omega} = \left[ \frac{m}{4\pi\hbar^2} \right]^2 \abs{\tilde{V}(\vv{k})}^2.
    \end{equation}
    If the particles are instead identical then there are two indistinguishable particles, as shown in \cref{fig:identical particle scattering}.
    Classically we would expect that the differential cross section is given by the sum of the squares,
    \begin{equation}
        \diff{\sigma}{\Omega}[\mathrm{cl}] = \abs{f(\vartheta)}^2 + \abs{f(\pi - \vartheta)}^2.
    \end{equation}
    In quantum mechanics the differential cross section is the square of the sum,
    \begin{equation}
        \diff{\sigma}{\Omega} = \abs{f(\vartheta) + f(\pi - \vartheta)}^2.
    \end{equation}
    We then find that \(\vartheta\) and \(\pi - \vartheta\) are the same, we just swap between direct and exchange or vice versa, and so for the total cross section we only integrate over \(\vartheta \in [0, \pi/2]\).
    
    \begin{figure}
        \tikzsetnextfilename{identical-particle-scattering}
        \begin{tikzpicture}
            \node[right] at (-2, 2) {Direct};
            \draw[very thick, ->] (-2, 0) -- (-0.5, 0) .. controls (0, 0) .. (45:2);
            \draw[very thick, ->] (2, 0) -- (0.5, 0) .. controls (0, 0) .. (-135:2);
            \coordinate (A) at (2, 0);
            \coordinate (B) at (0, 0);
            \coordinate (C) at (45:2);
            \path pic [draw, "\(\vartheta\)", angle radius = 0.7cm, angle eccentricity = 0.75] {angle};
            \begin{scope}[xshift=5cm]
                \node[right] at (-2, 2) {Exchange};
                \draw[very thick, ->] (-2, 0) -- (-0.5, 0) .. controls (0, 0) .. (-135:2);
                \draw[very thick, ->] (2, 0) -- (0.5, 0) .. controls (0, 0) .. (45:2);
                \coordinate (C) at (2, 0);
                \coordinate (B) at (0, 0);
                \coordinate (A) at (-135:2);
                \path pic [draw, "\(\pi - \vartheta\)", angle eccentricity = 1.3] {angle};
            \end{scope}
        \end{tikzpicture}
        \caption{Two identical particles can scatter in two indistinguishable ways.}
        \label{fig:identical particle scattering}
    \end{figure}
    
    % TODO: fill out details of calculation
    \begin{exm}{Coulomb Potential}{}
        Considering the Coulomb potential again the cross section for identical particles is
        \begin{equation}
            \diff{\sigma}{\Omega} = \left[ \frac{1}{4\pi\textcolor{gray}{\varepsilon_0}} \right]^2 \frac{e^4}{16E^2} \left[ \frac{1}{\sin^2(\vartheta/2)} + \frac{1}{\cos^2(\vartheta/2)} \right]^2 = \left[ \frac{1}{4\pi\textcolor{gray}{\varepsilon_0}} \right]^2\frac{e^4}{E^2}\frac{1}{\sin^4\vartheta}.
        \end{equation}
        Here \(E = p^2/(2\mu)\).
        For identical \enquote{spinless fermions}, which is to say identical fermions ignoring the effect of spin, the amplitudes have a relative minus sign and so we instad find that
        \begin{equation}
            \diff{\sigma}{\Omega} = \left[ \frac{1}{4\pi\textcolor{gray}{\varepsilon_0}} \right]^2 \frac{e^4}{E^2} \frac{\cos^2\vartheta}{\sin^4\vartheta}.
        \end{equation}
        This vanishes at \(\vartheta = \pi/2\), on the other hand the first result, for bosons, has a minimum at the positive value
        \begin{equation}
            \left[ \frac{1}{4\pi\textcolor{gray}{\varepsilon_0}} \right]\frac{e^4}{E^2}.
        \end{equation}
        We can therefore use Coulomb scattering to distinguish between bosons and fermions, as long as we can do so in such a way that spin effects are negligible, other than determining what happens upon exchange of identical particles.
    \end{exm}
    
    \chapter{Perturbation Theory in the Operator Formalism}
    \section{The Dirac Picture}
    In the Heisenberg picture the transition amplitude is
    \begin{equation}
        \braket{x_b, t_b}{x_a, t_a} = \bra{x_b} \operator{U}(t_b, t_a) \ket{x_a}.
    \end{equation}
    We now consider what happens if we introduce a perturbation to the Lagrangian that changes the time evolution operator from the unperturbed \(\operator{U}_0(t_b, t_a)\) to the perturbed \(\operator{U}(t_b, t_a)\).
    We then have
    \begin{multline}
        \int_{x_a}^{x_b} \!\! \DL{x} \, V(x(t_1), t_1) \dotsm V(x(t_n), t_n) \e^{iS_0/\hbar} \\
        = \tensor[_0]{\bra{x_b, t_b}}{} \timeorder(V(\operator{x}_0(t_1), t_1) \dotsm V(\operator{x}_0(t_n), t_n))\ket{x_a, t_a}_0.
    \end{multline}
    Here the states \(\ket{x, t}_0\), \(\tensor[_0]{\bra{x, t}}{}\), and the operators, \(\operator{x}_0(t)\), are in the Heisenberg picture with respect to the unperturbed action, \(S_0\), rather than the perturbed action, \(S\).
    
    The operator \(\operator{x}_0(t)\) is related to the the operator \(\operator{x}\) in the Schr\"odinger picture by the relation
    \begin{equation}
        \operator{x}_0(t) = \operator{U}_0^\hermit(t, t_0)\operator{x}\operator{U}_0(t, t_0).
    \end{equation}
    
    The unperturbed time evolution operator evolves according to the following differential equation with the unperturbed Hamiltonian, \(\operator{\hamiltonian}_0\):
    \begin{equation}
        i\hbar\diffp{}{t}\operator{U}_0 = \operator{\hamiltonian}_0\operator{U}_0.
    \end{equation}
    Evolving with the unperturbed action is called the \defineindex{Dirac picture}, also known as the \define{interaction picture}\index{interaction picture|see{Dirac picture}}.
    We denote quantities in this picture with subscript \(0\), or sometimes subscript \(\mathrm{D}\).
    The Dirac picture can be thought of as somewhere between the Schr\"odinger picture and Heisenberg picture, with operators evolving in time due to the unperturbed Hamiltonian and states evolving in time due to the perturbation.
    
    All other operators in the Dirac picture then evolve with \(\operator{U}_0\):
    \begin{equation}
        V(\operator{x}_0(t), t) = \operator{U}_0^\hermit (t, t_0) V(\operator{x}, t)\operator{U}_0(t, t_0).
    \end{equation}
    
    \section{Dyson Series}
    From \cref{eqn:series for perturbed transition amplitude,eqn:transition amplitude 2} we have
    \begin{align}
        \braket{x_b, t_b}{x_a, t_a} &= \tensor[_0]{\braket{x_a, t_b}{x_a, t_a}}{_0} + \sum_{n=1}^{\infty} \frac{1}{n!} \left( -\frac{i}{\hbar} \right)^n \int_{t_a}^{t_b} \!\! \dl{t_1} \dotsm \int_{t_a}^{t_b} \!\! \dl{t_n}\notag\\
        &\qquad\times \tensor[_0]{\bra{x_b, t_b}}{} \timeorder(V(\operator{x}_)(t_1), t_1) \dotsm V(\operator{x}_0(t_n), t_n) \ket{x_a, t_a}_0\\
        &= \tensor[_0]{\braket{x_b, t_b}{x_a, t_a}}{_0} + \sum_{n=1}^{\infty} \left( -\frac{i}{\hbar} \right)^{n} \int_{t_a}^{t_b} \!\! \dl{t_n} \int_{t_a}^{t_n} \!\! \dl{t_{n-1}} \dotsm \int_{t_a}^{t_2} \!\! \dl{t_1} \notag\\
        &\qquad\times \tensor[_0]{\bra{x_b, t_b}}{} V(\operator{x}_0(t_n), t_n) \dotsm V(\operator{x}_0(t_1), t_1)\ket{x_a, t_a}_0.
    \end{align}
    Here we have used the identity given in \cref{eqn:an identity}.
    
    The time dependence is governed by the unperturbed action, \(S_0\), and so in the Dirac picture the states are related to the Schr\"odinger picture states by
    \begin{equation}
        \ket{x_a, t_a}_0 = \operator{U}_0^\hermit(t_a, t_0)\ket{x_a}, \qqand \tensor[_0]{\bra{x_b, t_b}}{} = \bra{x_b}\operator{U}_0(t_b, t_0).
    \end{equation}
    Using identities for time evolution operators we have
    \begin{equation}
        \operator{U}_0(t_k, t_0)\operator{U}_0^\hermit(t_{k-1}, t_0) = \operator{U}_0(t_k, t_0)\operator{U}_0(t_0, t_{k-1}) = \operator{U}_0(t_k, t_{k-1}).
    \end{equation}
    Using this repeatedly gives
    \begin{multline}
        \bra{x_b} \operator{U}(t_b, t_a)\ket{x_a} = \bra{x_b} \operator{U}_0(t_b, t_a) \ket{x_a} + \sum_{n=1}^{\infty} \left( -\frac{i}{\hbar} \right)^n \int_{t_a}^{t_b} \!\! \dl{t_n} \dotsm \int_{t_a}^{t_2} \!\! \dl{t_1}\notag\\
        \times \bra{x_a} \operator{U}_0(t_b, t_n) V(\operator{x}, t_n) \operator{U}_0(t_n, t_{n-1}) V(\operator{x}, t_{n-1}) \dotsm V(\operator{x}, t_1)\operator{U}_0(t_1, t_a) \ket{x_a}.
    \end{multline}
    This holds for all states \(\ket{x_a}\) and \(\ket{x_b}\) and so we can derive an equation for the operator \(\operator{U}\):
    \begin{equation}
        \operator{U}(t_b, t_a) = \operator{U}_0(t_b, t_a) + \sum_{n=1}^{\infty} \left( -\frac{i}{\hbar} \right)^{n} \int_{t_a}^{t_b} \!\! \dl{t_n} \dotsm \int_{t_a}^{t_2} \!\! \dl{t_2} \operator{U}_0(t_b, t_n) V(\operator{x}, t_n)\operator{U}_0(t_n, t_{n-1}) \dotsm V(\operator{x}, t_1)\operator{U}_0(t_1, t_a).
    \end{equation}
    This is called the \defineindex{Dyson series} for \(\operator{U}\).
    
    This can be written more neatly as
    \begin{equation}\label{eqn:perturbed time evoloution operator equation}
        \operator{U}(t_b, t_a) = \operator{U}_0(t_b, t_a) - \frac{i}{\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \, \operator{U}_0(t_b, t) V(\operator{x}, t)\operator{U}(t, t_a).
    \end{equation}
    This is done in an analogous way to \cref{eqn:transition amplitude}.
    It is also possible to derive this equation, and the Dyson series, from the Schr\"odinger equation directly.
    Notice that \(\operator{x}\) is time independent here as we consider it in the Schr\"odinger picture.
    
    \section{Time Dependent Transitions}
    Suppose that the action, \(S = S_0 + S_1\), is such that \(S_0\) is an unperturbed time independent component and \(S_1\) is a time dependent perturbation.
    Often we choose \(S_0\) to be either the free particle in a box or the harmonic oscillator.
    Both of these have a discrete spectrum of energy eigenvalues, \(E_n\), which correspond to bound states, \(\ket{n}\), such that
    \begin{equation}
        \operator{\hamiltonian}_0\ket{n} = E_n\ket{n}
    \end{equation}
    or equivalently
    \begin{equation}
        \operator{\hamiltonian}_0\ket{n, t} = E_n\ket{n, t}.
    \end{equation}
    It is advantageous to work in this energy eigenbasis rather than the position basis for the time being.
    This is because the unperturbed transition amplitude is diagonal in the energy eigenbasis since
    \begin{equation}
        \braket{m, t}{n, t'} = \bra{m}\operator{U}_0(t, t')\ket{n} = \bra{m}\e^{-i(t - t')\operator{\hamiltonian}_0/\hbar}\ket{n} = \e^{-i(t-t')E_n/\hbar}\delta_{mn}.
    \end{equation}
    
    Taking \(\ket{a, t_a}\) and \(\ket{b, t_b}\) as the initial and final states using \cref{eqn:perturbed time evoloution operator equation} we see that to first order
    \begin{align}
        \braket{b, t_b}{a, t_a} &= \bra{b}\operator{U}(t_b, t_a)\ket{a}\\
        &= \bra{b} \operator{U}_0(t_b, t_a)\ket{a}\\
        &- \frac{i}{\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \, \bra{b} \operator{U}_0(t_b, t) V(\operator{x}, t)\operator{U}_0(t, t_a)\ket{a} + \dotsb \notag\\
        &= \bra{b} \operator{U}_0(t_b, t_a) \ket{a}\\
        &- \frac{i}{\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \sum_{m}\sum_{n} \bra{b} \operator{U}_0(t_b, t) \ket{m}\bra{m} V(\operator{x}, t) \ket{n}\bra{n} \operator{U}_0(t, t_a) \ket{a} + \dotsm \notag\\
        &= \e^{-i(t_b - t_a)E_a/\hbar}\delta_{ab} \label{eqn:first order transition amplitude}\\
        &- \frac{i}{\hbar} \e^{-i(E_bt_b - E_at_a)/\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \, \e^{it(E_b - E_a)/\hbar}V_{ba}(t) + \dotsb\notag
    \end{align}
    where we define \(V_{mn}(t)\) as the matrix elements of the potential:
    \begin{align}
        V_{mn}(t) &\coloneqq \bra{m} V(\operator{x}, t)\ket{n}\\
        &\hphantom{:}= \int_{-\infty}^{\infty} \bra{m}V(\operator{x}, t)\ket{x}\braket{x}{n} \dd{x}\\
        &=\hphantom{:}= \int_{-\infty}^{\infty} u_m^*(x)V(x, t)u_n(x) \dd{x}.
    \end{align}

    If we continued to second order in the same way by inserting the completeness relation and using \cref{eqn:an identity} then we would get the additional term
    \begin{equation*}
        \left( -\frac{i}{\hbar} \right)^{2} \sum_n \int_{t_a}^{t_b} \!\! \dl{t_2} \int_{t_a}^{t_2} \!\! \dl{t_1} \e^{-i(t_b - t_2)E_b/\hbar} V_{bn}(t_2) \e^{-i(t_2 - t_1)E_n/\hbar} V_{na}(t_1) \e^{-i(t_1 - t_a)E_a/\hbar}.
    \end{equation*}
    Higher order terms can be found in a similar way and have a similar form.
    
    This can be represented diagrammatically:
    \begin{multline}
        \tikzsetnextfilename{transition-amplitude-1st-order}
        \begin{tikzpicture}[baseline = (current bounding box)]
            \begin{feynman}
                \vertex (in) {\(t_a\)};
                \vertex (out) at (2, 0) {\(t_b\)};
                \vertex (bend) at (1, 1);
                \vertex (V1) at (1, 2) {\(V_{ba}(t_1)\)};
                \diagram {
                    (in) -- [edge label=\(a\)] (bend) -- [edge label=\(b\)] (out);
                    (bend) -- [boson] (V1);
                };
            \end{feynman}
        \end{tikzpicture}
        + \sum_n
        \tikzsetnextfilename{transition-amplitude-2nd-order}
        \begin{tikzpicture}[baseline = 1cm]
            \begin{feynman}
                \vertex (in) {\(t_a\)};
                \vertex (out) at (3, 1) {\(t_b\)};
                \vertex (bend1) at (1, 1);
                \vertex (bend2) at (2, 0);
                \vertex (V1) at (1, 2) {\(V_{na}(t_1)\)};
                \vertex (V2) at (2, -1) {\(V_{bn}(t_2)\)};
                \diagram {
                    (in) -- [edge label=\(a\)] (bend1) -- [edge label=\(n\)] (bend2) -- [edge label'=\(b\)] (out);
                    (bend1) -- [boson] (V1);
                    (bend2) -- [boson] (V2);
                };
            \end{feynman}
        \end{tikzpicture}
        \\
        + \sum_n\sum_m
        \tikzsetnextfilename{transition-amplitude-3rd-order}
        \begin{tikzpicture}[baseline = 1cm]
            \begin{feynman}
                \vertex (in) {\(t_a\)};
                \vertex (out) at (4, 0) {\(t_b\)};
                \vertex (bend1) at (1, 1);
                \vertex (bend2) at (2, 0);
                \vertex (bend3) at (3, 1);
                \vertex (V1) at (1, 2) {\(V_{na}(t_1)\)};
                \vertex (V2) at (2, -1) {\(V_{nm}(t_2)\)};
                \vertex (V3) at (3, 2) {\(V_{bm}(t_3)\)};
                \diagram {
                    (in) -- [edge label=\(a\)] (bend1) -- [edge label'=\(n\)] (bend2) -- [edge label'=\(m\)] (bend3) -- [edge label=\(b\)] (out);
                    (bend1) -- [boson] (V1);
                    (bend2) -- [boson] (V2);
                    (bend3) -- [boson] (V3);
                };
            \end{feynman}
        \end{tikzpicture}
        + \dotsb
    \end{multline}
    We sum over the intermediate virtual states, labelled \(m\) and \(n\), and the interaction times, at which the potential is inserted, are ordered such that \(t_i < t_{i+1}\) for all \(i\) and we implicitly integrate over all \(t_i\).
    
    Notice that unless \(a = b\) the first term in \cref{eqn:first order transition amplitude} vanishes due to the presence of the \(\delta_{ab}\) term.
    In this case the transition probability becomes
    \begin{equation}
        P(a \to b) = \abs{\braket{b, t_b}{a, t_a}}^2 = \frac{1}{\hbar^2} \abs*{\int_{t_a}^{t_b} \e^{i\omega_{ba}t}V_{ba}(t)\dd{t} + \order(V^2)}^2
    \end{equation}
    where we define the \defineindex{transition frequency}, \(\omega_{ba} = (E_b - E_a)/\hbar\).
    Interference between the higher order terms means that the next correction is \(\order(V^3)\), not \(\order(V^4) = \order(V^2)^2\) as one may expect initially.
    
    Recognising the integral as a Fourier transform we define
    \begin{equation}
        \tilde{V}_{ba} \coloneqq \int_{t_a}^{t_b} \e^{i\omega_{ba}t}V_{ba}(t) \dd{t}
    \end{equation}
    which allows us to write
    \begin{equation}
        P(a \to b) = \frac{1}{\hbar^2}\abs{\tilde{V}_{ba}}^2.
    \end{equation}
    
    \section{Fermi's Golden Rule}
    For a time independent perturbation we can do the integrals over \(t\), which are simply integrals of \(\e^{i\omega_{ab}t}\), and we find that
    \begin{equation}
        \braket{b, t_b}{a, t_a} = \e^{iE_aT/\hbar}\delta_{ab} + \frac{\e^{-iE_bT/\hbar} - \e^{-iE_aT/\hbar}}{E_b - E_a}V_{ba} + \order(V^2),
    \end{equation}
    where \(T \coloneqq t_b - t_a\).
    
    To first order therefore the transition probability for a time independent case with \(a \ne b\) is
    \begin{align}
        P(a \to b) &= \frac{1}{\hbar^2\omega_{ab}^2}\abs*{\e^{-iE_aT/\hbar}(\e^{-iT\omega_{ba}} - 1)}^2\abs{V_{ba}}^2\\
        &= \frac{f(T, \omega_{ba})}{\hbar^2}\abs{V_{ba}}^2.
    \end{align}
    Here we define the function \(f\) to be
    \begin{equation}
        f(t, \omega) \coloneqq \frac{\sin^2(\omega t/2)}{(\omega t/2)^2}t^2.
    \end{equation}
    For a given value of \(t\) this is \(\sinc^2\).
    The shape of the plot is a large peak centred at \(\omega = 0\) with height \(t^2\).
    There are zeros at \(\omega = 2n\pi/t\) for \(n \in \integers\).
    The width of the peak is then approximately \(2\pi/t\) and the function decays to zero as \(1/\omega^2\) with symmetrical oscillations.
    See \cref{fig:sinc squared}.
    
    \begin{figure}
        \tikzsetnextfilename{sinc-squared}
        \begin{tikzpicture}
            \pgfmathdeclarefunction{sinc}{1}{%
                \pgfmathparse{abs(#1)<0.01 ? int(1) : int(0)}%
                \ifnum\pgfmathresult>0 \pgfmathparse{1}\else\pgfmathparse{sin(#1 r)/#1}\fi%
            }
            \draw[very thick, ->] (-5, 0) -- (5, 0) node[below] {\(\omega\)};
            \draw[very thick, ->] (0, 0) -- (0, 5) node[above] {\(f(t, \omega)\)};
            \draw[highlight, very thick, domain=-5:4.9, samples=300] plot (\x, {4*sinc(2*\x)^2});
            \node[left] at (0, 4) {\(t^2\)};
        \end{tikzpicture}
        \caption{The function \(f(t, \omega) = \sin^2(\omega t/2)t^2/(\omega t/2)^2\).}
        \label{fig:sinc squared}
    \end{figure}
    
    Looking at the shape of this function we see that the only significant transition probability is to states whose energy places them in the central peak, which gives a range of possible energies \(\delta E \approx 2\pi\hbar/t\) about the initial energy, \(E_a\).
    
    Consider some arbitrary (sufficiently nice) test function \(h\), for large \(t\) we then have
    \begin{align}
        \int_{-\infty}^{\infty} f(t, \omega) h(\omega) \dd{\omega} &= 2t\int_{-\infty}^{\infty} \frac{\sin^2 x}{x^2} h(2x/t)\\
        &= 2t\int_{-\infty}^{\infty} \frac{\sin^2x}{x^2}[h(0) + \order(1/t)] \dd{t}\\
        &= 2\pi th(0) + \order(1).
    \end{align}
    Here we have used the standard result
    \begin{equation}
        \int_{-\infty}^{\infty} \frac{\sin^2 x}{x^2} \dd{x} = \pi,
    \end{equation}
    which is relatively simple to show with a contour integral\footnote{see methods of theoretical physics complex analysis notes.}.
    Alternatively we could simply have identified one of the standard representations of the delta distribution\footnote{see methods of mathematical physics notes.} as the limit as \(n \to \infty\) of \(\sin^2(nx)/(n\pi x^2)\).
    
    For large \(t\) the function \(f\) acts, to first order, as a delta function, such that
    \begin{equation}
        \lim_{t \to \infty} f(t, \omega) = 2\pi t \delta(\omega).
    \end{equation}
    
    Another case of interest is transition not to a single final state but to a range, \(\mathcal{R}\), of final states centred on some energy \(E_b\).
    We then find
    \begin{equation}
        P(a \to \mathcal{R}) = \int_{\mathcal{R}} P(a \to E) \rho(E) \dd{E}
    \end{equation}
    where \(\rho\) is the density of final states, which means that \(\rho(E)\) is the number of states with energy in the interval \([E, E + \dl{E}]\).
    
    For a sufficiently small interval, \(\mathcal{R}\), we can take \(\rho(E)\) and \(V_{ba}\) to be roughly constant in the region of integration.
    Since \(f(t, \omega)\) behaves like a delta distribution for large \(t\) we can then extend the limits of the integral without changing the value and so we have
    \begin{align}
        P(a \to \mathcal{R}) &= \frac{1}{\hbar^2} \abs{V_{ba}}^2 \rho(E_b) \int_{-\infty}^{\infty} 2\pi T[\delta(E/\hbar) + \order(1/T)] \dd{E}\\
        &= \frac{2\pi}{\hbar}\abs{V_{ba}}^2 \rho(E_b)T/
    \end{align}
    The transition rate is therefore
    \begin{equation}
        R = \frac{2\pi}{\hbar}\abs{V_{ba}}^2\rho(E_b).
    \end{equation}
    This is \defineindex{Fermi's golden rule}.
    
    
    \chapter{Feynman's Perturbation Theory}
    \section{Classical Mechanics}
    So far we have considered perturbation theory in quantum mechanics.
    The unperturbed amplitude is a Green's function for the Schr\"odinger equation, describing the quantum mechanical propagation.
    Perturbation theory also plays a role in classical mechanics for describing the effect that a small disturbance has on an otherwise stable system.
    
    For example, consider the forced anharmonic oscillator which has the Lagrangian
    \begin{equation}
        \lagrangian = \lagrangian_0 + \lagrangian_1 = \frac{m}{2}\dot{x}^2 - \frac{m}{2}\omega^2x^2 + Jx - \frac{\lambda}{4}x^4.
    \end{equation}
    Here we split this into two parts, \(\lagrangian_0\), which is the first three terms and has a contribution of \(S_0[x, J]\) to the action.
    \(J\) is the forcing term and is an arbitrary function of time, so \(J = J(t)\).
    This is the unperturbed case and is a forced harmonic oscillator, the solution to which is known.
    The final term of the Lagrangian is \(\lagrangian_1 = -\lambda x^4/4\), this is the anharmonic term, and is what we treat as a perturbation assuming that \(\lambda\) is relatively small.
    The factor of \(1/4\) in this term is for later convenience and it isn't uncommon to include different factors at this point, such as \(1/4!\).
    The classical equation of motion is
    \begin{equation}
        \left( \diffp[2]{}{t} + \omega^2 \right)\bar{x} = \frac{J}{m} - \frac{\lambda}{m}\bar{x}.
    \end{equation}
    Which follows simply from applying the Euler--Lagrange equations.
    
    \subsection{Homogeneous Equation}
    The homogeneous equation,
    \begin{equation}
        \left( \diffp[2]{}{t} + \omega^2 \right)\bar{x} = 0,
    \end{equation}
    is simply an undriven harmonic oscillator.
    Let \(\bar{x}_0(t)\) be the solution to this with the boundary conditions \(\bar{x}_0(t_a) = x_a\) and \(\bar{x}_0(t_b) = x_b\).
    
    In order to solve the full inhomogeneous equation we need to construct a Green's function, \(\Delta(t, t')\), such that
    \begin{equation}
        \left( \diffp[2]{}{t} + \omega^2 \right)\Delta(t, t') = -\delta(t - t').
    \end{equation}
    Notice that there is an additional minus sign here by convention, the standard Green's function is defined such that \(\mathfrak{L}G = \delta\), for some linear operator, \(\mathfrak{L}\), whereas we have chosen \(\mathfrak{L}\Delta = -\delta\).
    The boundary condition that this Green's function must satisfy is \(\Delta(t_b, t') = \Delta(t_a, t') = 0\).
    We can show without much difficulty but a lot of algebra that the following satisfies these requirements:
    \begin{multline}\label{eqn:driven SHO Greens function}
        \Delta(t, t') = \frac{1}{\omega\sin(\omega T)} [\theta(t - t')\sin[\omega(t_b - t)]\sin[\omega(t' - t_a)]\\
        + \theta(t' - t)\sin[\omega(t - t_a)\sin[\omega(t_b - t')]]].
    \end{multline}
    With \(T = t_b - t_a\).
    
    Notice that this is \emph{not} a causal Green's function.
    This is due to the choice of boundary conditions which mena that we have both a retarded (\(t' < t\)) and advanced (\(t' > t\)) part.
    We call \(\Delta\) the \defineindex{Feynman propagator}, or \define{Feynman Green's function}\index{Feynman Green's function|see{Feynman propagator}}.
    Notice that it is symmetric in its arguments so \(\Delta(t, t') = \Delta(t', t)\).
    
    \subsection{Full Solution}
    The full solution with the required boundary conditions is then
    \begin{equation}
        \bar{x}(t)= \bar{x}_0(t) + \frac{1}{m} \int_{t_a}^{t_b} \Delta(t, t')(-J(t') + \lambda\bar{x}(t')^3) \dd{t'}.
    \end{equation}
    The first term being the solution to the standard simple harmonic oscillator with the required boundary conditions and the integral the solution to the inhomogeneous driven anharmonic oscillator with vanishing boundary conditions.
    
    Notice that the integral giving the solution for \(\bar{x}\) has \(\bar{x}\) in the integrand.
    We therefore can solve for \(\bar{x}\) iteratively:
    \begin{multline}
        \bar{x}(t) = \bar{x}_0(t) - \frac{1}{m} \int_{t_a}^{t_b} \Delta(t, t') J(t') \dd{t'}\\
        + \frac{\lambda}{m} \int_{t_a}^{t_b} \Delta(t, t')\left[ \bar{x}_0(t') - \frac{1}{m}\int_{t_a}^{t_b} \Delta(t', t'')J(t'') \dd{t''} \right]^3 \dd{t'} + \dotsb.
    \end{multline}
    The problem is simplified somewhat if we take \(x_a = x_b = 0\), and so \(\bar{x}_0(t) = 0\).
    
    We can represent this series solution diagramatically with a line corresponding to a factor of \(\Delta(t, t')/m\), a cross for each \(-\int\dl{t} \, J(t)\), and a vertex for \(\lambda\int\dl{t}\).
    Diagrams at \(\order(\lambda^2)\) or higher also include some combinatorial factor from the expansion which must be calculated separately.
    The first few terms of the sum are then, up to the combinatorial factors and second order in \(\lambda\)
    \begin{multline}
        \tikzsetnextfilename{anharmonic-oscillator-diagram-1}
        \begin{tikzpicture}[baseline = -0.08cm]
            \begin{feynman}
                \vertex (t) {\(t\)};
                \vertex (n1) at (1, 0);
                \diagram {
                    (t) -- (n1);
                };
                \node[above] at (n1) {J};
                \node at (n1) {\(\times\)};
            \end{feynman}
        \end{tikzpicture}
        +
        \tikzsetnextfilename{anharmonic-oscillator-diagram-2}
        \begin{tikzpicture}[baseline = -0.08cm]
            \begin{feynman}
                \vertex (t) {\(t\)};
                \vertex (n1) at (1, 0);
                \vertex (n2) at ($(n1) + (1, 0.5)$);
                \vertex (n3) at ($(n1) + (1, 0)$);
                \vertex (n4) at ($(n1) + (1, -0.5)$);
                \diagram {
                    (t) -- (n1) -- (n2);
                    (n1) -- (n3);
                    (n1) -- (n4);
                };
                \foreach \n in {n2, n3, n4} \node at (\n) {\(\times\)};
                \node[above] at (n1) {\(t'\)};
            \end{feynman}
        \end{tikzpicture}
        +
        \tikzsetnextfilename{anharmonic-oscillator-diagram-3}
        \begin{tikzpicture}[baseline = -0.08cm]
            \begin{feynman}
                \vertex (t) {\(t\)};
                \vertex (n1) at (1, 0);
                \vertex (n2) at ($(n1) + (1, 0.5)$);
                \vertex (n3) at ($(n1) + (1, 0)$);
                \vertex (n4) at ($(n1) + (1, -0.5)$);
                \vertex (n5) at ($(n2) + (1, 0.5)$);
                \vertex (n6) at ($(n2) + (1, 0)$);
                \vertex (n7) at ($(n2) + (1, -0.5)$);
                \diagram {
                    (t) -- (n1) -- (n2);
                    (n1) -- (n3);
                    (n1) -- (n4);
                    (n2) -- (n5);
                    (n2) -- (n6);
                    (n2) -- (n7);
                };
                \foreach \n in {n3, n4, n5, n6, n7} \node at (\n) {\(\times\)};
                \node[above] at (n1) {\(t'\)};
                \node[above] at (n2) {\(t''\)};
            \end{feynman}
        \end{tikzpicture}
        \\
        +
        \tikzsetnextfilename{anharmonic-oscillator-diagram-4}
        \begin{tikzpicture}[baseline = -0.08cm]
            \begin{feynman}
                \vertex (t) {\(t\)};
                \vertex (n1) at (1, 0);
                \vertex (n2) at ($(n1) + (1, 0.5)$);
                \vertex (n3) at ($(n1) + (1, 0)$);
                \vertex (n4) at ($(n1) + (1, -0.5)$);
                \vertex (n5) at ($(n2) + (1, 0.5)$);
                \vertex (n6) at ($(n2) + (1, 0)$);
                \vertex (n7) at ($(n2) + (1, -0.5)$);
                \vertex (n8) at ($(n5) + (1, 0.5)$);
                \vertex (n9) at ($(n5) + (1, 0)$);
                \vertex (n10) at ($(n5) + (1, -0.5)$);
                \diagram {
                    (t) -- (n1) -- (n2);
                    (n1) -- (n3);
                    (n1) -- (n4);
                    (n2) -- (n5);
                    (n2) -- (n6);
                    (n2) -- (n7);
                    (n5) -- (n8);
                    (n5) -- (n9);
                    (n5) -- (n10);
                };
                \foreach \n in {n3, n4, n6, n7, n8, n9, n10} \node at (\n) {\(\times\)};
                \node[above] at (n1) {\(t'\)};
                \node[above] at (n2) {\(t''\)};
                \node[above] at (n5) {\(t'''\)};
            \end{feynman}
        \end{tikzpicture}
        +
        \tikzsetnextfilename{anharmonic-oscillator-diagram-5}
        \begin{tikzpicture}[baseline = -0.08cm]
            \begin{feynman}
                \vertex (t) {\(t\)};
                \vertex (n1) at (1, 0);
                \vertex (n2) at ($(n1) + (1, 0.5)$);
                \vertex (n3) at ($(n1) + (1, 0)$);
                \vertex (n4) at ($(n1) + (1, -0.5)$);
                \vertex (n5) at ($(n2) + (1, 0.3)$);
                \vertex (n6) at ($(n2) + (1, 0)$);
                \vertex (n7) at ($(n2) + (1, -0.3)$);
                \vertex (n8) at ($(n4) + (1, 0.3)$);
                \vertex (n9) at ($(n4) + (1, 0)$);
                \vertex (n10) at ($(n4) + (1, -0.3)$);
                \diagram {
                    (t) -- (n1) -- (n2);
                    (n1) -- (n3);
                    (n1) -- (n4);
                    (n2) -- (n5);
                    (n2) -- (n6);
                    (n2) -- (n7);
                    (n4) -- (n8);
                    (n4) -- (n9);
                    (n4) -- (n10);
                };
                \foreach \n in {n3, n5, n6, n7, n8, n9, n10} \node at (\n) {\(\times\)};
                \node[above] at (n1) {\(t'\)};
                \node[above] at (n2) {\(t''\)};
            \end{feynman}
        \end{tikzpicture}
        + \dotsb.
    \end{multline}
    
    We can interpret the different components as
    \begin{itemize}
        \item \(\Delta(t, t')\) propagates us from \(t'\) to \(t\).
        \item \(J(t)\) acts as a source (or sink) for motion.
        \item \(\lambda x^4/4\) is an interaction.
    \end{itemize}
    We then integrate over all intermediate times, time ordering is accounted for by using either the retarded or advanced part of \(\Delta(t, t')\), and we sum over all possible trees.
    This is very similar to the quantum mechanical perturbation result, the only difference being that there are no loops.
    
    \section{Quantum Mechanics}
    We now consider the same anharmonic oscillator but in quantum mechanics.
    The action is of the form \(S[x, J] = S_0[x, J] + S_1[x]\) where \(S_0\) is the action of the driven harmonic oscillator,
    \begin{equation}
        S_0[x, J] = \int_{t_a}^{t_b} \left[ \frac{m}{2}(\dot{x}^2 - \omega^2x^2) + Jx \right] \dd{t},
    \end{equation}
    and \(S_1\) is the anharmonic contribution,
    \begin{equation}
        S_1[x] = -\frac{\lambda}{4}\int_{t_a}^{t_b} x \dl{t}.
    \end{equation}
    We will start by considering the transition element
    \begin{equation}
        \bra{x_b, t_b} \timeorder(\operator{x}(t_1) \dotsm \operator{x}(t_m))\ket{x_a, t_a}_J = \int \!\! \DL{x} \, x(t_1) \dotsm x(t_m) \exp\left[ \frac{i}{\hbar}S[x, J] \right].
    \end{equation}
    In order to progress we will need functional derivatives.
    
    \subsection{Functional Derivatives}
    For discretely indexed \(x_i\) and \(J_i\) we have that
    \begin{equation}
        \diffp{}{J_i} \sum_j J_jx_j = x_i \implies \diffp*{J_j}{J_i} = \delta_{ij}.
    \end{equation}
    We can generalise this to continuously indexed \(J(t)\) and \(x(t)\) via
    \begin{equation}
        \diffd*{J(t')}{J(t)} = \delta(t - t').
    \end{equation}
    This is a \defineindex{functional derivative}.
    With an integral replacing the sum in the continuous case we get
    \begin{equation}\label{eqn:functional derivative delta distribution}
        \diffd{}{J(t)} \int_{t_a}^{t_b} J(t')x(t') \dd{t'} = \int_{t_a}^{t_b} \delta(t - t') \dd{t'} = x(t)
    \end{equation}
    assuming \(t \in (t_a, t_b)\), else we get \(0\).
    
    \subsection{Quantum Anharmonic Oscillator}
    Using functional derivatives we have
    \begin{align}
        &\bra{x_b, t_b}\timeorder(\operator{x}(t_1) \dotsm \operator{x}(t_m))\ket{x_a, t_a}_J\\
        \quad &= \int_{x_a}^{x_b} \!\! \DL{x} \, x(t_1) \dotsm x(t_m) \exp\left[ \frac{i}{\hbar}\left( S[x, 0] + \int_{t_a}^{t_b} J(t)x(t) \dd{t} \right) \right]\\
        \quad &= \int_{x_a}^{x_b} \!\! \DL{x} \, \left( \frac{\hbar}{i} \diffd{}{J(t_1)} \right) \dotsm \left( \frac{\hbar}{i} \diffd{}{J(t_m)} \right) \exp\left[ \frac{i}{\hbar} \left( S[x, 0] + \int_{t_a}^{t_b} J(t)x(t) \dd{t} \right) \right]\notag\\
        \quad &= \left( \frac{\hbar}{i} \right)^m \frac{\delta^m}{\delta J(t_1) \dotsm \delta J(t_m)} \braket{x_b, t_b}{x_a, t_a}_J.
    \end{align}
    Here we used functional derivatives and the relation in \cref{eqn:functional derivative delta distribution}, in particular we have used the chain rule, which works for functional derivatives the same as for regular derivatives, so that
    \begin{align}
        &\frac{\hbar}{i}\diffd{}{J(t')} \exp\left[ \frac{i}{\hbar} \int_{t_a}^{t_b} J(t)x(t) \dd{t} \right]\\
        \quad&= \frac{\hbar}{i}\left( \diffd{}{J(t')} \frac{i}{\hbar}\int_{t_a}^{t_b} J(t)x(t) \dd{t} \right) \exp\left[ \frac{i}{\hbar} \int_{t_a}^{t_b} J(t)x(t) \dd{t} \right]\\
        \quad&= x(t) \exp\left[ \frac{i}{\hbar} \int_{t_a}^{t_b} J(t)x(t) \dd{t} \right].
    \end{align}
    
    We have shown that the \(m\)-point transition element, that is
    \begin{equation}
        \bra{x_b, t_b} \timeorder(\operator{x}(t_1) \dotsm \operator{x}(t_m)) \ket{x_a, t_a}_J,
    \end{equation}
    can be written as a functional derivative of the exact transition amplitude, \(\braket{x_b, t_b}{x_a, t_a}_J\).

    So far all of this is valid for all (sufficiently nice) \(J\), including nonzero \(J\).
    If we set \(J = 0\) \emph{after} differentiation\footnote{it's important that it's after as we need the functional form in order to get as far as we have} then we get
    \begin{multline}
        \bra{x_b, t_b} \timeorder(\operator{x}(t_1) \dotsm \operator{x}(t_m)) \ket{x_a, t_a}\\
        = \left( \frac{\hbar}{i} \right)^m \frac{\delta^m}{\delta J(t_1) \dotsm \delta J(t_m)}\braket{x_b, t_b}{x_a, t_a}_J \bigg\vert_{J = 0},\label{eqn:m-point correlation}
    \end{multline}
    which will recover the result for \(J = 0\), the undriven harmonic oscillator.
    
    \subsection{Perturbation Expansion}
    To include the perturbation term we need to expand \(\e^{iS_1/\hbar}\) in \(\lambda\).
    This gives
    \begin{align}
        &\braket{x_b, t_b}{x_a, t_a}_J = \int_{x_a}^{x_b} \!\! \DL{x} \, \exp\left[ \frac{i}{\hbar}(S_0[x, J] + S_1[x]) \right]\\
        &= \sum_{n=0}^{\infty} \frac{1}{n!} \left( -\frac{i}{\hbar}\frac{\lambda}{4} \right)^{n} \int_{t_a}^{t_b} \!\! \dl{\tau_1} \dotsm \int_{t_a}^{t_b} \!\! \dl{\tau_n} \int_{x_a}^{x_b} \!\! \DL{x} \, x(\tau_1)^4 \dotsm x(\tau_n)^4\notag\\
        &\hspace{3.5cm}\times\exp\left[ \frac{i}{\hbar}\left( S_0[x, 0] + \int_{t_a}^{t_b} J(t)x(t) \dd{t} \right) \right].
    \end{align}
    Replacing factors of \(x(\tau_i)\) by functional derivatives with respect to \(J(\tau_i)\) we get
    \begin{multline*}
        \braket{x_b, t_b}{x_a, t_a}_J\\
        = \sum_{n=0}^{\infty} \frac{1}{n!} \left( -\frac{i}{\hbar}\frac{\lambda}{4} \right)^n \left[ \prod_{k=1}^{n} \int_{t_a}^{t_b} \!\! \dl{\tau_k} \left( \frac{\hbar}{i} \diffd{}{J(\tau_k)} \right)^4 \right] \int_{x_a}^{x_b} \!\! \DL{x}\ , \exp\left[ \frac{i}{\hbar}S_0[x, J] \right].
    \end{multline*}
    We can then write this as
    \begin{align*}
        \braket{x_b, t_b}{x_a, t_a}_J &= \exp\left[ -\frac{i}{\hbar}\frac{\lambda}{4} \int_{t_a}^{t_b} \dl{\tau} \left( \frac{\hbar}{i} \diffd{}{J(\tau)} \right)^4 \right] \int_{x_a}^{x_b} \!\! \DL{x} \, \exp\left[ \frac{i}{\hbar} S_0[x, J] \right]\\
        &= \exp\left[ -\frac{i}{\hbar}\frac{\lambda}{4} \int_{t_a}^{t_b} \dl{\tau} \left( \frac{\hbar}{i} \diffd{}{J(\tau)} \right)^4 \right]F_{\omega}(T) \exp\left[ \frac{i}{\hbar} S_0[\bar{x}, J] \right]
    \end{align*}
    where we recognise the path integral as the path integral for the forced harmonic oscillator, and include the solution found in \cref{sec:forced harmonic oscillator}.
    
    Combining this result with \cref{eqn:m-point correlation} we find the \(m\)-point transition amplitude to all orders in perturbation theory in terms of the transition amplitude for the forced harmonic oscillator:
    \begin{multline}\label{eqn:m-point correlation forced harmonic oscillator}
        \bra{x_b, t_b} \timeorder (\operator{x}(t_1) \dotsm \operator{x}(t_m)) \ket{x_a, t_a}_J\\
        \quad = \left( \frac{\hbar}{i} \right)^m \frac{\delta^m}{\delta J(t_1) \dotsm \delta J(t_m)} \exp\left[ -\frac{i}{\hbar}\frac{\lambda}{4} \int_{t_a}^{t_b} \!\! \dl{\tau} \left( \frac{\hbar}{i} \diffd{}{J(\tau)} \right)^4 \right]\\
        \times F_\omega(T) \exp\left[ \frac{i}{\hbar}S_0[\bar{x}, J] \right].
    \end{multline}
    
    \subsection{Further Evaluation}
    For simplicity we take homogeneous boundary conditions, \(x_a = x_b = 0\).
    The classical path then satisfies
    \begin{equation}
        \ddot{\bar{x}} + \omega^2\bar{x} = \frac{J}{m},
    \end{equation}
    and \(\bar{x}(t_a) = \bar{x}(t_b) = 0\).
    The solution to this is
    \begin{equation}
        \bar{x}(t) = -\frac{1}{m}\int_{t_a}^{t_b} \Delta(t, t')J(t')\dd{t'}
    \end{equation}
    where \(\Delta\) is the Green's function from \cref{eqn:driven SHO Greens function}, which is such that
    \begin{equation}
        \left( \diffp[2]{}{t} + \omega^2 \right)\Delta(t, t') = -\delta(t - t'), \quad\text{and}\quad \Delta(t_b, t') = \Delta(t_a, t') = 0.
    \end{equation}
    Integrating by parts for the \(\dot{\bar{x}}^2\) term we find that
    \begin{align}
        S_0[\bar{x}, J] &= \int_{t_a}^{t_b} \left[ \frac{m}{2}(\dot{\bar{x}}^2 - \omega^2\bar{x}^2) + J\bar{x}\right] \dd{t}\\
        &= \frac{m}{2}\underbrace{[ \ddot{\bar{x}}\bar{x} ]_{t_a}^{t_b}}_{=0} - \int_{t_a}^{t_b} \left[ \frac{m}{2}(\bar{x}\textcolor{highlightpurple}{\ddot{\bar{x}}} + \omega^2\bar{x}^2) - J\bar{x} \right] \dd{t}\\
        &= - \int_{t_a}^{t_b} \left[ \frac{m}{2}\left( \bar{x} \textcolor{highlightpurple}{\left( \frac{J}{m} + \omega^2\bar{x} \right)} - \omega^2\bar{x}^2 \right) - J\bar{x} \right] \dd{t}\\
        &= \frac{1}{2} \int_{t_a}^{t_b} J(t) \textcolor{highlight}{\bar{x}(t)} \dd{t}\\
        &= -\frac{1}{2m} \int_{t_a}^{t_b} \!\! \dl{t} \textcolor{highlight}{\int_{t_a}^{t_b} \!\! \dl{t'}} \, J(t) \textcolor{highlight}{\Delta(t, t')J(t')}.
    \end{align}
    
    Substituting this into \cref{eqn:m-point correlation forced harmonic oscillator} we get
    \begin{multline}\label{eqn:homogeneous m-point correlation function}
        \frac{1}{F_{\omega}(T)}\bra{0, t_b} \timeorder(\operator{x}(t_1) \dotsm \operator{x}(t_m)) \ket{0, t_a}_J\\
        \quad= \left( \frac{\hbar}{i} \right)^{m} \frac{\delta^m}{\delta J(t_1) \dotsm \delta J(t_m)} \exp\left[ -\frac{i}{\hbar} \frac{\lambda}{4} \int_{t_a}^{t_b} \!\! \dl{\tau} \left( \frac{\hbar}{i} \diffd{}{J(\tau)} \right)^4 \right]\\
        \times \exp\left[ -\frac{i}{2m\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \int_{t_a}^{t_b} \!\! \dl{t'} J(t)\Delta(t, t')J(t') \right].
    \end{multline}
    
    \section{Examples}
    \subsection{Harmonic Oscillator}
    For our first example we consider the driven harmonic oscillator, which means we set \(\lambda = 0\).
    \subsubsection{Two-Point Correlation Function}
    The two-point correlation function is
    \begin{equation}
        \bra{0, t_b} \timeorder(\operator{x}(t_1)\operator{x}(t_2))\ket{0, t_a}_{J=0},
    \end{equation}
    which we can compute using \cref{eqn:m-point correlation,eqn:homogeneous m-point correlation function}:
    \begin{align}
        &\frac{1}{F_{\omega}(T)}\bra{0, t_b} \timeorder(\operator{x}(t_1)\operator{x}(t_2))\ket{0, t_a}_{J=0}\\
        &= \left( \frac{\hbar}{i} \right)^2 \diffd{}{J(t_1), J(t_2)} \exp\left[ -\frac{i}{2m\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \int_{t_a}^{t_b} \!\! \dl{t'} \, J(t)\Delta(t, t')J(t') \right] \bigg\vert_{J=0}\\
        &= \left( \frac{\hbar}{i} \right)^{2} \diffd{}{J(t_1), J(t_2)} \left[ 1 - \frac{i}{2m\hbar}\int_{t_a}^{t_b} \!\! \dl{t} \int_{t_a}^{t_b} \!\! \dl{t'} \, J(t)\Delta(t, t')J(t') + \order(J^4) \right] \bigg\vert_{J = 0}\notag\\
        &= \frac{i\hbar}{2m} \diffd{}{J(t_1)} 2\int_{t_a}^{t_b} J(t)\Delta(t, t_2) \dd{t}\\
        &= \frac{i\hbar}{m} \Delta(t_1, t_2).
    \end{align}
    Note that the \(\order(J^4)\) become \(\order(J^2)\) after the derivatives and then vanish when we set \(J = 0\), so this is exact.
    The constant term vanishes when we take the derivatives.
    
    \subsubsection{Four-Point Correlation Function}
    The four-point correlation function can be calculated in a similar way:
    \begin{align}
        &\frac{1}{F_{\omega(T)}}\bra{0, t_b} \timeorder(\operator{x}(t_1)\operator{x}(t_2)\operator{x}(t_3)\operator{x}(t_4)) \ket{0, t_a}_{J=0} = \left( \frac{\hbar}{i} \right)^{4}\\
        &\diffd{}{J(t_1),J(t_2),J(t_3),J(t_4)} \exp\left[ -\frac{i}{2m\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \int_{t_a}^{t_b} \!\! \dl{t'} \, J(t)\Delta(t, t')J(t') \right] \bigg\vert_{J=0}\notag\\
        &= \left( \frac{\hbar}{i} \right)^{4} \diffd{}{J(t_1),J(t_2),J(t_3),J(t_4)} \frac{1}{2!}\left[ -\frac{i}{2m\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \int_{t_a}^{t_b} \!\! \dl{t'} \, J(t)\Delta(t, t')J(t') \right]^2\notag\\
        &= -\frac{\hbar^2}{m^2} [\Delta(t_1, t_2) \Delta(t_3, t_4) + \Delta(t_1, t_3)\Delta(t_2, t_4) + \Delta(t_1, t_4)\Delta(t_2, t_3)].
    \end{align}
    Note that we expand the exponential and drop all terms of order lower than 4, since these vanish in the derivative, and all terms of order greater than 4, since these vanish when we set \(J = 0\).
    
    \subsubsection{Diagrammatic Representation}
    We can also use a diagrammatic representation of this result where each time is represented by a point and each occurrence of \(i\hbar \Delta(t_i, t_j)/m\) is represented by a line joining the points representing \(t_i\) and \(t_j\).
    Using this the two-point correlation function is
    \begin{equation}
        \tikzsetnextfilename{two-point-correlation-harmonic-oscillator}
        \begin{tikzpicture}[baseline = (current bounding box)]
            \begin{feynman}
                \vertex (t1) {\(t_1\)};
                \vertex (t2) at (1, 0) {\(t_2\)};
                \diagram {
                    (t1) -- (t2);
                };
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    The four-point correlation function is
    \begin{equation}
        \tikzsetnextfilename{four-point-correlation-harmonic-osccilator-1}
        \begin{tikzpicture}[baseline = 0.4cm]
            \begin{feynman}
                \vertex (t1) {\(t_1\)};
                \vertex (t2) at (1, 0) {\(t_2\)};
                \vertex (t3) at (0, 1) {\(t_3\)};
                \vertex (t4) at (1, 1) {\(t_4\)};
                \diagram {
                    (t1) -- (t2);
                    (t3) -- (t4);
                };
            \end{feynman}
        \end{tikzpicture}
        +
        \tikzsetnextfilename{four-point-correlation-harmonic-osccilator-2}
        \begin{tikzpicture}[baseline = 0.4cm]
            \begin{feynman}
                \vertex (t1) {\(t_1\)};
                \vertex (t2) at (1, 0) {\(t_2\)};
                \vertex (t3) at (0, 1) {\(t_3\)};
                \vertex (t4) at (1, 1) {\(t_4\)};
                \diagram {
                    (t1) -- (t3);
                    (t2) -- (t4);
                };
            \end{feynman}
        \end{tikzpicture}
        +
        \tikzsetnextfilename{four-point-correlation-harmonic-osccilator-3}
        \begin{tikzpicture}[baseline = 0.4cm]
            \begin{feynman}
                \vertex (t1) {\(t_1\)};
                \vertex (t2) at (1, 0) {\(t_2\)};
                \vertex (t3) at (0, 1) {\(t_3\)};
                \vertex (t4) at (1, 1) {\(t_4\)};
                \diagram {
                    (t1) -- (t4);
                    (t2) -- [ultra thick, white] (t3);
                    (t2) -- (t3);
                };
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    
    \subsubsection{\(n\)-Point Correlation Function}
    As with the two and four-point correlation functions there is only one term in the expansion of the exponential which contributes to the final result.
    Notice also that the final result is symmetric with respect to swapping any two times.
    In general the \(n\) point correlation function is
    \begin{equation}
        \frac{1}{F_{\omega}(T)} \bra{0, t_b} \timeorder(\operator{x}(t_1) \dotsm \operator{x}(t_n)) \ket{0, t_a}_{J = 0} = \left( \frac{i\hbar}{m} \right)^{m/2} \sum_{\text{pairs}} \Delta(t_{i_1}, t_{i_2}) \dotsm \Delta(t_{i_{n-1}}, t_{i_n})
    \end{equation}
    for even \(n\) and zero for odd \(n\), since all terms in the exponential are even in order for \(J\).
    The sum is then over all \(\tensor[^n]{C}{_2}\) possible pairings of times, this will quickly mean that there are a lot of terms.
    This result is known as \defineindex{Wick's theorem}.
    
    \subsection{Anharmonic Oscillator}
    Now consider the case when \(\lambda \ne 0\).
    We will compute the four-point correlation function again.
    To first order in \(\lambda\) we have
    \begin{multline}
        \frac{1}{F_{\omega}(T)} \bra{0, t_b} \timeorder (\operator{x}(t_1)\operator{x}(t_2)\operator{x}(t_3)\operator{x}(t_4)) \bra{0, t_a}_{J=0}\\
        \quad= \left( \frac{\hbar}{i} \right)^4 \diffd{}{J(t_1),J(t_2),J(t_3),J(t_4)} \exp\left[ -\frac{i}{\hbar} \frac{\lambda}{4} \int_{t_a}^{t_b} \!\! \dl{\tau} \left( \frac{\hbar}{i} \diffd{}{J(\tau)} \right)^4 \right]\\
        \exp\left[ -\frac{i}{2m\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \int_{t_a}^{t_b} \!\! \dl{t'} \, J(t)\Delta(t, t')J(t') \right] \bigg\vert_{J=0}.
    \end{multline}
    
    We therefore find the first order contribution from the first term in the expansion of the first exponential.
    This gives four derivatives, from the \((\diffd{}/{J(\tau)})^4\) term, as well as the four derivatives with respect to \(J(t_i)\).
    This gives eight derivatives total.
    Therefore expanding the second exponential the only term that we keep is the one which has \(J^8\), which gives the result
    \begin{multline}
        \hbar^4 \diffd{}{J(t_1),J(t_2),J(t_3),J(t_4)} \left[ -\frac{i}{\hbar}\frac{\lambda}{4} \int_{t_a}^{t_b} \left( \hbar \diffd{}{J(\tau)} \right)^4 \right]\\
        \times\frac{1}{4!} \left[ -\frac{i}{2m\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \int_{t_a}^{t_b} \!\! \dl{t'} \, J(t)\Delta(t, t')J(t') \right]^4.
    \end{multline}
    We will start by performing the integrals with respect to \(J(\tau)\) and keep only the terms where each derivative acts on a different term in the second bracket.
    This gives the result
    \begin{multline}
        \hbar^4 \diffd{}{J(t_1),J(t_2),J(t_3),J(t_4)} \left( -\frac{i}{\hbar}\frac{\lambda}{4} \right) \hbar^4\\
        \quad\frac{8 \cdot 6 \cdot 4 \cdot 2}{4!} \int_{t_a}^{t_b} \!\! \dl{\tau} \left[ -\frac{i}{2m\hbar}\int_{t_a}^{t_b} \Delta(\tau, t')J(t') \dd{t'} \right]^4\\
        = -i\frac{3!\lambda}{\hbar} \int_{t_a}^{t_b} \!\! \dl{t} \prod_{i=1}^{4} \frac{i\hbar}{m}\Delta(t, t_i).
    \end{multline}
    For the method for finding the factor \(8 \cdot 6 \cdot 4 \cdot 2\) see \cref{app:combinatorial factor}.
    
    This can be represented by the connected Feynman diagram
    \begin{equation}
        \tikzsetnextfilename{4-point-correlation-connected-term}
        \begin{tikzpicture}[baseline = (current bounding box)]
            \begin{feynman}
                \vertex (t1) {\(t_1\)};
                \vertex (t2) at (1, 0) {\(t_2\)};
                \vertex (t3) at (0, 1) {\(t_3\)};
                \vertex (t4) at (1, 1) {\(t_4\)};
                \vertex (cross) at (0.5, 0.5);
                \diagram {
                    (t1) -- (cross) -- (t4);
                    (t2) -- (cross) -- (t3);
                };
                \fill (cross) circle [radius = 0.05];
                \node[right] at (cross) {\(t\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    
    The terms that we haven't considered, where the derivatives with respect to \(J(\tau)\) act on factors of \(J\) from the same \(J(t)\Delta(t, t')J(t')\) integral give disconnected terms proportional to \(\Delta(t, t)\), an example of such a term is
    \begin{equation}
        \tikzsetnextfilename{4-point-correlation-disconnected-term}
        \begin{tikzpicture}[baseline = (current bounding box)]
            \begin{feynman}
                \vertex (t1) {\(t_1\)};
                \vertex (t2) at (2, 0) {\(t_2\)};
                \vertex (t3) at (0, 1) {\(t_3\)};
                \vertex (t4) at (2, 1) {\(t_4\)};
                \vertex (cross) at (1, 1);
                \diagram {
                    (t1) -- (t2);
                    (t3) -- (cross) -- (t4);
                };
                \draw (1, 1.3) circle [radius = 0.3];
                \fill (cross) circle [radius = 0.05];
                \node[below] at (cross) {\(t\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Notice that we now have the possibility of loops, something that can't happen in the classical mechanics case.
    
    A summary of the Feynman rules for the anharmonic oscillator is
    \begingroup % change \arraystretch
    \tikzexternaldisable
    \def\arraystretch{2}
    \begin{equation}
        \begin{array}{ccc}
            \begin{tikzpicture}[baseline = (current bounding box)]
                \draw node[left] {\(t_1\)} (0, 0) -- ++ (1, 0) node[right] {\(t_2\)};
            \end{tikzpicture}
            &=&
            \displaystyle \frac{i\hbar}{m}\Delta(t_1, t_2)\\
            \tikzsetnextfilename{}
            \begin{tikzpicture}[baseline = (current bounding box)]
                \fill circle [radius = 0.05] node[right] {\(t\)} node[left, white] {\(t\)};
            \end{tikzpicture}
            &=&
            \displaystyle -i\frac{3!\lambda}{\hbar} \int_{t_a}^{t_b} \!\! \dl{t}\\
        \end{array}
    \end{equation}
    \tikzexternalenable
    \endgroup % reset \arraystretch
    
    \section{Limit as \(t_a \to -\infty\) and \(t_b \to \infty\)}
    Often we want to take \(t_a \to -\infty\) and \(t_b \to \infty\), for example in a scattering problem we typically deal with the particle a long time before and a long time after the interaction.
    The problem is that \(\Delta(t, t')\) oscillates wildly for large times.
    There are a few equivalent ways to deal with this:
    \begin{itemize}
        \item Take \(t = -i\tau\), this is what we do in statistical mechanics (see \cref{sec:statistical mechanics}).
        We can view this as rotating \(t\) by \(\pi/2\) (clockwise).
        \item Take \(t \to t - i\varepsilon\), we can see this as a small rotation by some small angle.
        This is what we did in \cref{sec:fourier transform of the amplitude} to take the Fourier transform of the transition amplitude.
        \item Dampen the path integral.
        This is also discussed at the end of \cref{sec:fourier transform of the amplitude}.
    \end{itemize}
    These last two are known collectively as the \(i\varepsilon\) prescription.
    
    We shall use the third option here.
    We take
    \begin{equation}
        \omega' = \omega - i\varepsilon
    \end{equation}
    for some infinitesimal \(\varepsilon > 0\).
    We then change the action to
    \begin{equation}
        S' = S + i\varepsilon \int_{-\infty}^{\infty} x^2 \dd{t}
    \end{equation}
    and so
    \begin{equation}
        \e^{iS'/\hbar} = \exp\left[ \frac{i}{\hbar} S - \varepsilon\int_{-\infty}^{\infty} x^2 \dd{t} \right].
    \end{equation}
    Therefore, for states with \(x \ne 0\), are exponentially suppressed as \(t_a \to -\infty\) and \(t_b \to \infty\).
    
    We now consider the propagator, \(\Delta(t, t')\), in this limit.
    The first term in \cref{eqn:driven SHO Greens function} in this limit gives
    \begin{multline}
        \lim_{t_b \to \infty} \lim_{t_a \to -\infty} \frac{\sin[\omega'(t_b - t)]\sin[\omega'(t' - t_a)]}{\sin(\omega' T)}\\
        = \lim_{t_b \to \infty} \lim_{t_a \to -\infty} \frac{\frac{1}{2i}\e^{i\omega'(t_b - t)} \frac{1}{2i}\e^{i\omega'(t' - t_a)}}{\frac{1}{2i}\e^{i\omega'T}} = \frac{1}{2i} \e^{-i\omega'(t - t')}
    \end{multline}
    where we have used
    \begin{equation}
        \sin[\omega'(t_b - t)] = \frac{1}{2i}[\e^{i\omega'(t_b - t)} - \e^{-i\omega'(t_b - t)}] \to \frac{1}{2i}\e^{i\omega'(t_b - t)}
    \end{equation}
    since 
    \begin{equation}
        \exp[-i\omega'(t_b - t)] = \exp[-i(\omega - i\varepsilon)(t_b - t)] = \exp[-\varepsilon(t_b - t)]\exp[-i\omega(t_b - t)] \to 0
    \end{equation}
    as \(t_b \to \infty\).
    We have also used a similar result for \(\sin[\omega'(t' - t_a)]\) and \(\sin(\omega' T)\), recalling that \(T = t_b - t_a \to \infty\) in this limit.
    
    Taking a similar limit for the second term in \cref{eqn:driven SHO Greens function} gives, dropping the prime on \(\omega'\),
    \begin{equation}
        \Delta_{\mathrm{F}}(t, t') = \Delta_{\mathrm{F}}(t - t') \coloneqq \lim_{t_b\to\infty} \lim_{t_a \to -\infty} \frac{1}{2i\omega} [\e^{-i\omega(t - t')}\theta(t - t') + \e^{i\omega(t - t')}\theta(t' - t)].
    \end{equation}
    This is called the \enquote{standard} \defineindex{Feynman propagator}.
    Note that \(\Delta_{\mathrm{F}}(t, t')\) is symmetric in \(t\) and \(t'\) and taking \(\vartheta(0) = 1/2\) we have \(\Delta(t, t) = 1/(2i\omega)\).
    
    \part{Relativistic Quantum Mechanics}
    \chapter{The Basics}
    We will now turn our focus to relativistic quantum mechanics.
    We will mostly abandon the path integral formalism that we have spent so long developing in this section, preferring instead the simpler operator formalism.
    Special relativity is needed to describe fast objects, and quantum mechanics is needed to describe small things, so they are needed in combination to do particle physics.
    Despite being in the \enquote{relativistic} section of the course we will spend a lot of time considering the non-relativistic limit of equations to ensure we can recover non-relativistic results.
    
    In relativistic quantum mechanics factors of the speed of light, \(c\), and reduced Planck's constant, \(\hbar\), are very common.
    For this reason many particle physicists prefer to work in natural units, a system where \(c = \hbar = 1\).
    This makes equations easier to remember and less cluttered.
    However, we will keep factors of \(c\) and \(\hbar\), occasionally writing important results with \(c = \hbar = 1\), which will be flagged with \natunitpic{} throughout.
    For example, in SI units
    \begin{equation}
        E^2 = m^2c^4 + \vv{p}^2c^2
    \end{equation}
    but in natural units
    \begin{equation}
        E^2 = m^2 + \vv{p}^2. \natunit
    \end{equation}
    
    In this section we will use the Einstein summation convention.
    This means any index repeated precisely twice, once up and once down, will be summed over.
    We also follow a convention where Latin indices, such as \(i\) or \(j\), run from 1 to 3 and Greek indices, such as \(\mu\) and \(\nu\), run from 0 to 3.
    
    \begin{rmk}
        For more details on relativity see the notes in the relativity section of the relativity, nuclear, and particle physics course.
    \end{rmk}
    
    \section{Four-Vectors}
    An \defineindex{event} is a point in four-dimensional Minkowski space, which is what we take as our model of space-time.
    The basis for these coordinates however is not orthogonal.
    This means we need to introduce the notions of contravariant and covariant components.
    
    A \defineindex{contravariant} four-vector has components with upper indices:
    \begin{equation}
        x^\mu = (x^0, x^1, x^2, x^3) = (ct, \vv{x}).
    \end{equation}
    We follow a convention where the \(0\)-component is time-like and the \(1\) through \(3\)-components are space-like.
    A \defineindex{covariant} four-vector has components with lower indices
    \begin{equation}
        x_\mu = (x_0, x_1, x_2, x_3) = (ct, -\vv{x}).
    \end{equation}
    Note the introduction of a negative sign.
    This is because
    \begin{equation}
        x_\mu \coloneqq \eta_{\mu\nu}x^{\nu}.
    \end{equation}
    Here \(\eta\) is the Minkowski metric tensor, sometimes denoted \(g\), which is given by
    \begin{equation}
        \eta^{\mu\nu} = \eta_{\mu\nu} \coloneqq
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & -1 & 0 & 0\\
            0 & 0 & -1 & 0\\
            0 & 0 & 0 & -1
        \end{pmatrix}
        = \diag(1, -1, -1, -1).
    \end{equation}
    Note that the exact form of \(\eta\) is a matter of convention, the two most common being \((+---)\), which is what we use, and \((-+++)\), which uses \(\eta_{\mu\nu} = \diag(-1,1,1,1)\).
    There are other conventions, including using complex time, but we will stick with the \(+---\) convention.
    
    The \defineindex{scalar product} of two four vectors, \(a^\mu = (a^0, \vv{a})\) and \(b^\mu = (b^0, \vv{b})\), is defined by
    \begin{equation}
        a \cdot b = a^\mu b_\mu = a_\mu b^\mu = \eta^{\mu\nu}a_\mu b_\nu = \eta_{\mu\nu}a^\mu b^\nu = a^0b^0 - \vv{a} \cdot \vv{b}
    \end{equation}
    where \(\vv{a} \cdot \vv{b}\) is the standard scalar product on \(\reals^3\):
    \begin{equation}
        \vv{a} \cdot \vv{b} \coloneqq a^ib_i = a^1b^1 + a^2b^2 + a^3b^3.
    \end{equation}
    
    The following derivative operators can be thought of as the contravariant and covariant versions of the gradient:
    \begin{align}
        \partial_\mu &= \diffp{}{x^\mu} \coloneqq \left( \frac{1}{c}\diffp{}{t}, \grad \right),\\
        \partial^\mu &= \diffp{}{x_\mu} \coloneqq \left( \frac{1}{c}\diffp{}{t}, -\grad \right).
    \end{align}
    Notice that the contravariant (covariant) derivative operator \(\partial^\mu\) (\(\partial_\mu\)) is the derivative with respect to a covariant (contravariant) quantity \(x_\mu\) (\(x^\mu\)).
    
    The \defineindex{d'Alembertian} is the four-vector version of the Laplacian, it's defined as
    \begin{equation}
        \dalembertian \coloneqq \partial_\mu\partial^\mu = \partial^\mu\partial_\mu = \frac{1}{c^2}\diffp[2]{}{t} - \laplacian.
    \end{equation}
    Note that this same operator is denoted both \(\square\) and \(\square^2\) in other sources\footnote{those are supposed to be squares, it's not a missing symbol}.
    
    The four-momentum is defined as
    \begin{align}
        p^\mu &\coloneqq \left( \frac{E}{c}, \vv{p} \right),\\
        p^\mu &\coloneqq (E, \vv{p}), \natunit
    \end{align}
    where \(\vv{p}\) is the relativistic three-momentum, \(\vv{p} \coloneqq \gamma m\vv{v}\), where the factor \(\gamma\) is to be defined later and \(\vv{v}\) is the three-velocity.
    
    For a free particle we have
    \begin{equation}
        p^2 = p_\mu p^\mu = \frac{E^2}{c^2} - \vv{p} \cdot \vv{p} = m^2c^2
    \end{equation}
    where \(m\) is the mass of the particle.
    This leads to the famous
    \begin{align}
        E^2 &= \vv{p}^2c^2 + m^2c^4,\\
        E^2 &= \vv{p}^2 + m^2. \natunit\\
    \end{align}
    
    \section{Lorentz Transformations}
    \define{Lorentz transformations}\index{Lorentz transformation} are how we move between frames in special relativity.
    It is a linear transformation on the components of a four-vector with the property that it leaves the scalar product invariant.
    Given a four-vector it's coordinates in one frame, \(a^\mu\), are related to it's coordinates in another frame, \(a'^\mu\), by
    \begin{equation}
        a'^\mu = \tensor{\Lambda}{^\mu_\nu}a^\nu
    \end{equation}
    where \(\tensor{\Lambda}{^\mu_\nu}\) is a Lorentz transform.
    The property that scalar products are invariant under Lorentz transformations means that
    \begin{equation}
        \tensor{\Lambda}{^\mu_\alpha}\tensor{\Lambda}{^\nu_\beta} = \varepsilon_{\alpha\beta}.
    \end{equation}
    Strictly what we discuss here are homogenous Lorentz transformations, we are not considering translations.
    
    The \enquote{standard} Lorentz transformation is for a boost between two frames which are in the \enquote{standard configuration} such that their respective space axes are parallel and coincide at \(t = 0\) and the velocity of one frame is along the \(x\)-axis of the other frame.
    In this case
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = 
        \begin{pmatrix}
            \cosh \omega & -\sinh\omega & 0 & 0\\
            -\sinh\omega & \cosh \omega & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    Here \(\omega\) is the \defineindex{rapidity} defined by
    \begin{equation}
        \tanh \omega = \beta \coloneqq \frac{v}{c}
    \end{equation}
    where \(v\) is the relative velocity of the frame.
    We then define another quantity
    \begin{equation}
        \gamma \coloneqq \cosh\omega = \frac{1}{\sqrt{\sech^2 \omega}} = \frac{1}{\sqrt{1 - \tanh^2\omega}} = \frac{1}{\sqrt{1 - v^2/c^2}}.
    \end{equation}
    We then have
    \begin{equation}
        \sinh\omega = \tanh\omega\cosh\omega = \gamma\beta.
    \end{equation}
    
    Hence we can write a Lorentz transform in the standard configuration as
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} =
        \begin{pmatrix}
            \gamma & -\beta\gamma & 0 & 0\\
            -\beta\gamma & \gamma & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    
    The coordinates of \(x^\mu = (ct, x, y, z)\) are related to the coordinates of the same point in the other frame, \(x'^\mu = (ct', x', y', z')\) by
    \begin{align}
        ct' &= \gamma ct - \beta\gamma x,\\
        x' &= \gamma x - \beta\gamma ct,\\
        y' &= y,\\
        z' &= z.
    \end{align}
    
    The group of homogenous Lorentz equations includes the group of rotations as a subgroup.
    Elements of this subgroup take the form
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = 
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & \cos\varphi & \sin\varphi & 0\\
            0 & -\sin\varphi & \cos\varphi & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    Notice the similarity to the standard configuration Lorentz transformation, we just change the angle, \(\varphi\), to the rapidity, \(\omega\), and trig to hyperbolic trig.
    Using this comparison we can think of Lorentz transformations as rotations of spacetime.
    
    \chapter{Klein--Gordon Equation}
    The Schr\"odinger equation for a free particle is
    \begin{align}
        i\hbar\diffp{}{t} \psi(\vv{r}, t) &= -\frac{\hbar^2}{2m}\laplacian\psi(\vv{r}, t),\\
        i\diffp{}{t} \psi(\vv{r}, t) &= -\frac{1}{2m}\laplacian\psi(\vv{r}, t).
    \end{align}
    We can obtain this from the non-relativistic total energy,
    \begin{equation}
        E = H = \frac{\vv{p}^2}{2m}
    \end{equation}
    by substituting in the operators
    \begin{equation}
        E \to i\hbar\diffp{}{t}, \qqand \vv{p} \to -i\hbar\grad \implies \vv{p}^2 \to -\hbar^2\laplacian.
    \end{equation}
    
    This leads us naturally to a relativistic version of the Schr\"odinger equation where we start with the relativistic \(E^2 = \vv{p}^2c^2 + m^2c^4\) and making the same operator substitutions we get
    \begin{equation}
        -\hbar^2\diffp[2]{}{t}\varphi(\vv{r}, t) = -\hbar^2c^2\laplacian\varphi(\vv{r}, t) + m^2c^4\varphi(\vv{r}, t).
    \end{equation}
    This can be written more compactly using the d'Alembertian as
    \begin{align}
        (\hbar^2\dalembertian + m^2c^2)\varphi(x) = 0,\\
        (\dalembertian + m^2)\varphi(x) = 0. \natunit
    \end{align}
    This is the \defineindex{Klein--Gordon equation} for the wavefunction \(\varphi\) for a free relativistic particle.
    Note that this applies only to a spin zero particle.
    
    From the Klein--Gordon equation we see that the appropriate operator prescription is
    \begin{equation}
        p^\mu \to \operator{p}^\mu = i\hbar\left( \frac{1}{c}\diffp{}{t}, -\grad \right) = i\hbar\diffp{}{x)\mu} = i\hbar\partial^\mu.
    \end{equation}
    
    \section{Free Particle Solution}
    Plane waves of the form
    \begin{equation}
        \varphi(\vv{r}, t) = \exp[i\vv{k}\cdot\vv{r} - i\omega t]
    \end{equation}
    are solutions to the Klein--Gordon equation, which we can see by substituting this in.
    We then find that \(\omega\), \(\vv{k}\), and \(m\) are related by
    \begin{equation}
        \hbar^2\omega^2 = \hbar^2c^2\vv{k}^2 + m^2c^4.
    \end{equation}
    The square root of this gives
    \begin{equation}
        \hbar\omega = \pm \sqrt{\hbar^2c^2\vv{k}^2 + m^2c^4}.
    \end{equation}
    
    We can also show relatively easily that plane waves of this form are eigenfunctions of the energy and momentum operators, with eigenvalues \(\hbar\omega\) and \(\vv{p} \coloneqq \hbar\vv{k}\) respectively.
    
    Interpreting \(\hbar\omega\) as the total allowed energy of the free particle we see that there is an ambiguity in the sign of the energy, there are both positive and negative energy solutions.
    Defining \(E \coloneqq \sqrt{\vv{p}^2c^2 + m^2c^4}\) as the positive solution we have
    \begin{equation}
        \hbar\omega = \pm E.
    \end{equation}
    
    The positive energy eigenvalues agree with the classical relativistic relation between energy, mass, and momentum.
    We will come back to the interpretation of the negative energy solutions shortly.
    
    Defining the four vector \(k^\mu \coloneqq (\omega/c, \vv{k})\), and so \(p^\mu = \hbar k^\mu\) we can write the solution in a covariant form:
    \begin{equation}
        \varphi(x) = \e^{-ik\cdot x} = \e^{-ip\cdot x / \hbar}.
    \end{equation}
    
    \section{Continuity Equation}
    \subsection{Schr\"odinger Equation}
    Denoting the Schr\"odinger equation by \((\mathrm{SE})\), so
    \begin{equation}
        (\mathrm{SE}) = \left[ i\hbar\diffp{}{t} + \frac{\hbar^2}{2m}\laplacian - V \right]\psi = 0
    \end{equation}
    we then have
    \begin{equation}
        (\mathrm{SE})^* = \left[ -i\hbar\diffp{}{t} + \frac{\hbar^2}{2m}\laplacian - V \right]\psi^* = 0.
    \end{equation}
    
    Considering \(\psi^*(\mathrm{SE}) - \psi(\mathrm{SE})^* = 0\) we have
    \begin{align}
        0 &= \psi^*(\mathrm{SE}) - \psi(\mathrm{SE})^*\\
        &= \psi^* i\hbar\diffp{\psi}{t} + \psi^*\frac{\hbar^2}{2m} \laplacian \psi - \psi^*V\psi + \psi i\hbar \diffp{\psi^*}{t} - \psi \frac{\hbar^2}{2m} \laplacian \psi^* - \psi V \psi^*\\
        &= i\hbar \diffp*{(\psi\psi^*)}{t} + \frac{\hbar^2}{2m} [\psi^*\laplacian \psi - \psi \laplacian \psi^*].
    \end{align}
    Now considering
    \begin{align}
        \div (\psi^* \grad \psi - \psi \grad \psi^*) &= \partial_i (\psi^*\partial_i\psi - \psi\partial_i\psi^*)\\
        &= (\partial_i\psi^*)(\partial_i\psi) + \psi^*\partial_i\partial_i\psi - (\partial_i\psi)(\partial_i\psi^*) - \psi\partial_i\partial_i\psi^*\\
        &= \psi^* \laplacian \psi - \psi \laplacian \psi^*.
    \end{align}
    We therefore have
    \begin{equation}
        0 = \diffp*{(\psi\psi^*)}{t} - \frac{i\hbar}{2m}\div(\psi^*\grad \psi - \psi \grad \psi^*).
    \end{equation}
    Defining
    \begin{equation}
        \rho \coloneqq \psi^* \psi,
    \end{equation}
    and
    \begin{equation}
        \vv{j} \coloneqq -\frac{i\hbar}{2m}(\psi^* \grad \psi - \psi \grad \psi^*)
    \end{equation}
    we have
    \begin{equation}
        0 = \diffp{\rho}{t} + \div \vv{j}.
    \end{equation}
    Interpreting \(\rho = \abs{\psi}^2\) as the probability density and \(\vv{j}\) as a probability density current we see that this is a continuity equation for probability density.
    
    \subsection{Klein--Gordon Equation}
    Denoting the Klein--Gordon equation by \((\mathrm{KG})\), so
    \begin{equation}
        (\mathrm{KG}) = (\hbar^2\dalembertian + m^2c^2)\varphi = 0
    \end{equation}
    we then have
    \begin{equation}
        (\mathrm{KG})^* = (\hbar^2\dalembertian + m^2c^2)\varphi^* = 0.
    \end{equation}
    
    Considering \(\varphi^*(\mathrm{KG}) - \varphi(\mathrm{KG}) = 0\) we have
    \begin{align}
        0 &= \varphi^*(\mathrm{KG}) - \varphi(\mathrm{KG})\\
        &= \varphi^*\hbar^2\dalembertian\varphi + \varphi^*m^2c^2\varphi - \varphi\hbar^2\dalembertian\varphi^* - \varphi m^2c^2\varphi^*\\
        &= \varphi^*\hbar^2\dalembertian\varphi - \varphi\hbar^2\dalembertian\varphi\\
        &= \hbar^2\varphi^*\left( \frac{1}{c^2}\diffp[2]{}{t} - \laplacian \right)\varphi - \hbar\varphi\left( \frac{1}{c^2}\diffp[2]{}{t} - \laplacian \right)\varphi^*\\
        &= \frac{\hbar^2}{c^2}\left( \varphi^*\diffp[2]{\varphi}{t} - \varphi\diffp[2]{\varphi}{t} \right) + \hbar^2(-\varphi^*\laplacian\varphi + \varphi\laplacian\varphi^*).
    \end{align}
    Using
    \begin{align}
        \diffp{}{t} \left[ \varphi^*\diffp{\varphi}{t} - \varphi\diffp{\varphi^*}{t} \right] &= \diffp{\varphi^*}{t}\diffp{\varphi}{t} + \varphi^*\diffp[2]{\varphi}{t} - \diffp{\varphi}{t}\diffp{\varphi^*}{t} - \varphi\diffp[2]{\varphi^*}{t}\\
        &= \varphi^*\diffp[2]{\varphi}{t} - \varphi\diffp[2]{\varphi^*}{t}
    \end{align}
    we then have
    \begin{equation}
        0 = \frac{\hbar^2}{c^2} \diffp{}{t}\left[ \varphi^*\diffp{\varphi}{t} - \varphi\diffp{\varphi^*}{t} \right] - \hbar^2 \div[\varphi^*\grad\varphi - \varphi\grad\varphi^*].
    \end{equation}
    Multiplying through by \(i/(2m)\) we have
    \begin{equation}
        0 = \diffp{}{t}\left( \frac{i\hbar^2}{2mc^2} \left[ \varphi^*\diffp{\varphi}{t} - \varphi\diffp{\varphi^*}{t} \right] \right) - \div \left( \frac{i\hbar^2}{2m} [\varphi^*\grad\varphi - \varphi\grad\varphi^*] \right).
    \end{equation}
    Defining
    \begin{equation}
        \rho \coloneqq \frac{i\hbar}{2mc^2}\left[ \varphi^*\diffp{\varphi}{t} - \varphi\diffp{\varphi^*}{t} \right],
    \end{equation}
    and
    \begin{equation}
        \vv{j} = -\frac{i\hbar^2}{2m}(\varphi^*\grad\varphi - \varphi\grad\varphi^*)
    \end{equation}
    we have
    \begin{equation}
        0 = \diffp{\rho}{t} + \div\vv{j}.
    \end{equation}
    Which is another continuity equation.
    Note that we choose the constant factor such that the definition of \(\vv{j}\) is consistent for both the Schr\"odinger equation and Klein--Gordon equation.
    
    We now see another problem with the Klein--Gordon equation.
    The quantity \(\rho\), which we would like to interpret as a probability density function, is not positive definite.
    In particular for plane waves with negative energy solutions \(\rho < 0\).
    In the non-relativistic case we can show that \(\rho\) reduces to \(\varphi^*\varphi\).
    
    \chapter{Dirac Equation}
    \section{The Form of the Equation}
    Unhappy with the negative energy solutions to the Klein--Gordon equation Dirac posited a new equation.
    In doing so his requirements were that it be first order in \(\diffp{}/{t}\), to ensure that the sign ambiguity in the energy being a square root didn't occur, and that it be relativistic.
    This means that it should also be first order in \(\diffp{}/{x}\), since in relativity we must treat time and space equally.
    Dirac also hoped that this would avoid having time derivatives in the probability density.
    
    Dirac's proposal was to look for an equation of the form of the Schr\"odinger equation:
    \begin{equation}
        i\hbar\diffp{}{t}\psi(\vv{r}, t) = \operator{\hamiltonian}\psi(\vv{r}, t),
    \end{equation}
    where \(\operator{\hamiltonian}\) is linear in spatial derivatives.
    In particular, since the spatial derivatives correspond to momentum operators, the Hamiltonian must be of the form
    \begin{equation}
        \operator{\hamiltonian} = c\vv{\alpha}\cdot\vecoperator{p} + \beta mc^2
    \end{equation}
    with
    \begin{equation}
        \vv{\alpha} \cdot \vecoperator{p} = -i\hbar \alpha^i\diffp{}{x^i} = -i\hbar \alpha^i\partial_i,
    \end{equation}
    We assume that \(\alpha^i\) and \(\beta\) are independent of the derivatives, so commute with \(\vv{r}\), \(t\), \(\vecoperator{p}\), and \(E\), but not necessarily with each other.
    
    Requiring that this equation be relativistic we must have \(E^2 = \vv{p}^2c^2 + m^2c^4\), and so we must have
    \begin{equation}
        E^2  \operator{\hamiltonian}^2\psi(\vv{r}, t) = (c^2\vecoperator{p}^2 + m^2c^4)\psi(\vv{r}) = c^2\vv{p}^2 + m^2c^4.
    \end{equation}
    We therefore have
    \begin{align}
        \operator{\hamiltonian}^2\psi(\vv{r}, t) &= (c\vv{\alpha} \cdot \vecoperator{p} + \beta mc^2)^2 \psi(\vv{r}, t)\\
        &= [c^2(\vv{\alpha} \cdot \vecoperator{p})^2 + \beta^2m^2c^4 + mc^3(\vv{\alpha}\cdot\vecoperator{p}\beta + \beta\vv{\alpha}\cdot\vecoperator{p})] \psi(\vv{r}, t)\\
        &= (c^2\vecoperator{p}^2 + m^2c^4)\psi(\vv{r}, t).
    \end{align}
    Notice the care taken in the second line not to commute \(\vv{\alpha}\) and \(\beta\).
    
    \section{What are \(\alpha^i\) and \(\beta\)?}
    Requiring that the last equality above holds considering the \(c^2\) term gives
    \begin{equation}
        c^2(\vv{\alpha} \cdot \vecoperator{p})^2 = c^2\vecoperator{p}^2.
    \end{equation}
    This means that we must have
    \begin{equation}
        \vecoperator{p}^2 = (\vv{\alpha} \cdot \vecoperator{p})^2 = \alpha^i\operator{p}_i\alpha^j\operator{p}_j = \alpha^i\alpha^j\operator{p}^i\operator{p}_j.
    \end{equation}
    We should also have
    \begin{equation}
        (\vv{\alpha} \cdot \vecoperator{p})^2 = a^j\operator{p}_ja^i\operator{p}_i = \alpha^j\alpha^i\operator{p}_i\operator{p}_j.
    \end{equation}
    Adding these we have
    \begin{equation}
        2\vecoperator{p}^2 = \alpha^i\alpha^j\operator{p}_i\operator{p}_j + \alpha^j\alpha^i\operator{p}_j\operator{p}_i
    \end{equation}
    We see that this result holds if
    \begin{equation}
        \anticommutator{\alpha^i}{\alpha^j} = 2\delta^{ij},
    \end{equation}
    where \(\anticommutator{A}{B} \coloneqq AB + BA\) is the \defineindex{anticommutator}.
    
    We can see from the \(c^4\) term that we must have \(\beta^2 = \ident\).
    
    Considering the \(c^3\) term we must have that 
    \begin{equation}
        \vv{\alpha}\cdot\vecoperator{p}\beta + \beta\vv{\alpha} \cdot \vecoperator{p} = 0.
    \end{equation}
    It then follows that
    \begin{equation}
        \anticommutator{\alpha^i}{\beta} = 0.
    \end{equation}
    
    It is not possible that these results can all hold if \(\alpha^i\) and \(\beta\) are just numbers.
    The simplest mathematical objects for which these requirements hold are matrices.
    
    Considering the cyclic property of the trace we see that they must be traceless, first
    \begin{equation}
        \tr(\alpha^i) = \tr(\alpha^i\beta^2) = \tr(\beta \alpha^i \beta) = -\tr(\alpha^i\beta^2) = -\tr(\alpha^i)
    \end{equation}
    where we use \(\beta^2 = \ident\) in the first equality, the cyclic property of the trace in the second equality, and the anticommutativity of \(\alpha^i\) and \(\beta\), meaning \(\alpha^i\beta = -\beta\alpha^i\), in the third equality, and then \(\beta^2 = \ident\) again in the final equality.
    The only way to have \(\tr(\alpha^i) = -\tr(\alpha^i)\) is if \(\tr(\alpha^i) = 0\).
    We can prove \(\tr(\beta) = 0\) in a similar way since we also have \((\alpha^i)^2 = \ident\), which follows from \((\alpha^i)^2 = \anticommutator{\alpha^i}{\alpha^i}/2 = 2\delta^{ij}/2 = \ident\).
    
    In order for \(\operator{\hamiltonian}\) to be Hermitian \(\alpha^i\) and \(\beta\) must also be Hermitian.
    This means that their eigenvalues are real.
    Since \((\alpha^i)^2 = \beta^2 = \ident\) the eigenvalues must all be \(\pm 1\).
    
    Since the eigenvalues are all \(\pm 1\) and the trace is the sum of the eigenvalues this means the eigenvalues must pair up in positive-negative pairs to cancel to give zero trace.
    Therefore the dimension of the matrices, which is the number of eigenvalues, must be even.
    
    There is no set of four traceless \(2\times 2\) matrices satisfying these anticommutation relations.
    This can be seen by noticing that the Pauli matrices,
    \begin{equation}
        \sigma^1 \coloneqq 
        \begin{pmatrix}
            0 & 1\\ 1 & 0
        \end{pmatrix}
        , \qquad \sigma^2 \coloneqq
        \begin{pmatrix}
            0 & -i\\ i & 0
        \end{pmatrix}
        , \qqand \sigma^3 \coloneqq 
        \begin{pmatrix}
            1 & 0\\ 0 & -1
        \end{pmatrix}
        ,
    \end{equation}
    satisfy the required \(\anticommutator{\sigma^i}{\sigma^j} = 2\delta^{ij}\), but these three matrices span the space of \(2\times 2\) traceless Hermitian matrices and as such there is no fourth independent matrix satisfying the necessary requirements.
     
    The smallest representation\footnote{see notes from symmetries of quantum mechanics} of \(\alpha^i\) and \(\beta\) is \(4\times 4\).
    One \(4\times 4\) representation can be written using the \(2\times 2\) identity matrix, \(\ident_2\), and the Pauli matrices, \(\sigma^i\), as block diagonal matrices:
    \begin{equation}
        \beta = 
        \begin{pmatrix}
            \ident_2 & 0\\
            0 & -\ident_2
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & -1 & 0\\
            0 & 0 & 0 & -1
        \end{pmatrix}
        = \sigma^3 \otimes \ident_2,
    \end{equation}
    and
    \begin{equation}
        \vv{\alpha} = 
        \begin{pmatrix}
            0 & \vv{\sigma}\\
            \vv{\sigma} & 0
        \end{pmatrix}
        = \sigma^1 \otimes \vv{\sigma}.
    \end{equation}
    Here \(\vv{\alpha}\) and \(\vv{\sigma}\) are an abuse of notation for a vector of three matrices, and \(\otimes\) is the tensor product\footnote{see notes from principles of quantum mechanics for details on the tensor product}.
    
    At this point we note that it can be shown that the Pauli matrices satisfy the identity
    \begin{equation}
        \sigma^i\sigma^j = \delta^{ij} + i\varepsilon^{ijk}\sigma^k.
    \end{equation}
    Importantly this means that
    \begin{equation}
        \anticommutator{\sigma^i}{\sigma^j} = \sigma^i\sigma^j + \sigma^j\sigma^i = \delta^{ij} + i\varepsilon^{ijk}\sigma^k + \delta^{ji} + i\varepsilon^{jik}\sigma^k = 2\delta^{ij},
    \end{equation}
    and
    \begin{equation}
        \commutator{\sigma^i}{\sigma^j} = \sigma^i\sigma^j - \sigma^j\sigma^i = \delta^{ij} + i\varepsilon^{ijk}\sigma^k - \delta^{ji} - i\varepsilon^{jik}\sigma^k = 2i\varepsilon^{ijk}\sigma^k.
    \end{equation}
    
    Since \(\operator{\hamiltonian}\) is the sum of \(4\times 4\) matrices (in this representation) this means that \(\operator{\hamiltonian}\) is a \(4\times 4\) matrix.
    This means it must act on a four component column matrix which we call a four component \defineindex{spinor}\footnote{This is one case where a column matrix is \emph{not} a vector, since the components don't transform as a vector}:
    \begin{equation}
        \psi(\vv{r}, t) = 
        \begin{pmatrix}
            \psi_1(\vv{r}, t)\\ \psi_2(\vv{r}, t)\\ \psi_3(\vv{r}, t)\\ \psi_4(\vv{r}, t)\\
        \end{pmatrix}
    \end{equation}
    The Dirac equation for a free particle is them
    \begin{align}
        i\hbar\diffp{}{t}\psi(\vv{r}, t) &= (-i\hbar c\vv{\alpha}\cdot\grad + \beta mc^2)\psi(\vv{r}, t),\\
        i\diffp{}{t}\psi(\vv{r}, t) &= (-i\vv{\alpha} \cdot \grad + \beta m) \psi(\vv{r}, t). \natunit
    \end{align}
    
    \section{Probability Density}
    In this section we will consider operators acting both right and left, in particular the operator will be grad, \(\grad\).
    We will denote a right acting operator by \(\rightgrad\) and a left acting operator by \(\leftgrad\).
    
    Denote by \((\mathrm{DE})\) the Dirac equation
    \begin{equation}
        (\mathrm{DE}) = i\hbar\diffp{}{t}\psi + (i\hbar c\vv{\alpha} \cdot \rightgrad - \beta mc^2)\psi = 0,
    \end{equation}
    and take the Hermitian conjugate:
    \begin{equation}
        (\mathrm{DE})^\hermit = -i\hbar\diffp{}{t}\psi^\hermit + \psi^\hermit (-i\hbar c\vv{\alpha} \cdot \leftgrad - \beta mc^2) = 0.
    \end{equation}
    Note that \(\vv{\alpha}\) and \(\beta\) are both Hermitian so \(\vv{\alpha}^\hermit = \vv{\alpha}\) and \(\beta^\hermit = \beta\).
    We also use that the definition of the adjoint is such that the adjoint of an operator is the operator acting left, so \(\rightgrad^\hermit = \leftgrad\).
    The object \(\psi^\hermit\) is a row matrix whose components are the complex conjugate of the components of \(\psi\).
    
    Now consider
    \begin{align}
        0 &= \psi^\hermit (\mathrm{DE}) - (\mathrm{DE})\psi\\
        &= i\hbar \psi^\hermit \diffp{}{t}\psi + \psi^\hermit(i\hbar c\vv{\alpha} \cdot \rightgrad - \beta mc^2)\psi \\
        &\qquad+ i\hbar \psi\diffp{}{t}\psi^\hermit + \psi^\hermit(i\hbar c\vv{\alpha}\cdot\leftgrad + \beta mc^2)\psi\\
        &= i\hbar\left( \psi^\hermit \diffp{}{t}\psi + \psi\diffp{}{t}\psi^\hermit \right) + i\hbar c \psi^\hermit(\vv{\alpha} \cdot \rightgrad - \vv{\alpha} \cdot \leftgrad) \psi\\
        &= i\hbar \diffp*{(\psi^\hermit\psi)}{t} + i\hbar \div(\psi^\hermit \vv{\alpha} \psi).
    \end{align}
    Dividing through by \(i\hbar c\) we get
    \begin{align}
        0 &= \frac{1}{c}\diffp{}{t} \rho + \div \vv{j},\\
        0 &= \diffp{}{t} \rho + \div \vv{j}, \natunit
    \end{align}
    with
    \begin{equation}
        \rho \coloneqq \psi^\hermit\psi, \qqand \vv{j} \coloneqq \psi^\hermit \vv{\alpha} \psi.
    \end{equation}
    This is a continuity equation.
    The probability current, \(\rho\), is positive definite, as required.
    
    We can write the continuity equation in a covariant form as
    \begin{equation}
        \partial_\mu j^\mu = 0
    \end{equation}
    where we have defined the four-probability-density-current,
    \begin{equation}
        j^\mu \coloneqq (\rho, \vv{j}).
    \end{equation}
    This means that the probability density, \(\rho\), transforms as the time component of a four-vector, and \(\vv{j}\) transforms as the spatial component of a four-vector.
    
    \section{Free Particle Solution}
    We will look for plane wave solutions of the form
    \begin{align}
        \psi(\vv{r}, t) &= \e^{-i k\cdot x} w(p)\\
        &= \e^{-ip\cdot x/\hbar}w(p)\\
        &= \exp\left[ -\frac{i}{\hbar}(c p^0 t - \vv{p} \cdot \vv{r}) \right]w(p)
    \end{align}
    where \(w(p)\) is a four component spinor which is a function of the four-momentum, \(p\).
    Substituting this into the Dirac equation the left hand side gives
    \begin{equation}
        i\hbar\diffp{}{t}\exp\left[ -\frac{i}{\hbar}(cp^0 t - \vv{p}\cdot\vv{r}) \right]w(p) = cp^0\exp\left[ -\frac{i}{\hbar}(cp^0t - \vv{p} \cdot \vv{r}) \right]w(p).
    \end{equation}
    The right hand side gives
    \begin{multline}
        (-i\hbar c\vv{\alpha} \cdot \grad + \beta mc^2)\exp\left[ -\frac{i}{\hbar}(cp^0t - \vv{p}\cdot\vv{r}) \right]\\
        = (-\vv{\alpha}\cdot\vv{p} + \beta mc^2)\exp\left[ -\frac{i}{\hbar}(cp^0t - \vv{p}\cdot\vv{r}) \right]w(p).
    \end{multline}
    Dividing out the exponential and a factor of \(c\) we have
    \begin{align}
        p^0w(p) &= (\vv{\alpha} \cdot \vv{p} + \beta mc)w(p),\\
        p^0w(p) &= (\vv{\alpha} \cdot \vv{p} + \beta m)w(p). \natunit
    \end{align}
    This is sometimes called the momentum-space Dirac equation.
    
    Recalling that \(p^0 = E/c\) we see that this trial solution corresponds to a particle of energy \(cp^0\) and three-momentum \(\vv{p}\).
    
    \subsection{Block Form}
    The momentum-space Dirac equation can be solved for \(w(p)\) by brute force.
    However, a more elegant approach is to write the four component spinor, \(w(p)\), as two two-component spinors, \(\varphi(p)\) and \(\chi(p)\):
    \begin{equation}
        w(p) = 
        \begin{pmatrix}
            \varphi(p)\\ \chi(p)
        \end{pmatrix}
        .
    \end{equation}
    Writing \(\vv{\alpha}\) and \(\beta\) in block diagonal form the momentum-space Dirac equation becomes
    \begin{equation}
        p^0
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        =
        \begin{pmatrix}
            mc & \vv{\sigma} \cdot \vv{p}\\
            \vv{\sigma} \cdot \vv{p} & -mc
        \end{pmatrix}
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        .
    \end{equation}
    Which gives a system of linear equations:
    \begin{align}
        p^0\varphi &= mc\varphi + \vv{\sigma}\cdot\vv{p} \chi,\label{eqn:dirac eqn solution step eq1}\\
        p^0\chi &= \vv{\sigma} \cdot \vv{p}\varphi - mc \chi.\label{eqn:dirac eqn solution step eq2}
    \end{align}
    Rearranging the second equation to get \(\chi\) in terms of \(\varphi\) we have
    \begin{equation}
        \chi = \frac{\vv{\sigma} \cdot \vv{p}}{p^0 + mc}\varphi.
    \end{equation}
    Substituting this into the first equation we get
    \begin{equation}
        p^0\varphi = \left( mc + \frac{(\vv{\sigma}\cdot\vv{p})^2}{p^0 + mc} \right)\varphi.
    \end{equation}
    
    We have that
    \begin{align}
        (\vv{\sigma} \cdot \vv{p})^2 &= \sigma^ip^i\sigma^jp^j\ident\\
        &= \sigma^i\sigma^jp^ip^j\ident\\
        &= (\delta^{ij} + \varepsilon^{ijk}\sigma^k)p^ip^j\ident\\
        &= \delta^{ij}p^ip^j\ident\\
        &= \vv{p}^2\ident.
    \end{align}
    Here we have used the fact that \(p^ip^j\) is symmetric in \(i\) and \(j\) so \(e^{ijk}p^ip^j\) is the product of a symmetric and antisymmetric term and so vanishes.
    There is also a factor of \(\ident\) here since \(\vv{\sigma}\) is a matrix, not a vector.
    
    It follows that \(w(p)\) is a free particle solution of the Dirac equation for all two-component spinors, \(\varphi\), if
    \begin{equation}
        (p^0)^2 = (m^2c^2) + \vv{p}^2.
    \end{equation}
    Multiplying by \(c^2\) this becomes
    \begin{equation}
        (p^0c)^2 = E^2 = m^2c^4 + \vv{p}^2c^2,
    \end{equation}
    which is the energy-momentum relation that we hoped would hold.
    
    \subsection{Positive Energy Solutions}
    For positive energy solutions we have \(p^0 > 0\).
    We therefore have
    \begin{equation}
        p^0 = \sqrt{m^2 c^2 + \vv{p}^2} = \frac{E}{c},
    \end{equation}
    where as usual we take \(E\) to be positive and the negative energy solutions have \(-E\).
    
    We then define the two Dirac spinors \(w^{(1)}(p)\) and \(w^{(2)}(p)\) to be
    \begin{equation}
        w^{(1),(2)}(p) = \renewcommand{\arraystretch}{1.5}
        \begin{pmatrix}
            \varphi^{(1),(2)}\\
            \dfrac{c\vv{\sigma}\cdot\vv{p}}{E + mc^2}\varphi^{(1),(2)}
        \end{pmatrix}
        .
    \end{equation}
    Since \(\varphi^{(1),(2)}\) are arbitrary assuming that the energy-momentum relation holds it is conventional to choose the linearly-independent spinors
    \begin{equation}
        \varphi^{(1)} \coloneqq 
        \begin{pmatrix}
            1\\ 0
        \end{pmatrix}
        , \qqand \varphi^{(2)} \coloneqq
        \begin{pmatrix}
            0\\ 1
        \end{pmatrix}
        .
    \end{equation}
    
    \subsection{Negative Energy Solutions}
    For negative energy solutions we have \(p^0 < 0\).
    We therefore have
    \begin{equation}
        p^0 = -\sqrt{m^2c^2 + \vv{p}^2} = -\frac{E}{c}.
    \end{equation}
    
    It is conventional to write down the two negative energy solutions for negative spatial momentum, \(-\vv{p}\).
    The process is similar to the positive energy solution but we instead rearrange \cref{eqn:dirac eqn solution step eq1} to get
    \begin{equation}
        \varphi = \frac{\vv{\sigma}\cdot\vv{p}}{p^0 - mc}\chi.
    \end{equation}
    Substituting this into \cref{eqn:dirac eqn solution step eq2} we get
    \begin{equation}
        p^0\chi = \left( \frac{(\vv{\sigma} \cdot \vv{p})^2}{p^0 - mc} - mc \right)\chi.
    \end{equation}
    We are dealing with negative energies, so \(p^0 \to -p^0\), and by convention negative momenta, so \(\vv{p} \to -\vv{p}\), giving
    \begin{equation}
        p^0\chi = \left( c\frac{\vv{\sigma}\cdot\vv{p}}{E + mc^2} \right).
    \end{equation}
    
    We then define the two Dirac spinors \(w^{(3),(4)}(-p)\) to be
    \begin{equation}
        w^{(3),(4)}(-p) = \renewcommand{\arraystretch}{1.5}
        \begin{pmatrix}
            \dfrac{c\vv{\sigma}\cdot\vv{p}}{E + mc^2}\chi^{(1),(2)}\\
            \chi^{(1),(2)}
        \end{pmatrix}
    \end{equation}
    where by convention we choose the two linearly-independent spinors
    \begin{equation}
        \chi^{(1)} \coloneqq 
        \begin{pmatrix}
            0\\ 1
        \end{pmatrix}
        , \qqand \chi^{(2)} \coloneqq
        \begin{pmatrix}
            1\\ 0
        \end{pmatrix}
        .
    \end{equation}
    
    \subsection{Summary}
    For a particle with four momenta \(p^\mu_+ = (E/c, \vv{p})\) the solutions are
    \begin{equation}
        w^{(1)}(p) =
        \begin{pmatrix}
            1\\ 0\\
            \dfrac{cp^3}{E + mc^2}\\[1.5ex]
            \dfrac{c(p^1 + ip^2)}{E + mc^2}
        \end{pmatrix}
        , \qqand w^{(2)}(p) = 
        \begin{pmatrix}
            0\\ 1\\
            \dfrac{c(p^1 - ip^2)}{E + mc^2}\\[1.5ex]
            \dfrac{-cp^3}{E + mc^2}
        \end{pmatrix}
        .
    \end{equation}
    For a particle with four momenta \(p^\mu_- = (-E/c, -\vv{p}) = -p^\mu_+\) the solutions are
    \begin{equation}
        w^{(3)}(-p) =
        \begin{pmatrix}
            \dfrac{c(p^1 - ip^2)}{E + mc^2}\\[1.5ex]
            \dfrac{-cp^3}{E + mc^2}\\
            0\\ 1
        \end{pmatrix}
        , \qqand w^{(4)}(-p) = 
        \begin{pmatrix}
            \dfrac{cp^3}{E + mc^2}\\[1.5ex]
            \dfrac{c(p^1 + ip^2)}{E + mc^2}\\
            1\\ 0
        \end{pmatrix}
        .
    \end{equation}
    
    Recall that \(E > 0\), and so we still have solutions with negative energies, however the probability density is now positive definite so we have solved one problem with the Klein--Gordon equation, and we will see later that the negative energy solutions actually have a physical meaning.
    
    \begin{wrn}
        Conventions on labelling the spinors \(w^{(i)}\) vary so be careful to check which is being used.
    \end{wrn}
    
    \section{Rest Frame Solutions}
    For \(\vv{p} = \vv{0}\) we have
    \begin{equation}
        w^{(1)} = 
        \begin{pmatrix}
            1\\ 0\\ 0\\ 0
        \end{pmatrix}
        , \quad
        w^{(2)} = 
        \begin{pmatrix}
            0\\ 1\\ 0\\ 0
        \end{pmatrix}
        , \quad
        w^{(3)} = 
        \begin{pmatrix}
            0\\ 0\\ 0\\ 1
        \end{pmatrix}
        , \quad\text{and}\quad w^{(4)} = 
        \begin{pmatrix}
            0\\ 0\\ 1\\ 0
        \end{pmatrix}
        .
    \end{equation}
    The positive energy solutions are then
    \begin{equation}
        \psi^{(1)} = \e^{-imc^2t/\hbar} 
        \begin{pmatrix}
            1\\ 0\\ 0\\ 0
        \end{pmatrix}
        , \qqand \psi^{(2)} = \e^{-imc^2t/\hbar}
        \begin{pmatrix}
            0\\ 1\\ 0\\ 0
        \end{pmatrix}
        .
    \end{equation}
    These are degenerate in energy.
    This means there must be some other conserved operator which commutes with the Hamiltonian, which for \(\vv{p} = \vv{0}\) is \(\operator{\hamiltonian} = \beta mc^2\), which is diagonal.
    The eigenvalues of this operator will then label the two energy eigenstates allowing us to distinguish between them.
    
    One such operator is
    \begin{equation}
        \operator{\Sigma}^3 \coloneqq
        \begin{pmatrix}
            \sigma^3 & 0\\
            0 & \sigma^3
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & -1 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 0 & 0 & -1
        \end{pmatrix}
        = \ident \otimes \sigma^3.
    \end{equation}
    The rest frame four-component spinors \(w^{(i)}(p^0, \vv{0})\) are eigenvectors of \(\operator{\Sigma}^3\) with eigenvalues \(\pm 1\).
    
    \subsection{Spin}
    The presence of the Pauli matrices suggests that we interpret the Dirac equation as describing a spin \(1/2\) particle.
    As the \(\operator{\Sigma}^3\) notation suggests we now introduce the \(4\times 4\) matrices
    \begin{equation}
        \operator{\Sigma}^i \coloneqq 
        \begin{pmatrix}
            \sigma^i & 0\\
            0 & \sigma^i
        \end{pmatrix}
        = \ident \otimes \sigma^i
    \end{equation}
    or with a slight abuse of notation
    \begin{equation}
        \vecoperator{\Sigma} = 
        \begin{pmatrix}
            \vv{\sigma} & 0\\
            0 & \vv{\sigma}
        \end{pmatrix}
        .
    \end{equation}
    
    We can easily check that
    \begin{equation}
        \left( \frac{1}{2}\hbar\vecoperator{\Sigma} \right)^2 = \frac{3}{4}\hbar^2\ident = \frac{1}{2}\left( \frac{1}{2} + 1 \right)\hbar^2I
    \end{equation}
    and \(\hbar\operator{\Sigma}^3/2\) has eigenvalues \(\pm \hbar/2\).
    These are the required properties to interpret
    \begin{equation}
        \vecoperator{s} \coloneqq \frac{1}{2}\hbar\vecoperator{\Sigma}
    \end{equation}
    as the spin operator for a particle described by the Dirac equation, which necessarily has spin \(1/2\).
    
    We then use the eigenvalues of \(\operator{s}^3 = \hbar\operator{\Sigma}^3/2\) to distinguish between the two degenerate energy eigenstates.
    
    Note that \(\vecoperator{\Sigma}\) only commutes with the Hamiltonian in the rest frame.
    In other frames we have an extra \(c\vv{\alpha} \cdot \vv{p}\) term which in general doesn't commute with \(\vecoperator{\Sigma}\) for \(\vv{p} \ne \vv{0}\).
    This means that the expectation of \(\vecoperator{\Sigma}\) is not a conserved quantity for \(\vv{p} \ne \vv{0}\).
    
    The operator \(\vecoperator{L} \coloneqq \vecoperator{r}\times\vecoperator{p}\) also doesn't commute with the Hamiltonian in any frame other than the rest frame and so orbital angular momentum is also not a conserved quantity in frames other than the rest frame.
    
    The operator
    \begin{equation}
        \vv{J} \coloneqq \vecoperator{L} + \frac{1}{2}\hbar\vecoperator{\Sigma} = \vecoperator{L} + \vecoperator{s}
    \end{equation}
    does commute with the Hamiltonian in all frames.
    We interpret \(\vecoperator{J}\) as the total angular momentum and this is a conserved quantity.
    
    \subsection{Helicity}
    The choice of \(\vecoperator{\Sigma}\) and related quantities is not the only way to distinguish between the two degenerate eigenstates.
    We could also use the \define{helicity operator} which is a momentum-space operator defined as
    \begin{equation}
        \operator{\hamiltonian}(\vv{p}) \coloneqq
        \begin{pmatrix}
            \dfrac{\vv{\sigma}\cdot\vv{p}}{\abs{\vv{p}}} & 0\\
            0 & \dfrac{\vv{\sigma}\cdot\vv{p}}{\abs{\vv{p}}}
        \end{pmatrix}
        \propto \frac{\vecoperator{\Sigma} \cdot \vv{p}}{\abs{\vv{p}}}.
    \end{equation}
    This has eigenvalues \(\pm 1\) which give the projection of the particle's spin along the direction of motion, which is given by the unit vector \(\vv{p}/\abs{\vv{p}}\).
    
    It is possible to choose plane wave states with \(\vv{p} \ne \vv{0}\) to be eigenstates of the helicity operator.
    
    \section{Interpretation of Negative Energy Solutions}
    We've seen that the plane wave solutions to the Dirac equation satisfy the energy-momentum relation
    \begin{equation}
        p^0c = \pm \sqrt{\vv{p}^2c^2 + m^2c^4}.
    \end{equation}
    This means that either \(p^0c \ge mc^2\), or \(p^0c \le -mc^2\), since \(\vv{p}^2\) is non-negative.
    There is then a continuum of positive energy states with the lowest energy being \(mc^2\), and a continuum of negative energy states with the highest energy being \(-mc^2\).
    
    The Dirac equation describes spin \(1/2\) particles, which we will assume are electrons for simplicity.
    Suppose that the negative energy solutions are valid.
    Then we have the following problem: what stops a positive energy electron from transitioning to a negative energy state?
    
    One solution to this problem was proposed by Dirac, and is known as the \defineindex{Dirac sea}.
    Dirac proposed that the negative energy states are all filled, each energy level containing two electrons (with opposite spins).
    The Pauli exclusion principle is then what prevents positive energy electrons from transitioning to negative energies.
    
    In this picture the ground, or vacuum, state is an infinite sea of negative energy electrons.
    Dirac proceeded to argue that this sea is unobservable and we can only make measurements relative to the vacuum.
    
    One immediate consequence of this idea is we can excite a negative energy particle from the sea to a positive energy state.
    For example, a negative energy electron could absorb photons with total energy greater than \(2mc^2\), the electron will then be excited to a positive energy state.
    The result is that we observe an electron of charge \(-e\) and energy \(E_1 > 0\), and also a \defineindex{hole} in the sea.
    This hole, which is just the absence of an electron in a state with charge \(-e\) and energy \(-E_2 < 0\), can be interpreted as a particle with charge \(e\) and energy \(E_2 > 0\).
    That is as a positive energy \defineindex{antiparticle}, in the case of an electron the hole can be interpreted as a \defineindex{positron}.
    
    The minimum energy required for this process is \(2mc^2\), and the result is the production of an electron-positron pair.
    This means we can interpret this process as \defineindex{pair production}.
    
    This whole thought process lead Dirac to predict the existence of antimatter.
    Just a few years after suggesting this the positron was discovered confirming Dirac's prediction.
    We now know that all fundamental particles have antiparticles.
    
    Notice that we started with the Dirac equation describing a single particle but the existence of negative energy states forces us to consider a many-particle interpretation.
    Quantum mechanics with a fixed number of particles is inadequate for this.
    We need quantum field theory.
    
    The absence of a negative energy, spin up, particle in its rest frame can be interpreted as a positive energy, spin down, particle.
    This is one reason why we chose \(\chi^{(1),(2)}\) to be \((0, 1)^\trans\), and \((1, 0)^\trans\) respectively, rather than the more obvious initial choice of \((1, 0)^\trans\) and \((0, 1)^\trans\).
    
    The Dirac sea picture doesn't work for bosons, since there is no Pauli exclusion principle to stop the positive energy particles from decaying to negative states.
    
    We can multiply the probability density by the particle charge so that \(\rho \propto eE\), if we then change the charge to \(-e\) and the energy to \(-E\) we still have \(\rho \propto (-e)(-E) = eE\).
    This means that particles with momentum \(p^\mu\) are equivalent to antiparticles with momentum \(-p^\mu\).
    This works for both bosons and fermions.
    
    A more general interpretation is the Feynman--St\"uckelberg interpretation, in this for negative energy solutions we replace \(t \to -t\) and \(\vv{r} \to -\vv{r}\), so that the wave function doesn't change:
    \begin{equation}
        \e^{-i((-E)t - (-\vv{p})\cdot\vv{r})/\hbar} \to \e^{-i(E(-t) - \vv{p}\cdot(-\vv{r}))/\hbar}.
    \end{equation}
    This means we can interpret negative energy particles (that is antiparticles) moving forward in time as positive energy particles (that is normal particles) moving backwards in time.
    
    These observations and requirements lead to (relativistic) quantum field theory, (RQFT)\glossary[acronym]{RQFT}{relativistic quantum field theory}.
    This is beyond the scope of this course
    \footnote{}{For an introduction to QFT exploring some of these ideas, particularly the particle-going-backward-in-time-is-antiparticle idea see the notes from the particle physics part of the relativity, nuclear, and particle physics course.}.
    
    \section{Covariant Form}
    So far we have been writing the Dirac equation as
    \begin{equation}
        i\hbar\diffp{}{t}\psi(\vv{r}, t) = (-i\hbar c\vv{\alpha} \cdot \grad + \beta mc^2)\psi(\vv{r}, t).
    \end{equation}
    This form doesn't demonstrate the Lorentz invariance of the Dirac equation.
    A better form to demonstrate this is the covariant form that we will find in this section.
    
    First define the \define{gamma matrices}\index{gamma matrix}
    \begin{equation}
        \gamma^0 \coloneqq \beta, \qqand \gamma^i \coloneqq \beta\alpha^i.
    \end{equation}
    Rearranging the Dirac equation to get everything on the same side and then left multiplying by \(\beta\) we get
    \begin{equation}
        \left[ i\hbar\left( \beta\diffp{}{t} + c \beta\vv{\alpha}\cdot\grad \right) - \beta^2 mc^2 \right]\psi(\vv{r}, t) = 0.
    \end{equation}
    Recalling that \(\beta^2 = \ident\), and leaving the factor of \(\ident\) implicit, as well as dividing by \(c\) we get
    \begin{equation}
        \left[ i\hbar\left( \beta\frac{1}{c}\diffp{}{t} + \beta \alpha^i\partial_i \right) - mc \right]\psi(\vv{r}, t) = 0.
    \end{equation}
    Identifying the gamma matrices and changing from \(t\) and \(\vv{r}\) to four-vector notation we get
    \begin{equation}
        \left[ i\hbar\left( \gamma^0\diffp{}{x^0} + \gamma^i\diffp{}{x^i} \right) - mc \right] \psi(x) = 0.
    \end{equation}
    More compactly
    \begin{align}
        (i\hbar \gamma^\mu \partial_\mu - mc)\psi(x) &= 0,\\
        (i\gamma^\mu \partial_\mu - m)\psi(x) &= 0. \natunit
    \end{align}
    
    \begin{ntn}{Feynman Slash Notation}{}
        We now introduce the \defineindex{Feynman slash notation}\index{slash notation|see{Feynman slash notation}}, where for a four-vector, \(a^\mu\), we define
        \begin{equation}
            \slashed{a} \coloneqq \gamma^\mu a_\mu = \gamma_\mu a^\mu.
        \end{equation}
    \end{ntn}
    
    Using Feynman slash notation we can write the covariant form the Dirac equation incredibly compactly:
    \begin{align}
        (i\hbar\slashed{\partial} - mc)\psi(x) &= 0,\\
        (i\slashed{\partial} - m)\psi(x) &= 0. \natunit
    \end{align}
    
    The positive energy plane wave solutions are of the form 
    \begin{equation}
        \psi(x) = \e^{ip\cdot x/\hbar}u(p, s),
    \end{equation}
    where we now change the notation we use to \(u(p, s) = w^{(s)}(p)\), for \(s = 1, 2\).
    Similarly the negative energy solutions are of the form
    \begin{equation}
        \psi(x) = \e^{ip\cdot x/\hbar}v(p, s)
    \end{equation}
    where \(v(p, s) = w^{(s + 2)}(-p)\) for \(s = 1, 2\).
    
    Substituting these plane wave solutions into the Dirac equation and then cancelling out the common exponential terms we get
    \begin{equation}
        (\gamma^\mu p_\mu - mc)u(p, s) = (\slashed{p} - mc)u(p, s) = 0,
    \end{equation}
    and
    \begin{equation}
        (\gamma^\mu p_\mu + mc)v(p, s) = (\slashed{p} - mc)v(p, s) = 0.
    \end{equation}
    
    It can be shown that the gamma matrices satisfy the anticommutation relations
    \begin{equation}
        \gamma^\mu \gamma^\nu + \gamma^\nu\gamma^\mu = \anticommutator{\gamma^\mu}{\gamma^\nu} = 2\eta^{\mu\nu}.
    \end{equation}
    
    \itshape
    These anticommutation relations mean that the gamma matrices satisfy the requirements to represent the Clifford algebra \(\mathrm{Cl}_{1,3}(\reals)\) (here \(1, 3\) means there is 1 time dimension and 3 space dimensions).
    This Clifford algebra over space time, \(V\), can be thought of as the set of real linear operators from \(V\) to itself, also known as \(\mathrm{End}(V)\).
    
    A real Clifford algebra equips \(\reals^n\) with a quadratic form, \(Q\colon\reals^n \to \reals\), where, with a slight abuse of notation, we write \(Q(x) = \vv{x}^\trans Q \vv{x}\) for some symmetric \(n\times n\) matrix \(Q\).
    Since \(Q\) is symmetric its eigenvalues are real.
    We also require that \(Q\) is nondegenerate, so its eigenvalues are all distinct.
    \(Q\) will then have \(p\) positive eigenvalues and \(q\) negative eigenvalues, and \(p + q = n\).
    We call \((p, q)\) the signature of \(Q\), in our case \(p = 1\) and \(q = 3\).
    
    We can define an orthonormal basis for \(\reals^n\), such as \(\{e_1, e_2, \dotsc, e_n\}\).
    When we say orthonormal here we mean with respect to \(Q\), so \(e_i^\trans Qe_j = 0\) if \(i \ne j\), and \(e_i^\trans Qe_i = \pm 1\) with \(+1\) if \(i = 1, \dotsc, p\), and \(-1\) if \(i = p+1, \dotsc, n\).
    
    We then define multiplication to be associative and satisfy \(e_i^2 = \pm 1\), with \(+1\) if \(i = 1, \dotsc, p\) and \(-1\) if \(i = p + 1, \dotsc, n\), and anticommutativity, so \(e_ie_j = e_je_i\).
    This allows us to define any product of basis vectors.
    We write \(e_{A} = e_{i_1}e_{i_2} \dotsm e_{i_m}\) where \(A = i_1i_2\dotsm i_m\).
    
    The real Clifford algebra \(\mathrm{Cl}_{p, q}(\reals)\) is then the vector space spanned by the products \(e_A\) where \(A\) is an arbitrary subset of \(\{1, \dotsc, n\}\).
    \normalfont
    
    With the standard representation for \(\alpha^i\) and \(\beta\) we have
    \begin{equation}
        \gamma^i = 
        \begin{pmatrix}
            0 & \sigma^i\\
            -\sigma^i & 0
        \end{pmatrix}
        = i\sigma^2 \otimes \sigma^i,
    \end{equation}
    and
    \begin{equation}
        \gamma^0 = 
        \begin{pmatrix}
            \ident & 0\\
            0 & -\ident
        \end{pmatrix}
        = \sigma^3 \otimes \ident.
    \end{equation}
    Writing these out in full the gamma matrices are
    \begin{alignat}{3}
        \gamma^0 &\coloneqq 
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & -1 & 0\\
            0 & 0 & 0 & -1
        \end{pmatrix}
        , \qquad &
        \gamma^1 &= 
        \begin{pmatrix}
            0 & 0 & 0 & 1\\
            0 & 0 & 1 & 0\\
            0 & -1 & 0 & 0\\
            -1 & 0 & 0 & 0
        \end{pmatrix}
        ,\\
        \gamma^2 &\coloneqq 
        \begin{pmatrix}
            0 & 0 & 0 & \mathrlap{-i}\hphantom{-1}\\
            0 & 0 & i & 0\\
            0 & i & 0 & 0\\
            \mathrlap{-i}\hphantom{-1} & 0 & 0 & 0
        \end{pmatrix}
        , \qquad &
        \gamma^3 &= 
        \begin{pmatrix}
            0 & 0 & 1 & 0\\
            0 & 0 & 0 & -1\\
            -1 & 0 & 0 & 0\\
            0 & 1 & 0 & 0
        \end{pmatrix}
        .
    \end{alignat}
    
    From this it is easy to show that \(\gamma^0\) is Hermitian, \({\gamma^0}^\hermit = \gamma^0\), and \(\gamma^i\) are anti-Hermitian, \({\gamma^i}^\hermit = -\gamma^i\).
    This means that \((\gamma^0)^2 = \ident\), and \((\gamma^i)^2 = -\ident\).
    
    \begin{ntn}{Bar Notation}{}
        Given some spinor, \(u\), we define
        \begin{equation}
            \overline{u} \coloneqq u^\hermit \gamma^0.
        \end{equation}
    \end{ntn}
    
    The notation we have introduced in this section treats space and time even more equally.
    This is known as the \defineindex{covariant formulation}.
    
    It is possible to derive properties of the Dirac equation under Lorentz boosts and verify that the conserved current
    \begin{equation}
        j^\mu = (\psi^\hermit \psi, \psi^\hermit \vv{\alpha} \psi) = \overline{\psi} \gamma^\mu \psi
    \end{equation}
    transforms as a four-vector under Lorentz transforms, which was what we concluded after deriving the continuity equation.
    We can also show that \(\overline{\psi}\psi\) is invariant under Lorentz transformations, that is its a Lorentz scalar.
    
    \section{General Free Particle Solution and RQFT}
    In QFT we consider the general solution to the Dirac equation for a free particle to be a superposition of plane wave solutions, allowing both positive and negative energies.
    We write the solution as
    \begin{equation}
        \psi(x) = \sum_{s = 1, 2} \int \frac{\dl{^3p}}{(2\pi)^32E_p} [a_s(\vv{p})u(p, s)\e^{-ip\cdot x/\hbar} + b_s^*v(p, s)\e^{ip\cdot x/\hbar}].
    \end{equation}
    This looks complicated but isn't too bad, it's just an integral over the solutions with momenta \(\pm p^\mu\), and sum over the spins to account for all possible plane waves.
    The normalisation factor \(E_p = \sqrt{\vv{p}^2c^2 + m^2c^4}\) is just a matter of convention.
    The coefficient functions, \(a_s(\vv{p})\) and \(b_s^*(\vv{p})\) are important in QFT where they become particle/antiparticle annihilation/creation operators, \(\operator{a}_s\) and \(\operator{b}_s^\hermit\).
    
    The Hamiltonian is then
    \begin{equation}
        \operator{\hamiltonian} = \sum_{s = 1, 2} \int \frac{\dl{^3p}}{(2\pi)^2 2E_p} E_p[\operator{a}_s^\hermit(\vv{p}) \operator{a}_s(\vv{p}) + \operator{b}^\hermit(\vv{p})\operator{b}_s(\vv{p})].
    \end{equation}
    This is always positive definite.
    
    It is also possible to write the free particle solutions to the Klein--Gordon equation in this way:
    \begin{equation}
        \varphi(x) = \int \frac{\dl{^3p}}{(2\pi)^32E_p} [a(\vv{p}) \e^{-ip\cdot x/\hbar} + b^\hermit \e^{ip\cdot x/\hbar}].
    \end{equation}
    
    \chapter{Dirac Equation in an EM Field}\glossary[acronym]{EM}{electromagnetic}
    The Dirac equation (or Klein--Gordon equation) for a particle of charge \(q\) interacting with an electromagnetic field described by the four-potential \(A^\mu = (A^0, \vv{A})\), can be obtained from the \defineindex{minimal coupling prescription}:
    \begin{equation}
        p^\mu \to p^\mu - \frac{q}{c}A^\mu.
    \end{equation}
    This is a generalisation of the prescription followed for a nonrelativistic particle:
    \begin{equation}
        H = E = \frac{\vv{p}^2}{2m} \to \frac{1}{2m}\left( \vv{p} - \frac{q}{c}\vv{A} \right) + q\varphi,
    \end{equation}
    where we identify \(\varphi = A^0\) and write this in four-vector notation.
    See \cref{sec:hamiltonian} for more details.
    
    We then have
    \begin{equation}
        \vecoperator{p} = -i\hbar\grad \to \vecoperator{p} - \frac{1}{c}\vv{A} = -i\hbar\grad - \frac{q}{c}\vv{A}
    \end{equation}
    and
    \begin{equation}
        \operator{p}^0 = i\hbar\frac{1}{c}\diffp{}{t} \to \operator{p}^0 - \frac{1}{c}A^0 = \frac{1}{c}\left( i\hbar\diffp{}{t} - qA^0 \right).
    \end{equation}
    
    The Dirac equation then becomes
    \begin{equation}
        i\hbar\diffp{}{t} \psi(\vv{r}, t) = \left[ c\vv{\alpha} \cdot \left( \vv{p} - \frac{q}{c}\vv{A} \right) + \beta mc^2 + qA^0 \right] \psi(\vv{r}, t) = \operator{\hamiltonian}\psi(\vv{r}, t).
    \end{equation}
    
    \section{Pauli Equation}
    We will investigate the nonrelativistic limit of the Dirac equation for a charged particle in an EM field.
    Recall that a nonrelativistic description of spin requires two-component spinors.
    For the free particle, positive energy solution we had
    \begin{equation}
        \psi \propto 
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
    \end{equation}
    where
    \begin{equation}
        \chi = \frac{c\vv{\sigma}\cdot\vv{p}}{E + mc^2}\varphi.
    \end{equation}
    From this in the nonrelativistic limit \(\vv{p} \propto \vv{v}\) and \(E \ll mc^2\), we then have
    \begin{equation}
        \frac{c\vv{\sigma}\cdot\vv{p}}{E + mc^2} \approx \frac{c\vv{\sigma} \cdot m\vv{v}}{mc^2} \approx \frac{v}{c} \implies \chi \approx \frac{v}{c}\varphi.
    \end{equation}
    This suggests that we can write
    \begin{equation}
        \psi(\vv{r}, t) = 
        \begin{pmatrix}
            \Psi(\vv{r}, t)\\ \Phi(\vv{r}, t)
        \end{pmatrix}
        \e^{-imc^2t/\hbar},
    \end{equation}
    and regard the upper component as \enquote{large} and the lower component as \enquote{small} in the nonrelativistic limit.
    
    For the positive solution we write the Hamiltonian as \(\operator{\hamiltonian} = \operator{\hamiltonian}' + mc^2\) where \(\operator{\hamiltonian}\) is the Dirac equation Hamiltonian, so
    \begin{equation}
        \operator{\hamiltonian}' = c\vv{\alpha} \cdot \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right) + \beta mc^2 + qA^0 - mc^2.
    \end{equation}
    
    In two component notation the Dirac equation is then
    \begin{equation}
        \operator{\hamiltonian}'
        \begin{pmatrix}
            \Psi\\ \Phi
        \end{pmatrix}
        =
        \begin{pmatrix}
            qA^0 & c\vv{\sigma} \cdot \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)\\
            c\vv{\sigma} \cdot \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right) & qA^0 - 2mc^2
        \end{pmatrix}
        \begin{pmatrix}
            \Psi\\ \Phi
        \end{pmatrix}
        .
    \end{equation}
    This gives
    \begin{align}
        \operator{\hamiltonian}'\Psi &= c\vv{\sigma} \cdot \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right) \Phi + qA^0\Psi,\\
        \operator{\hamiltonian}'\Phi &= c\vv{\sigma} \cdot \left( \vecoperator{p} - \frac{1}{c}\vv{A} \right) \Psi + qA^0\Phi - 2mc^2\Phi.
    \end{align}
    We can rewrite the second of these as
    \begin{equation}
        (\operator{H'} - qA^0 + 2mc^2)\Phi = c\vv{\sigma} \cdot \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right) \Psi.
    \end{equation}
    
    Provided that the matrix elements of \(\operator{\hamiltonian}'\) are much less than \(mc^2\) and that \(\abs{qA^0} \ll mc^2\) we can drop these terms on the left hand side and so
    \begin{equation}
        \Phi \approx \frac{1}{2mc} \vv{\sigma} \cdot \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)\Psi.
    \end{equation}
    
    Substituting this into the first we then get
    \begin{equation}\label{eqn:precursor to pauli equation}
        \operator{\hamiltonian}'\Psi \approx \frac{1}{2m}\left[ \vv{\sigma} \cdot \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right) \right]^2 \Psi + qA^0 \Psi.
    \end{equation}
    The operator
    \begin{equation}
        \frac{1}{2m}\left[ \vv{\sigma} \cdot \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right) \right]^2 + qA^0
    \end{equation}
    is called the \defineindex{Pauli Hamiltonian}.
    
    \subsection{Reduction of the Pauli Hamiltonian}
    Consider the first term of the Pauli Hamiltonian:
    \begin{align}
        \left[ \vv{\sigma} \cdot \left( \vecoperator{p} - \frac{1}{c}\vv{A} \right) \right]^2 &= \sigma^i\left( \operator{p}^i - \frac{q}{c}A^i \right) \sigma^j\left( \operator{p}^j - \frac{q}{c}A^j \right)\\
        &= \sigma^i\sigma^j\left( \operator{p}^i - \frac{q}{c}A^i \right)\left( \operator{p}^j - \frac{q}{c}A^j \right)\\
        &= (\delta^{ij} + i\varepsilon^{ijk}\sigma^k)\left( \operator{p}^i - \frac{q}{c}A^i \right)\left( \operator{p}^j - \frac{q}{c}A^j \right)\\
        &= \delta^{ij}\left( \operator{p}^i - \frac{q}{c}A^i \right)\left( \operator{p}^j - \frac{q}{c}A^j \right)\notag\\
        &\qquad+ i\varepsilon^{ijk}\sigma^k\left( \operator{p}^i - \frac{q}{c}A^i \right)\left( \operator{p}^j - \frac{q}{c}A^j \right)\\
        &= \left( \operator{p}^i - \frac{q}{c}A^i \right)\left( \operator{p}^i - \frac{q}{c}A^i \right)\notag\\
        &\qquad+ i\varepsilon^{ijk}\sigma^k\left( \operator{p}^i - \frac{q}{c}A^i \right)\left( \operator{p}^j - \frac{q}{c}A^j \right)\notag\\
        &= \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)^2 + i\varepsilon^{ijk}\sigma^k\operator{p}^i\operator{p}^j + i\varepsilon^{ijk}\sigma^k\frac{q^2}{c^2}A^iA^j\notag\\
        &\qquad- i\varepsilon^{ijk}\frac{q}{c}(\operator{p}^iA^j + A^i\operator{p}^j).
    \end{align}
    We now notice that \(\operator{p}^i\operator{p}^j\) and \(A^iA^j\) are symmetric in \(i\) and \(j\) whereas \(\varepsilon^{ijk}\) is antisymmetric in \(i\) and \(j\), hence their product vanishes leaving us with
    \begin{equation}
        \left[ \vv{\sigma} \cdot \left( \vv{p} - \frac{1}{c} \right) \right] = \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)^2 - i\varepsilon^{ijk}\sigma^k\frac{q}{c}(\operator{p}^iA^j + A^i\operator{p}^j).
    \end{equation}
    Now considering the final term in brackets and adding in a test function, \(\psi\), we have
    \begin{align}
        (\operator{p}^iA^j + A^i\operator{p}^j)\psi &= -i\hbar(\partial^iA^j + A^i\partial^j)\psi\\
        &= -i\hbar(\partial^i(A^j\psi) + A^i\partial^j\psi)\\
        &= -i\hbar(\psi\partial^iA^j + A^j\partial^i \psi + A^i\partial^j \psi)
    \end{align}
    which shows that
    \begin{equation}
        \operator{p}^i A^j + A^i\operator{p}^j = -i\hbar(\partial^iA^j + A^j\partial^i + A^i\partial^j).
    \end{equation}
    Substituting this in we get
    \begin{equation}
        \left[ \vv{\sigma} \cdot \left( \vv{p} - \frac{1}{c} \right) \right] = \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)^2 - \hbar \varepsilon^{ijk}\sigma^k \frac{q}{c} (\partial^iA^j + A^j\partial^i + A^i\partial^j).
    \end{equation}
    Now noticing that \(\partial^iA^j + \partial^jA^i\) is symmetric in \(i\) and \(j\) and so its product with the antisymmetric \(\varepsilon^{ijk}\) vanishes we are left with
    \begin{equation}
        \left[ \vv{\sigma} \cdot \left( \vv{p} - \frac{1}{c} \right) \right] = \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)^2 - \hbar\varepsilon^{ijk}\sigma^k\frac{q}{c} \partial^iA^j.
    \end{equation}
    We now use
    \begin{align}
        \vv{B} = \curl \vv{A} \implies B^k = \varepsilon^{ijk}\partial^iA^j
    \end{align}
    which means that
    \begin{align}
        \left[ \vv{\sigma} \cdot \left( \vv{p} - \frac{1}{c} \right) \right] &= \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)^2 - \hbar\sigma^k\frac{q}{c}B^k\\
        &= \left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)^2 - \hbar\frac{q}{c} \vv{\sigma} \cdot \vv{B}.
    \end{align}
    
    Substituting this result into \cref{eqn:precursor to pauli equation} we get
    \begin{equation}
        i\hbar\diffp{}{t}\Psi = \left[ \frac{1}{2m}\left( \vecoperator{p} - \frac{q}{c}\vv{A} \right)^2 + qA^0 - \frac{\hbar}{2m}\frac{q}{c}\vv{\sigma} \cdot \vv{B} \right]\Psi.
    \end{equation}
    This is called the \defineindex{Pauli equation}.
    
    An alternative derivation starts by factoring out all time dependence,
    \begin{equation}
        \psi(\vv{r}, t) = 
        \begin{pmatrix}
            \Psi(\vv{r})\\ \Phi(\vv{r})
        \end{pmatrix}
        \e^{-iEt/\hbar},
    \end{equation}
    and then substituting \(i\hbar\diffp{}/{t} \to E'\) where \(E = E' + mc^2\).
    
    \section{Spin Term}
    The \defineindex{spin term} of the Pauli equation is the final term
    \begin{equation}
        -\frac{\hbar}{2m}\frac{q}{c} \vv{\sigma} \cdot \vv{B} = -\vv{m}\cdot\vv{B} = -\frac{q}{mc}\vecoperator{s}\cdot\vv{B} = g_{s}\frac{e}{2mc}\vecoperator{s}\cdot\vv{B}
    \end{equation}
    where \(\vecoperator{s} = \hbar\vv{\sigma}/2\) for an electron of charge \(q = -e\), and \(\vv{m} = \hbar q/(2mc)\vv{\sigma}\) is the magnetic moment.
    
    It follows that for this equality to hold we must have that the \defineindex{gyromagnetic ratio}m \(g_s\) is equal to 2.
    This agrees well with experiment.
    On the other hand a classical prediction would have \(g_s = 1\), since for a small uniformly charged sphere in classical physics we have \(\vv{m} = 1/(2mc)\vv{L}\).
    This means that we have to put in \(g_s = 2\) \enquote{by hand} when doing nonrelativistic atomic physics.
    
    When doing full QFT with the standard model we get small corrections due to virtual particles.
    These mean that the value of \(g_s\) is not quite \(2\), but it is close.
    Currently our best measurements for the value \(a = (g_s - 2)/2\), called the \defineindex{anomalous magnetic moment}, for both the electron and muon are
    \begin{align}
        a_{\mathrm{e}} &= \num{0.00115965218073(28)},\\
        a_{\upmu} &= \num{0.00116592061(41)}.
    \end{align}
    
    For the muon there is a currently a \(4.2\sigma\) discrepancy between the theoretical value, \(a_{\upmu} = \num{0.00116591828(50)}\), and the global experimental average value above.
    There is currently an experiment, called \(g - 2\), running at Fermilab which hopes to reduce this error by a factor of about 4.
    The initial results from this experiment give the experimental value \(a_{\upmu} = \num{0.00116592040(54)}\), which is within \(3.3\sigma\) of the theoretical value \cite{g-2}.
    
    This is an active area of research and it is not yet clear whether experimental results will align with theory or if this is a hint at physics beyond the standard model.
    
    It should be noted that the result \(g_s = 2\) doesn't \emph{require} relativity, although it arises naturally this way, we can instead start with the two-component wave equation
    \begin{equation}
        i\hbar\diffp{}{t}\Psi = \frac{(\vv{\sigma} \cdot \vecoperator{p})^2}{2m}\Psi.
    \end{equation}
    This is equivalent to the Schr\"odinger equation for a free particle since \((\vv{\sigma} \cdot \vecoperator{p})^2 = \vecoperator{p}^2\).
    The minimal coupling prescription will then yield the same result.
    However, this approach requires us to realise that this is the correct starting point, which is not obvious if we don't already know what result we are looking for, which requires relativity.
    
    \chapter{Fine Structure of Hydrogen}
    In this section we will apply the Dirac equation to a central potential, \(V(\vv{r}) = V(r)\), in particular we will focus on the Hydrogen atom, with the potential
    \begin{equation}
        qA^0(r) = V(r) = -\frac{e^2}{4\pi r}.
    \end{equation}
    There are two approaches that we can take: solve the Dirac equation exactly, or perform the nonrelativistc reduction early and hope we get the same result as solving the Dirac equation and then reducing to the nonrelativistic case.
    The first approach is long and technical, but not that different to the second pedagogically so we shall follow the second.
    
    \section{Nonrelatvistic Reduction}
    We will look for positive energy eigenstates to the Dirac equation.
    These will be of the form
    \begin{equation}
        \psi(\vv{r}, t) = \e^{iEt/\hbar}
        \begin{pmatrix}
            \Psi(\vv{r})\\ \Phi(\vv{r})
        \end{pmatrix}
        .
    \end{equation}
    Substituting this into the Dirac equation we get
    \begin{align}
        (E - V - mc^2) \Psi - c(\vv{\sigma} \cdot \vecoperator{p})\Phi &= 0,\\
        (E - V + mc^2) \Phi - c(\vv{\sigma} \cdot \vecoperator{p})\Psi &= 0.
    \end{align}
    Solving the second equation for \(\Phi\) we get
    \begin{equation}
        \Phi = \frac{c(\vv{\sigma} \cdot \vecoperator{p})}{E - V + mc^2} \Psi.
    \end{equation}
    Substituting this into the first equation, and taking care since \([\vecoperator{p}, V] \ne 0\), we get
    \begin{equation}
        (E - V - mc^2) \Psi = c(\vv{\sigma} \cdot \vecoperator{p}) \frac{1}{E - V + mc^2} c(\vv{\sigma} \cdot \vecoperator{p}) \Psi.
    \end{equation}
    We now define \(E' = E - mc^2\) and expand the fraction taking \(\varepsilon = (E' - V)/(2mc^2)\) to be small we can use
    \begin{multline}
        \frac{1}{E - V + mc^2} = \frac{1}{2mc^2 + E' - V} = (2mc^2 + E' - V)^{-1}\\
        = \frac{1}{2mc^2}\left( 1 + \varepsilon \right)^{-1}
        \approx \frac{1}{2mc^2} \left( 1 - \varepsilon \right) = \frac{1}{2mc^2} - \frac{E' - V}{2m^2c^4}.
    \end{multline}
    Substituting this in we get
    \begin{equation}
        E' \Psi = \left[ \frac{\vecoperator{p}^2}{2m} + V - \frac{(\vv{\sigma} \cdot \vecoperator{p})(E' - V)(\vv{\sigma} \cdot \vecoperator{p})}{4mc^2c^2} \right]\Psi.
    \end{equation}
    Here we have once again made use of the fact that \((\vv{\sigma} \cdot \vecoperator{p})^2 = \vecoperator{p}^2\), and that \(1/(2mc^2)\) commutes with \(\vecoperator{p}\).
    
    This isn't quite yet in a suitable form since \(E'\) appears on both sides.
    For nonrelativistic energy, \(E'\) is \(\order(v^2)\), since the kinetic energy reduces to the nonrelativistic kinetic energy.
    The third term is therefore \(\order(v^2(v/c)^2)\) compared to the first term, since the two factors of \(\vecoperator{p}\) will both give something order \(\order(v)\).
    
    We can iterate this equation to get the required form.
    To leading order we have
    \begin{equation}
        (E' - V) \Psi \approx \frac{\vecoperator{p}^2}{2m}\Psi.
    \end{equation}
    This means that we have
    \begin{align}
        (E' - V)(\vv{\sigma} \cdot \vecoperator{p})\Psi &= (\vv{\sigma} \cdot \vecoperator{p}) (E' - V) \Psi + \vv{\sigma} \cdot \commutator{(E'- V)}{\vecoperator{p}}\Psi\\
        &\approx (\vv{\sigma} \cdot \vecoperator{p}) \frac{\vecoperator{p}^2}{2m} \Psi + \vv{\sigma} \cdot \commutator{\vecoperator{p}}{V} \Psi.
    \end{align}
    Substituting this back into our equation we get
    \begin{equation}
        E' \Psi \approx \left[ \frac{\vecoperator{p}^2}{2m} + V - \frac{(\vecoperator{p}^2)^2}{8m^3c^2} - \frac{(\vv{\sigma} \cdot \vecoperator{p})(\vv{\sigma} \cdot \commutator{\vecoperator{p}}{V})}{4m^2c^2} \right]\Psi.
    \end{equation}
    Some further algebraic manipulation can show that this can be written as
    \begin{equation}
        E' \Psi = \left[ \frac{\vecoperator{p}^2}{2m} - \frac{(\vecoperator{p}^2)^2}{8m^3c^2} + V - \frac{i\vv{\sigma} \cdot (\vecoperator{p} \times \commutator{\vecoperator{p}}{V})}{4m^2c^2} - \frac{\vecoperator{p} \cdot \commutator{\vecoperator{p}}{V}}{4m^2c^2} \right]\Psi.
    \end{equation}
    
    The interpretation of the first two terms is simple, its just the nonrelativist kinetic energy and the first relativistic correction to the kinetic energy.
    The last two terms aren't so obvious.
    
    \section{Spin-Orbit Interaction}
    The fourth term is the spin-orbit interaction.
    To see this we use
    \begin{align}
        (\vecoperator{p} \times \commutator{\vecoperator{p}}{V})^i &= \varepsilon^{ijk}\operator{p}^j \commutator{\vecoperator{p}}{V}^k\\
        &= \varepsilon^{ijk}\operator{p}^j\commutator{-i\hbar\grad}{V}^k\\
        &= \varepsilon^{ijk}\operator{p}^j(-i\hbar)(\grad V - V\grad)^k
    \end{align}
    Considering the last factor with a test function, \(\psi\), we have
    \begin{equation}
        (\grad V - V\grad)\psi = \grad(V\psi) - V\grad\psi = V\grad \psi + \psi\grad V - V\grad\psi = \psi\grad V
    \end{equation}
    and so
    \begin{align}
        (\vecoperator{p} \times \commutator{\vecoperator{p}}{V})^i &= -i\hbar\varepsilon^{ijk}\operator{p}^j(\grad V)^k\\
        &= -i\hbar\varepsilon^{ijk}\operator{p}^j\partial^k V.
    \end{align}
    We can then use
    \begin{equation}
        \partial^k V(r) = r^k \frac{V'}{r}
    \end{equation}
    and so
    \begin{align}
        (\vecoperator{p} \times \commutator{\vecoperator{p}}{V})^{i} &= -i\hbar\varepsilon^{ijk}\operator{p}^jr^k\frac{V'}{r}\\
        &= i\hbar\frac{V'}{r}(\vv{r} \times \vecoperator{p})^i.
    \end{align}
    We then have
    \begin{align}
        \operator{\hamiltonian}_{\mathrm{SO}} &\coloneqq -\frac{i\vv{\sigma} \cdot (\vecoperator{p} \times \commutator{\vecoperator{p}}{V})}{4m^2c^2}\\
        &\hphantom{:}= \frac{\hbar}{4m^2c^2}\frac{V'}{r}\vv{\sigma} \cdot (\vv{r} \times \vecoperator{p})\\
        &\hphantom{:}= \frac{V'}{2m^2c^2r}\vecoperator{s} \cdot \vecoperator{L}.
    \end{align}
    Where we have identified the spin and angular orbital momentum operators.
    This term is proportional to \(\vecoperator{s}\cdot\vecoperator{L}\), which is a measure of how the spin and orbital angular momentum align, and so we call this the spin-orbit coupling term.
    
    For hydrogen we can substitute in \(V(r) = -e^2/(4\pi r)\), so the derivative gives a \(1/r^2\) term, which combines with the existing \(1/r\) term to give
    \begin{equation}
        \operator{\hamiltonian}_{\mathrm{SO}} = \frac{e^2}{8\pi m^2c^2r^3} \vecoperator{s} \cdot \vecoperator{L}.
    \end{equation}
    
    Notice how the spin-orbit term drops out naturally from the relativistic formalism when in the past we have had to add it in manually.
    
    In a more careful classical calculation we would include an extra \enquote{Thomas precession factor} of \(1/2\), which arises due to the electron being in a rotating frame with respect to the stationary nucleus.
    This term cancels with \(g_s = 2\) to give the same naive classical result.
    Importantly we don't have to consider this here since it drops out of the Dirac equation naturally since the Dirac equation is covariant.
    
    \section{Darwin Term}
    Defining \(\operator{\hamiltonian}'\Psi = E'\Psi\) and writing the fourth term as the spin-orbit term we have
    \begin{equation}
        \operator{\hamiltonian}'\Psi = \left[ \frac{\vecoperator{p}^2}{2m} - \frac{(\vecoperator{p}^2)^2}{8m^3c^2} + V + \frac{V'}{2m^2c^3r}\vecoperator{s}\cdot\vecoperator{L} - \frac{\vecoperator{p} \cdot \commutator{\vecoperator{p}}{V}}{4m^2c^2} \right]\Psi.
    \end{equation}
    The final term causes problems since it is not Hermitian.
    
    A non-Hermitian Hamiltonian means that the evolution is nonunitary.
    This means that the integral of the probability density,
    \begin{equation}
        \int \Psi^\hermit \Psi \dd{^3r}
    \end{equation}
    is not constant in time.
    However, the full probability density from the Dirac equation is \(\psi^\hermit \psi\) where \(\psi\) is the 4-component wave function.
    Therefore the correct probability conservation law is actually
    \begin{equation}
        \int \psi^\hermit \psi \dd{^3r} = \int (\Psi^\hermit \Psi + \Phi^\hermit \Phi) \dd{^3r}
    \end{equation}
    which must be constant in time.
    So far we have simply ignored the \(\Phi^\hermit \Phi\) part since we have been treating this term as \enquote{small}.
    
    What all of this means is that \(\Psi\) is not a good candidate for the wave function to this order, it was acceptable to \(\order(v^2)\) though.
    We need to account for the \enquote{small} component of the wave function.
    
    Recall that
    \begin{equation}
        \Phi = \frac{1}{E - V + mc^2} c(\vv{\sigma} \cdot \vecoperator{p}) \Psi \approx \frac{\vv{\sigma} \cdot \vecoperator{p}}{2mc}\Psi
    \end{equation}
    for \(E - V \ll mc^2\).
    We therefore have
    \begin{equation}
        \Phi^\hermit \Phi \approx \Psi^\hermit\frac{(\vv{\sigma} \cdot \vecoperator{p})}{2mc}\frac{(\vv{\sigma} \cdot \vecoperator{p})}{2mc}\Psi = \Psi^\hermit \frac{\vecoperator{p}^2}{4m^2c^2}\Psi.
    \end{equation}
    Substituting this into the probability density integral we have
    \begin{align}
        \int (\Psi^\hermit \Psi + \Phi^\hermit \Phi) \dd{^3r} &\approx \int \Psi^\hermit \left( 1 + \frac{\vecoperator{p}^2}{4m^2c^2} \right) \Psi \dd{^3r}\\
        &\approx \int \left[ \left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2}\Psi \right) \right]^\hermit \cdot \left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right) \dd{^3r}
    \end{align}
    where we have expanded \(1 + x = (1 + x/2)^2 + \order(x^2)\) and used the fact that \(\vecoperator{p}\) is Hermitian.
    
    We can now identify as a good candidate for the Pauli/Schr\"odinger Wave function
    \begin{equation}
        \Psi_{\mathrm{S}} \coloneqq \left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right)\Psi.
    \end{equation}
    
    We therefore have
    \begin{align}
        E'\Psi &= E'\left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right)^{-1}\Psi_{\mathrm{S}} = \operator{\hamiltonian}\Psi = \operator{\hamiltonian}'\left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right)^{-1}\Psi_{\mathrm{S}}\\
        \implies E'\Psi_{\mathrm{S}} &= \left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right)\operator{\hamiltonian}'\Psi = \left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right) \operator{\hamiltonian}' \left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right)^{-1}\Psi_{\mathrm{S}}.
    \end{align}
    Hence
    \begin{align}
        E'\Psi_{\mathrm{S}} &= \left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right) \operator{\hamiltonian}' \left( 1 + \frac{\vecoperator{p}^2}{8m^2c^2} \right)^{-1} \Psi_{\mathrm{S}}\\
        &\approx \left( \operator{\hamiltonian}' + \commutator*{\frac{\vecoperator{p}^2}{9m^2c^2}}{\operator{\hamiltonian}'} \right)\Psi_{\mathrm{S}}\\
        &\approx \left( \operator{\hamiltonian}' + \commutator*{\frac{\vecoperator{p}^2}{9m^2c^2}}{V} \right)\Psi_{\mathrm{S}}.
    \end{align}
    In order to evaluate the commutator we need only the \(\order(v^2)\) part of \(\operator{\hamiltonian}'\) since \(\vecoperator{p}^2/(8m^2c^2)\) is already \(\order(v^2/c^2)\) and we are only working to \(\order(v^2(v/c)^2)\).
    Therefore we define
    \begin{equation}
        \operator{\hamiltonian}_{\mathrm{S}} \coloneqq \operator{\hamiltonian}' + \commutator*{\frac{\vecoperator{p}^2}{8m^2c^2}}{V}
    \end{equation}
    to be the Schr\"odinger Hamiltonian.
    The extra term combines with the last term to give the \defineindex{Darwin term}\footnote{Named after Charles Galton Darwin, grandson of Charles Robert Darwin of natural selection fame. Darwin (the physicist) was at the University of Edingburgh when he worked on solving the Dirac equation for hydrogen.}
    \begin{align}
        \operator{\hamiltonian}_{\mathrm{Darwin}} &= \frac{1}{8m^2c^2} (-2\vecoperator{p} \cdot \commutator{\vecoperator{p}}{V} + \commutator{\vecoperator{p}^2}{V})\\
        &= -\frac{1}{8m^2c^2} \commutator{\vecoperator{p}^2 \cdot {}}{\commutator{\vecoperator{p}}{V}}\\
        &= \frac{\hbar^2}{8m^2c^2} \laplacian V.
    \end{align}
    Here we have used \(\commutator{\vecoperator{p}^2}{V} = \vecoperator{p} \cdot \commutator{\vecoperator{p}}{V} + \commutator{\vecoperator{p}}{V} \cdot \vecoperator{p}\) and then twice applied \(\commutator{\vecoperator{p}^i}{V} = -i\hbar\partial^iV\), which can be verified by adding a test function and expanding the commutator.
    
    For the Coulomb potential we use the result
    \begin{equation}
        \laplacian \frac{1}{r} = -4\pi \delta(\vv{r})
    \end{equation}
    where \(\delta\) is the Dirac delta distribution, which gives us
    \begin{equation}
        \operator{\hamiltonian}_{\mathrm{Darwin}} = \frac{e^2\hbar^2}{8m^2c^2}\delta(\vv{r}).
    \end{equation}
    
    The Darwin term affects only \(\mathrm{s}\) states since all other states vanish at the origin.
    
    \section{Notes}
    Now that we have these extra terms in the Hamiltonian we can treat them as perturbations to the Schr\"odinger equation\footnote{This was done in principles of quantum mechanics so see the notes for that course}.
    Doing so to lowest order we get
    \begin{equation}
        E_{nj} = mc^2\left[ 1 - \frac{\alpha^2}{2n^2} - \frac{\alpha^4}{2n^4}\left( \frac{n}{j + 1/2} - \frac{3}{4} \right) \right] + \order(\alpha^6)
    \end{equation}
    where \(\alpha \coloneqq e^2/(4\pi \hbar c)\) is the \defineindex{fine structure constant} and \(j\) is the total angular momentum quantum number.
    That is the eigenvalues of \(\vecoperator{J}^2\) are \(\hbar^2 j(j + 1)\) and \(\vecoperator{J} = \vecoperator{L} + \vecoperator{s}\).
    This result agrees well with experiments.
    
    Our solution is correct to \(\order(v^4/c^2)\), however, the Dirac equation can be solved exactly for the hydrogen atom and the resulting energy spectrum is
    \begin{equation}
        E_{nj} = mc^2\left[ 1 + \frac{\alpha^2}{(n - (j + 1/2) + \sqrt{(j + 1/2)^2 - \alpha^2})^2} \right]^{-1/2}.
    \end{equation}
    Expanding this in powers of \(\alpha\) we recover the \(\order(v^4/c^2)\) expression above plus higher order terms.
    
    Since this is exact we see that all states for some given \(n\) and \(j\) are degenerate to all orders in \(\alpha\).
    This suggests that there are other effects we aren't considering which lift this degeneracy, and indeed there are.
    Two such effects are
    \begin{itemize}
        \item The \defineindex{Lamb shift} which is due to the interaction of the electron with vacuum fluctuations of the electromagnetic field.
        This removes the remaining degeneracy between the \(2\mathrm{S}_{1/2}\) and \(2\mathrm{P}_{1/2}\) states.
        Recall that we use the notation \(nL_j\) for hydrogen energy levels, so \(2\mathrm{S}_{1/2}\) and \(2\mathrm{P}_{1/2}\) both have \(n = 2\) and \(j = 1/2\), with \(l = 0\) and \(l = 1\) respectively, from which we see they have \(s = -1/2\) and \(s = 1/2\) respectively.
        
        \item The \defineindex{hyperfine splitting} due to the magnetic spin-spin interaction between the spin \(1/2\) electron and spin \(1/2\) proton.
        This removes the degeneracy between the spin singlet (total spin \(s = 0\)) and spin triplet (total spin \(s = 1\)).
    \end{itemize}
    Including these terms the agreement with experiment is exceptional.
    
    The process of reducing the Dirac equation to a nonrelativistic Schr\"odinger is called the \defineindex{Foldy--Wouthuysen transformation}.
    It is historically important as it demonstrates that the Dirac equation naturally leads to the correct nonrelativistic limit of the hydrogen atom as demonstrated here.
    It is also used currently to study other interactions in the nonrelativistic limit, such as heavy quarks interacting with the gluon field in nonrelativistic QCD\glossary[acronym]{QCD}{quantum chromodynamics}.
    
%   Appdendix
    \appendixpage
    \begin{appendices}
        \include{parts/appendix-identities}
        \include{parts/appendix-formal-definitions}
        \include{parts/appendix-integrals}
        \include{parts/appendix-combinatorial-factor}
    \end{appendices}
    
    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
    \begin{thebibliography}{9}
        \bibitem{g-2} B.~Abi et al. \textit{Measurement of the Positive Muon Anomalous Magnetic Moment to \qty{0.46}{ppm}} 07/04/2021 Physics Review Letters \textbf{126}.14 pages 141801--141812 DOI: \url{https://link.aps.org/doi/10.1103/PhysRevLett.126.141801}
    \end{thebibliography}
\end{document}
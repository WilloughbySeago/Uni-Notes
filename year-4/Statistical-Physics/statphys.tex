\documentclass[fleqn]{NotesClass}

\strictpagecheck

\usepackage{siunitx}
\usepackage{csquotes}
\usepackage{blkarray}

% Tikz stuff
\usepackage{tikz}
\tikzset{>=latex}
% external
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
%\tikzexternaldisable
% other libraries
\usetikzlibrary{hobby}
\usetikzlibrary{calc}

% PGF plots
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% References, should be last things loaded
\usepackage{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{mathtools}
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

\diffdef{p}{long-var-wrap=dv}

% Title page info
\title{Statistical Physics}
\author{Willoughby Seago}
\date{February 18, 2022}
% \subtitle{}
% \subsubtitle{}

% Highlight colour
\definecolor{highlight}{HTML}{6B668E}
\definecolor{my blue}{HTML}{677C8E}
\definecolor{my red}{HTML}{8E6C67}
\definecolor{my green}{HTML}{678E6C}
\definecolor{my purple}{HTML}{8E6789}

% Commands
% Maths
\newcommand*{\boltzmann}{k_{\mathrm{B}}}
\newcommand*{\cpartition}{Z_{\mathrm{c}}}
\newcommand*{\gcpartition}{\mathcal{Z}_{\mathrm{gc}}}
\newcommand*{\e}{\mathrm{e}}
\newcommand*{\order}{\mathcal{O}}
\newcommand*{\ident}{I}
\newcommand*{\hermit}{\dagger}
\DeclareMathOperator{\tr}{tr}
\undef\Re
\DeclareMathOperator{\Re}{Re}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\artanh}{artanh}
\newcommand*{\hamiltonian}{\mathcal{H}}
\DeclarePairedDelimiterX{\poissonbracket}[2]{\lbrace}{\rbrace}{#1, #2}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1, #2}
\DeclareMathOperator{\Var}{Var}

% Include
\includeonly{}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/comparison-dos-crystal}
    \tableofcontents
    \listoffigures
    \mainmatter
    
    \chapter{Missing Information}
    \section{Probability}
    There are two common interpretations of probability.
    The first is the \defineindex{frequentist} approach, in which the probability of getting outcome \(i\), denoted \(p_i\), is defined by
    \begin{equation}
        p_i \coloneqq \lim_{N \to \infty} \frac{\text{number of times outcome \(i\) occurs}}{N}
    \end{equation}
    where \(N\) is the number of trials that we make.
    In this approach the probability of \(i\) is the proportion of trials that we expect to give \(i\) as a result.
    The frequentist definition is often the simplest, but it doesn't apply very well to one-off events.
    
    The second approach is \defineindex{Bayesian} statistics, in which \(p_i\) is a quantitative measure of the degree of belief that a rational observer has that the result of a given process will be \(i\).
    This approach can apply to a one-off event.
    
    Whichever approach we take there are some rules regarding the values that \(p_i\) can take.
    In particular if there are \(r\) mutually exclusive outcomes each with probability \(p_i\) for \(i \in \{1, \dotsc, r\}\) then
    \begin{itemize}
        \item \(p_i \in [0, 1]\), with \(0\) representing no chance of happening and \(1\) representing certainty.
        \item \(p_{i\text{ or } j} = p_i + p_j\), this only applies to mutually exclusive outcomes.
        \item \(\sum_{i=1}^{r} p_i = 1\), this is simply \(p_{1 \text{ or } 2 \text{ or } \dotsb \text{ or } r}\), which is to say that the probability that one of the outcomes occurs is certain, since something must happen.
        \item \(\expected{y} = \mean{y} = \sum_{i=1}^{r} p_iy_i\), this is the mean value of \(y\), which is a quantity that takes the value \(y_i\) when outcome \(i\) occurs.
    \end{itemize}
    
    \section{Entropy}
    There are various definitions of entropy, including, but not limited to
    \begin{itemize}
        \item a measure of disorder, and
        \item a measure of uncertainty.
    \end{itemize}
    The second definition here is more useful to us and by the end of this chapter we will have made it rigorous.
    
    \subsection{Thermodynamics Definition}
    We start with the entropy as defined in thermodynamics:
    \begin{equation}
        \dl{S} = \frac{\dl{Q_{\mathrm{rev}}}}{T}
    \end{equation}
    where \(\dl{Q_{\mathrm{rev}}}\) is the heat absorbed by the system during a reversible process and \(T\) is the temperature.
    This gives the change in entropy of the system, \(\dl{S}\).
    In thermodynamics we almost exclusively deal with changes in entropy rather than the absolute value of entropy.
    One useful thing that we find from this is that entropy has units of \([\mathrm{energy}][\mathrm{temperature}]^{-1}\), so typically we measure entropy in units of \unit{\joule\per\kelvin}.
    
    Recall that the second law of thermodynamics states that the entropy of an isolated system can only increase or stay the same, that is
    \begin{equation}
        \diffp{S}{t} \ge 0.
    \end{equation}
    
    \subsection{Boltzmann Entropy}
    The \define{Boltzmann entropy}\index{entropy!Boltzmann} is defined as
    \begin{equation}
        S_{\mathrm{B}} \coloneqq \boltzmann \ln \Omega.
    \end{equation}
    Here \(\boltzmann = \qty{1.380649e-23}{\joule\per\kelvin}\)\index{kB@\(\boltzmann\), Boltzmann's constant} (exact) is Boltzmann's constant.
    Recall that a macrostate describes the bulk properties of a system, such as temperature, pressure, or volume.
    A microstate describes the microscopic properties of a system, such as the velocity or energy of all the particles.
    In general for each macrostate there are multiple microstates which the system could be in and still have the same macroscopic properties.
    The quantity \(\Omega\) is then the \defineindex{weight} of the macrostate, which is the number of microstates that correspond to the macrostate.
    
    We expect that the entropy is an extrinsic property, that is the more matter we have the greater the entropy.
    In particular we expect that the entropy is of the form \(S = sN\), where \(N\) is the number of constituents in the system, for example the number of particles.
    We then have
    \begin{equation}
        \Omega = \exp\left( \frac{sN}{\boltzmann} \right).
    \end{equation}
    So \(\Omega\) is exponential in \(N\).
    Therefore the larger \(N\) is the larger \(\Omega\) is, and hence the larger \(S_{\mathrm{B}}\) is.
    This means we can interpret \(S_{\mathrm{B}}\) as a measure of the uncertainty about which microstate the system is in.
    
    \subsection{Gibbs Entropy}
    The \define{Gibbs entropy}\index{entropy!Gibbs} is defined as
    \begin{equation}
        S_{\mathrm{G}} \coloneqq -\boltzmann \sum_{\mathclap{\mathrm{microstates}, i}} p_i \ln p_i
    \end{equation}
    where the sum is over microstates and \(p_i\) is the probability that the system is in microstate \(i\).
    
    We can show that both the Boltzmann and Gibbs entropies agree when we have an isolated system in a given macrostate with \(\Omega\) microstates.
    With no further information the only sensible choice is \(p_i = 1/\Omega\), this is called the principle of equal \textit{a priori} probabilities.
    The sum for the Gibbs entropy then runs over all microstates associated with the given macrostate, meaning that it goes from 1 to \(\Omega\).
    Hence we have
    \begin{align}
        S_{\mathrm{G}} &= -\boltzmann \sum_{i=1}^{\Omega} \frac{1}{\Omega} \ln \frac{1}{\Omega}\\
        &= \boltzmann \sum_{i=1}^{\Omega} \frac{1}{\Omega} \ln \Omega\\
        &= \boltzmann \ln \Omega\\
        &= S_{\mathrm{B}}.
    \end{align}
    Here we have used the fact that the terms in the sum are constant so the sum from \(1\) to \(\Omega\) is just \(\Omega\) times this constant.
    
    The Gibbs entropy uses the probability of microstates, \(p_i\), whereas the Boltzmann entropy uses the weight of a macrostate.
    This means that the Gibbs entropy is a microscopic picture of entropy whereas the Boltzmann entropy is a macroscopic picture.
    This makes the Gibbs entropy a more appropriate definition for developing a fundamental understanding of entropy.
    
    The Gibbs entropy also has the advantage of applying to systems which are not large, this allows us to break systems up into smaller subsystems.
    The Gibbs entropy also applies to systems which are not in thermal equilibrium.
    
    \section{Missing Information}
    Information is a quantity measured in bits.
    A bit is a quantity that can take one of two value, usually we think of these as 0 or 1, or in physics we may consider spins being up or down.
    If a system has a single bit that can take either value then we have missing information, the value of the bit.
    Suppose we then apply some process which fixes the value of the bit, such as applying a magnetic field such that the spin aligns with the field.
    We then have gained some information since we now know the value of the bit, we have gained \qty{1}{\bit} of information.
    If we had started off knowing the value of the bit then we would not have gained any information.
    
    We wish to generalise this idea of missing some information to systems of more than two states where the probabilities of different states are not necessarily equal.
    
    In order to find the correct function to determine the missing information we now list some requirements of this function.
    To do so we consider the missing information of a system with \(r\) mutually exclusive outcomes, each with probability \(p_i\) of occurring.
    \begin{enumerate}
        \item The missing information should be a continuos function of the probabilities, \(p_i\).
        This means that changing one of the probabilities by only a very small amount should change the missing information by only a small amount, which makes intuitive sense.
        This means we are looking for a continuous function of the form
        \begin{equation}
            S \colon [0, 1]^r \to \reals.
        \end{equation}
        \item The missing information should be symmetric in the probabilities.
        This means if we label the probabilities differently the missing information doesn't change.
        This makes sense since the missing information should be independent of our choices, such as the way we order outcomes.
        \item For the case where \(p_1 = \dotsb = p_r = 1/r\) the missing information should reduce to an increasing function of \(r\).
        That is for equally likely outcomes the more possible outcomes there are the more information will be missing, since there is more uncertainty.
        \item The missing information should not change based on how we group the outcomes.
    \end{enumerate}
    
    This last point is non-trivial and we will expand upon it.
    We can divide the outcomes into \(n\) groups labelled \(j = 1, \dotsc, n\), such that each group has \(r_j\) outcomes and the probability of an outcome in group \(j\) is \(w_j\).
    We should then be able to write the missing information
    \begin{align}
        S(\{p\}_r) &= S(\{w\}_n) + w_1S\left( \frac{p_1}{w_1}, \dotsc, \frac{p_{r_{1}}}{w_1} \right) + w_2S\left( \frac{p_{r_1+1}}{w_2}, \dotsc, \frac{p_{r_1 + r_2}}{w_2} \right) + \dotsb\\
        &= S(\{w\}_{n}) + \sum_{j=1}^{n} w_jS\left( \frac{p_{r_1 + \dotsb + r_{j-1} + 1}}{w_j}, \dotsc, \frac{p_{r_1 + \dotsb + r_j}}{w_j} \right).\label{eqn:entropy grouping}
    \end{align}
    The interpretation here is that the first term, \(S(\{w\}_n)\) deals with the missing information about which group the outcome is in, and then the relevant second term deals with the missing information about which element within that group the outcome is.
    
    This becomes clearer with an example.
    Suppose that we have three outcomes, say possible energies of a particle, and these outcomes have associated probabilities \(p_1\), \(p_2\), and \(p_3\).
    Suppose that outcomes 1 and 2 differ only due to the spin of the particle, and that there is a second observer who can't make measurements as precisely and therefore can't differentiate between the first two outcomes.
    In this case all they can do is ascribe a single probability, \(p_1 + p_2\), that one of the first two outcomes occurs.
    The missing information is then
    \begin{align}
        S(p_1, p_2, p_3) &= S(w_1, w_2) + w_1S\left( \frac{p_1}{w_1} + \frac{p_2}{w_1} \right) + S\left( \frac{p_3}{w_2} \right)\\
        &= S(w_1, w_2) + w_1S\left( \frac{p_1}{w_1} + \frac{p_2}{w_1} \right)
    \end{align}
    where \(w_1 = p_1 + p_2\) and \(w_2 = p_3\).
    We have then used \(S(p_3/w_2) = S(1) = 0\).
    
    For an even more explicit example consider a procedure with four possible outcomes with probabilities \(p_1 = 1/6\), \(p_2 = 1/3\), and \(p_3 = p_4 = 1/4\).
    Suppose that the first two outcomes can be grouped together as can the second two.
    Then we expect
    \begin{align}
        S\left( \frac{1}{6}, \frac{1}{3}, \frac{1}{4}, \frac{1}{4} \right) &= S\left( \frac{1}{2}, \frac{1}{2} \right) + \frac{1}{2}S\left( \frac{1/6}{1/2}, \frac{1/3}{1/2} \right) + \frac{1}{2} S\left( \frac{1/4}{1/2}, \frac{1/4}{1/2} \right)\\
        &= S\left( \frac{1}{2}, \frac{1}{2} \right) + \frac{1}{2}S\left( \frac{1}{3}, \frac{2}{3} \right) + \frac{1}{2} S\left( \frac{1}{2}, \frac{1}{2} \right).
    \end{align}
    
    We make the following ansatz for the form of \(S\):
    \begin{equation}
        S(\{p\}_r) = \sum_{i=1}^{r} \varphi(p_i)
    \end{equation}
    for some function \(\varphi\).
    The fact that addition is commutative takes care of the requirement that \(S\) be symmetric.
    Requiring \(\varphi\) to be continuous also takes care of our continuity conditions.
    
    Noticing that adding in outcomes with zero probability cannot change the information content, since we know for certain that these outcomes won't occur, we see that we must have \(\varphi(0) = 0\).
    Similarly if one outcome is certain, so \(p_i = 1\) for some fixed value of \(i\), and all other outcomes cannot occur, so \(p_j = \delta_{ij}\), then we also have no missing information since the outcome is certain and so we must have \(\varphi(1) = 0\) also so that the sum is zero.
    
    Considering the case where \(p_i = 1/r\) we have
    \begin{equation}
        S(\{p\}_r) = \sum_{i=1}^{r} \varphi\left( \frac{1}{r} \right) = r\varphi\left( \frac{1}{r} \right).
    \end{equation}
    Now dividing the outcome into \(n\) groups with \(m\) outcomes, so \(r = mn\), we have that the probability of being in the \(j\)th group is \(w_j = 1/n = m/r\),.
    We then have
    \begin{equation}
        S(\{w\}_n) = \sum_{j=1}^{n} \varphi\left( \frac{1}{n} \right) = n\varphi\left( \frac{1}{n} \right)
    \end{equation}
    and
    \begin{align}
        \sum_{j=1}^{n} w_j S\left( \frac{\{p\}_m}{w_j} \right) &= \sum_{j=1}^{n} \frac{1}{n} \varphi\left( \frac{n}{r} \right)\\
         &= n \frac{1}{n}\varphi\left( \frac{n}{r} \right)\\
         &= m\varphi\left( \frac{1}{m} \right).
    \end{align}
    Hence \cref{eqn:entropy grouping} becomes
    \begin{equation}
        r\varphi\left( \frac{1}{r} \right) = \frac{r}{m} \varphi\left( \frac{m}{r} \right) + m \varphi\left( \frac{1}{m} \right).
    \end{equation}
    
    This isn't simple to solve, fortunately others have solved it and we can just check their solution:
    \begin{equation}
        \varphi(x) = -kx\ln x
    \end{equation}
    for some constant \(k\).
    With this form the left hand side becomes
    \begin{equation}
        r\varphi\left( \frac{1}{r} \right) = r\left( -k\frac{1}{r}\ln\frac{1}{r} \right) = k\ln r.
    \end{equation}
    The right hand side becomes
    \begin{align}
        \frac{r}{m} \varphi\left( \frac{m}{r} \right) + m \varphi\left( \frac{1}{m} \right)&= \frac{r}{m}\left( -k\frac{m}{r}\ln\frac{m}{r} \right) + m\left( -k\frac{1}{m}\ln\frac{1}{m} \right)\\
        &= k\ln\frac{r}{m} + k\ln\frac{1}{m} = k\ln r.
    \end{align}
    So we see that this is indeed a solution,
    We also have
    \begin{equation}
        \varphi(1) = -k\ln 1 = 0 
    \end{equation}
    and
    \begin{equation}
        \lim_{x\to 0} -kx\ln x = 0
    \end{equation}
    also.
    This therefore gives all the properties required for the missing information function:
    \begin{equation}
        S(\{p\}_r) = -k\sum_{i=1}^{r} p_i\ln p_i.
    \end{equation}
    It can be shown that this is the only function satisfying the necessary properties, up to the value of \(k\).
    We take \(k\) to be positive so that \(S\) is non-negative, since \(p_i \in [0, 1]\) and so \(\ln p_i \le 0\).
    It can be shown that \(S\) is additive, that \(S\) is maximised when all probabilities are equal, which is when we know the least about the system, and that \(S = 0\) when \(p_j = \delta_{ij}\) for some fixed \(i\), which is when we know everything about the system.
    
    To uniquely specify \(S\) all we need to do is pick a value for \(k\).
    When Claude Shannon first found this function he was studying information theory.
    For this reason he chose \(k = 1/\ln 2\).
    His reasoning being if he had a string of \(B\) bits, each being 0 or 1, so some element of \(\{0, 1\}^{B}\), then the total number of possible states for this string is \(2^{B}\).
    If all states are equally likely then we have
    \begin{equation}
        S = -k\sum_{i=1}^{2^B} \frac{1}{2^{B}} \ln \frac{1}{2^{B}} = k\sum_{i=1}^{2^B} \frac{1}{2^B}\ln 2^B = k2^B\frac{1}{2^B}\ln 2^B = k\ln 2^B = Bk\ln 2
    \end{equation}
    and so the choice of \(k = 1/\ln 2\) gives the missing information as \(S = B\), which is measured in bits.
    In this case we call this the \define{Shannon entropy}\index{entropy!Shannon}.
    
    On the other hand in statistical mechanics we typically take \(k = \boltzmann\), which has units of \([\boltzmann] = [\mathrm{energy}]/[\mathrm{temperature}]\), most commonly \unit{\joule \per \kelvin}.
    This gives
    \begin{equation}
        S = -\boltzmann \sum_{i=1}^{r} p_i\ln p_i = S_{\mathrm{G}}.
    \end{equation}
    That is with this choice we have that the missing information is exactly the Gibbs entropy.
    This is the precise meaning in the statement \enquote{entropy is a measure of disorder}, entropy is a measure of how much information we are missing from a system, which relates to the disorder since the more ordered a system is the more we know and the less information we are missing.
    
    \chapter{Formulating Statistical Mechanics}
    \section{Assignment of Probability}
    Suppose that we have a system with some constraints, for example the energy may be fixed, or the average of some value might be fixed.
    We want to assign probabilities, \(\{p\}_r\), to the states.
    The values of these probabilities should reflect only the information that we have available.
    Otherwise there would be some bias in the assignment and we cannot then argue that it is a rational assignment of the probabilities.
    To ensure that we aren't accidentally including more information in the assignment than we actually know we should aim to maximise the missing information.
    This leads to the following principle.
    \begin{important}
        Probabilities should be assigned to states such as to maximise \(S\) subject to known constraints.
    \end{important}
    
    For our purposes constraints usually take the form of fixed expectation values of some observables.
    We also always have the constraint that
    \begin{equation}
        \sum_{i=1}^{r} p_i = 1.
    \end{equation}
    We can maximise a quantity subject to constraint's using Lagrange multipliers.
    
    \subsection{Lagrange Multipliers}
    \begin{rmk}
        For more details about Lagrange multipliers see the notes from the Lagrangian dynamics course.
    \end{rmk}
    Suppose we wish to extremise some function, \(f\), which is a function of independent variables, \(x_i\).
    We want to find a point where \(f\) is stationary so we need
    \begin{equation}
        \dl{f} = \sum_{i=1}^{r} \diffp{f}{x_i}\dl{x_i} = 0.
    \end{equation}
    Suppose also that we have the constraint that \(g(\{x\}_r) = g_0 = \text{constant}\) for some function \(g\).
    We cannot therefore assume that \(\diffp{f}/{x_i} = 0\) gives the desired point.
    
    What we do is construct the function \(h(\{x\}_r) = f(\{x\}_r) - \lambda g(\{x\}_r)\) where \(\lambda\) is some constant.
    We then have
    \begin{equation}
        \dl{h} = \dl{f} - \lambda\dl{g} = \sum_{i=1}^{r} \left( \diffp{f}{x_i} - \lambda\diffp{g}{x_i}  \right)\dl{x_i} = 0.
    \end{equation}
    We can choose \(\lambda\) such that \(\diffp{f}/{x_r} - \lambda \diffp{g}/{x_r} = 0\).
    Since \(x_i\) are independent variables we can vary them separately and the sum must still give zero, it follows then that for this same value of \(\lambda\) we will have \(\diffp{f}/{x_i} - \lambda \diffp{g}/{x_i} = 0\).
    
    The general method then to minimise \(f\) subject to the constraint that \(g(\{x\}_r) = g_0\) is to define
    \begin{equation}
        h(\{x\}_r) = f(\{x\}_r) - \lambda g(\{x\}_r).
    \end{equation}
    Then extermise \(h\) by requiring that
    \begin{equation}
        \diffp{h}{x_i} = 0
    \end{equation}
    for all \(x_i\).
    
    We apply this method with \(x_i\) being the probabilities \(p_i\), and \(f\) being the entropy, \(S\).
    We then choose \(\lambda\) such that the constrained quantities have the desired values.
    If there are several constraints then each constraint gets its own Lagrange multiplier.
    
    It is easiest to see why this works geometrically.
    Suppose we want to extremise \(f(\vv{x})\) subject to the constraint \(g(\vv{x}) = 0\), if instead the constraint is \(g(\vv{x}) = g_0\) we can redefine \(g\), so \(g(\vv{x}) \to g(\vv{x}) - g_0\), to get the desired form.
    Recall that \(\dl{f} = \dl{\vv{x}} \cdot \grad f\).
    We must also choose \(\dl{\vv{x}}\) such that \(g\) doesn't change.
    This means that \(\dl{\vv{x}}\) must be along the level surfaces of \(g\).
    The level surfaces of \(g\) are perpendicular to \(\grad g\), therefore \(\dl{\vv{x}}\) must be perpendicular to \(\grad g\).
    This means that we must have \(\dl{\vv{x}} \cdot \grad g = 0\).
    
    Therefore at the extremum we have both \(\grad g\) and \(\grad f\) parallel, which means that for some value of \(\lambda\) we must have \(\grad f - \lambda \grad g = 0\).
    In component form this is
    \begin{equation}
        \diffp{f}{x_i} - \lambda \diffp{g}{x_i} = 0,
    \end{equation}
    which is exactly what we had before.
    
    \begin{exm}{}{exm:microcanonical partition func}
        Suppose we have no constraints other than the mandatory \(\sum_i p_i = 1\).
        We then have
        \begin{equation}
            h(\{p\}_r) = -k \sum_i p_i \ln p_i - \lambda \sum_i p_i.
        \end{equation}
        Taking the derivative we use
        \begin{equation}
            \diffp{}{x} x \ln x = \ln x + 1
        \end{equation}
        and so
        \begin{equation}
            \diffp{h}{p_j} = -k\ln p_j - k - \lambda = 0
        \end{equation}
        since all terms with \(p_i\) for \(i \ne j\) vanish in the derivative.
        Rearranging we have
        \begin{equation}
            p_j = \exp\left[ -1 - \frac{\lambda}{k} \right].
        \end{equation}
        Noticing that the right hand side here is constant we have
        \begin{equation}
            1 = \sum_{i=1}^{r} p_i = \sum_{i=1}^{r} \exp\left[ -1 - \frac{\lambda}{k} \right] = r\exp\left[ -1 - \frac{\lambda}{k} \right].
        \end{equation}
        Rearranging this we have
        \begin{equation}
            \frac{1}{r} = \exp\left[ -1 - \frac{\lambda}{k} \right] = p_i.
        \end{equation}
        So we see that we recover the principle of equal \textit{a priori} probabilities.
    \end{exm}
    
    \begin{exm}{}{exm:grand canonical partition func}
        Suppose we have two constraints.
        Namely that the expectation values of two observables, \(y\) and \(z\), are fixed.
        That is
        \begin{equation}
            \expected{y} = \sum_{i} p_iy_i, \qqand \expected{z} = \sum_{i} p_iz_i
        \end{equation}
        are fixed values.
        As well as this we have the requirement that \(\sum_{i} p_i = 1\).
        
        We then define
        \begin{equation}
            h(\{p\}_r) = -k\sum_{i} p_i\ln p_i - \lambda_1 \sum_i p_i - \lambda_y \sum_i p_iy_i - \lambda_z \sum_i p_i z_i.
        \end{equation}
        Differentiating we have
        \begin{equation}
            \diffp{h}{p_j} = -k\ln p_j - k - \lambda_1 - \lambda_y y_j - \lambda_z z_j = 0.
        \end{equation}
        Thus,
        \begin{equation}
            p_j = \exp\left[ -1 - \frac{\lambda_1}{k} - \frac{\lambda_y y_j}{k} - \frac{\lambda_z z_j}{k} \right].
        \end{equation}
        Now considering the requirement that \(\sum_j p_j = 1\) we have
        \begin{align}
            1 &= \sum_j p_j\\
            &= \sum_j \exp\left[ -1 - \frac{\lambda_1}{k} - \frac{\lambda_y y_j}{k} - \frac{\lambda_z z_j}{k} \right]\\
            &= \exp\left[ -1 - \frac{\lambda_1}{k} \right] \sum_j \exp\left[ -\frac{\lambda_y y_j}{k} - \frac{\lambda_z z_j}{k} \right]\\
            &= Z \exp\left[ -1 - \frac{\lambda_1}{k} \right]\label{eqn:deriving gc partition func}
        \end{align}
        where we have defined
        \begin{equation}
            Z \coloneqq \sum_j \exp\left[ -\frac{\lambda_y y_j}{k} - \frac{\lambda_z z_j}{k} \right].
        \end{equation}
        Rearranging \cref{eqn:deriving gc partition func} we have
        \begin{equation}
            \exp\left[ -1 - \frac{\lambda_1}{k} \right] = \frac{1}{Z}.
        \end{equation}
        We then have
        \begin{align}
            p_j &= \exp\left[ -1 - \frac{\lambda_1}{k} - \frac{\lambda_y y_j}{k} - \frac{\lambda_z z_j}{k} \right]\\
            &= \exp\left[ -1 - \frac{\lambda_1}{k} \right]\exp\left[ -\frac{\lambda_y y_j}{k} - \frac{\lambda_z z_j}{k} \right]\\
            &= \frac{1}{Z}\exp\left[ -\frac{\lambda_y y_j}{k} - \frac{\lambda_z z_j}{k} \right].
        \end{align}
        We can fix the values of \(\lambda_y\) and \(\lambda_z\) using the constraints that \(\expected{y}\) and \(\expected{z}\) are known, fixed values.
    \end{exm}
    
    \section{Application to Statistical Mechanics}
    Recall that a microstate is the most detailed description possible of a system.
    Typically this will correspond to the \(i\)th microstate being the \(i\)th solution to the Schr√∂dinger equation.
    Let \(E_i\) be the energy of the \(i\)th microstate, this will typically be a function of extensive thermodynamic properties, such as volume.
    
    The equilibrium state is specified by the expectation values of extensive observables, such as the internal energy, the expectation value of which is
    \begin{equation}
        \mean{E} = \sum_{i} p_i E_i.
    \end{equation}
    Note that it is common to drop the overline and simply write \(E\) for this quantity.
    
    From here the exact physics depends on which constraints are imposed.
    We classify the constraints into categories of ensembles, which for now we can think of as being synonymous with probability distributions.
    
    \subsection{Microcanonical Ensemble}
    A \defineindex{microcanonical ensemble} is a completely isolated system.
    This means that the energy is fixed and so all microstates must have the same energy, \(E_i = \mean{E}\).
    The only constraint therefore is that \(\sum_i = 1\).
    This is analogous to \cref{exm:microcanonical partition func} and so
    \begin{equation}
        p_i = \frac{1}{\Omega}
    \end{equation}
    where \(\Omega\) is the number of microstates, which we called \(r\) in \cref{exm:microcanonical partition func}.
    So maximising \(S\) in a microcanonical ensemble corresponds to the principle of equal \textit{a priori} probabilities.
    
    \subsection{Canonical Ensemble}
    A \defineindex{canonical ensemble} can explore states of different energies, \(E_i\), we can think of it as being isolated but in contact with some heat reservoir which allows it to change energy.
    The observable \(\mean{E}\) specifies the equilibrium state.
    Therefore we need to maximise \(S\) subject to the constraint that \(\mean{E}\) is fixed, as well as the constraint that \(\sum_i p_i = 1\).
    We therefore have
    \begin{equation}
        h(\{p\}_r) = -k\sum_i p_i \ln p_i - \lambda_1 \sum_i p_i - \lambda_E \sum_i p_i E_i.
    \end{equation}
    Extremising this we have
    \begin{equation}
        \diffp{h}{p_j} = -k\ln p_j - k - \lambda_1 - \lambda_E E_j = 0.
    \end{equation}
    Rearranging this gives 
    \begin{equation}
        p_j = \exp\left[ -1 - \frac{\lambda_1}{k} - \frac{\lambda_E E_j}{k} \right].
    \end{equation}
    The constraint that \(\sum_j p_j = 1\) gives us
    \begin{align}
        1 &= \sum_j p_j = \sum_j \exp\left[ -1 - \frac{\lambda_1}{k} - \frac{\lambda_E E_j}{k} \right]\\
        &= \exp\left[ -1 - \frac{\lambda_1}{k} \right]\sum_j \exp\left[ -\frac{\lambda_E E_j}{k} \right]\\
        &= \cpartition\exp[-1 - \frac{\lambda_1}{k}]
    \end{align}
    where we have defined
    \begin{equation}
        \cpartition \coloneqq \sum_j \exp\left[ -\frac{\lambda_E E_j}{k} \right].
    \end{equation}
    We therefore have
    \begin{equation}
        \exp\left[ -1 - \frac{\lambda_1}{k} \right] = \frac{1}{\cpartition}.
    \end{equation}
    It follows that
    \begin{align}
        p_j &= \exp\left[ -1 - \frac{\lambda_1}{k} - \frac{\lambda_E E_j}{k} \right]\\
        &= \exp\left[ -1 - \frac{\lambda_1}{k} \right] \exp\left[ -\frac{\lambda_E E_j}{k} \right]\\
        &= \frac{1}{\cpartition} \exp\left[ -\frac{\lambda_E E_j}{k} \right].
    \end{align}
    Here \(\cpartition\) is the \defineindex{canonical partition function}\index{partition function!canonical}.
    We will see later that we can identify \(\lambda_E\) in such a way that we recover the Boltzmann distribution.
    
    \subsection{Grand Canonical Ensemble}
    A \defineindex{grand canonical ensemble} can explore states of different energies, but also different numbers of particles.
    We can think of the system as being in contact with a heat reservoir and a reservoir of particles.
    We now need to label states both by \(i\), which corresponds to the energy, \(E_i\), but also by the number of particles, \(N\), since in general if \(N\) changes then \(E_i\) will change also.
    
    The constraints are then that the following are fixed
    \begin{equation}
        \mean{E} = \sum_{i, N} p_{iN} E_{iN}, \qqand \mean{N} = \sum_{i, N} p_{iN}N.
    \end{equation}
    As well as the normal \(\sum_{i, N} p_{iN} = 1\).
    This corresponds to \cref{exm:grand canonical partition func} and we identify
    \begin{equation}
        p_{iN} = \frac{1}{\gcpartition} \exp\left[ -\frac{\lambda_E E_{iN}}{k} - \frac{\lambda_N N}{k} \right]
    \end{equation}
    where
    \begin{equation}
        \gcpartition \coloneqq \sum_{i, N} \exp\left[ -\frac{\lambda_E E_{iN}}{k} - \frac{\lambda_N N}{k} \right]
    \end{equation}
    is the \defineindex{grand canonical partition function}\index{partition function!grand canonical}.
    
    The next step is to identify the physical interpretation of the Lagrange multipliers.
    This will be the focus of the next chapter.
    
    \chapter{Identifying the Lagrange Multipliers}
    \section{Thermodynamics Review}
    \begin{rmk}
        For more details on thermodynamics see the Thermodynamics notes from the Thermal Physics course.
    \end{rmk}
    In order to identify the physical meaning of the Lagrange multipliers we will need to recap some basic thermodynamics.
    Equilibrium thermodynamic variables are variables which are stationary when the system is in equilibrium and have well defined values, we call these functions of state.
    The equation of state for a system relates different thermodynamic variables, for example \(PV = nRT\) relates the pressure, \(P\), volume, \(V\), number of moles, \(n\), and temperature, \(T\).
    
    In statistical mechanics we replace thermodynamic variables with their expectation values.
    This is valid since in the thermodynamic limit, where the number of constituents, \(N\), tends to infinity, we get distributions which are sharply peaked about the expected value.
    
    The zeroth law of thermodynamics is that heat flows from a hotter body to a colder body.
    This defines what we mean when we say hot and cold and can be used to define temperature.
    
    The first and second laws of thermodynamics can be combined into
    \begin{equation}
        \dl{\mean{E}} = T\dd{S} - P\dd{V}
    \end{equation}
    where \(\mean{E}\) is the mean (internal) energy (denoted \(U\) in thermodynamics), \(T\) is the temperature, \(S\) is the entropy, \(P\) is the pressure and \(V\) is the volume.
    This form of the first and second laws is valid for a PVT system, which is a system where the thermodynamic variables are pressure, volume, and temperature.
    
    A more general form of this law is
    \begin{equation}
        \dl{\mean{E}} = T \dd{S} + \sum_{\gamma} f_\gamma \dd{X_\gamma}.
    \end{equation}
    Here \(f_\gamma\) is what we call a \define{thermodynamic force}\index{thermodynamic force|see{generalised force}}, or a \defineindex{generalised force}\footnote{for more on generalised forces and conjugate variables see the notes from the Lagrangian Dynamics course.}.
    \(X_\gamma\) is then the \define{thermodynamic displacement}\index{thermodynamic displacement|see{conjugate field}}, also known as the \defineindex{conjugate field}.
    These quantities have units such that their product has dimensions of energy.
    
    We've already seen the example of a PVT system where we can identify \(-P\) as a generalised force with the associated conjugate variable \(V\).
    Another example would be the magnetic field, \(\mu_0\vv{\vv{H}}\), with the conjugate field being the magnetisation, \(\vv{M}\).
    In this case the term that contributes to \(\dl{\mean{E}}\) is \(\mu_0 \vv{H} \cdot \vv{M}\), that is each component of \(\mu_0\vv{H}\) and \(\vv{M}\) acts as a conjugate pair.
    
    Importantly the forces are \defineindex{intensive}, meaning they don't depend on the size of the system, and the displacements are \defineindex{extensive}, meaning they scale linearly with the size of the system.
    
    Being even more general the total change in internal energy is
    \begin{equation}
        \dl{\mean{E}} = T \dd{S} + \sum_{\gamma} f_\gamma \dd{X_\gamma} + \sum_{\alpha} \mu_\alpha \dd{\mean{N_\alpha}}
    \end{equation}
    where \(\mean{N_\alpha}\) is the mean number of particles of species\footnote{this is just a fancy word for \enquote{type of particle} which allows for us to talk about atoms, molecules, colloidal particles, etc.\@ using the same word.} \(\alpha\) and \(\mu_\alpha\) is the associated \defineindex{chemical potential}.
    We can take this equation to define \(\mu_\alpha\) and so we see that
    \begin{equation}
        \mu_\alpha \coloneqq \diffp{\mean{E}}{\mean{N}}[S,\{X\}]
    \end{equation}
    where the notation
    \begin{equation}
        \diffp{f}{x}[y]
    \end{equation}
    means the derivative of \(f\) with respect to \(x\) holding \(y\) constant.
        
    The final takeaway is that the entropy, \(S\), conjugate fields, \(\{X\}\), and number of particles, \(\{N\}\), are the natural variables for \(\mean{E}\), meaning that \(\mean{E} = \mean{E}(S, \{X\}, \{N\})\).
    
    \section{Identifying the Lagrange Multipliers}
    \subsection{Canonical}
    Recall that for the canonical ensemble
    \begin{equation}
        p_i = \frac{1}{\cpartition} \exp\left[ -\frac{\lambda_E E_i}{k} \right], \qqwhere \cpartition = \sum_i \exp\left[ -\frac{\lambda_E E_i}{k} \right].
    \end{equation}
    We take \(k = \boltzmann\), the Boltzmann constant, in order to get results familiar from thermodynamics.
    We start by calculating \(\dl{\mean{E}}\).
    Starting with the definition
    \begin{equation}
        \mean{E} \coloneqq \sum_i p_i E_i
    \end{equation}
    we see that we can change \(\mean{E}\) in two ways.
    First, we could change \(p_i\), second we can change \(E_i\), to change \(E_i\) we have to change properties of the system.
    Say we change the system by \(\dl{X_\gamma}\), then there will be an associated change in \(E_i\), and hence \(\mean{E}\).
    
    Putting this together
    \begin{align}
        \dl{\mean{E}} &= \dl{\left( \sum_i E_i p_i \right)}\\
        &= \sum_i \diffp{\mean{E}}{p_i} \dd{p_i} + \sum_\gamma \diffp{\mean{E}}{X_\gamma} \dd{X_\gamma}.
    \end{align}
    Identifying
    \begin{equation}
        \diffp{\mean{E}}{p_i} = \diffp{}{p_i} \sum_j E_jp_j = E_i
    \end{equation}
    we can write this as
    \begin{equation}\label{eqn:canonical lagrange multiplier derivation}
        \dl{\mean{E}} = \sum_i E_i \dd{p_i} + \sum_\gamma \diffp{\mean{E}}{X_\gamma} \dd{X_\gamma}.
    \end{equation}
    
    Now consider the entropy
    \begin{equation}
        S = -\boltzmann\sum_i p_i \ln p_i.
    \end{equation}
    The change in entropy for a change in probabilities is given by the chain rule as
    \begin{equation}
        \dl{S} = \sum_i \diffp{S}{p_i} \dl{p_i} = -\boltzmann \sum_i (\ln p_i + 1) \dd{p_i}.
    \end{equation}
    Since the total probability must stay constant we must have \(\sum_i \dl{p_i} = 0\) and hence the second term in this sum vanishes and we have
    \begin{equation}
        \dl{S} = -\boltzmann \sum_i \ln(p_i)\dd{p_i}.
    \end{equation}
    Substituting in 
    \begin{equation}
        p_i = \frac{1}{\cpartition} \exp\left[ -\frac{\lambda_E E_i}{\boltzmann} \right]
    \end{equation}
    we get
    \begin{equation}
        \dl{S} = -\boltzmann \sum_i \left[ -\frac{\lambda_E E_i}{\boltzmann} - \ln \cpartition \right] \dd{p_i}.
    \end{equation}
    The second term vanishes again since \(\ln\cpartition\) is a constant and the sum of the changes in probabilities must vanish to maintain constant total probability.
    We therefore have
    \begin{equation}
        \dl{S} = \lambda_E \sum_i E_i \dd{p_i}.
    \end{equation}

    Substituting this result into \cref{eqn:canonical lagrange multiplier derivation} we get
    \begin{equation}
        \dl{\mean{E}} = \frac{1}{\lambda_E} \dd{S} + \sum_\gamma \diffp{\mean{E}}{X_\gamma} \dd{X_\gamma}.
    \end{equation}
    Comparing this to
    \begin{equation}
        \dl{\mean{E}} = T \dd{S} + \sum_\gamma f_\gamma \dd{X_\gamma}
    \end{equation}
    we identify
    \begin{equation}
        \lambda_E = \frac{1}{T}.
    \end{equation}
    We also get an expression for the thermodynamic force:
    \begin{equation}\label{eqn: f = dE/dX}
        f_\gamma = \diffp{\mean{E}}{X_\gamma}.
    \end{equation}
    For example,
    \begin{equation}
        -P - \diffp{\mean{E}}{V} = \sum_i p_i \diffp{E_i}{V}.
    \end{equation}
    We can use this to identify the \defineindex{instantaneous pressure}
    \begin{equation}
        P_i \coloneqq - \diffp{E_i}{V}
    \end{equation}
    so that the mean pressure takes the expected form
    \begin{equation}
        P = \sum_i p_i P_i.
    \end{equation}
    
    Now that we have identified the Lagrange multiplier we can write the canonical distribution in its usual form:
    \begin{equation}
        p_i = \frac{1}{\cpartition} \exp\left[ -\frac{E_i}{\boltzmann T} \right], \qqwhere \cpartition = \sum_i \exp\left[ -\frac{E_i}{\boltzmann T} \right].
    \end{equation}
    Introducing \(\beta \coloneqq 1/(\boltzmann T)\), which we will use throughout this course as we see fit, we can write this as
    \begin{important}
        \vspace{-2.5ex}
        \begin{equation}
            p_i = \frac{1}{\cpartition} \e^{-\beta E_i}, \qqwhere \cpartition = \sum_i \e^{-\beta E_i}.
        \end{equation}
    \end{important}
    
    Considering again the entropy we have
    \begin{align}
        S &= -\boltzmann \sum_i p_i \ln p_i\\
        &= -\boltzmann \sum_i p_i[-\beta E_i - \ln \cpartition]\\
        &= -\boltzmann \sum_i p_i\left[ -\frac{E_i}{\boltzmann T} - \ln \cpartition \right]\\
        &= \frac{1}{T} \sum_i p_iE_i + \boltzmann\ln\cpartition \sum_i p_i.
    \end{align}
    Identifying the final sum as 1 and rearranging this we see that
    \begin{equation}
        -\boltzmann T \ln \cpartition = \mean{E} - TS \eqqcolon F
    \end{equation}
    where \(F\) is the \defineindex{Helmholtz free energy}, which we will discuss more in the next chapter.
    This equation is called a \defineindex{bridge equation} because the left hand side is in terms of the partition function, which is a sum over microstates, and the right hand side is in terms of macroscopic properties such as the energy.
    Notice that
    \begin{equation}
        \dl{F} = \dl{\mean{E}} - T \dd{S} - S\dd{T} = -S\dd{T} + \sum_\gamma f_\gamma \dd{X_\gamma}.
    \end{equation}
    Here we used the chain rule to compute \(\dl{(ST)}\) and then recognised
    \begin{equation}
        \dl{\mean{E}} - T\dd{S} = \sum_\gamma f_\gamma \dd{X_\gamma}
    \end{equation}
    as a rearrangement of the first and second law.
    From this we see that \(T\) and \(X_\gamma\) are the natural variables to express \(F\).
    
    \subsection{Grand Canonical}
    Recall that for the grand canonical ensemble
    \begin{equation}
        p_{iN} = \frac{1}{\gcpartition}\exp\left[ -\frac{\lambda_E E_{iN}}{k} - \frac{\lambda_N N}{k}\right], \qqwhere \gcpartition = \sum_{i, N} \exp\left[ -\frac{\lambda_E E_{iN}}{k} - \frac{\lambda_N N}{k} \right].
    \end{equation}
    We take \(k = \boltzmann\), the Boltzmann constant, in order to get results familiar from thermodynamics.
    We start by calculating \(\dl{\mean{E}}\).
    By the same logic as the canonical case
    \begin{align}
        \dl{\mean{E}} &= \sum_{i,N} \diffp{\mean{E}}{p_{iN}}\dd{p_{iN}} + \sum_{\gamma} \diffp{\mean{E}}{X_\gamma} \dd{X_\gamma}\\
        &= \sum_{i,N} E_{iN} \dd{p_{iN}} + \sum_\gamma \diffp{\mean{E}}{X_\gamma} \dd{X_\gamma}.
    \end{align}
    
    Now consider the entropy.
    The change in entropy for a change in probabilities is
    \begin{align}
        \dl{S} &= \sum_{i,N} \diffp{S}{p_{iN}} \dd{p_{iN}}\\
        &= -\boltzmann \sum_{i,N} (\ln p_{iN} + 1) \dd{p_{iN}}\\
        &= -\boltzmann \sum_{i,N} \ln (p_{iN})\dd{p_{iN}}\\
        &= -\boltzmann \sum_{i,N} \left[ -\frac{\lambda_E E_{iN}}{\boltzmann} - \frac{\lambda_N N}{\boltzmann} - \ln \gcpartition \right] \dd{p_{iN}}\\
        &= \lambda_E \sum_{i,N} E_{iN} \dd{p_{iN}} + \lambda_N \sum_{i,N} N\dd{p_{iN}}.
    \end{align}
    Here we substituted in the definition of \(S\) to get to the second line.
    We then identified that \(\sum_{i,N}p_{i,N} = 1\) and so \(\sum_{i,N}\dd{p_{iN}} = 0\) so we can neglect the second term in the second line.
    In the fourth line we substituted in the definition of \(p_{iN}\) in the grand canonical distribution to get the fourth line.
    We then identify that \(\ln\gcpartition\) is constant and so the final term of the fourth line vanishes when we sum over \(\dl{p_{iN}}\).
    
    We now write
    \begin{equation}
        \dl{\mean{N}} = \sum_{i,N} N \dd{p_{iN}}
    \end{equation}
    which follows from
    \begin{equation}
        \mean{N} = \sum_{i,N} p_{iN}N
    \end{equation}
    and noticing that \(N\) is the index of the sum, so is constant in any given term, we have
    \begin{equation}
        \diffp{\mean{N}}{p_{iN}} = Np_{iN}.
    \end{equation}
    
    We can then write
    \begin{equation}
        \dl{S} = \lambda_E \sum_{i, N} E_{i, N} \dd{p_{i,N}} + \lambda_N \dd{\mean{N}}.
    \end{equation}
    Rearranging this we get
    \begin{equation}
        \dl{\mean{E}} = \frac{1}{\lambda_E} \dd{S} - \frac{\lambda_N}{\lambda_E} \dd{\mean{N}}.
    \end{equation}
    Comparing this to the most general form of the first and second law,
    \begin{equation}
        \dl{\mean{E}} = T\dd{S} + \sum_\gamma f_\gamma \dd{X_\gamma} + \mu \dd{N}
    \end{equation}
    for a single particle species we can identify
    \begin{equation}
        \lambda_E = \frac{1}{T}, \qqand \lambda_N = -\lambda_E \mu = -\frac{\mu}{T}.
    \end{equation}
    
    We have found the standard form of the grand canonical ensemble distribution:
    \begin{important}
        \vspace{-2.5ex}
        \begin{equation}
            p_{iN} = \frac{1}{\gcpartition} \e^{-\beta(E_{iN} - N\mu)}, \qwhere \gcpartition = \sum_{i,N} \e^{-\beta(E_{iN} - N\mu)}.
        \end{equation}
    \end{important}
    
    Considering the entropy we have
    \begin{align}
        S &= \sum_{i,N} p_{iN} \ln p_{iN}\\
        &= -\boltzmann\sum_{i,N} p_{iN}[-\beta(E_{iN} - N\mu) - \ln\gcpartition]\\
        &= \frac{1}{T}\sum_{i,N} p_{iN}E_{iN} - \frac{\mu}{T}\sum_{i,N} p_{iN} N - \boltzmann\ln\gcpartition \sum_{i,N}p_{i,N}\\
        &= \frac{\mean{E}}{T} - \mu\mean{N} + \boltzmann\ln\gcpartition.
    \end{align}
    Rearranging this we have
    \begin{equation}
        -\boltzmann T \ln \gcpartition = \mean{E} - TS - \mu \mean{N} = \Phi
    \end{equation}
    where \(\Phi\) is the \defineindex{grand potential}, which we will discuss more in the next chapter.
    This is another bridge equation relating microscopic and macroscopic quantities.
    
    \chapter{Thermodynamic Potentials}
    \section{Thermodynamic Potentials}
    Recall that for a PVT system with a single species of particle the first and second laws of thermodynamics combine to give the central equation
    \begin{equation}
        \dl{\mean{E}} = T\dd{S} - P\dd{V} + \mu\dd{\mean{N}}.
    \end{equation}
    There are three conjugate pairs of variables here, \((T, S)\), \((P, V)\), and \((\mu, \mean{N})\).
    Of these \(S\), \(V\), and \(\mean{N}\) are extensive, and \(T\), \(P\), and \(\mu\) are intensive.
    The natural variables for \(\mean{E}\) are \(S\), \(V\), and \(\mean{N}\).
    We can use this to calculate the temperature, pressure, and chemical potential:
    \begin{equation}\label{eqn:T P chem potential from mean E derivatives}
        T = \diffp{\mean{E}}{S}[P, \mean{N}], \qquad P = -\diffp{\mean{E}}{V}[S, \mean{N}], \qqand \mu = \diffp{\mean{E}}{\mean{N}}[S, V].
    \end{equation}
    
    The \defineindex{Helmholtz free energy} is defined as
    \begin{equation}
        F \coloneqq \mean{E} - TS.
    \end{equation}
    Therefore
    \begin{align}
        \dl{F} &= \dl{(\mean{E} - TS)}\\
        &= \dl{\mean{E}} - T\dd{S} - S\dd{T}\\
        &= T\mean{S} - P\dd{V} + \mu\dd{\mean{N}} - T\dd{S} - S\dd{T}\\
        &= -S\dd{T} - P\dd{V} - \mu\dd{\mean{N}}.
    \end{align}
    This means that \(T\), \(V\), and \(\mean{N}\) are the natural variables for \(F\).
    This also gives us knew ways to calculate the entropy, pressure, and chemical potential:
    \begin{equation}
        S = -\diffp{F}{T}[V, \mean{N}], \qquad P = -\diffp{F}{V}[T, \mean{N}], \mu = -\diffp{\mean{E}}{\mean{N}}[T, V].
    \end{equation}
    
    The \defineindex{grand potential} is defined as
    \begin{equation}
        \Phi \coloneqq F - \mu \mean{N}.
    \end{equation}
    Hence,
    \begin{align}
        \dl{\Phi} &= \dl{(F - \mu\mean{N})}\\
        &= \dl{F} - \mu\dd{\mean{N}} - \mean{N}\dd{\mu}\\
        &= -S\dd{T} - P\dd{V} 0 \mu\dd{\mean{N}} - \mu\dd{\mean{N}} - \mean{N}\dd{\mu}\\
        &= -S\dd{T} - P\dd{V} + \mean{N}\dd{\mu}.
    \end{align}
    This means that \(T\), \(V\), and \(\mu\) are the natural variables for \(\Phi\).
    We can also use this to calculate the entropy, pressure, and mean number of particles:
    \begin{equation}
        T = -\diffp{\Phi}{T}[V, \mu], \qquad P = -\diffp{\Phi}{V}[T, \mu], \qqand \mean{N} = \diffp{\Phi}{\mu}[T, V].
    \end{equation}
    
    At this point we see a pattern forming.
    We take a potential and then subtract the product of conjugate variables to get the next potential.
    Since we have three pairs of conjugate variables for a PVT single species system there are \(2^3 = 8\) possible potentials, each with three different natural variables.
    The important ones when it comes to physics are given below.
    
    \begin{center}
         \begin{tabular}{cclc}\toprule
             Symbol & Definition & Name & Natural Variables \\ \midrule
             \(\mean{E}\) & & Energy & \(S\), \(V\), \(\mean{N}\)\\
             \(F\) & \(\mean{E} - TS\) & Helmholtz free energy & \(T\), \(V\), \(\mean{N}\)\\
             \(H\) & \(\mean{E} + PV\) & Enthalpy & \(S\), \(P\), \(\mean{N}\)\\
             \(G\) & \(F + PV\) & Gibbs Free Entropy & \(T\), \(P\), \(\mean{N}\)\\
             \(\Phi\) & \(F - \mu \mean{N}\) & Grand Potential & \(T\), \(V\), \(\mu\)\\ \bottomrule
         \end{tabular}
    \end{center}
    
    \subsection{Legendre Transforms}
    \begin{rmk}
        For more examples of Legendre transforms see the notes from the Lagrangian dynamics course, in particular the Hamiltonian is the Legendre transform of the Lagrangian.
    \end{rmk}
    The similarity in these definitions is no accident.
    The thermodynamic potentials are related by Legendre transforms, which we can think of as a way to define a new function with different variables related to the previous variables in a non-trivial way such that the new function contains all the information of the first.
    
    Consider some function, \(f\), which is a function of \(x_i\) for \(i = 1, \dotsc, k\).
    Then
    \begin{equation}
        \dl{f} = \sum_{i = 1}^{k} \diffp{f}{x_i}\dd{x_i} = \sum_{i = 1}^{k} u_i\dd{x_i}, \qqwhere u_i = \diffp{f}{x_i}.
    \end{equation}
    We then define a new function,
    \begin{equation}
        g \coloneqq f - \sum_{i = r + 1}^{k} u_ix_i.
    \end{equation}
    Which means
    \begin{align}
        \dd{g} &= \dl{\left( f - \sum_{i = r + 1}^{k} u_i x_i \right)}\\
        &= \dl{f} - \sum_{i = r + 1}^{k} (u_i \dd{x_i} + x_i \dd{u_i})\\
        &= \sum_{i=1}^{k} u_i\dd{x_i} - \sum_{i = r + 1}^{k} (u_i \dd{x_i} + x_i \dd{u_i})\\
        &= \sum_{i=1}^{r} u_i \dd{x_i} - \sum_{i = r + 1}^{k} x_i \dd{u_i}.
    \end{align}
    The function \(g\) is then a natural function of the variables \(x_1, \dotsc, x_r, u_{r+1}, \dotsc, u_k\).
    We say that \(g\) is the \defineindex{Legendre transform} of \(f\).
    
    The logic behind Legendre transforms can be explained by a one-dimensional example.
    Let \(f\) be a function of the single variable \(x\).
    This function can be defined by the value \(f(x)\) at all possible points \(x\), which is usually done through an equation like \(f(x) = \text{something with }x\).
    We can also specify \(f\), up to some constant, with the values of the derivative, \(u(x) = \diffp{f}/{x}\), at all points.
    We can think about this as specifying the gradient of the tangent at each point, \(x\).
    These tangents are lines with a slope \(u\) and a \(y\)-intercept, which we'll call \(g\).
    This means that we can express the value at \(x\) as \(f(x) = g + ux\).
    We can invert this and get \(g(u) = f(x) - ux\).
    We see that \(g\) contains the same information as \(f\) and is its Legendre transform.
    
    \section{Gibbs--Duhem Relation}
    Consider the energy of the system.
    This is an extensive value, therefore it should be proportional to the size of the system.
    Similarly the natural variables for energy, \(S\), \(X_\gamma\), and \(\mean{N_\alpha}\), are extensive and so also proportional to the size of the system.
    Consider then what happens if we scale the size of the system by some factor, \(b\).
    On the one hand, the energy is extensive and so scales by \(b\) also, meaning \(\mean{E} \to b\mean{E}\).
    On the other hand we can also view this as scaling all of the natural variables for \(\mean{E}\) by \(b\), so
    \begin{equation}
        \mean{E}(S, \{X\}, \{\mean{N}\}) \to \mean{E}(bS, \{bX\}, \{b\mean{N}\}).
    \end{equation}
    We therefore have
    \begin{equation}
        b\mean{E}(S, \{X\}, \{\mean{N}\}) = \mean{E}(bS, \{bX\}, \{b\mean{N}\}).
    \end{equation}
    
    Now consider what happens if we differentiate both sides with respect to \(b\), the left hand side is simple:
    \begin{equation}
        \diffp{}{b} b\mean{E}(S, \{X\}, \{\mean{N}\}) = \mean{E}(S, \{X\}, \{\mean{N}\}).
    \end{equation}
    The right hand side is slightly more complex:
    \begin{align}
        \diffp{}{b}\mean{E}(bS, \{bX\}, \{b\mean{N}\}) &= S\diffp{}{S}\mean{E}(bS, \{bX\}, \{b\mean{N}\})\\
        &\quad+ \sum_{\gamma} X_\gamma\diffp{}{X_{\gamma}}\mean{E}(bS, \{bX\}, \{b\mean{N}\})\\
        &\quad+ \sum_{\alpha} \mean{N_\alpha} \diffp{}{\mean{N_{\alpha}}}\mean{E}(bS, \{bX\}, \{b\mean{N}\})
    \end{align}

    Now evaluating at \(b = 1\) the left hand side gives
    \begin{equation}
        \diffp{}{b} b\mean{E}(S, \{X\}, \{\mean{N}\}) \bigg\vert_{b=1} = \mean{E}(S, \{X\}, \{\mean{N}\}).
    \end{equation}
    The right hand side gives
    \begin{equation}
        \diffp{}{b}\mean{E}(bS, \{bX\}, \{b\mean{N}\}) \bigg\vert_{b=1} = S\diffp{\mean{E}}{S} + \sum_{\gamma} X_\gamma\diffp{\mean{E}}{X_\gamma} + \sum_{\alpha} \mean{N_{\alpha}} \diffp{\mean{E}}{\mean{N_\alpha}}.
    \end{equation}
    Recognising \(T = \diffp{\mean{E}}{S}\), \(f_\gamma = \diffp{\mean{E}}{X_\gamma}\), and \(\mu_\alpha = \diffp{\mean{E}}{\mean{N_\alpha}}\) from \cref{eqn:T P chem potential from mean E derivatives,eqn: f = dE/dX} we have
    \begin{equation}
        \mean{E} = TS \sum_{\gamma} f_\gamma X_\gamma + \sum_{\alpha} \mu_\alpha \mean{N_\alpha}.
    \end{equation}
    
    We can rewrite the two sums using the Gibbs free energy and the grand potential:
    \begin{align}
        G &= \mean{E} - TS - \sum_{\gamma} - \sum_{\gamma} f_\gamma X_\gamma = \sum_\alpha \mu_\alpha \mean{N_\alpha}\\
        \Phi &= \mean{E} - TS - \sum_\alpha \mu_\alpha \mean{N_\alpha} = \sum_\gamma f_\gamma X_\gamma.
    \end{align}
    Hence 
    \begin{equation}
        \mean{E} = TS + G + \Phi.
    \end{equation}
    
    The important thing is that we can now compute \(\mean{E}\) based solely on the assumption that this is the form of \(\mean{E}\).
    Doing so we find that
    \begin{equation}
        \dl{\mean{E}} = T\dd{S} + S\dd{T} + \sum_\gamma (f_\gamma\dd{X_\gamma} + X_\gamma\dd{f_\gamma}) + \sum_\alpha (\mu_\alpha \dd{\mean{N_\alpha}} + \mean{N_\alpha}\dd{\mu_\alpha}).
    \end{equation}
    But from the first and second law we know that
    \begin{equation}
        \dl{\mean{E}} = T\dd{S} + \sum_\gamma f_\gamma\dd{X_\gamma} + \sum_\alpha \mu_\alpha \dd{\mean{N_\alpha}}.
    \end{equation}
    Subtracting this from the previous expression for \(\dl{\mean{E}}\) we get
    \begin{equation}
        0 = S\dd{T} + \sum_\gamma X_\gamma \dd{f_\gamma} + \sum_\alpha \mean{N_\alpha} \dd{\mu_\alpha}.
    \end{equation}
    This is the \defineindex{Gibbs-Duhem relation}.
    It shows that the intensive variables \(T\), \(f_\gamma\), and \(\mu_\alpha\), are not independent.
    
    For example, in a PVT system with a single species we have
    \begin{equation}
        0 = S\dd{T} - V\dd{P} + \mean{N}\dd{\mu}
    \end{equation}
    which means that an increase in, say, the temperature must be balanced by an increase in pressure and/or decrease in the chemical potential.
    
    \section{Ensembles}
    At this point we look slightly more at what we mean by an ensemble.
    So far we have seen that maximising \(S\) subject to constraints gives the probability associated with each microstate of the system.
    Different ensembles correspond to different sets of constraints.
    This works well with the Bayesian view of probability.
    In this section we will use the frequentist view.
    
    We can think of an ensemble as a very large number, \(M\), of the same assembly.
    Using the frequentist definition of probability the probability of being in microstate \(i\) is given by
    \begin{equation}
        p_i = \lim_{M \to \infty} \frac{m_i}{M}
    \end{equation}
    where \(m_i\) is the number of assemblies in microstate \(i\).
    
    We can think of a volume being divided up into smaller pieces.
    Although each small piece is small relative to the entire ensemble it still has a large number of particles, say on the order of Avogadro's number.
    
    The entire megasystem is isolated from the rest of the universe so all microstates of the megasystem are equally likely, that is the megasystem is a microcanonical ensemble.
    If we allow the subsystems to exchange energy then each one is a canonical ensemble.
    If we allow them to exchange particles also then each one is a grand canonical ensemble.
    
    It can be shown that using the Boltzmann distribution to describe the entropy of the whole system we get the Gibbs entropy as the definition of the entropy for each assembly.
    
    \chapter{Fluctuations}
    \section{Energy Fluctuations}
    Consider a canonical ensemble.
    The internal energy fluctuates randomly about the mean value, \(\mean{E}\).
    In the thermodynamic limit we expect these fluctuations to vanish and we get a sharply defined mean energy, which we take to be \emph{the} energy.
    For a canonical ensemble we can express the mean energy as
    \begin{align}
        \mean{E} &= \sum_i p_iE_i\\
        &= \frac{1}{\cpartition} \sum_i E_i\e^{-\beta E_i}\\
        &= \frac{1}{\cpartition} \sum_i \diffp{}{\beta} \e^{-\beta E_i}\\
        &= \frac{1}{\cpartition} \diffp{}{\beta} \sum_i \e^{-\beta E_i}\\
        &= \frac{1}{\cpartition} \diffp{\cpartition}{\beta}\\
        &= -\diffp{}{\beta} \ln \cpartition.
    \end{align}
    We will soon see that being able to write thermodynamic variables as \enquote{logarithmic derivatives} of the partition function, by which we mean derivatives of the log of the partition function, is a useful way to write things.
    
    We wish to estimate the size of the fluctuation.
    To do this we consider the heat capacity at constant volume, \(C_V\):
    \begingroup
    \allowdisplaybreaks
    \begin{align}
        C_V &\coloneqq \diffp{\smash{\mean{E}}}{T}[V]\\
        &\hphantom{:}= \diff{\beta}{T}\diffp{\mean{E}}{\beta}\\
        &\hphantom{:}= -\frac{1}{\boltzmann T^2} \diffp{}{\beta}\left( -\frac{1}{\cpartition} \diffp{\cpartition}{\beta} \right)\\
        &\hphantom{:}= -\frac{1}{\boltzmann T^2} \left[ -\frac{1}{\cpartition} \diffp[2]{\cpartition}{\beta} + \frac{1}{\cpartition^2} \left( \diffp{\cpartition}{\beta} \right)^2 \right]\\
        &\hphantom{:}= -\frac{1}{\boltzmann T^2} \left[ -\frac{1}{\cpartition} \diffp[2]{}{\beta} \sum_i \e^{-\beta E_i} + \left( \frac{1}{\cpartition} \diffp{\cpartition}{\beta} \right)^2 \right]\\
        &\hphantom{:}= -\frac{1}{\boltzmann T^2} \left[ -\frac{1}{\cpartition}\sum_i E_i^2\e^{-\beta E_i} + (-\mean{E})^2 \right]\\
        &\hphantom{:}= -\frac{1}{\boltzmann T^2} \left[ -\sum_i E_i^2p_i + \mean{E}^2 \right]\\
        &\hphantom{:}= -\frac{1}{\boltzmann T^2} \left[ -\mean{E^2} + \mean{E}^2 \right]\\
        &\hphantom{:}= \frac{1}{\boltzmann T^2} \left[ \mean{E}^2 - \mean{E^2} \right].
    \end{align}
    \endgroup
    Considering the fluctuation of microstate \(i\) from the mean, \(\Delta E_i \coloneqq E_i - \mean{E}\), we get that the mean-square fluctuation is
    \begin{align}
        \mean{\Delta E^2} &\coloneqq \sum_i \Delta E_i^2 p_i\\
        &\hphantom{:}= \sum_i (E_i - \mean{E})^2p_i\\
        &\hphantom{:}= \sum_i (E_i^2 + \mean{E}^2 - 2E_i\mean{E})p_i\\
        &\hphantom{:}= \sum_i E_i^2p_i + \mean{E}^2\sum_i p_i - 2\mean{E}\sum_i E_ip_i\\
        &\hphantom{:}= \mean{E^2} + \mean{E}^2 - 2\mean{E}^2\\
        &\hphantom{:}= \mean{E^2} - \mean{E}^2.
    \end{align}
    
    Comparing this to our result for \(C_V\) we see that the root-mean-square fluctuation is
    \begin{equation}
        \Delta E_{\mathrm{rms}} \coloneqq \sqrt{\smash{\mean{\Delta E^2}}\rule{0ex}{2.2ex}} = \sqrt{\boltzmann T^2 C_V}.
    \end{equation}
    This quantity is extensive and scales with the total energy of the system.
    It is more useful to consider a measure of \emph{relative} fluctuation size.
    Normalising by the mean energy we have
    \begin{equation}
        \frac{\Delta E_{\mathrm{rms}}}{\mean{E}} = \frac{\sqrt{\boltzmann T^2C_V}}{\mean{E}} \sim \frac{\sqrt{N}}{N} = \frac{1}{\sqrt{N}}.
    \end{equation}
    Hence as the size of the system increases the size of the relative fluctuations vanishes.
    This ratio, of the standard deviation divided by the mean, is called the \defineindex{coefficient of variation}.
    For a system of one mole we have \(N \approx 10^{24}\) and so the coefficient of variation is approximately \(10^{-12}\).
    This fluctuation size is far lower than can be detected by experiments on this scale and so for all intents and purposes the energy is fixed at \(\mean{E}\).
    If the total energy isn't fluctuating then we expect the canonical ensemble to behave exactly like the microcanonical ensemble in this limit.
    
    \section{Magnetic Fluctuations}
    As a second example consider a system in an external magnetic field, \(\vv{H}\).
    The energy of a microstate is then given by the normal energy without the field, minus the decrease in energy due to the systems magnetisation aligning with the external field.
    If the magnetisation of system in microstate \(i\) is \(\vv{M}_i\) then this contribution is \(\mu_0\vv{M}_i \cdot \vv{H}\) and so
    \begin{equation}
        E_i(\vv{H}) = E_i(\vv{H} = \vv{0}) - \mu_0\vv{M}_i \cdot \vv{H}.
    \end{equation}
    For simplicity we will now only consider the one dimensional case, so \(\vv{M}_i\cdot\vv{H}\) becomes \(M_iH\).
    
    Following similar logic to the energy case we have
    \begin{align}
        \mean{M} &\coloneqq \sum_i p_iM_i\\
        &\hphantom{:}= \frac{1}{\cpartition} \sum_i M_i \e^{-\beta E_i(H)}\\
        &\hphantom{:}= \frac{1}{\cpartition} \sum_i M_i \exp[-\beta(E_i(H = 0) - \mu_0 M_i H)]\\
        &\hphantom{:}= \frac{1}{\cpartition} \sum_i \frac{1}{\beta \mu_0} \diffp{}{H} \exp[-\beta(E_i(H = 0) - \mu_0 M_i H)]\\
        &\hphantom{:}= \frac{1}{\cpartition} \frac{1}{\beta \mu_0} \diffp{}{H} \sum_i \exp[-\beta(E_i(H = 0) - \mu_0 M_i H)]\\
        &\hphantom{:}= \frac{1}{\cpartition} \frac{1}{\beta \mu_0} \diffp{\cpartition}{H}\\
        &\hphantom{:}= \frac{1}{\beta \mu_0} \diffp{}{H} \ln \cpartition.
    \end{align}
    
    It can then be shown that
    \begin{equation}
        \mean{\Delta M^2} = \frac{\boltzmann T}{\mu_0} \chi
    \end{equation}
    where \(\chi\) is the isothermal magnetic susceptibility, defined as
    \begin{equation}
        \chi \coloneqq \diffp{\smash{M}}{H}[T, V].
    \end{equation}
    
    \section{Density Fluctuations}
    Now consider grand canonical ensemble and consider the fluctuation of the number of particles.
    First notice that we can write \(\mean{N}\) as
    \begin{align}
        \mean{N} &\coloneqq \sum_{i, N} p_{iN}N\\
        &\hphantom{:}= \frac{1}{\gcpartition} \sum_{i, N} N\e^{-\beta(E_{iN} - \mu N)}\\
        &\hphantom{:}= \frac{1}{\gcpartition} \sum_{i, N} \frac{1}{\beta}\diffp{}{\mu} \e^{-\beta(E_{iN} - \mu N)}\\
        &\hphantom{:}= \frac{1}{\beta} \frac{1}{\gcpartition} \diffp{}{\mu} \sum_{i, N} \e^{-\beta(E_{iN} - \mu N)}\\
        &\hphantom{:}= \frac{1}{\beta} \frac{1}{\gcpartition} \diffp{\gcpartition}{\mu} = \frac{1}{\beta} \diffp{}{\mu} \ln \gcpartition.
    \end{align}
    
    Recall that in the grand canonical ensemble we have \(\Phi = -\boltzmann T\ln\gcpartition\).
    This allows us to write
    \begin{equation}
        \mean{N} = -\diffp{\Phi}{\mu}[T, V].
    \end{equation}
    We can also write
    \begin{align}
        \diffp{\smash{\mean{N}}}{\mu}[T,V] &= \frac{1}{\beta} \diffp[2]{}{\mu}\ln\gcpartition\\
        &= \frac{1}{\beta} \left[ \frac{1}{\gcpartition}\diffp[2]{\gcpartition}{\mu} - \frac{1}{\gcpartition^2}\left( \diffp{\gcpartition}{\mu} \right)^2 \right]\\
        &= \beta[\mean{N^2} - \mean{N}^2].
    \end{align}
    Hence,
    \begin{equation}
        \mean{\Delta N^2} = \boltzmann T \diffp{\smash{\mean{N}}}{\mu}[T,V].
    \end{equation}
    This is extensive and so it follows that
    \begin{equation}
        \frac{\Delta N_{\mathrm{rms}}}{\mean{N}} = \frac{\sqrt{\mean{\Delta N^2}}}{\mean{N}} \sim \frac{1}{\sqrt{\mean{N}}}
    \end{equation}
    and so the fluctuations again vanish in the thermodynamic limit.
    Therefore in this limit we expect the grand canonical ensemble to behave like the canonical ensemble, which we have already shown behaves like a microcanonical ensemble in this limit.
    
    It is possible to rewrite this result in terms of the isothermal compressibility, \(\kappa_T\), which is somewhat analogous to \(C_V\), and is defined as
    \begin{equation}
        \kappa_T \coloneqq -\frac{1}{v} \diffp{v}{P}[T] = -\frac{1}{V}\diffp{V}{P}[T, \mean{N}]
    \end{equation}
    where \(v = V/\mean{N} = 1/\rho\) is the volume per particle.
    It can then be shown that
    \begin{equation}
        \diffp{\mean{N}}{\mu}[T,V] = \rho \kappa_T \mean{N}.
    \end{equation}
    From which it again follows that
    \begin{equation}
        \frac{\Delta N_{\mathrm{rms}}}{\mean{N}} = \sqrt{\frac{\boltzmann T \rho \kappa_T}{\mean{N}}} \sim \frac{1}{\sqrt{\mean{N}}}.
    \end{equation}
    
    \section{General Theory}\label{sec:fluctuations general theory}
    We can now formulate our observations above into a general procedure for some observable, \(A\), with conjugate field \(f\), which play the roles of the magnetisation and magnetic field respectively.
    If in microstate \(i\) the observable takes the value \(A_i\) then the energy can be written as
    \begin{equation}
        E_i(f) = E_i(f = 0) - fA_i.
    \end{equation}
    In the canonical ensemble we then have
    \begin{equation}
        \beta\mean{A} = \diffp{}{f}\ln \cpartition,
    \end{equation}
    which follows exactly the same derivation as used for \(\mean{M}\).
    We can then define the generalised susceptibility, which we call the \defineindex{response function}:
    \begin{equation}
        \chi_{AA} = \diffp{\mean{A}}{f}
    \end{equation}
    and it can be shown that
    \begin{equation}
        \mean{\Delta A^2} = \frac{\chi_{AA}}{\beta}.
    \end{equation}
    The double subscript \(A\) on \(\chi_{AA}\) denotes that this is the response of \(A\) in a change to the conjugate field to \(A\).
    This notation suggests that we may also consider the response of \(A\) due to a change in the conjugate field to some other observable, \(B\), and then define the response function \(\chi_{AB}\), but we won't do so here.
    
    In general \(\chi_{AA}\) will be extensive and therefore
    \begin{equation}
        \frac{\Delta A_{\mathrm{rms}}}{\mean{N}} = \frac{\sqrt{\mean{\Delta A^2}}}{\mean{N}} \sim \frac{1}{\sqrt{N}}.
    \end{equation}
    We therefore expect fluctuations in \(A\) to vanish in the large \(\mean{N}\) limit.
    
    The only time that there may be issues with this is if the coefficient of proportionality in \(\chi_{AA} \propto N\) diverges at some point.
    In this case we get large fluctuations.
    This typically happens near phase transitions.
    
    It is possible to derive a similar general theory for the grand canonical ensemble.
    
    \chapter{Weakly Interacting Systems}
    A system is \defineindex{weakly interacting} if no energy is stored in any interaction potential.
    This means the energy of the system is entirely due to the kinetic energy of the particles and their interactions with some external potential.
    The only interactions between particles are exchanging kinetic energy and momentum.
    Weakly interacting is essentially the closest we can get to no interactions without just having particles that behave like single particles.
    
    \section{Localised Particles}
    Consider a system of \(N\) particles fixed on a lattice.
    For example we could consider a crystal with atoms fixed at the lattice points.
    Each particle can be distinguished from the others by its location.
    We say that the particles are \define{distinguishable}\index{distinguishable particles}.
    
    Each particle in this case will have its own energy spectrum.
    As with a single particle we can label the states by an integer, but we now need another subscript to denote the particle.
    So the \(m\)th particle may be in state \(j_m\), in which it has energy \(\varepsilon_{j_m}\).
    The microstate is then a set of labels, \(i = \{j_1, j_2, \dotsc, j_N\}\).
    For weakly interacting particles (we will show later that) the energy of a given microstate is the sum of the energies of the particles in this microstate:
    \begin{equation}
        E_i = \varepsilon_{j_1} + \varepsilon_{j_2} + \dotsb + \varepsilon_{j_N} = \sum_{n=1}^{N} \varepsilon_{j_n}.
    \end{equation}
    The partition function in the canonical ensemble is then
    \begin{align}
        \cpartition &= \sum_{i} \e^{-\beta E_i}\\
        &= \sum_{j_1}\sum_{j_2}\dotso\sum_{j_N} \exp\left[ -\beta \sum_{n=1}^{N} \varepsilon_{j_n} \right]\\
        &= \sum_{j_1}\sum_{j_2}\dotso\sum_{j_N} \prod_{n=1}^{N} \e^{-\beta\varepsilon_{j_n}}\\
        &= \left( \sum_{j_1} \e^{-\beta\varepsilon_{j_1}} \right)\left( \sum_{j_2} \e^{-\beta\varepsilon_{j_2}} \right) \dotsm \left( \sum_{j_N} \e^{-\beta\varepsilon_{j_N}} \right)\\
        &= [Z(1)]^N
    \end{align}
    where
    \begin{equation}
        Z(1) = \sum_{j} \e^{-\beta \varepsilon_j}
    \end{equation}
    is the single particle partition function and this sum is over single particle states.
    
    The thermodynamics of a weakly interacting system can then be derived using the bridge equation
    \begin{equation}
        F = -\boltzmann T \ln \cpartition = -N\boltzmann T \ln(Z(1)).
    \end{equation}
    The probability of finding a particular particle in state \(j\) is given by
    \begin{equation}
        p_j = \frac{1}{Z(1)}\e^{-\beta \varepsilon_j}.
    \end{equation}
    This is the canonical, or Boltzmann, distribution for a single particle.
    
    \section{Non-Localised Particles}
    If the particles aren't localised then there is no way to keep track of them and so we can't distinguish them.
    We must therefore get the same state if we permute the particles since the final result cannot depend on what we call the particles.
    
    The microstate, \(i\), of the assembly is specified by stating how many particles are in each single particle state, \(j\).
    Therefore \(i\) is specified by \(\{n_j\}\), where \(n_j\) is the number of particles in the single particle state \(j\).
    
    A sum over microstates then becomes a sum over allowed occupation numbers.
    We can write the energy of a microstate as
    \begin{equation}
        E_{iN} = \sum_j n_j\varepsilon_j.
    \end{equation}
    The total number of particles is simply
    \begin{equation}
        N = \sum_{j} n_j.
    \end{equation}
    
    If we work in the canonical ensemble then we are constrained to microstates with a fixed value of \(N\).
    This makes the sums difficult since we would have to be cautious about partitioning the particles into states.
    Instead we work in the grand canonical ensemble and we can relax this limit to just having the average number of particles fixed.
    Doing so we find that
    \begin{equation}
        \gcpartition = \sum_{n_1}\sum_{n_2} \dotso \sum_{n_N} \exp\left[ -\beta\left( \sum_{j} n_j\varepsilon_j - \mu\sum_{j} n_j \right) \right] = \prod_j \mathcal{Z}_j.
    \end{equation}
    We arrive at this in the same way as we did in the localised canonical case.
    Again
    \begin{equation}
        \mathcal{Z}_j = \sum_{n_j} \e^{-\beta n_j(\varepsilon_j - \mu)}
    \end{equation}
    is the single state partition function for state \(j\).
    Notice that the factorisation is over states here, rather than particles as with the localised canonical case.
    
    The probability of finding the system in the microstate \(\{n_j\}\) is
    \begin{equation}
        p_{\{n_j\}} = \frac{1}{\gcpartition} \exp\left[ \beta \left( \mu \sum_j n_j - \beta \sum_j n_j\varepsilon_j \right) \right] = \prod_j p_{n_j}.
    \end{equation}
    The last term is a product over states and \(p_{n_j}\) is the probability of finding exactly \(n_j\) particles in state \(j\).
    This is given by
    \begin{equation}
        p_{n_j} = \frac{1}{\mathcal{Z}_j} \e^{\beta n_j(\mu - \varepsilon_j)}.
    \end{equation}
    
    The mean number of particles in state \(j\) is then
    \begin{align}
        \mean{n_j} &= \sum_{n_k} n_j p_{\{n_k\}}\\
        &= \sum_{n_j} n_j p_{n_j}\\
        &= \sum_{n_j} n_j \frac{1}{\mathcal{Z}_j} \e^{\beta n_j(\mu - \varepsilon_j)}\\
        &= \sum_{n_j} \frac{1}{\mathcal{Z}_j} \frac{1}{\beta} \diffp{}{\mu} \e^{\beta n_j(\mu - \varepsilon_j)}\\
        &= \frac{1}{\mathcal{Z}_j} \frac{1}{\beta} \diffp{}{\mu} \sum_{n_j} \e^{\beta n_j(\mu - \varepsilon_j)}\\
        &= \frac{1}{\mathcal{Z}_j} \frac{1}{\beta} \diffp{}{\mu} \mathcal{Z}_j\\
        &= \frac{1}{\beta} \diffp{}{\mu} \ln \mathcal{Z}_j\\
        &= \boltzmann T \diffp{}{\mu} \ln \mathcal{Z}_j.
    \end{align}
    
    \section{Fermions and Bosons}
    In order to perform the sum over allowed occupation numbers we need to distinguish between fermions and bosons.
    Recall that \define{fermions}\index{fermion} have half integers spin, \(n/2\) for odd \(n\), and the Pauli exclusion principle applies meaning that no two fermions can be in the same state.
    On the other hand, \define{bosons}\index{boson} have integer spin and there is no Pauli exclusion principle.
    
    \subsection{Fermions}
    For fermions the Pauli exclusion principle means that the only allowed occupation numbers are \(0\) and \(1\) since if \(n_j = 2\) then there are two particles in state \(j\), which isn't allowed.
    Hence
    \begin{equation}
        \mathcal{Z}_j = \sum_{n_j} \e^{\beta n_j(\mu - \varepsilon_j)} = \e^{0} + \e^{\beta (\mu - \varepsilon_1)} = 1 + \e^{\beta(\mu - \varepsilon_1)}.
    \end{equation}
    
    \subsection{Bosons}
    For bosons the occupation number can be any natural number\footnote{\(\naturals = \{0, 1, 2, \dotsc\}\).}.
    Hence
    \begin{align}
        \mathcal{Z}_j &= \sum_{n_j = 0}^{\infty} \e^{\beta n_j(\mu - \varepsilon_j)}\\
        &= \sum_{n_j = 0}^{\infty} \left( \e^{\beta(\mu - \varepsilon_j)} \right)^{n_j}\\
        &= \frac{1}{1 - \e^{\beta(\mu - \varepsilon_j)}}.
    \end{align}
    Here we have identified a geometric series, \(\sum_{n = 0}^{\infty} x^n = 1/(1 - x)\), which converges uniformly for \(\abs{x} < 1\), which is indeed the case here since \(\mu\) is negative and \(-\varepsilon_j\) is negative and \(\beta\) is positive so we have \(x\) being \(e\) raised to some negative power, which will always give something in the range \((0, 1)\).
    
    \subsection{Both}
    It is possible to write both of these in one by taking \(+\) for the case of fermions and \(-\) for bosons in the following:
    \begin{equation}\label{eqn:single particle grand canonical partition function}
        \mathcal{Z}_j = [1 \pm \e^{\beta(\mu - \varepsilon_j)}]^{\pm 1}.
    \end{equation}
    
    We can then compute the mean number of particles in state \(n_j\):
    \begin{align}
        \mean{n_j} &= \frac{1}{\beta} \diffp{}{\mu} \ln \mathcal{Z}_j\\
        &= \frac{1}{\beta} \diffp{}{\mu} \ln [1 \pm \e^{\beta(\mu - \varepsilon_j)}]^{\pm 1}\\
        &= \pm \frac{1}{\beta} \diffp{}{\mu} \ln [1 \pm \e^{\beta(\mu - \varepsilon_j)}]\\
        &= \frac{\e^{\beta(\mu - \varepsilon_j)}}{1 \pm \e^{\beta(\mu - \varepsilon_j)}}\\
        &= \frac{1}{\e^{\beta(\varepsilon_j - \mu)} \pm 1}\\
        &= f_{\pm}(\varepsilon_j, \mu).
    \end{align}
    Taking \(+\) for fermions and \(-\) for Bosons we get the \defineindex{Fermi--Dirac distribution},
    \begin{equation}
        \mean{n} = f_{+}(\varepsilon, \mu) = \frac{1}{\e^{\beta(\varepsilon - \mu)} + 1},
    \end{equation}
    and \defineindex{Bose--Einstein distribution},
    \begin{equation}
        \mean{n} = f_{-}(\varepsilon, \mu) = \frac{1}{\e^{\beta(\varepsilon - \mu)} - 1},
    \end{equation}
    respectively.
    To remember the sign we can use the fact that \(+\) represents \textbf{\textcolor{highlight}{a}}ddition and this goes with the Fermi--Dir\textbf{\textcolor{highlight}{a}}c distribution and \(-\) represents \textbf{\textcolor{highlight}{s}}ubtraction and this goes with the Bo\textbf{\textcolor{highlight}{s}}e--Ein\textbf{\textcolor{highlight}{s}}tein distribution.
    
    \chapter{Dilute Limit}
    \section{Dilute Limit}
    At high temperatures and low particle densities, this is known as the \defineindex{dilute limit}.
    In this limit the de Broglie wavelength is much less than the mean interparticle separation, and so particles are sufficiently separated that they behave as distinct particles and are essentially distinguishable.
    The chance of any one state being occupied in this limit is low and so we don't need to worry about the fermion/boson distinction since the probability of two particles trying to occupy the same state is negligible.
    We therefore expect the Fermi--Dirac and Bose--Einstein distributions to be the same in this limit.
    
    The way we achieve this limit mathematically is by taking \(\e^{\beta\mu} \ll 1\).
    We have to be careful since \(\mu\) is a typically increasing function of \(T\) and \(\beta \propto 1/T\).
    
    Without loss of generality we can take the lowest energy state to define zero energy, \(\varepsilon_0 = 0\), so for all other states, \(i\), we have \(\varepsilon_i \ge 0\).
    In this case we have
    \begin{equation}
        \e^{\beta(\mu - \varepsilon_i)} \ll 1 \implies \e^{\beta(\varepsilon_i - \mu)} \gg 1.
    \end{equation}
    This means that adding or subtracting 1 from this quantity is negligible so we have
    \begin{equation}
        \mean{n_i} = f_{\pm}(\varepsilon_i, \mu) = \frac{1}{\e^{\beta(\varepsilon_i - \mu)} \pm 1} \approx \frac{1}{\e^{\beta(\varepsilon_i - \mu)}} = \e^{\beta(\mu - \varepsilon_i)}.
    \end{equation}
    So we get something similar to the Boltzmann distribution, \(\e^{-\beta\varepsilon_i}\), and this result is the same for both fermions and bosons.
    
    If we had classically distinguishable particles then we would have
    \begin{equation}
        \mean{n_i} = \mean{N} p_i = \mean{N}\frac{\e^{-\beta\varepsilon_i}}{Z(1)}.
    \end{equation}
    Therefore in the dilute limit we have
    \begin{equation}
        \mean{n_i} \approx \e^{\beta(\mu - \varepsilon_i)} \approx \frac{\mean{N}}{Z(1)}\e^{-\beta\varepsilon_i} \implies \e^{\beta\mu} \approx \frac{\mean{N}}{Z(1)}.
    \end{equation}
    
    \section{Semi-Classical Approximation}
    Consider \(\ln\mathcal{Z}_j\) in the dilute limit, using \cref{eqn:single particle grand canonical partition function} we have
    \begin{equation}
        \ln\mathcal{Z}_j = \pm \ln[1 \pm \e^{\beta(\mu - \varepsilon_j)}] \approx \e^{\beta(\mu - \varepsilon_j)}.
    \end{equation}
    Where we've used the Taylor expansion \(\ln(1 \pm x) \approx \pm x\) for small \(x\).
    
    Now consider the grand potential, \(\Phi\):
    \begin{align}
        \Phi &= -\boltzmann T\ln \gcpartition\\
        &= -\boltzmann T \sum_j \ln \mathcal{Z}_j\\
        &\approx -\boltzmann T\sum_j \e^{\beta(\mu - \varepsilon_j)}\\
        &= -\boltzmann T \e^{\beta\mu} \sum_j \e^{-\beta\varepsilon_j}\\
        &\approx -\boltzmann T\e^{\beta\mu}Z(1)\\
        &\approx -\mean{N}\boltzmann T.
    \end{align}
    Here we have used the approximation \(\e^{\beta\mu}Z(1) = \mean{N}\) in the dilute limit.
    
    Consider instead the Helmholtz free energy, using \(\e^{\beta\mu}Z(1) = \mean{N}\) again, now rearranged to give \(\mu = \ln(\mean{N}/Z(1))/\beta\), we get
    \begin{align}
        F &= \Phi + \textcolor{highlight}{\mu}\mean{N}\\
        &\approx -\boltzmann T \mean{N} + \textcolor{highlight}{\boltzmann T}\mean{N} \textcolor{highlight}{\ln\left[ \frac{\mean{N}}{Z(1)} \right]}\\
        &= \boltzmann T(\textcolor{my blue}{\mean{N}\ln \mean{N} - \mean{N}} - \mean{N}\ln[Z(1)])\\
        &\approx \boltzmann T(\textcolor{my blue}{\ln(\mean{N}!)} - \ln(Z(1)^{\mean{N}}))\\
        &= -\boltzmann T \mean{N} \ln\left[ \frac{Z(1)^{\mean{N}}}{\mean{N}!} \right].
    \end{align}
    Here we have used \defineindex{Stirling's approximation}
    \begin{equation}
        \ln(N!) \sim N\ln N - N
    \end{equation}
    for large \(N\).
    Using
    \begin{equation}
        F = -\boltzmann T\ln \cpartition
    \end{equation}
    we can identify that in the dilute limit
    \begin{equation}
        \cpartition = \frac{Z(1)^{\mean{N}}}{\mean{N}!}.
    \end{equation}
    This is the \defineindex{semi-classical approximation}.
    \begin{rmk}
        Recall that in the statistical mechanics part of the thermal physics course we arrived at this result by a not particularly convincing argument of fixing the overcounting of states which occurs due to failing to account for the indistinguishability of particles.
    \end{rmk}

    \chapter{Density of States}
    To calculate thermodynamic properties we need to perform sums over states.
    This is not that easy to do.
    Instead we can often approximate the sums as integrals, which can then be computed more easily, or numerically, or just looked up.
    The goal of this section will be to find the \defineindex{density of states} in energy-space, \(g\), which is a function such that \(g(\varepsilon)\dd{\varepsilon}\) is the number of states with energies in the interval \([\varepsilon, \varepsilon + \dl{\varepsilon}]\).
    This function is such that
    \begin{equation}
        \mean{N} = \sum_j \mean{n_j} = \sum_j f(\varepsilon_j, \mu) \approx \int_{0}^{\infty} f(\varepsilon, \mu) g(\varepsilon) \dd{\varepsilon}.
    \end{equation}
    
    This approximation is generally valid when there are many states with similar energies, so that the discrete steps between states in the sum are small, and the sum can be approximated by the continuous integral.
    Generally we get states with close energy levels at high temperatures.
    So we expect the density of states to be a good approximation when the temperature of the system is high enough that the higher energy states dominate.
    
    \section{Density of States for a Particle in a Box}
    Consider a particle in a box, which we take to be a cube for simplicity.
    Mathematically the states are given by the solutions to the time independent Schr√∂dinger equation for a free particle,
    \begin{equation}
        -\frac{\hbar^2}{2m}\laplacian \psi = \varepsilon\psi,
    \end{equation}
    over the region \([0, L]^3\), with the boundary condition that \(\psi = 0\) when any of \(x\), \(y\), or \(z\) are either \(0\) or \(L\).
    This is simply an eigenvalue problem.
    
    The eigenfunctions of the Laplacian are \(\sin\) and \(\cos\), for our particular boundary conditions we take
    \begin{equation}
        \psi = A\sin(k_x x) \sin(k_y y) \sin(k_z z).
    \end{equation}
    If we take \(k_i = n_i\pi/L\) for \(n_i = 1, 2, \dotsc\) then we satisfy the boundary conditions.
    We can easily show that
    \begin{equation}
        \laplacian\psi = -(k_x^2 + k_y^2 + k_z^2)^2\psi = -k^2\psi
    \end{equation}
    where \(k = \abs{\vv{k}}\) with \(\vv{k} = (k_x, k_y, k_z)\) being a vector in \(k\)-space.
    We can also write\footnote{\(\integers_{>0} = \{1, 2, \dotsc\} = \naturals\setminus\{0\}\)} \(\vv{k} = \pi\vv{n}/L\) where \(\vv{n} \in \integers_{>0}^3\) is a vector in \(n\)-space.
    We can then identify
    \begin{equation}
        \varepsilon = \frac{\hbar^2}{2m}k^2.
    \end{equation}
    
    \subsection{Two Dimensions}
    The states are spaced \(\pi/L\) apart in \(k\)-space.
    In two dimensions this means that the area per state is \((\pi/L)^2\).
    We are only interested in states with \(k_x, k_y \ge 0\).
    This corresponds to only considering the top right quadrant, which in turn we can do by considering all four quadrants and including factors of \(1/4\) as necessary.
    
    The area of an annulus of inner radius \(k\) and thickness \(\dl{k}\) in the top right quadrant is
    \begin{equation}
        A = \frac{1}{4} [\pi (k + \dl{k})^2 - \pi k^2] = \frac{1}{4}2\pi k\dd{k} + \order(\dl{k}^2) \approx \frac{\pi}{2}k\dl{k}.
    \end{equation}
    This means that this quarter annulus contains
    \begin{equation}
        \frac{\pi k \dl{k}/2}{(\pi/L)^2} = \frac{L^2}{2\pi} k\dd{k}
    \end{equation}
    states.
    
    \subsection{Three Dimensions}
    In three dimensions the volume per state is \((\pi/L)^3\).
    Again we are only interested in states where \(k_x, k_y, k_z \ge 0\).
    This now means we only care about one out of eight octants, so we include a factor of \(1/8\).
    The volume of a spherical shell of inner radius \(k\) and thickness \(\dl{k}\) in this octant is
    \begin{equation}
        \frac{1}{8}\left[ \frac{4}{3}\pi(k + \dl{k})^3 - \frac{4}{3}\pi k^3 \right] = \frac{1}{8}4\pi k^2\dl{k} + \order(\dl{k}^2) \approx \frac{\pi}{2}k^2\dd{k}.
    \end{equation}
    The number of states in this eighth of a spherical shell is then
    \begin{equation}
        \frac{\pi k^2\dd{k}/2}{(\pi/L)^3} = \frac{L^3}{2\pi^2} k^2\dd{k}.
    \end{equation}
    
    Now define \(\Gamma\) to be the density of states in \(k\)-space, so that \(\Gamma(k)\dl{l}\) is the number of states with wave vectors in the interval \([k, k + \dl{k}]\).
    We can relate this to the density of states in energy-space by a simple change of variables.
    First we notice that since |\(\varepsilon = \hbar^2k/(2m)\)
    \begin{equation}
        \diff{\varepsilon}{k} = \frac{\hbar^2}{m}k = \sqrt{\frac{2\hbar^2}{m}}\sqrt{\varepsilon}.
    \end{equation}
    Hence
    \begin{equation}
        g(\varepsilon)\dd{\varepsilon} = \Gamma(k)\dd{k} = \frac{L^3}{2\pi^2} k^2 \dd{k} = \frac{V}{2\pi^2}k^2\dd{k}
    \end{equation}
    where \(V = L^3\) is the volume of the cube.
    Putting in
    \begin{equation}
        \dl{k} = \sqrt{\frac{2\hbar^2}{m}} \sqrt{\varepsilon} \dd{\varepsilon}
    \end{equation}
    and \(k^2 = 2m\varepsilon/\hbar^2\) we get
    \begin{equation}
        g(\varepsilon) = \left( \frac{2m}{\hbar} \right)^{3/2} \frac{V}{4\pi^2} \sqrt{\varepsilon}.
    \end{equation}
    The important thing here is that \(g(\varepsilon)\) scales linearly with the volume and as \(\sqrt{\varepsilon}\).
    
    \chapter{Statistical Mechanics and Quantum Mechanics}
    \section{Many Particle Schr√∂dinger Equation}
    In statistical mechanics we are interested in many particle systems.
    To fully treat these systems we need to solve the time independent many particle Schr√∂dinger equation:
    \begin{equation}
        \operator{H}\Psi(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) = E\Psi(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}).
    \end{equation}
    Here \(\vv{r_i}\) is the position of the \(i\)th particle, \(\Psi\) is the wave function for the state, \(E\) is the energy of the state and \(\operator{H}\) is the Hamiltonian.
    
    The most general Hamiltonian is
    \begin{equation}
        \operator{H} = -\sum_{k=1}^{N} \frac{\hbar^2}{2m_k}\laplacian[k] + U(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}).
    \end{equation}
    Here \(m_k\) is the mass of the \(k\)th particle, from now on we will assume all particles have the same mass.
    \(\laplacian[k]\) is the Laplacian operator acting only on \(\vv{r_k}\), that is if \(\vv{r_k} = (x_k, y_k, z_k)\) then
    \begin{equation}
        \laplacian[k] \coloneqq \diffp[2]{}{x_k} + \diffp[2]{}{{y_k}} + \diffp[2]{}{z_k}.
    \end{equation}
    \(U\) is the interaction potential, which in general depends on the positions of all particles.
    
    Often we can write \(U\) as a sum of interactions between pairs of particles, such as the case where the only interactions are electrostatic, we then have
    \begin{equation}
        U(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) = \sum_{l < k}\sum_{k = 1}^{N} \varphi(\abs{\vv{r_k} - \vv{r_l}})
    \end{equation}
    where \(\varphi(r) = q^2/(r\pi\varepsilon_0 r^2)\) where \(q\) is the charge carried by each particle.
    
    For the case of weakly interacting particles the potential depends only on the external potential, \(V\), felt by each particle and so
    \begin{equation}
        U(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) = \sum_{k = 1}^{N} V(\vv{r_k}).
    \end{equation}
    In this case we can write the Hamiltonian as
    \begin{equation}
        \operator{H}(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) = \sum_{k = 1}^{N} \operator{h}_k(\vv{r_k}).
    \end{equation}
    Here
    \begin{equation}
        \operator{h}_k(\vv{r_k}) \coloneqq -\frac{\hbar^2}{2m} \laplacian[k] + V(\vv{r_k}).
    \end{equation}
    
    The single particle Hamiltonians \(\operator{h}_k\) have wave functions \(\psi_i\) as solutions and corresponding energies \(\varepsilon_i\).
    These solutions are common between all particles.
    The many body wave function is then factorised in terms of these single-particles solutions so
    \begin{equation}
        \Psi(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) = \prod_{k = 1}^{N} \psi_{\alpha_k} (\vv{r_k})
    \end{equation}
    and
    \begin{equation}
        E = \sum_{k = 1}^{N} \varepsilon_{\alpha_k}
    \end{equation}
    where \(\alpha_k\) are integers labelling the states of each individual particle.
    
    It should be noted that this form is for localised or otherwise distinguishable particles.
    For indistinguishable particles this form over counts and instead we take suitable symmetrised combinations of the single-particle eigenstates to get the multi-particle state.
    What the \enquote{suitable} symmetrised combination is depends on whether we are dealing with fermions or bosons.
    
    For bosons the wave function should be symmetric under exchanging two particles.
    In this case the most general wave function is a sum over all permutations:
    \begin{equation}
        \Psi(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) \propto \sum_{\sigma \in S_N} \psi_{\alpha_{\sigma(1)}} (\vv{r_1}) \psi_{\alpha_{\sigma(2)}}(\vv{r_2}) \dotsm \psi_{\alpha_{\sigma(N)}} (\vv{r_N}).
    \end{equation}
    Here \(S_n\) is the permutation group on \(n\) objects and \(\sigma\) is a permutation.
    For the case of \(N = 2\) this means that
    \begin{equation}
        \Psi(\vv{r_1}, \vv{r_2}) = \frac{\sqrt{2}}{2}[\psi_{\alpha_1}(\vv{r_1})\psi_{\alpha_2}(\vv{r_2}) + \psi_{\alpha_2}(\vv{r_1})\psi_{\alpha_1}(\vv{r_2})].
    \end{equation}
    
    For fermions the wave function should be antisymmetric under exchanging two particles.
    Therefore the most general wave function comes from summing over all permutations and the totally antisymmetric Levi-Civita symbol, \(\varepsilon(\sigma) = \varepsilon_{\sigma(1)\sigma(2)\dotso\sigma(N)}\), which is defined to be 1 if \(\sigma\) is an even permutation and \(-1\) if \(\sigma\) is an odd permutation:
    \begin{equation}
        \Psi(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) \propto \sum_{\sigma \in S_N} \varepsilon(\sigma)\psi_{\alpha_{\sigma(1)}}(\vv{r_1}) \psi_{\alpha_{\sigma(2)}}(\vv{r_2}) \dotsm \psi_{\alpha_{\sigma(N)}}(\vv{r_N}).
    \end{equation}
    For the case of \(N = 2\) this means that
    \begin{equation}
        \Psi(\vv{r_1}, \vv{r_2}) = \frac{\sqrt{2}}{2} [\psi_{\alpha_1}(\vv{r_1})\psi_{\alpha_2}(\vv{r_2}) - \psi_{\alpha_2}(\vv{r_1})\psi_{\alpha_1}(\vv{r_2})].
    \end{equation}
    
    \section{A Tale of Two Probabilities}
    Things get a bit confusing when we start to introduce quantum mechanics to statistical mechanics\footnote{or introduce statistical mechanics to quantum mechanics. See the principles of quantum mechanics notes for that perspective} due to the fact that we have two notions of probability.
    In statistical mechanics we have been considering classical probabilities, such as the probability of finding a system in a given state, \(p_i = \e^{-\beta E_i}/\cpartition\) for a canonical ensemble.
    
    On the other hand in quantum mechanics we usually assume that the state is known and that the inherent probability aspect to quantum mechanics occurs when making measurements which give specific results with specific probabilities given by projecting the state onto the relevant eigenspace of the measurement operator.
    
    We now have to deal with both of these and we will do so by coming up with a mathematical notion unifying them.
    But first, a recap of quantum mechanics.
    
    \section{Quantum Mechanics Recap}
    \begin{rmk}
        For more details on quantum mechanics see the notes from principles of quantum mechanics.
    \end{rmk}
    In quantum mechanics we typically use braket notation.
    The state of a particle is a vector, \(\ket{\psi}\), in some Hilbert space.
    The energy eigenstates are the eigenvectors of the Hamiltonian operator, \(\operator{H}\), and the energy is the associated eigenvalue.
    That is \(\operator{H}\ket{i} = E_i\ket{i}\), where \(\operator{H}\) is the Hamiltonian operator, \(\{\ket{i}\}\) are the energy eigenvalues and \(E_i\) is the energy of a particle in state \(\ket{i}\).
    
    Since \(\operator{H}\) is Hermitian \(\{\ket{i}\}\) are orthogonal, and we take them to be normalised so the energy eigenstates are orthonormal and so
    \begin{equation}
        \braket{i}{j} = \delta_{ij}.
    \end{equation}
    
    The identity can then be written as
    \begin{equation}
        \ident = \sum_i \ket{i}\bra{i}.
    \end{equation}
    We can write a general wave function using projection operators as
    \begin{equation}
        \ket{\psi} = \ident\ket{\psi} = \sum_i \ket{i}\braket{i}{\psi} = \sum_i c_i\ket{i}
    \end{equation}
    where \(c_i = \braket{i}{\psi} \in \complex\).
    
    Given an observable, \(A\), there is an associated operator, \(\operator{A}\).
    The expected value of a measurement of \(A\) is
    \begin{equation*}
        \expected{A} = \bra{\psi}\operator{A}\ket{\psi} = \left( \sum_i c_i^*\bra{i} \right) \operator{A} \left( \sum_j \ket{j} \right) = \sum_i\sum_j c_i^*c_j\bra{i}\operator{A}\ket{j} = \sum_i \sum_j c_i^* c_j A_{ij}
    \end{equation*}
    where \(A_{ij} \coloneqq \bra{i}\operator{A}\ket{j}\) are the matrix elements of \(\operator{A}\) in the energy eigenbasis.
    
    For example, in the energy eigenbasis \(\bra{i}\operator{H}\ket{j} = E_i\delta_{ij}\) and so
    \begin{equation}
        \expected{E} = \sum_i\sum_j c_i^*c_j E_i\delta_{ij} = \sum_i \abs{c_i}^2 E_i.
    \end{equation}
    
    The problem that we face when it comes to including statistical mechanics is that in statistical mechanics the mean value of an observable \(A\) is
    \begin{equation}
        \mean{A} = \sum_i p_i A_i.
    \end{equation}
    Identifying \(A_{ii} = A_i\) we therefore want the mean in statistical mechanics to be
    \begin{equation}
        \mean{A} = \sum_i p_i A_{ii} = \sum_i p_i \bra{i} \operator{A} \ket{i}.
    \end{equation}
    Unfortunately this can't be written as a simple superposition.
    Suppose we could write it as
    \begin{equation}
        \ket{\psi} = \sum_i \sqrt{p_i} \ket{i},
    \end{equation}
    then
    \begin{equation}
        \expected{A} = \sum_i \sum_j \sqrt{p_ip_j}A_{ij}.
    \end{equation}
    Identifying \(A_i = A_{ii}\) we see that this is almost what we wanted but we have off diagonal terms which don't necessarily vanish.
    
    \section{Density Matrix}
    The resolution to this problem is to introduce the \defineindex{density matrix}, \(\rho\).
    First we define the density matrix as the diagonal matrix with the classical probabilities as its diagonal, that is \(\rho_{ij} \coloneqq p_i\delta_{ij}\).
    Then
    \begin{equation}
        \tr(\rho\operator{A}) = \sum_i (\rho\operator{A})_{ii} = \sum_i \sum_j \rho_{ij}A_{ji} = \sum_i\sum_j p_i \delta_{ij}A_{ji} = \sum_{i} p_i A_{ii} = \mean{A}.
    \end{equation}
    So we see that we recover the statistical mechanics mean in this case by taking \(\expected{A} \coloneqq \tr(\rho\operator{A})\).
    
    We can do this same calculation in braket notation as
    \begin{equation}
        \rho \coloneqq \sum_i p_i \ket{i}\bra{i}.
    \end{equation}
    Then
    \begin{multline}
        \tr(\rho\operator{A}) = \sum_j \bra{j} \rho\operator{A}\ket{j} = \sum_j \sum_i \bra{j} p_i \ket{i}\bra{i} \operator{A} \ket{j}\\
        = \sum_i \sum_j p_{ij} \delta_{ij} \bra{j}\operator{A}\ket{j} = \sum_i p_i \bra{i} \operator{A} \ket{i}.
    \end{multline}
    Which was what we wanted the statistical mechanics mean to be in braket notation.
    
    Recall that we can define the exponential of an operator through a power series:
    \begin{equation}
        \e^{\operator{A}} \coloneqq \sum_{n = 0}^{\infty} \frac{1}{n!} \operator{A}^n = \operator{1} + \operator{A} + \frac{1}{2}\operator{A}^2 + \frac{1}{3!}\operator{A}^3 + \dotsb.
    \end{equation}
    We therefore have
    \begin{equation}
        \e^{-\beta\operator{H}}\ket{i} = \sum_{n=0}^{\infty} \frac{1}{n!} (-\beta)^n \operator{H}^n \ket{i} = \sum_{n = 0}^{\infty} \frac{1}{n!} (-\beta)^n E_i^n = \e^{-\beta E_i}.
    \end{equation}
    Hence we have
    \begin{multline}
        \rho \coloneqq \sum_i p_i \ket{i}\bra{i} = \frac{1}{\cpartition}\sum_i \e^{-\beta E_i}\ket{i}\bra{i} = \frac{1}{\cpartition} \sum_i \e^{-\beta\operator{H}}\ket{i}\bra{i}\\
        = \frac{1}{\cpartition} \e^{-\beta\operator{H}} \sum_{i} \ket{i}\bra{i} = \frac{1}{\cpartition} \e^{-\beta\operator{H}}.
    \end{multline}
    Notice the resemblance between \(p_i = \e^{-\beta E_i}/\cpartition\) and \(\rho = \e^{-\beta \operator{H}}\).
    
     We can write the canonical partition function as the trace of \(\e^{-\beta\operator{H}}\) since
    \begin{equation}
        \tr(\e^{-\beta\operator{H}}) = \sum_{i = 0}^{\infty} \bra{i} \e^{-\beta\operator{H}}\ket{i} = \sum_{i = 0}^{\infty} \e^{-\beta E_i} \braket{i}{i} = \sum_{i = 0}^{\infty} \e^{-\beta E_i} = \cpartition.
    \end{equation}
    
    Now consider what happens if instead \(\rho \coloneqq \ket{\psi}\bra{\psi}\).
    Then
    \begin{multline}
        \expected{A} = \tr(\rho\operator{A}) = \sum_j \braket{j}{\psi}\bra{\psi}\operator{A}\ket{j} = \sum_{j} c_j\bra{\psi}\operator{A}\ket{j}\\
        = \sum_j\sum_i c_jc_i^* \bra{j}A\ket{i} = \sum_j\sum_i c_i^*c_j A_{ij}.
    \end{multline}
    So we recover the expectation value in quantum mechanics.
    Now we just need to find a way to make these two definitions work together so we can do quantum mechanics and statistical mechanics at the same time.
    
    What we have been discussing so far is the density matrix for a canonical ensemble.
    The most general form of density matrix arises when we have double probability.
    First we have the inherent quantum probability, but suppose we also don't know what state the system is in, just that it is in a state \(\ket{\psi_i}\) with probability \(p_i\).
    We then can define the density operator
    \begin{equation}
        \rho = \sum_i p_i \ket{\psi_i}\bra{\psi_i}.
    \end{equation}
    This is called a \defineindex{mixed state}, as opposed to when we know for sure the state of the system is \(\ket{\psi}\), which we call a \defineindex{pure state}.
    In this case the expectation value for an operator is still given as
    \begin{equation}
        \expected{A} = \mean{A} = \tr(\rho\operator{A})
    \end{equation}
    and this correctly accounts for both the statistical mechanics and quantum mechanics.
    
    \subsection{Von Neumann Entropy}
    We can further generalise the entropy of the system to the \define{von Neumann entropy}\index{entropy!von Neumann}, which is defined to be
    \begin{equation}
        S \coloneqq -k\tr(\rho\ln \rho).
    \end{equation}
    The logarithm is defined through its power series:
    \begin{multline}
        \ln \rho = \ln[1 + (\rho - 1)] = \sum_{n = 1}^{\infty} \frac{(-1)^{n + 1}}{n}(\rho - 1)^n\\
        = (\rho - 1) - \frac{1}{2}(\rho - 1)^2 + \frac{1}{3}(\rho - 1)^3 + \dotsb.
    \end{multline}
    Taking \(k = \boltzmann\) in this gives a definition that coincides with the Gibbs entropy for a canonical ensemble.
    
    \chapter{Vibrations in a Solid}
    In this chapter we will look at vibrations in a solid as an example of a many body problem.
    In a general many body problem we want to solve the time independent Schr√∂dinger equation
    \begin{equation}
        \operator{H}\Psi(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) = E\Psi(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N})
    \end{equation}
    where
    \begin{equation}
        \operator{H} = -\sum_{k = 1}^{N} \frac{\hbar^2}{2m} \laplacian[k] + U(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}).
    \end{equation}
    The problem is if \(U\) is not a sum of single particle potentials we can't, except in very special cases, solve the many body Schr√∂dinger equation.
    
    If \(U\) is a sum of single particle potentials, \(V\), then the Hamiltonian can be written as a sum of single particle Hamiltonians:
    \begin{equation}
        \operator{H}(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) = \sum_{k = 1}^{N} \operator{h}_k(\vv{r_k}), \qwhere \operator{h}_k(\vv{r_k}) = -\frac{\hbar^2}{2m} \laplacian[k] + V(\vv{r_k}).
    \end{equation}
    This then has a factorised solution
    \begin{equation}
        \Psi(\vv{r_1}, \vv{r_2}, \dotsc, \vv{r_N}) = \prod_{k = 1}^{N} \psi_{\alpha_k}(\vv{r_k})
    \end{equation}
    which has energy
    \begin{equation}
        E = \sum_{k = 1}^{N} \varepsilon_{\alpha_k}.
    \end{equation}
    Here \(\psi_{\alpha_k}\) is the wave function of the \(k\)th particle in state \(\alpha_k\) and \(\varepsilon_{\alpha_k}\) is the energy of this particle.
    We call this the diagonal form of the Hamiltonian since the particles aren't coupled by off diagonal terms.
    A lot of the work of many body physics involves making appropriate transformations and approximations to get the Hamiltonian into a diagonal form.
    
    \section{Einstein Model}
    Consider a crystalline solid of \(N\) atoms arranged in a regular lattice.
    Since the atoms are localised they are distinguishable.
    Let \(\vv{r_k}\) be the displacement of the \(k\)th atom from its equilibrium position.
    
    The basis of the Einstein model is to replace the complicated potential that each atom experiences with a single particle potential.
    The obvious choice being the harmonic potential, either because we can imagine expanding the potential to second order around a minimum and choosing for the constant term to be zero or just because physicists love harmonic oscillators.
    If we do this then homogeneity and isotropy of the crystal means that all atoms should have the same spring constant and hence the same angular momentum.
    The form of the Hamiltonian is then
    \begin{equation}
        \operator{H} = \sum_{k = 1}^{N} \left[ -\frac{\hbar^2}{2m}\laplacian[k] + \frac{1}{2}m\omega^2\abs{\vv{r_k}}^2 \right].
    \end{equation}
    The angular frequency, \(\omega\), is to be determined from experimental measurements.
    
    The Hamiltonian is then a sum of \(N\) three-dimensional quantum harmonic oscillators.
    This further simplifies since it is easy to show via separation of variables that a three-dimensional quantum harmonic oscillator behaves as three independent one-dimensional quantum harmonic oscillators.
    Therefore the system can be considered as \(3N\) one-dimensional quantum harmonic oscillators, which we label with \(k\).
    The energy of a quantum harmonic oscillator is \(\hbar\omega(n + 1/2)\) where \(n \in \naturals\)\footnote{\(\naturals = \{0, 1, 2, \dotsc\}\)}.
    The total energy is then
    \begin{equation}
        E = \sum_{k = 1}^{3N} \varepsilon_k \qqwhere \varepsilon_k = \hbar\omega\left( n_k + \frac{1}{2} \right)
    \end{equation}
    for non-negative integer \(n_k\).
    
    As the number of atoms is fixed we are working in the canonical ensemble and since the particles are weakly interacting the partition function is
    \begin{equation}
        \cpartition = [Z(1)]^{3N}
    \end{equation}
    with the single particle partition function given by
    \begin{align}
        Z(1) &= \sum_{n = 0}^{\infty} \exp\left[ -\beta\hbar\omega\left( n + \frac{1}{2} \right) \right]\\
        &= \e^{-\beta\hbar\omega/2} \sum_{n = 0}^{\infty} \e^{-\beta\hbar\omega n}\\
        &= \frac{\e^{-\beta\hbar\omega/2}}{1 - \e^{-\beta\hbar\omega}}.
    \end{align}
    Here we have used the fact that this is a geometric series, and \(\abs{\e^{-\beta\hbar\omega}} < 1\) so it converges.
    Notice that up to a factor of \(\e^{-\beta\hbar\omega}\) this is the same as the single particle partition function for bosons and so we can think of the quanta of energy in the harmonic oscillator as a boson.
    
    We can calculate the average energy:
    \begin{align}
        \mean{E} &= - \diffp{}{\beta} \ln \cpartition\\
        &= -\diffp{}{\beta} \ln\left[ \left( \frac{\e^{-\beta\hbar\omega/2}}{1 - \e^{-\beta\hbar\omega}} \right)^{3N} \right]\\
        &= -3N \diffp{}{\beta} \left[ -\frac{1}{2}\beta\hbar\omega - \ln(1 - \e^{-\beta\hbar\omega}) \right]\\
        &= \frac{3}{2}N\hbar\omega + 3N\hbar\omega\frac{\e^{-\beta\hbar\omega}}{1 - \e^{-\beta\hbar\omega}}\\
        &= \frac{3}{2}N\hbar\omega + \frac{3N\hbar\omega}{\e^{\beta\hbar\omega} - 1}
    \end{align}
    The heat capacity is then given by
    \begin{align}
        C_V &\coloneqq \diffp{\smash{\mean{E}}}{T}[V]\\
        &\hphantom{:}= \diffp{\beta}{T}\diffp{\mean{E}}{\beta}\\
        &\hphantom{:}= -\frac{1}{\boltzmann T^2} \diffp{}{\beta} \frac{3N\hbar\omega}{\e^{\beta\hbar\omega} - 1}\\
        &\hphantom{:}= \frac{1}{\boltzmann T^2} 3N (\hbar\omega)^2 \frac{\e^{\beta\hbar\omega}}{(1 - \e^{-\beta\hbar\omega})^2}\\
        &\hphantom{:}= 3N\boltzmann(\hbar\omega\beta)^2 \frac{\e^{\beta\hbar\omega}}{(\e^{\beta\hbar\omega} - 1)^2}.
    \end{align}
    
    At high temperatures \(\beta\) is small and so we can expand about \(\beta = 0\), doing so we have
    \begin{equation}
        \e^{\beta\hbar\omega} \approx 1,
    \end{equation}
    and
    \begin{equation}
        (\e^{\beta\hbar\omega} - 1)^{2} \approx (1 + \beta\hbar\omega - 1)^2 = (\beta\hbar\omega)^2,
    \end{equation}
    hence,
    \begin{equation}
        C_V \approx 3N\boltzmann.
    \end{equation}
    This is the same result that we get by applying the equipartition theorem to the \(3N\) harmonic oscillators, each of which has two quadratic degrees of freedom, the kinetic energy and potential energy, each quadratic degree of freedom contributing \(\boltzmann T/2\) to the specific heat capacity.
    
    At low temperatures \(\beta\) is large and so \(\e^{\beta\hbar\omega} - 1 \approx \e^{\beta\hbar\omega}\) meaning
    \begin{equation}
        C_V \approx 3N\boltzmann(\hbar\omega\beta)^2 \e^{-\beta\hbar\omega}.
    \end{equation}
    This tends to zero as \(T \to 0\), as it should.
    However, the Einstein model predicts that the heat capacity goes to zero exponentially, when careful measurements show that the heat capacity follows a cubic law and so a better model is needed.
    The problem is that the Einstein model doesn't account for interactions between atoms, the next model does.
    
    \section{Debye Model}
    Keeping with the notation of \(\vv{r_k}\) as the displacement of the \(k\)th atom from its equilibrium we introduce \(x_k^i\) as the \(i\)th Cartesian coordinate of this displacement.
    The general potential, \(U\), could be a very complicated function of the displacements of the atoms, however, as long as these displacements are small we can always Taylor expand.
    This is slightly complicated because \(U\) is a function of \(3N\) variables, \(x_k^i\), but not that much more complicated that the one dimensional case, we just need sums:
    \begin{equation}
        U = U_0 + \sum_{k}\sum_{i} \diffp{U}{x_k^i}\bigg|_{0}x_k^i + \frac{1}{2} \sum_{k}\sum_{i}\sum_{l}\sum_{j} \diffp{U}{x_k^i, x_l^j} \bigg|_{0} x_k^ix_l^j + \dotsb.
    \end{equation}
    We are free to choose \(U_0 = 0\) and the first derivative vanishes since we are expanding about the equilibrium, which is a minimum.
    Therefore the lowest order term is the quadratic term.
    We don't know the second derivative, since we don't know \(U\), but we can just hide all of that away by defining
    \begin{equation}
        A_{kl}^{ij} \coloneqq \diffp{U}{x_k^i, x_l^i}
    \end{equation}
    and so the Hamiltonian takes the form
    \begin{equation}
        \operator{H} = -\frac{\hbar^2}{2m} \sum_{k = 1}^{N} \laplacian[k] + \frac{1}{2}\sum_{\clap{k, l, i, j}} A_{kl}^{ij} x_k^ix_l^j.
    \end{equation}
    This leaves us with a pairwise potential which is quadratic in the displacements.
    
    By choosing an appropriate set of coordinates it is possible to diagonalise the Hamiltonian.
    We know this is possible since this is the Hamiltonian of 3N coupled harmonic oscillators.
    Recall that coupled oscillators have normal modes in which all displacements oscillate with the same frequency.
    The actual motion of the system is then a superposition of these normal modes and the energy is the sum of the energies of each normal mode.
    We define some transformation matrix, \(L_r\), such that we can define \defineindex{normal coordinates}, \(q_r\) which diagonalise the Hamiltonian:
    \begin{equation}
        q_r \coloneqq \sum_{k,i} L_{r,ki}x_k^i
    \end{equation}
    for \(r = 1, \dotsc, 3N\).
    Notice that these normal coordinates in general depend on all \(3N\) physical coordinates and describe oscillations of the whole system.
    Since there are \(3N\) coordinates there are \(3N\) normal modes.
    After diagonalising the Hamiltonian becomes
    \begin{equation}
        \operator{H} = \sum_{r = 1}^{3N} \left[ -\frac{\hbar^2}{2m} \diffp[2]{}{q_r} + \frac{1}{2}m\omega_r^2q_r^2 \right].
    \end{equation}
    From here the process is very similar to the Einstein model so we won't go into too much detail.
    
    The normal modes are quantum oscillators.
    We call the quanta of energy \defineindex{phonons}.
    The total energy is
    \begin{equation}
        E = \sum_{r = 1}^{3N} \hbar\omega_r\left( n_r + \frac{1}{2} \right)
    \end{equation}
    where \(n_r\) is the number of phonons in mode \(r\).
    The canonical partition function is then
    \begin{equation}
        \cpartition = \prod_{r=1}^{3N} Z_r
    \end{equation}
    where \(Z_r\) is the partition function for mode \(r\) and is given by
    \begin{equation}
        Z_r = \frac{\e^{\beta\hbar\omega_r/2}}{1 - \e^{-\beta\hbar\omega_r}}.
    \end{equation}
    The average energy is then
    \begin{equation}
        \mean{E} = -\diffp{}{\beta} \ln \cpartition = \text{constant} + \sum_r \frac{\hbar\omega_r}{\e^{\beta\hbar\omega_r} - 1}.
    \end{equation}
    The constant term isn't important since we are interested in the heat capacity, which is defined as the temperature derivative of this.
    
    The second term is the same as an ideal Bose gas at \(\mu = 0\), so phonons behave as if they were bosons.
    The factor \(1/(\exp(\beta\hbar\omega) - 1)\) can then be interpreted as the mean number of phonons in mode \(r\) and \(\hbar\omega_r = E_r\) as the energy of a phonon in mode \(r\).
    We can treat the system as an ideal Bose gas of free phonons, free since \(\mu = 0\).
    The number of phonons is not conserved, which is why \(\mu = 0\), essentially it takes no energy to create or remove phonons, so long as the net energy remains constant the number of phonons is free to change.
    
    If we assume that the density of modes, \(g\), is known as a function of \(\omega\) then we can write the sum as an integral over the density of modes:
    \begin{equation}
        \mean{E} \approx \text{constant} + \int \frac{\hbar\omega}{\e^{\beta\hbar\omega} - 1} g(\omega) \dd{\omega}.
    \end{equation}
    The problem is that we cannot in general accurately calculate \(g\), since for any reasonable number of particles the calculation becomes quickly infeasible.
    Instead we need an approximation.
    
    \subsection{Debye Theory}
    Debye theory is an approximation of the density of modes.
    Recall that for a gas of free particles, which we can treat the phonons as, the density of states in energy-space is
    \begin{equation}
        g(\varepsilon) = \frac{\Gamma(k)}{\diff{\varepsilon}/{k}} \qqwhere \Gamma(k) = g_{\mathrm{s}} \frac{V}{2\pi^2}k^2
    \end{equation}
    where we have added a spin degeneracy factor, \(g_{\mathrm{s}}\).
    
    In Debye theory we regard phonons as sound waves and we can then use the same density of modes that follows from the relation
    \begin{equation}
        k = \frac{\omega}{c_{\mathrm{s}}}
    \end{equation}
    where \(c_{\mathrm{s}}\) is the speed of sound in the material, which we assume to be constant.
    There are two transverse and one longitudinal wave modes and so \(g_{\mathrm{s}} = 3\).
    Changing variables to \(\omega = \varepsilon/\hbar\) we have
    \begin{equation}
        g(\omega) \dd{\omega} = \Gamma(k) \dd{k}
    \end{equation}
    which gives
    \begin{equation}
        g(\omega) \dd{\omega} = 3\frac{V}{2\pi^2} \left( \frac{\omega}{c_{\mathrm{s}}} \right)^{2} \frac{1}{c_{\mathrm{s}}} \dd{\omega}
    \end{equation}
    and so
    \begin{equation}
        g(\omega) = 3\frac{V}{2\pi^2} \frac{\omega^2}{c_{\mathrm{s}}^3} \eqqcolon AV\omega^2
    \end{equation}
    where we define the constant \(A \coloneqq 3/(2\pi^2 c_{\mathrm{s}}^3)\) for notational simplicity.
    Debye theory simply posits that this is the correct form for the density of modes.
    
    Recall that there are \(3N\) normal modes.
    We therefore need to introduce an cut-off frequency, \(\omega_{\mathrm{max}}\) so that
    \begin{equation}
        \int_{0}^{\omega_{\mathrm{max}}} g(\omega) \dd{\omega} = 3N.
    \end{equation}
    This gives
    \begin{equation}
        \omega_{\mathrm{max}} = \left( \frac{9}{A} \frac{N}{V} \right)^{1/3} = \left( 6\pi^2 \frac{N}{V} \right)^{1/3} c_{\mathrm{s}}.
    \end{equation}
    Notice that this depends only on the number density, \(N/V\), of the phonons.
    We can define a characteristic temperature, \(\Theta_{\mathrm{D}}\), called the \defineindex{Debye temperature}:
    \begin{equation}
        \boltzmann\Theta_{\mathrm{D}} = \hbar\omega_{\mathrm{max}}.
    \end{equation}
    We can then take \(\Theta_{\mathrm{D}}\) as a free material dependent parameter to be determined from experiments.
    
    The mean energy is then
    \begin{align}
        \mean{E} &= \int_{0}^{\infty} \hbar\omega g(\omega) \mean{n(\omega)} \dd{\omega}\\
        &= AV\hbar \int_{0}^{\omega_{\mathrm{max}}} \frac{\omega^3}{\e^{\beta\hbar\omega} - 1}\dd{\omega}.
    \end{align}

    We first consider the high temperature limit, \(T \gg \Theta_{\mathrm{D}}\).
    This implies \(\hbar\omega/(\boltzmann T) \ll 1\) and we can expand \(\e^{\beta\hbar\omega} \approx 1 + \beta\hbar\omega\).
    We then get
    \begin{equation}
        \mean{E} \approx AV\hbar \int_{0}^{\omega_{\mathrm{max}}} \frac{\omega^3}{\beta\hbar\omega} \dd{\omega} = 3N\boltzmann T.
    \end{equation}
    This is, as expected, the same result as we get from equipartition and the Einstein model, which we know work well in the high temperature regions.
    
    In the lower temperature limit, \(T \ll \Theta_{\mathrm{D}}\) a change of variables to \(x = \beta\hbar\omega\) gives the mean energy as a constant times a dimensionless integral:
    \begin{equation}
        \mean{E} = \frac{AV\hbar}{(\beta\hbar)^4} \int_{0}^{\Theta_{\mathrm{D}}/T} \frac{x^3}{\e^{x} - 1}.
    \end{equation}
    For \(T \ll \Theta_{\mathrm{D}}\) we can replace the upper limit with \(\infty\).
    This integral can be done analytically or numerically estimated, either way its value is \(\pi^4/15\).
    The important thing is it is a finite, nonzero, constant.
    At low temperatures we can see that \(\mean{E} \propto T^4\) and so \(C_V \propto T^3\), which agrees well with experiments.
    
    At low temperatures only low frequency (and so low energy) modes are excited and it is these modes which are modelled well by the Debye theory as sound waves.
    This allows Debye theory to work at low and high temperatures, but in between it is approximate.
    
    \subsection{The Integral}
    While the exact value of the integral isn't important it is my favourite integral so it would be amiss not to demonstrate how one can come to an analytic solution.
    We do so for a more general integral,
    \begin{equation}
        I = \int_{0}^{\infty} \frac{x^{p - 1}}{\e^{x} - 1} \dd{x}.
    \end{equation}
    First we need a few facts from complex analysis, first the Riemann zeta function is defined for \(s\in\complex\) with \(\Re(s) \ge 1\) as
    \begin{equation}
        \zeta(s) \coloneqq \sum_{n = 1}^{\infty} \frac{1}{n^s}.
    \end{equation}
    The gamma function is defined for \(z \in \complex\) with \(\Re(z) > 0\) by the integral
    \begin{equation}
        \Gamma(z) \coloneqq \int_{0}^{\infty} t^{z - 1} \e^{-t} \dd{t}.
    \end{equation}
    
    We therefore have
    \begin{align}
        I &= \int_{0}^{\infty} \frac{x^{p - 1}}{\e^{x} - 1} \dd{x}\\
        &= \int_{0}^{\infty} x^{p - 1} \e^{-x} \frac{1}{1 - \e^{-x}} \dd{x}\\
        &= \int_{0}^{\infty} x^{p - 1} \e^{-x} \sum_{n = 0}^{\infty} \e^{-nx} \dd{x}.
    \end{align}
    Here we have identified the geometric series which converges uniformly for \(\abs{\e^{-x}} < 1\), which is always the case for \(x \in \reals\) and so we can exchange the sum and the integral giving
    \begin{align}
        I &= \sum_{n = 0}^{\infty} \int_{0}^{\infty} x^{p - 1} \e^{-x} \e^{-nx} \dd{x}\\
        &= \sum_{n = 0}^{\infty} \int_{0}^{\infty} x^{p - 1} \e^{-x(n + 1)} \dd{x}.
    \end{align}
    Reindexing the sum to start at \(n = 1\) this becomes
    \begin{equation}
        I = \sum_{n = 1}^{\infty} \int_{0}^{\infty} x^{p - 1} \e^{-xn} \dd{x}.
    \end{equation}
    Now let \(y = nnx\) and so \(\dl{y} = n\dd{x}\) and we have
    \begin{align}
        I &= \sum_{n = 1}^{\infty} \int_{0}^{\infty} \frac{y^{p - 1}}{n^{p - 1}} \e^{-y} \frac{\dl{y}}{n}\\
        &= \sum_{n = 1}^{\infty} \frac{1}{n^p} \int_{0}^{\infty} y^{p - 1} \e^{-y} \dd{y}\\
        &= \zeta(p)\Gamma(p).
    \end{align}
    
    The Riemann zeta function and gamma function can be evaluated to arbitrary precision, and for some values can be evaluated exactly.
    \(p = 4\) is one of these values giving \(\zeta(4) = \pi^4/90\) and \(\Gamma(4) = 3! = 6\), hence
    \begin{equation}
        \int_{0}^{\infty} \frac{x^3}{\e^{x} - 1} \dd{x} = \zeta(4)\Gamma(4) = \frac{\pi^4}{15}.
    \end{equation}

    \section{Comparison of Models}
    In the Einstein model we assume independent oscillators all of the same frequency, \(\omega_{\mathrm{E}}\).
    This corresponds to a density of states given by \(\delta(\omega - \omega_{\mathrm{E}})\).
    In the Debye model we allow for two-point correlations, that is interactions between two particles, but not interactions involving three particles.
    This happens because we expand the potential to second order, so we have terms like \(x_k^i x_l^j\).
    The result is that we end up with a quadratic density of states, up to a certain cut-off frequency, \(\omega_{\mathrm{D}}\), above which the density of states vanishes.
    
    The true density of states is nearly impossible to actually know, but there are improvements we can make upon the Debye model, such as density functional theory.
    The results broadly follow the quadratic shape of Debye theory, after all to second order they agree, but there are typically more details that are missed by Debye theory.
    Qualitatively we can compare the three densities of states in \cref{fig:compare dos}.
    
    \begin{figure}[b]
        \tikzsetnextfilename{comparison-dos-crystal}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 5) node[left] {\(g(\omega)\)} -- (0, 0) -- (5, 0) node[below] {\(\omega\)};
            \draw[highlight, ultra thick] (0, 0) -- (4.9, 0);
            \draw[highlight, ultra thick] (2.5, 0) -- ++ (0, 5);
            \draw[my blue, ultra thick, domain=0:3, rounded corners=0.01] plot (\x, 0.5*\x*\x) -- (3, 4.5) -- (3, 0) -- (4.9, 0);
            \def\ycoord#1{\pgfmathparse{{0.5*#1*#1 + (rand + 1)/2}} \pgfmathresult}
            \draw[my red, ultra thick, rounded corners=0.01] (0, 0) -- (0.1, 0) -- (0.2, 0.153) -- (0.3, 0.214) -- (0.4, 0.244) -- (0.5, 0.193) -- (0.6, 0.444) -- (0.7, 0.087) -- (0.8, 0.035) -- (0.9, 0.707) -- (1, 0.527) -- (1.1, 1.1) -- (1.2, 1.09) -- (1.3, 0.726) -- (1.4, 0.938) -- (1.5, 1.032) -- (1.6, 1.388) -- (1.7, 1.806) -- (1.8, 1.856) -- (1.9, 1.391) -- (2, 2.391) -- (2.1, 2.648) -- (2.2, 2.285) -- (2.3, 2.474) -- (2.4, 2.554) -- (2.5, 2.697) -- (2.6, 2.953) -- (2.7, 3.868) -- (2.8, 4.046) -- (2.9, 3.756) -- (3, 4.845) -- (3.01, 4) -- (3.02, 3.5) -- (3.04, 3) -- (3.06, 2.7) -- (3.1, 2) -- (3.13, 1.5) -- (3.17, 1) -- (3.2, 0.5) -- (3.24, 0) -- (4.9, 0);
            \draw[highlight, ultra thick] (4, 3) -- ++ (0.5, 0) node[right, black] {Einstein};
            \draw[my blue, ultra thick] (4, 2.5) -- ++ (0.5, 0) node[right, black] {Debye};
            \draw[my red, ultra thick] (4, 2) -- ++ (0.5, 0) node[right, black] {Realistic};
            \node[below] at (2.5, 0) {\(\omega_{\mathrlap{\mathrm{E}}}\)};
            \node[below] at (3, 0) {\(\omega_{\mathrlap{\mathrm{D}}}\)};
        \end{tikzpicture}
        \caption[Density of States Comparison.]{The density of states for the Einstein and Debye models and a qualitative depiction of the true density of states.}
        \label{fig:compare dos}
    \end{figure}
    
    
    \chapter{Interactions in Classical Fluids}
    \section{Interactions and Correlations}
    Two random variables, \(x\) and \(y\), are \define{independent}\index{independent variables} if their joint probability distribution factorises:
    \begin{equation}
        P(x, y) = P(x)P(y).
    \end{equation}
    This extends to \(N\) random variables, \(x_i\), which are independent if
    \begin{equation}
        P(x_1, \dotsc, x_N) = \prod_{i = 1}^{N} P(x_i).
    \end{equation}
    
    If two random variables \emph{aren't} independent then they are \define{dependent}\index{dependent variables}.
    For any two random variables, \(x\) and \(y\), we can always write their joint probability distribution as
    \begin{equation}
        P(x, y) = P(x \mid y)P(y) = P(y \mid x) P(x)
    \end{equation}
    where \(P(x \mid y)\) is the probability distribution for \(x\) given a particular value of \(y\).
    In the case of independent variables the probability distribution of \(x\) doesn't depend on the value of \(y\) and so \(P(x \mid y) = P(x)\), giving the factorisation result above.
    
    \begin{exm}{}
        Consider the result of rolling two dice.
        The result of the first dice doesn't effect the result of the second dice and for fair six sided dice all outcomes of a single dice have probability \(1/6\), so the probability of getting one result on the first dice and another result on the second is \(1/36\).
        
        In the grand canonical ensemble we have
        \begin{equation}
            P(\{n_j\}) = \prod_j P(n_j),
        \end{equation}
        which shows that the occupation numbers of each state are independent.
        
        An example we will come to soon is a gas.
        Let \(\rho_1(\vv{r})\) be the probability of finding a gas particle at \(\vv{r}\) and \(\rho_2(\vv{r_1}, \vv{r_2})\) the probability of finding a gas particle at both \(\vv{r_1}\) and \(\vv{r_2}\).
        For an ideal gas, which has no interactions, the particles are all independent and so the presence or lack of presence of a particle at \(\vv{r_1}\) doesn't effect the probability of finding a particle at \(\vv{r_2}\) and so we have
        \begin{equation}
            \rho_2(\vv{r_1}, \vv{r_2}) = \rho_1(\vv{r_1}) \rho_1(\vv{r_2}).
        \end{equation}
        On the other hand if we consider a gas of particles with some nonzero volume, such as a van der Waals gas, then if \(\vv{r_1}\) and \(\vv{r_2}\) are sufficiently close together we can't have a particle at both of them, and so we lose the independence of the ideal gas.
    \end{exm}
    
    The last example here points to a key physical idea.
    If we have interactions then this causes some dependence between different variables.
    To quantify this we use the \defineindex{covariance}, which is defined for random variables \(x\) and \(y\) as
    \begin{equation}
        \Cov(x, y) \coloneqq \mean{(x - \mean{x})(y - \mean{y})} = \mean{\Delta x\, \Delta y}.
    \end{equation}
    We use the covariance as a measure of correlation, in particular if \(\Cov(x, y) > 0\) we say that \(x\) and \(y\) are \defineindex{correlated}, if \(\Cov(x, y) < 0\) we say that \(x\) and \(y\) are \defineindex{anticorrelated}, and if \(\Cov(x, y) = 0\) we say that \(x\) and \(y\) are \defineindex{uncorrelated}.
    We also say the variables are correlated when we mean that they are either correlated or anticorrelated, that they are not uncorrelated.
    
    In general independent variables are uncorrelated, however, it is possible to have zero correlation between dependent variables.
    Examples of this tend to be quite perverse though and are more of an issue for mathematicians than in the real world.
    As an example, consider \(x\) to be uniformly distributed on \((-1, 1)\) and set \(y = x^2\).
    Clearly \(y\) depends on \(x\).
    However, \(\mean{x} = 0\) and \(\mean{y} = 0.5\), so
    \begin{equation}
        \Cov(x, y) = \mean{x(y - 0.5)} = \mean{xy} - 0.5\mean{x} = \mean{x^3} = 0.
    \end{equation}
    
    Correlations between particles typically imply a gain in information, that is if two particles are (anti)correlated and we know a property of one particle then we are more likely to be able to correctly predict something about the second particle.
    This means that interactions generally generate correlations which reduce the missing information, and hence entropy.
    
    \section{Classical Statistical Mechanics}\label{sec:classical stat mech}
    In the coming section we will consider a classical fluid and try to improve on the ideal gas model.
    A classical fluid of \(N\) particles of mass \(m\) in a volume \(V\) has an average (number) density \(\rho = N/V\).
    An ideal gas has no interactions, which is of course not realistic.
    We will assume that this density is low enough that there are relatively few interactions and the interactions that do occur are weak.
    We can then treat the interactions as a perturbation on the ideal gas.
    We will, mostly, ignore quantum effects, such as quantisation of states, keeping only the ones known to be important to give the correct result, we give no further motivation for which quantum effects to consider other than someone else has already done this and worked out which effects can't be neglected.
    
    For a system of \(N\) particles the \(i\)th particle has position\footnote{we denote positions by \(q\) in this section to emphasise the conjugate nature of position and momenta, see the notes from the Lagrangian dynamics course for more on conjugate variables.} \(\vv{q}_i\) and momentum \(\vv{p}_i\), which means that we have \(6N\) degrees of freedom in the system, assuming a gas in three-dimensional space.
    This means we have a \(6N\)-dimensional phase space, recall that the phase space is the space of all possible configurations of the system, which in this case means each point in phase space corresponds to a set of positions and momenta for all \(N\) particles.
    
    The problem which we now face is that \(\vv{q}\) and \(\vv{p}\) are continuous and so there is an uncountably infinite number of states.
    This breaks several things, such as the Boltzmann definition of the entropy, since the weight of any macrostate will always be infinite and so its logarithm will diverge.
    
    The solution is to quantise phase space into cells of side \(\delta q_i\) and \(\delta p_i\), note that \(i = x, y, z\) here, rather than running over particle numbers.
    For simplicity, we take \(\delta q_i\) to be the same for all directions and \(\delta p_i\) to be the same for all directions, so we drop the \(i\) label.
    This results in cells of \enquote{volume} \(h^3 \coloneqq \delta q^3 \delta p^3\).
    Notice that \([h] = [q][p] = LMLT^{-1} = ML^2T^{-1}\), which are units of angular momentum.
    
    In quantum mechanics we would interpret this as an example of the uncertainty principle, interpreting \(h\) as Planck's constant.
    In classical mechanics we can instead interpret the cells as uncertainty in some measurement of position and momentum, which was the view of Maxwell when he developed this theory before quantum mechanics was well understood.
    It turns out that \(h\) vanishes in our final result so we can think of it just as a mathematical trick.
    Of course, if we do interpret \(h\) as Planck's constant then it must vanish from our formulae if we are to demand \enquote{classical} results.
    We can then interpret a single state with each cell in phase space, and there are a countable number of cells.
    
    We can use the semi-classical approximation for the canonical partition function, which fixes over-counting by dividing by \(N!\).
    We then have
    \begin{equation}
        \cpartition = \frac{1}{N!} \sum_{\mathclap{\mathrm{cells}}} \exp[-\beta E(\{\vv{q}\}, \{\vv{p}\})]
    \end{equation}
    where \(E\) is some function of the positions, \(\{\vv{q}\}\), and momenta, \(\{\vv{p}\}\), giving the energy in a given state.
    
    For a sufficiently small cell size the energy varies slowly between neighbouring cells and we can approximate the sum as an integral over phase space:
    \begin{equation}
        \sum_{\mathclap{\mathrm{cells}}} \to \frac{1}{h^3} \int \dl{q_x} \dd{q_y} \dd{q_z} \int \dl{p_x} \dd{p_y} \dd{p_z}.
    \end{equation}
    Notice that we divide by the \enquote{volume} of the cells so that this integral over the constant function \(1\) gives the volume of phase space divided by the cell volume, which gives the number of cells, which is the number of states.
    
    The canonical partition function is then given by integrating over each cell:
    \begin{equation}
        \cpartition = \frac{1}{N!} \frac{1}{h^{3N}} \prod_{i = 1}^{N} \int \dl{^3q_i} \dd{^3p_i} \exp[-\beta E(\{\vv{q}\}, \{\vv{p}\})].
    \end{equation}
    
    \section{Recovery of Ideal Gas}
    To demonstrate the use fo this partition function we use it to recover the ideal gas.
    For a classical ideal gas there are no interactions and so the energy of the \(i\)th particle is then
    \begin{equation}
        \varepsilon_i(\vv{q}_i, \vv{p}_i) = \frac{\vv{p}_i^2}{2m}.
    \end{equation}
    The energy of the system is simply the sum of the energies of the particles.
    Since the energy is independent of position the volume integrals in the partition function simply give the spatial volume, \(V\), and so in the product we get \(V^N\):
    \begin{align}
        Z_{\mathrm{ideal}} &= \frac{1}{N!} \frac{1}{h^{3N}} \prod_{i = 1}^{N} \int \dl{^3q_i} \dd{^3p_i} \exp\left[ -\beta\frac{\vv{p}_i^2}{2m} \right]\\
        &= \frac{V^N}{N! h^{3N}} \prod_{i = 1}^{N} \int \dl{^3p_i} \exp\left[ -\beta\frac{\vv{p}_i^2}{2m} \right]\\
        &= \frac{V^N}{N! h^{3N}} \left( \int \dl{^3p_i} \exp\left[ -\beta\frac{\vv{p}_i^2}{2m} \right] \right)^N\\
        &= \frac{V^N}{N! h^{3N}} \left( \int \dl{^3p_i} \exp\left[ -\frac{\beta}{2m}(p_{ix}^2 + p_{iy}^2 + p_{iz}^2) \right] \right)^N\\
        &= \frac{V^N}{N! h^{3N}} \bigg ( \int \exp\left[ -\frac{\beta}{2m}p_{ix}^2 \right] \dd{p_{ix}} \int \exp\left[ -\frac{\beta}{2m}p_{iy}^2 \right] \dd{p_{iy}}\\
        &\times \int \exp\left[ -\frac{\beta}{2m}p_{iz}^2 \right] \dd{p_{iz}} \bigg)^N\\
        &= \frac{V^N}{N! h^{3N}} \left( \int \dl{p} \exp\left[ -\frac{\beta}{2m}p^2 \right] \right)^{3N}\\
        &= \frac{V^N}{N! h^{3N}} \left( \sqrt{\frac{2m\pi}{\beta}} \right)^{3N}\\
        &= \frac{1}{N!} \left( \frac{V}{\lambda_{T}^3} \right)^N
    \end{align}
    where
    \begin{equation}
        \lambda_T \coloneqq \sqrt{\frac{h^2}{2\pi m\boltzmann T}}.
    \end{equation}
    The subscript \(T\) reminds us that \(\lambda\) depends on the temperature, which will be important later.
    
    Here we have used the fact that the momentum integral can be split into three separate integrals over the three components of the momentum of the \(i\)th particle, and that this integral is identical for all particles.
    We then recognise the resulting integral as a Gaussian and use the standard result
    \begin{equation}
        \int_{-\infty}^{\infty} \e^{-\alpha x^2} \dd{x} = \sqrt{\frac{\pi}{\alpha}}.
    \end{equation}
    In our case \(\alpha = \beta/(2m)\).
    
    Notice that \(Z_{\mathrm{ideal}}\) depends on \(h\) through \(\lambda_T\).
    This means that quantities like the free energy or entropy will depend on \(h\) as an additive constant.
    However, it is only differences in these quantities that we can measure and in these \(h\) will cancel out, as it must for classical results.
    
    The free energy of an ideal gas is therefore
    \begin{align}
        F_{\mathrm{ideal}} &= -\boltzmann T \ln Z_{\mathrm{ideal}}\\
        &= -\boltzmann T \ln\left( \frac{1}{N!} \left( \frac{V}{\lambda_T^3} \right)^N \right)\\
        &= -\boltzmann T \left[ N\ln \frac{V}{\lambda_T^3} - \ln N! \right]\\
        &\approx -\boltzmann T N \left[ \ln\frac{V}{\lambda_T^3} - \ln N + 1 \right]\\
        &= -\boltzmann T N \left[ \ln \frac{V}{N\lambda_T^3} + 1 \right].
    \end{align}
    The pressure is then
    \begin{align}
        P_{\mathrm{ideal}} &= - \diffp{F_{\mathrm{ideal}}}{V}[T, N]\\
        &= -\diffp{}{V} \left( -\boltzmann T N \left[ \ln\frac{V}{N\lambda_T^3} + 1 \right] \right)\\
        &= \boltzmann T N \diffp{}{V} \ln V\\
        &= \frac{N \boltzmann T}{V},
    \end{align}
    which is exactly what we expect for an ideal gas.
    The entropy is
    \begin{align}
        S_{\mathrm{ideal}} &= -\diffp{F_{\mathrm{ideal}}}{T}[V, N]\\
        &= -\diffp{}{T}\left( -\boltzmann T N \left[ \ln\frac{V}{N\lambda_T^3} + 1 \right] \right)\\
        &= \boltzmann N \left( \ln \frac{V}{N\lambda_T^3} + 1 \right) + \boltzmann T N \diffp{}{T} \ln \frac{V}{N\lambda_T^3}\\
        &= \boltzmann N \left( \ln\frac{V}{N\lambda_T^3} + 1 \right) - \frac{3 \boltzmann T N}{\lambda_T} \diffp{\lambda_T}{T}\\
        &= \boltzmann N \left( \ln\frac{V}{N\lambda_T^3} + 1 \right) - 3 \boltzmann T N \sqrt{\frac{2\pi m \boltzmann T}{h^2}} \diffp{}{T} \sqrt{\frac{h^2}{2\pi m\boltzmann T}}\\
        &= \boltzmann N \left( \ln\frac{V}{N\lambda_T^3} + 1 \right) + \frac{3}{2} \boltzmann T N \sqrt{\frac{2\pi m \boltzmann T}{h^2}}\sqrt{\frac{h^2}{2\pi m \boltzmann T^3}}\\
        &= \boltzmann N \left( \ln\frac{V}{N\lambda_T^3} + 1 \right) + \frac{3}{2}\boltzmann N\\
        &= \boltzmann N \left( \ln\frac{V}{N\lambda_T^3} + \frac{5}{2} \right).
    \end{align}
    This is exactly the result we would expect for an ideal gas.
    
    \section{Configurational Integral}
    Returning to a gas with interactions we can make an important simplification by assuming that the interaction potential doesn't depend on the particle momenta.
    That is, the energy is given by
    \begin{equation}
        E(\{\vv{q}\}, \{\vv{p}\}) = \sum_{i = 1}^{N} \frac{\vv{p}_i^2}{2m} + U(\vv{q}_1, \dotsc, \vv{q}_N).
    \end{equation}
    In this case \(\cpartition\) factors into two separate products:
    \begin{equation*}
        \cpartition = \frac{1}{N!h^{3N}} \left[ \prod_{i = 1}^{N} \int \exp\left[ -\beta\sum_{i=1}^{N} \frac{\vv{p}_i^2}{2m} \right] \dd{^3 p_i} \right] \left[ \prod_{i=1}^{N} \int \exp[ -\beta U(\vv{q}_1, \dotsc, \vv{q}_N) ] \dd{^3q_i} \right].
    \end{equation*}
    We can then identify the momentum integral and the prefactor as corresponding to \(Z_{\mathrm{ideal}} / V^N\), in which case we can write
    \begin{equation}
        \cpartition = Z_{\mathrm{ideal}} Q
    \end{equation}
    where
    \begin{equation}
        Z_{\mathrm{ideal}} = \frac{1}{N!} \left( \frac{V}{\lambda_T^3} \right)^N,
    \end{equation}
    and
    \begin{equation}
        Q \coloneqq \frac{1}{V^N} \prod_{i = 1}^{N} \int \exp[-\beta U(\vv{q}_1, \dotsc, \vv{q}_N)] \dd{^3q_i}.
    \end{equation}
    This is called the \defineindex{configurational integral}.
    Interactions between particles enter only through \(Q\).
    If we can evaluate \(Q\) we can find pretty much any quantity we want, for example
    \begin{equation}
        F = -\boltzmann T \ln \cpartition = -\boltzmann T \ln Z_{\mathrm{ideal}} - \boltzmann T \ln Q
    \end{equation}
    which we can view as the ideal gas free energy plus a correction term due to interactions.
    Similarly
    \begin{equation}
        P = - \diffp{F}{V} = \frac{N\boltzmann T}{V} + \boltzmann T \diffp{}{V} \ln Q = P_{\mathrm{ideal}} + P_{\mathrm{conf}}
    \end{equation}
    where \(P_{\mathrm{conf}}\) is the configurational pressure, which is the pressure due to interactions.
    
    The problem is that computing \(Q\) is often not easy.
    
    \section{Virial Expansion}
    An important case where we can approximate the configuration integral is when the interaction potential can be written as
    \begin{equation}
        U(\vv{q}_1, \dotsc, \vv{q}_N)  = \frac{1}{2} \sum_{i \ne j} \varphi(\abs{\vv{q}_i - \vv{q}_j}) = \sum_{i < j} \varphi_{ij}
    \end{equation}
    where \(\varphi_{ij} \coloneqq \varphi(\abs{\vv{q}_i - \vv{q}_j})\).
    This assumption implies only two-body interactions, which is reasonable for a dilute gas as if two-particle interactions are rare then three-particle interactions will be even rarer.
    It also assumes a central potential which is isotropic, meaning that only the distance between the particles is important.
    
    There are a variety of common interaction potentials which take this form.
    The simplest example is the hardcore potential, which is zero above a certain distance and infinite below it:
    \begin{equation}
        \varphi(r) \coloneqq
        \begin{cases}
            \infty & r \le a,\\
            0 & r > a.
        \end{cases}
    \end{equation}
    We can think of this as modelling the particles as hard spheres which can't overlap.
    Another example would be the Lennard-Jones potential\footnote{see the notes for either the matter part of the fields and matter course, or introduction to condensed matter physics for more on the Lennard-Jones potential.}, which is strongly repulsive at low distances, attractive at medium distances and goes to zero at infinity:
    \begin{equation}
        \varphi(r) \coloneqq 4\varepsilon \left[ \left( \frac{\sigma}{r} \right)^{12} - \left( \frac{\sigma}{r} \right)^6 \right].
    \end{equation}
    Here \(\varepsilon\) is related to the depth of the attractive well and \(\sigma\) to the hardcore radius.
    
    We wish to compute the configurational integral in a general way for any interaction potential which can be written as a sum over an isotropic two particle interaction potential.
    As such we start by writing the configurational integral as
    \begin{equation}
        Q = \frac{1}{V^N} \prod_{i = 1}^{N} \prod_{i < j} \int F_{ij} \dd{^3q_i}
    \end{equation}
    where
    \begin{equation}
        F_{ij} \coloneqq \exp[-\beta \varphi_{ij}] = \exp[-\beta \varphi(\abs{\vv{q}_i - \vv{q}_j})].
    \end{equation}
    This double product is slightly confusing so we write it out for clarity:
    \begin{equation}
        Q = \frac{1}{V^N} \int \dl{^3q_1} \dotsm \dl{^3q_N} F_{12}F_{13} \dotsm F_{1N}F_{23} \dotsm F_{2N} \dotsm F_{N-1,N}.
    \end{equation}
    Notice that there are \(N - 1\) \(F_{ij}\) factors with \(i = 1\) and \(j = 2, \dotsc, N\), similarly there are \(N - 2\) \(F_{ij}\) factors with \(i = 2\) and \(j = 3, \dotsc, N\), and so on until there is a single \(F_{ij}\) term with \(i = N - 1\) and \(j = N\).
    Therefore there are
    \begin{equation}
        (N - 1) + (N - 2) + \dotsb + 1 = \sum_{k = 1}^{N} k = \frac{N}{2}(N - 1) = \binom{N}{2}.
    \end{equation}
    factors of \(F_{ij}\) appearing in this product.
    
    The form of the \(Q\) integral is reminiscent of an average, integrating over space and then dividing by the volume.
    We therefore write this integral as
    \begin{equation}
        Q = \expected[\bigg]{ \raisebox{0.6ex}{\(\displaystyle\prod_{\smash{i < j}}\)} F_{ij} }
    \end{equation}
    where
    \begin{equation}
        \expected{x} \coloneqq \frac{1}{V^N}\prod_{i=1}^{N} \int x \dd{^3q_i}.
    \end{equation}
    In general, it is not easy to compute \(Q\) this way because \(F_{ij}\) and \(F_{ik}\) are correlated.
    However, we can approximate \(F_{ij}\) and \(F_{ik}\) as uncorrelated in which case \(Q\) factorises as
    \begin{equation}
        Q = \prod_{i < j} \expected{F_{ij}} = \expected{F}^{\binom{N}{2}}.
    \end{equation}
    Since we expect that \(F_{ij}\) doesn't depend on the indices of the particles, which are just labels we give them, so we let \(\expected{F_{ij}} = \expected{F}\).
    
    We can then compute \(\expected{F}\):
    \begin{equation}
        \expected{F_{ij}} = \frac{1}{V^N} \prod_{k = 1}^{N} \int F_{ij} \dd{^3 q_k}.
    \end{equation}
    First notice that for \(k \ne i, j\) this is an integral over \(1\) and just gives \(V\) so this quickly simplifies to
    \begin{equation}
        \expected{F_{ij}} = \frac{1}{V^2} \int \exp[-\beta \varphi(\abs{\vv{q}_i - \vv{q}_j})] \dd{^3q_i} \dd{^3q_j}.
    \end{equation}
    In order to compute this we switch to centre-of-mass coordinates, \(\vv{r} = \vv{q}_i - \vv{q}_j\) and \(\vv{R} = (\vv{q}_i + \vv{q}_j)/2\).
    This then turns our integral into
    \begin{equation}
        \expected{F_{ij}} = \frac{1}{V^2} \int \exp[-\beta \varphi(r)] \dd{^3r}\dd{^3R} = \frac{1}{V} \int \exp[-\beta \varphi(r)] \dd{^3r}.
    \end{equation}
    Here we have used the fact that the integral has no \(R\) dependence and so the integral over \(R\) just gives another factor of \(V\).
    
    We now rewrite \(F_{ij}\) as
    \begin{equation}
        F_{ij} = 1 + f_{ij}
    \end{equation}
    with
    \begin{equation}
        f_{ij} \coloneqq \exp[-\beta \varphi_{ij}] - 1.
    \end{equation}
    Note that this is exact, not an approximation.
    Considering the limits of \(f_{ij}\) we have
    \begin{equation}
        f_{ij} \to
        \begin{cases}
            0 & \text{as } r \to \infty,\\
            -1 & \text{as } r \to 0. 
        \end{cases}
    \end{equation}
    In a dilute gas, where separations are large, we therefore expect \(f_{ij}\) to be small.
    Using this
    \begin{align}
        \expected{F_{ij}} &= \frac{1}{V} \int (1 + f_{ij}) \dd{^3r}\\
        &= \frac{1}{V} \int \dd{^3r} + \frac{1}{V}\int (\exp[-\beta \varphi(r)] - 1) \dd{^3r}\\
        &= 1 + \frac{1}{V} \int (\exp[-\beta\varphi(r)] - 1) \dd{^3 r}\\
        &= 1 - \frac{2}{V}B_2
    \end{align}
    where
    \begin{equation}
        B_2 \coloneqq -\frac{1}{2} \int (\exp[-\beta \varphi(r)] - 1) \dd{^3r} = -2\pi \int (\exp[-\beta\varphi(r)] - 1)r^2 \dd{r}.
    \end{equation}
    This is called the \defineindex{second virial coefficient}\index{B2@\(B_2\)|see{second virial coefficient}}, the reason it is the second, and the reason for the factor of \(-1/2\), is that it appears as part of an expansion as we will soon see.
    
    Within this approximation we then have the configurational integral as
    \begin{equation}
        Q = \left( 1 - \frac{2}{V}B_2 \right)^{\binom{N}{2}}.
    \end{equation}
    We therefore have
    \begin{equation}
        F = F_{\mathrm{ideal}} - \boltzmann T \ln Q \approx F_{\mathrm{ideal}} + \frac{N^2 \boltzmann T}{V} B_2.
    \end{equation}
    Here we have made the approximation
    \begin{equation}
        \binom{N}{2} = \frac{N}{2}(N - 1) \approx \frac{N^2}{2}
    \end{equation}
    for large \(N\).
    We have also expanded
    \begin{equation}
        \ln\left( 1 - \frac{2}{V}B_2 \right) \approx -\frac{2}{V}B_2.
    \end{equation}
    We therefore have that the pressure is given
    \begin{equation}
        \beta P = \rho + B_2 \rho^2
    \end{equation}
    where \(\rho = N / V\) is the number density.
    This corresponds to the first two terms in the \defineindex{virial expansion} of the pressure in terms of the density, hence calling \(B_2\) the \emph{second} virial coefficient, the first being \(B_1 = 1\), and the choice of normalisation factor \(-1/2\).
    
    Note that we can also compute the entropy in this approximation:
    \begin{equation}
        S = -\diffp{F}{T}[V,N] = S_{\mathrm{ideal}} - N\boltzmann \rho \diffp{}{T}(TB_2).
    \end{equation}
    
    The virial expansion is a perturbation expansion in the density about the ideal gas.
    That is we have a series
    \begin{equation}
        \beta P = \sum_{n = 1}^{\infty} B_n \rho^n.
    \end{equation}
    The virial coefficients are then computed in a similar way to \(B_2\) as functions of the interaction potential \(\varphi\).
    A simple approximation for a generic \(\varphi\), which is repulsive nearby, attractive in the middle, and goes to zero at infinity, yields \(B_2 = b_0 - a_0\beta\) for positive constants \(b_0\) and \(a_0\).
    This allows us to recover the equation of state for a van der Waals gas,
    \begin{equation}
        (P + \rho^2 a_0) = \frac{N \boltzmann T}{V - N b_0}
    \end{equation}
    from the second order virial expansion.
    
    It can also be shown that the entropy here is lower than for an ideal gas.
    This is what we would expect from an information theoretic point of view since knowledge of the interactions should increase the information available so the entropy, which measures missing information, should decrease.
    
    We can expand the series to higher order.
    To do so we write
    \begin{equation}
        F_{ij} = \expected{F} + \lambda_{ij}
    \end{equation}
    where \(\lambda_{ij}\) is the deviation of \(F_{ij}\) from its average spatial value, which is such that \(\expected{\lambda_{ij}} = 0\).
    We then expand in powers of \(\lambda_{ij}\) and the result is
    \begin{align}
        Q &= \expected[\Bigg]{\raisebox{0.6ex}{\(\displaystyle\prod_{i<j}\)} (\expected{F} + \lambda_{ij})}\\
        &= \expected{F}^{\binom{N}{2}} + \binom{N}{2}\expected{F}^{\binom{N}{2} - 1} \expected{\lambda_{ij}}\\
        &\qquad+ \expected{F}^{\binom{N}{2} - 2} \left[ \binom{N}{4} \expected{\lambda_{ij}\lambda_{kl}} + \binom{N}{3}\expected{\lambda_{ij}\lambda_{jl}} \right] + \order(\lambda^3).
    \end{align}
    Note that we have to account for when the indices match or don't match.
    It turns out that expectation value of the \(\lambda\)s vanishes a lot of the time.
    The first non-vanishing erm in this series, other than \(\expected{F}^{\binom{N}{2}}\), is the \(\expected{\lambda_{ij}\lambda_{jk}\lambda_{ik}}\) term.
    
    It is possible to construct the non-vanishing terms using a diagrammatic process.
    In this we write out all indices appearing in a term and connect two indices, say \(i\) and \(j\), if \(\lambda_{ij}\) appears in the term.
    The only non-vanishing terms are then those where the diagram forms a closed loop.
    We can then compute the sum as a sum over closed diagrams.
    For example, the \(\expected{\lambda_{ij}\lambda_{jk}\lambda_{kl}}\) term corresponds to the diagram
    \begin{equation}\tikzsetnextfilename{diagrammatic-expansion-1}
        \begin{tikzpicture}
            \coordinate (i) at (210:0.5);
            \coordinate (k) at (90:0.5);
            \coordinate (j) at (-30:0.5);
            \node[below left] at (i) {\(i\)};
            \node[above] at (k) {\(k\)};
            \node[below right] at (j) {\(j.\)};
            \draw (i) -- (j) -- (k) -- cycle;
        \end{tikzpicture}
    \end{equation}
    Another non-vanishing term would be given by the diagram
    \begin{equation}\tikzsetnextfilename{diagrammatic-expansion-2}
        \begin{tikzpicture}
            \draw (0, 0) rectangle (0.75, 0.75);
            \node[below left] at (0, 0) {\(i\)};
            \node[below right] at (0.75, 0) {\(j.\)};
            \node[above right] at (0.75, 0.75) {\(k\)};
            \node[above left] at (0, 0.75) {\(l\)};.
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term \(\expected{\lambda_{ij}\lambda_{jk}\lambda_{kl}\lambda_{li}}\).
    
    At high densities the expansion can fail, meaning either the series doesn't converge or one of the virial coefficients diverges.
    Both of these typically indicate a phase transition.
    
    The higher order terms represent interactions between more particles, so the \(\expected{\lambda_{ij}\lambda_{jk}\lambda_{ki}}\) term represents three particle interactions and so on.
    For this reason this is called a \defineindex{cluster expansion} or a \defineindex{Mayer expansion!see{cluster expansion}}.
    
    \chapter{Reduced Density Functions}
    \section{Distribution Functions}
    In this section we consider \define{reduced density functions}\index{reduced density function}.
    In general
    \begin{equation}
        \rho_m(\vv{r}_1, \dotsc, \vv{r}_2)
    \end{equation}
    is the probability density function for any particle to be at \(\vv{r}_1\), some other particle to be at \(\vv{r}_2\), and so on.
    
    The one-particle reduced density function, \(\rho_1(\vv{r})\), is the probability density function for a particle to be at location \(\vv{r}\).
    By this we mean the probability of finding a particle in a region \(\dl{^3r}\) centred on \(\vv{r}\) is \(\rho_1(\vv{r})\dd{^3r}\).
    For a homogeneous material the one-particle reduced density function doesn't depend on position so
    \begin{equation}
        \rho_1(\vv{r}) = \frac{N}{V} = \rho.
    \end{equation}
    
    The two-particle reduced density function, \(\rho_2(\vv{r}_1, \vv{r}_2)\) is the probability density function for finding a particle at \(\vv{r}_1\) and simultaneously at \(\vv{r}_2\).
    That is \(\rho_2(\vv{r}_1, \vv{r}_2)\dd{^3r_1}\dd{^3r_2}\) is the probability that there is a particle in the volume \(\dl{^3r_1}\) centred on \(\vv{r}_1\) and a particle in the volume \(\dl{^3r_2}\) centred on \(\vv{r}_2\).
    For a homogeneous material the two-particle reduced density function doesn't depend on the absolute position of the particles, only their relative positions, so it is a function of \(\vv{r}_1 - \vv{r}_2\).
    If the material is also isotropic then the two-particle reduced density function doesn't depend on the orientation of the particles, only the distance between them, and so it is a function of \(\abs{\vv{r}_1 - \vv{r}_2}\).
    We will assume that this is the case so we can write
    \begin{equation}
        \rho_2(\vv{r}_1, \vv{r}_2) = \rho^2 g(\abs{\vv{r}_1 - \vv{r}_2}).
    \end{equation}
    This defines \(g\), which we call the \defineindex{radial distribution function} or \define{pair distribution function}\index{pair distribution function|see{radial distribution function}} or \define{pair correlation function}\index{pair correlation function|see{radial distribution function}}.
    
    To understand what \(g\) represents recall that we can write the two-particle reduced density function as
    \begin{equation}
        \rho_2(\vv{r}_1, \vv{r}_2) = \rho_2(\vv{r}_1 \mid \vv{r}_2) \rho_1(\vv{r}_2)
    \end{equation}
    where \(\rho_2(\vv{r}_1 \mid \vv{r}_2)\) is the conditional probability density of a particle being at \(\vv{r}_1\) given that there is a particle at \(\vv{r}_2\), and \(\rho_1\) is the one-particle reduced density function.
    Since we are considering a homogeneous material we have \(\rho_1(\vv{r}_2) = \rho\) and so we have
    \begin{equation}
        \rho_2(\vv{r}_1, \vv{r}_2) = \rho_2(\vv{r}_1 \mid \vv{r}_2) \rho = \rho g(\abs{\vv{r}_1 - \vv{r}_2}).
    \end{equation}
    This means we can interpret \(\rho g\) as the conditional probability of their being a particle at a point a distance \(r\) from the origin given that there is a particle at the origin.
    Therefore \(g\) measures the local density at a distance \(r\) given that there is a particle at the origin.
    We can ensure that this condition is satisfied by working in the rest frame of some randomly chosen particle.
    
    For non-interacting particles the particle positions are uncorrelated and we have \(\rho_2 = (N/V)^2 = \rho^2\).
    This means that \(g(r) = 1\).
    
    For a real gas \(g(r) \to 1\) as \(r \to \infty\), which is to say that interactions only have a finite range and so can't effect the distribution of particles at infinity.
    We also have \(g(0) = 0\), which reflects the fact that we can't have two particles at the same point.
    If \(g(r) > 1\) then the probabilities of finding particles at \(\vv{r}_1\) and \(\vv{r}_2\) are correlated.
    If \(g(r) < 1\) then the probabilities of finding particles at \(\vv{r}_1\) and \(\vv{r}_2\) are anticorrelated.
    
    The idea of a radial distribution function is very general.
    We have already discussed the ideal gas case (\(g(r) = 1\)).
    Another simple example is a crystal.
    Here the particles are highly localised in a regular pattern and so \(g\) is sharply peaked at each nearest neighbour distance.
    Typically the height of these peaks increases with distance since the number of \(k\)-nearest neighbours typically increases with \(k\).
    Something similar happens in glasses and liquids, but less regularly.
    The radial distribution will oscillate as we go through coordination shells.
    This is demonstrated in \cref{fig:radial distribution function}.
    
    \begin{figure}
        \tikzsetnextfilename{radial-distribution-function}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 5) node[left] {\(g(r)\)} -- (0, 0) -- (5, 0) node[below] {\(r\)};
            \draw[highlight, ultra thick] (0, 3) -- (5, 3);
            \draw[my blue, ultra thick, use Hobby shortcut] (0, 0) .. (0.2, 0.5) .. (0.5, 1) .. (0.75, 2.3) .. (1, 3) .. (1.2, 3.3) .. (1.5, 3.5) .. (1.8, 3.2) .. (2, 3) .. (2.4, 2.4) .. (2.6, 2.6) .. (3, 3) .. (3.5, 3.4) .. (3.9, 3) .. (4.2, 2.5) .. (4.8, 2.6) .. (5, 2.9);
            \draw[my red, ultra thick, rounded corners] (1, 0) -- (1.1, 4) -- (1.2, 0);
            \draw[my red, ultra thick, rounded corners] (3, 0) -- (3.1, 4) -- (3.2, 0);
            \draw[highlight, ultra thick] (5.1, 3) -- ++ (0.5, 0) node[right, black] {Ideal Gas};
            \draw[my blue, ultra thick] (5.1, 2.5) -- ++ (0.5, 0) node[right, black] {Liquid};
            \draw[my red, ultra thick] (5.1, 2) -- ++ (0.5, 0) node[right, black] {Crystal};
        \end{tikzpicture}
        \caption[Radial distribution function.]{The radial distribution function for an ideal gas (\(g(r) = 1\)), liquid, and crystal (sharply peaked).}
        \label{fig:radial distribution function}
    \end{figure}

    The radial distribution function can be measured by diffraction experiments, but is generally difficult to calculate exactly.
    However, a simple approximation for \(g(r)\) is
    \begin{equation}
        g(r) = \exp[-\beta \varphi(r)]
    \end{equation}
    where \(\varphi\) is the two-particle interaction potential.
    This ignores interactions between more than two particles.
    This approximation is motivated by giving the right limiting behaviour, as \(r \to \infty\) we have \(\varphi \to 0\), since we assume short ranged interactions, and so \(g \to 1\), and as \(r \to 0\) we have \(\varphi \to \infty\), representing a hardcore potential, and so \(g \to 0\).
    The other motivation is simply that this is statistical mechanics and so Boltzmann factors like this appear everywhere.
    
    \section{Virial Equation of State}
    It can be shown that the assumption that \(g\) is of the form \(g(r) = \exp[-\beta\varphi(r)]\) is equivalent to the assumption that \(Q = \expected{F}^{\binom{N}{2}}\), that is that the two particle correlation functions, \(F_{ij}\) and \(F_{ik}\), are uncorrelated.
    We start with the assumption that the interaction energy is pairwise additive, so
    \begin{equation}
        U(\{\vv{r}\}) = \sum_{i < j} \varphi(r_{ij})
    \end{equation}
    where \(r_{ij} = \abs{\vv{r}_i - \vv{r}_j}\).
    We then ave a general formula for the pressure given by
    \begin{equation}
        P = P_{\mathrm{ideal}} + P_{\mathrm{conf}} = \rho \boltzmann T - \frac{\rho^2}{6} \int_{0}^{\infty} r\diff{\varphi}{r} g(r) 4\pi r^2 \dd{r}.
    \end{equation}
    This is the \defineindex{virial equation of state}, which is more general than the previous virial expansion as we have not yet made any approximations.
    We will now demonstrate how one arrives at this formula.
    
    We start by introducing the coordinates \(\tilde{\vv{r}} = \vv{r} / L\) where \(L = V^{1/3}\).
    We then have
    \begin{align}
        Q &= \frac{1}{V^N} \prod_{i = 1}^{N} \exp[-\beta U(\{\vv{r}\})] \dd{^3r_i}\\
        &= \prod_{i = 1}^{N} \int \tilde{r}_i \exp\left[ -\beta \sum_{i < j} \varphi(L \tilde{r}_{ij}) \right] \dd{^3\tilde{r}_i}
    \end{align}
    where \(\tilde{r}_{ij} = r_{ij}/L\).
    The limits of integration are independent of \(L\), whereas before the change of variable they were from 0 to \(L\).
    We then have the configurational free energy, \(F_{\mathrm{conf}} = -\boltzmann T \ln Q\), which gives the configurational pressure
    \begin{align}
        P_{\mathrm{conf}} &= -\diffp{F_{\mathrm{conf}}}{V}[T]\\
        &= \boltzmann T \diffp{}{V}\ln Q\\
        &= \frac{\boltzmann T}{3L^2} \frac{1}{Q} \diffp{}{L} \prod_{i = 1}^{N} \int \exp\left[ -\beta \sum_{i < j} \varphi(L\tilde{r}_{ij}) \right] \dd{^3\tilde{r}_i}\\
        &= - \frac{1}{3V} \frac{1}{V^NQ} \prod_{i = 1}^{N} \int \sum_{i < j} r_{ij} \varphi'(r_{ij}) \exp\left[ -\beta \sum_{i < j} \varphi(r_{ij}) \right] \dd{^3r_i}\\
        &= - \frac{1}{3V} \prod_{i = 1}^{N} \int \sum_{i < j} r_{ij} \varphi'(r_{ij}) p(\vv{r}_1, \dotsc, \vv{r}_N) \dd{^3r_i}.
    \end{align}
    Performing all but the \(i\)th and \(j\)th integrals this becomes
    \begin{align}
        P_{\mathrm{conf}} &= -\frac{1}{3V} \sum_{i < j} \int r_{ij} \varphi'(r_{ij}) \frac{\rho_2(\vv{r}_i, \vv{r}_2)}{N(N - 1)} \dd{^3r_i} \dd{^3r_j}\\
        &= -\frac{1}{6V} \int r_{12} \varphi'(r_{12}) \rho_2(\vv{r}_1, \vv{r}_2) \dd{^3r_1} \dd{^3r_2}\\
        &- -\frac{\rho^2}{6V} \int r_{12} \varphi'(r_{12}) g(r_{12}) \dd{^3r_1}\dd{^3r_2}.
    \end{align}
    Here we have simply chosen to number our particles such that the two particles we care about are particles 1 and 2.
    Note that the factor of \(1/2\) taking us from \(1/(3V)\) to \(1/(6V)\) comes in the sum which gives a factor of \(N(N - 1)/2\).
    We then identify \(\rho^2g(r_{12}) = \rho_2(\vv{r}_1, \vv{r}_2)\), and we arrive at this result.
    
    The final step is to change variables to centre of mass coordinates again, so \(\vv{r} = \vv{r}_1 - \vv{r}_2\), so \(r = r_{12}\), and \(\vv{R} = (\vv{r}_1 + \vv{r}_2) / 2\), and use the fact that the integral over \(R\) just gives a factor of \(V\), since there is no \(R\) dependence in the integrand and so we achieve the result:
    \begin{align}
        P_{\mathrm{conf}} &= -\frac{\rho^2}{6V} \int r \varphi'(r) g(r) \dd{^3r} \dd{^3R}\\
        &= -\frac{\rho^2}{6} r \diff{\varphi}{r} g(r) 4\pi r^2 \dd{r}
    \end{align}
    where the factor of \(r^2\) comes from the Jacobian in spherical polar coordinates and the factor of \(4\pi\) comes from performing the integrals over the angular variables, since our isotropic functions have no angular dependence.
    
    The virial equation of state shows that for pairwise interactions both the configurational integral and the raidal distribution function contain the same information.
    Unfortunately neither can be computed exactly and so we resort to approximations.
    
    It is possible to recover the virial expansion from the virial equation of state by expanding \(g\) in powers of \(\rho\):
    \begin{equation}
        g(r) = g_0(r) + \rho g_1(r) + \order(\rho^2).
    \end{equation}
    The leading correction to the pressure is obtained from \(g_0(r)\), which gives
    \begin{align}
        P_{\mathrm{conf}} &= B_2 \rho^2 \boltzmann T\\
        &= -\boltzmann T\frac{\rho^2}{2} \int_{0}^{\infty} (\exp[-\beta \varphi(r)] - 1) 4\pi r^2 \dd{r}\\
        &= -\boltzmann T\frac{\rho^2}{2} \bigg( \left[(\exp[-\beta \varphi(r)] - 1)\frac{4}{3}\pi r^3 \right]_{0}^{\infty}\notag\\
        &\qquad\qquad- \int_0^\infty \diff{}{r}\left[(\exp[-\beta \varphi(r) - 1]) \frac{4}{3}\pi r^3 \right] \dd{r} \bigg)\\
        &= -\frac{\rho^2}{6} \int_{0}^{\infty} r \varphi'(r) \exp[-\beta\varphi(r)] 4\pi r^2 \dd{r}.
    \end{align}
    This is achieved by integrating by parts and noticing that for large \(r\) we need \(\varphi(r) \sim 1/r^{3 + \varepsilon}\) for some \(\varepsilon > 0\) so that \(r^3(\exp[-\beta\varphi(r) - 1]) \approx -r^3\beta\varphi(r) \sim 1/r^\varepsilon\) vanishes for large \(r\).
    If this is the case we recover the previous virial expansion.
    Therefore the approximation that \(g(r) = \exp[-\beta\varphi(r)]\) gives the same as the approximation that \(Q = \expected{F}^{\binom{N}{2}}\), that is if we want \(P\) to second order in \(\rho\) we need only expand \(g\) to zeroth order in \(\rho\), which gives the Boltzmann factor for two particle interaction energy, evaluated as if there were no other particles.
    
    \subsection{High Density Fluids}
    For an ordinary, uncharged, dense fluid the main problem is short range forces.
    For a single species fluid there are several approaches, but we won't go into detail on any.
    
    We can start with the pair correlation function, \(\rho_2\), and write down an expression for this quantity in terms of the unknown three-particle distribution function, \(\rho_3\).
    We can then express \(\rho_3\) in terms of \(\rho_4\) and so on.
    The higher order distributions are more complicated but the advantage of this is that we can truncate this process at some point with a \defineindex{closure equation}, such as
    \begin{equation}
        \rho_3(\vv{r}_1, \vv{r}_2, \vv{r}_3) = \rho_2(\vv{r}_1, \vv{r}_2) \rho_2(\vv{r}_2, \vv{r}_3) \rho_2(\vv{r}_1, \vv{r}_3).
    \end{equation}
    This scheme is called a Kirkwood hierarchy.
    With this assumption we get a closed system of equations from which we can work out \(g(r)\), and hence the virial equation of state.
    
    The exact way to perform the closure is somewhat of a guess and there are various schemes which give fairly good results, many of which can accurately predict a gas-liquid phase transition.
    Since closure schemes ignore or approximate higher order correlation functions they are an example of a mean field theory.
    
    \chapter{Debye--H\"uckel Theory}
    A \defineindex{plasma} is a fluid of charged particles.
    The problem with this is that the Coulomb potential goes as \(\varphi(r) \sim 1 / r\), which means that the virial equation of state will diverge, since it requires \(\varphi(r) \sim 1 / r^{3 + \varepsilon}\) for \(\varepsilon > 0\).
    The solution to this will become clear later but involves shielding of charges by the background charge.
    
    \begin{ntn}{}{}
        Unfortunately we now have a notation clash.
        We have been using \(\rho\) for number densities and \(\varphi\) for potential energies.
        In electromagnetism we usually reserve \(\rho\) for charge densities and \(\varphi\) for electric potentials.
        We will change to denoting number densities by \(n\) and the potential energy due to the electric potential \(\varphi\) is simply \(q\varphi\), where \(q\) is the charge of our particle.
    \end{ntn}
    
    We will consider a one component plasma, which is a gas of point particles of charge \(q\) with average number density \(n_{\infty}\).
    We can think of this as the number density infinitely far away.
    We will consider the case where there is a constant background density \(-qn_{\infty}\), which ensures that the net charge is zero.
    An example of such a plasma would be electrons in a semiconductor where the background is provided by the fixed ions and the electron density is sufficiently low that the classical limit holds.
    It is possible to carry out similar calculations to those we are about to do but allowing both positive and negative charges to move.
    This might be needed to consider a salt solution or interstellar gas.
    
    \subsection{Poisson--Boltzmann Equation}
    The radial distribution function, \(g\), is defined such that \(n(r) = n_{\infty}g(r)\) is the number density at a distance \(r\) from the origin, assuming there is a particle at the origin.
    Using our previous approximation of \(g(r) = \exp[-\beta q\varphi(r)]\) (where we now use \(q\varphi\) as the interaction energy since \(\varphi\) is the electromagnetic potential) we have
    \begin{equation}
        n(r) = n_{\infty} \exp[-\beta q \varphi(r)].
    \end{equation}
    Here \(\varphi(r) = kq/r\) is the Coulomb potential due to another particle of charge \(q\) at the origin.
    In SI units \(k = 1/(r\pi\varepsilon)\), where \(\varepsilon = \varepsilon_0 \varepsilon_{\mathrm{r}}\) is the permittivity of the medium and \(\varepsilon_{\mathrm{r}}\) is the relative permittivity.
    
    The electric potential, \(\varphi\), obeys Poisson's equation:
    \begin{equation}
        \laplacian \varphi = -\frac{\rho}{\varepsilon}
    \end{equation}
    where \(\rho\) is the charge density, which in general depends on the distance from the origin.
    There are three contributions to the charge density in this case:
    \begin{enumerate}
        \item The point charge, \(q\), at the origin which contributes
        \begin{equation}
            \rho_0(r) = q\delta(\vv{r}) = q\delta(r)
        \end{equation}
        where \(\delta\) is a Dirac delta distribution.
        \item The fixed background charge density,
        \begin{equation}
            \rho_{\mathrm{fixed}}(r) = -qn_{\infty}.
        \end{equation}
        \item The charge density due to all other free charges in the plasma.
        We assume that these charges are arranged around the central charge according to \(g\) and so
        \begin{equation}
            \rho_{\mathrm{free}}(r) = qn_{\infty}g(r) = qn_{\infty} \exp[-\beta q\varphi(r)].
        \end{equation}
    \end{enumerate}
    The total charge density is then
    \begin{equation}
        \rho = \rho_0 + \rho_{\mathrm{fixed}} + \rho_{\mathrm{free}} = -\frac{n_{\infty}q}{\varepsilon}(\exp[-\beta q\varphi(r)] - 1) - \frac{q}{\varepsilon}\delta(r).
    \end{equation}
    Hence Poisson's equation becomes
    \begin{equation}
        \laplacian\varphi = -\frac{n_{\infty}q}{\varepsilon}(\exp[-\beta q\varphi(r)] - 1) - \frac{q}{\varepsilon}\delta(r).
    \end{equation}
    This is called the \defineindex{Poisson--Boltzmann} equation.
    Note that this is not exact since we have ignored higher order correlations in the arrangement of free charges about the origin.
    
    The Poisson--Boltzmann equation is nonlinear, due to the \(\exp[-\beta q\varphi(r)]\) term.
    It can be solved numerically in order to find \(\varphi\).
    We can also consider the case where thermal fluctuations dominate, so \(q\varphi \ll \boltzmann T\), which requires low charge densities or high temperatures.
    In this case we Taylor expand the exponential giveing
    \begin{equation}
        \exp[-\beta q \varphi(r)] - 1 \approx -\beta q \varphi(r)
    \end{equation}
    and we get a linearised version of the Poisson--Boltzmann equation:
    \begin{equation}
        \laplacian \varphi = \frac{n_{\infty}q}{\varepsilon} \beta\varphi - \frac{q}{\varepsilon}\delta(r).
    \end{equation}
    Rearranging this we get
    \begin{equation}
        \laplacian\varphi - \frac{\varphi}{\lambda_{\mathrm{D}}^2} = -\frac{q}{\varepsilon}\delta(r)
    \end{equation}
    where
    \begin{equation}
        \lambda_{\mathrm{D}} \coloneqq \sqrt{\frac{\boltzmann T\varepsilon}{q^2n_{\infty}}}.
    \end{equation}
    This is called the \defineindex{Debye--H\"uckel equation}.
    The constant \(\lambda_{\mathrm{D}}\) has units of length and is called the \defineindex{Debye screening length}.
    We will see that it plays the role in our equation of a decay constant, hence our choice of calling it \(\lambda\).
    
    The Debye--H\"uckel equation is a Green's function for a charge \(q\) at the origin.
    The equation can be solved by Fourier transforming to \(k\)-space.
    However, we will simply state and check the result.
    First recall that for a function, \(f\), with no angular dependence
    \begin{equation}
        \laplacian f = \frac{1}{r^2} \diffp{}{r}\left( r^2 \diffp{f}{r} \right) = \frac{1}{r} \diffp[2]{}{r} (rf).
    \end{equation}
    We make an ansatz, based on the similarity to a point charge in free space, that the solution is of the form
    \begin{equation}
        \varphi(r) = \frac{A}{r}f(r)
    \end{equation}
    for some function \(f\) to be determined.
    We then have
    \begin{equation}
        \laplacian\varphi = \frac{1}{r}\diffp[2]{f}{r}, \qqand -\frac{1}{\lambda_{\mathrm{D}}^2} \varphi(r) = -\frac{1}{\lambda_{\mathrm{D}}^2} \frac{A}{r}f(r).
    \end{equation}
    Putting this together we have
    \begin{equation}
        \frac{A}{r} \left[ f''(r) - \frac{1}{\lambda_{\mathrm{D}}^2} f(r) \right] = -\frac{q}{\varepsilon}\delta(r).
    \end{equation}
    For \(r \ne 0\) this is a homogeneous second order differential equation with the solution
    \begin{equation}
        f(r) = B\exp[r/\lambda_{\mathrm{D}}] + C\exp[-r/\lambda_{\mathrm{D}}].
    \end{equation}
    In order for this not to diverge at infinity we require that \(B = 0\) and so we have
    \begin{equation}
        \varphi(r) = \frac{A}{r}\exp[-r/\lambda_{\mathrm{D}}],
    \end{equation}
    where we absorb the constant \(C\) into the definition of \(A\).
    
    We can find \(A\) by requiring Gauss' law to hold, namely that for some small volume, \(V\), given by a sphere of radius \(\delta\) centred on the origin
    \begin{align}
        \int_V \left[ \laplacian \varphi - \frac{\varphi}{\lambda_{\mathrm{D}}^2} \right] \dd{^3r} &= \int_{r \le \delta} \left[ \laplacian \varphi - \frac{\varphi}{\lambda_{\mathrm{D}}^2} \right] \dd{^3r}\\
        &= \int_{r = \delta} \grad \varphi \cdot \dd{\vv{S}} + \order(\delta^2)\\
        &= -4\pi A + \order(\delta^2)
    \end{align}
    and since there is a point charge, \(q\), at the origin we also have
    \begin{equation}
        \int_V -\frac{q}{\varepsilon}\delta(r) \dd{^3r} = -\frac{q}{\varepsilon}.
    \end{equation}
    Combining these and taking \(\delta \to 0\) we get
    \begin{equation}
        A = \frac{q}{4\pi\varepsilon},
    \end{equation}
    which is exactly the same factor we would get for a point charge in a vacuum.
    
    Therefore the solution is
    \begin{equation}
        \varphi(r) = \frac{q}{4\pi\varepsilon} \frac{1}{r} \exp[-r/\lambda_{\mathrm{D}}].
    \end{equation}
    This is called the \defineindex{screened Coulomb potential}, since for small \(r\) it goes as \(1/r\) like a normal Coulomb potential but for larger \(r\) it rapidly goes to zero.
    The interpretation is that at larger distances the background negative charge screens the charge at the origin and the electromagnetic effects are negligible.
    The distance at which this happens is controlled by \(\lambda_{\mathrm{D}}\).
    The radial density distribution and number density are plotted in \cref{fig:plasma distribution}.
    
    \begin{figure}
        \tikzsetnextfilename{screened-coulomb}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 4) node[left] {\(g(r)\)} -- (0, 0) -- (4, 0) node[below] {\(r\)};
            \begin{scope}
                \clip (0, 0) rectangle (4, 4);
                \draw[highlight, thick, domain=0.25:4, dashed, samples=100] plot (\x, 1/\x);
                \draw[highlight, ultra thick, domain=0.2:4, samples=100] plot (\x, {exp(-\x)/\x});
            \end{scope}
            \draw[|-|] (0, -0.3) -- ++ (1, 0) node[midway, below] {\(\lambda_{\mathrm{D}}\)};
            \draw[thick, dashed, highlight] (1, -1) -- ++ (0.5, 0) node[right, black] {\(q/(4\pi\varepsilon r)\)};
            \draw[ultra thick, highlight] (1, -1.5) -- ++ (0.5, 0) node[right, black] {\(g(r)\)};
            \begin{scope}[xshift=5cm]
                \draw[very thick, <->] (0, 4) node[left] {\(n(r)\)} -- (0, 0) -- (4, 0) node[below] {\(r\)};
                \begin{scope}
                    \clip (0, 0) rectangle (4, 4);
                    \draw[highlight, thick, domain=0.25:4, dashed] (0, 3) -- (4, 3);
                    \draw[highlight, ultra thick, domain=0.01:4, samples=100] plot (\x, {3*exp(-exp(-\x)/\x)});
                    \node[left] at (0, 3) {\(n_\infty\)};
                \end{scope}
                \draw[thick, dashed, highlight] (1, -1) -- ++ (0.5, 0) node[right, black] {\(n_{\infty}\)};
                \draw[ultra thick, highlight] (1, -1.5) -- ++ (0.5, 0) node[right, black] {\(n(r)\)};
                \draw[|-|] (0, -0.3) -- ++ (1, 0) node[midway, below] {\(\lambda_{\mathrm{D}}\)};
            \end{scope}
        \end{tikzpicture}
        \caption[Plasma distributions]{The radial density distribution, \(g\), and number density, \(n\), for a plasma. Note the characteristic length \(\lambda_{\mathrm{D}}\) and the equivalent quantities ignoring all correlations shown as dashed lines.}
        \label{fig:plasma distribution}
    \end{figure}
    
    This process can readily be modified for slightly different setups.
    For example, we can include multiple species of charged particles.
    The main change is then that we end up with \(\lambda_{\mathrm{D}}\) as a sum over the species.
    We could also consider a charged plane of with a plasma on one side.
    We then account for the plane by treating it as a boundary condition for the resulting differential equation.
    
    Debye--H\"uckel theory is a self consistent theory, in the sense that \(\varphi\) depends on \(n\) and \(n\) determines \(\varphi\), so we get the values of \(\varphi\) and \(n\) by looking for solutions consistent with these interdependencies.
    It is also a non-perturbative theory, there isn't a systematic way to improve by including higher order terms in the same way there was for the virial expansion.
    In order to improve the results here we would need new insights.
    
    Debye--H\"ukel theory is a mean field theory, meaning it ignores higher order correlations.
    In this case we consider two-point correlations, that is interactions of two particles, through the two particle potential, \(\varphi\), but we ignore three-point correlations.
    
    \chapter{Phase Transitions}
    \section{Solid-Liquid-Gas}
    Consider a system with three phases, solid, liquid, or gas.
    We can plot a phase diagram of pressure against temperature separating the plane into regions associated with each phase.
    Along the boundaries the neighbouring phases can coexist.
    A phase diagram is shown in \cref{fig:phase diagram}, this is qualitative and the exact details will depend on the material in question.
    
    \begin{figure}
        \tikzsetnextfilename{phase-diagram}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 5) node[left] {\(P\)} -- (0, 0) -- (6, 0) node[below] {\(T\)};
            \draw[highlight, ultra thick, domain=1:4, samples=100] plot (\x, {sqrt(\x - 1)});
            \fill[highlight] (4, {sqrt(3)}) circle [radius = 0.07];
            \node[below right, font=\footnotesize] at (3.9, 1.8) {\parbox{1cm}{\centering Critical Point}};
            \draw[dashed, highlight, very thick] (4, {sqrt(3)}) -- (4, 0) node[below, black] {\(T_{\mathrm{c}}\)};
            \draw[dashed, highlight, very thick] (4, {sqrt(3)}) -- (0, {sqrt(3)}) node[left, black] {\(P_{\mathrm{c}}\)};
            \fill[highlight] (2, 1) circle [radius = 0.07];
            \node[below right, font=\footnotesize] at (1.8, 1) {\parbox{1cm}{\centering Triple Point}};
            \draw[highlight, ultra thick, domain=0:0.75, xshift=2cm, yshift=1cm, ->] plot (\x, {4*sqrt(\x)});
            \node[highlight, font=\sffamily\bfseries] at (1, 3) {Solid};
            \node[highlight, font=\sffamily\bfseries] at (3, 2.5) {Liquid};
            \node[highlight, font=\sffamily\bfseries] at (3.4, 0.75) {Gas};
            \node[highlight, font=\footnotesize\sffamily\bfseries, right] at (3.8, 2.3) {\parbox{2cm}{\centering Supercritical Fluid}};
        \end{tikzpicture}
        \caption[Phase diagram]{A \((T, P)\) diagram showing the triple point and critical point, \((T_{\mathrm{c}}, P_{\mathrm{c}})\), as well as regions where the phase is solid, liquid, gas, and supercritical fluid.}
        \label{fig:phase diagram}
    \end{figure}
    
    The \defineindex{triple point} is defined as the point where all three phases can coexist, that is where the sold-liquid, solid-gas, and liquid-gas phase boundaries meet.
    The \defineindex{critical point} is defined as the end of the coexistence curve, beyond this point instead of liquid and gas as two separate phases we instead get a supercritical fluid.
    The critical point has the unique thermodynamic coordinates \(T_{\mathrm{c}}\), \(P_{\mathrm{c}}\), and \(\rho_{\mathrm{c}}\), for temperature, pressure and number density respectively.
    
    The critical isotherm, that is the isotherm at temperature \(T_{\mathrm{c}}\), has zero slope at \(\rho_{\mathrm{c}}\).
    This means that the \defineindex{isothermal compressibility}, given by
    \begin{equation}
        \kappa_{T} = -\frac{1}{V} \diffp{V}{P}[T],
    \end{equation}
    diverges at the critical point.
    This means that we will see large scale fluctuations in the volume and density.
    
    Below the critical isotherm there is a coexistence region where the density of the gas phase, \(\rho_{\mathrm{g}}\), and the density of the liquid phase, \(\rho_{\mathrm{l}}\), combine to give the total density, \(\rho\).
    In this region changing the overall density doesn't effect the pressure, it just causes the system to move toward one of the phases, increasing density promotes the liquid phase and decreasing promotes the gas phase.
    This region is shown in \cref{fig:phase diagram density}
    
    \begin{figure}
        \tikzsetnextfilename{phase-diagram-density}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 5) node[left] {\(P\)} -- (0, 0) -- (5, 0) node[below] {\(\rho\)};
            \draw[ultra thick, highlight, use Hobby shortcut] (0, 0) .. (0.5, 0.5) .. (1, 0.8) .. (2, 1.3) .. (3, 1.6) .. (4, 2.5) .. (5, 4);
            \node[right] at (5, 4) {\(T > T_{\mathrm{c}}\)};
            \draw[ultra thick, my blue, use Hobby shortcut] (0, 0) .. (0.5, 0.4) .. (1, 0.5) .. (2, 0.6) .. (3, 1.2) .. (4, 2) .. (5, 3);
            \node[right] at (5, 3) {\(T = T_{\mathrm{c}}\)};
            \node[above, font=\scriptsize] (poi) at (1, 2) {\parbox{1cm}{\centering Point of Inflection}};
            \draw[->, thick] (1.4, 0.52) to[bend left] (poi.south);
            \fill[my blue] (1.4, 0.52) circle [radius = 0.075];
            \draw[ultra thick, my red, use Hobby shortcut] (0, 0) .. (0.3, 0.2) .. (0.5, 0.25) .. (1, 0.29) -- (1.8, 0.31) .. (2.4, 0.5) .. (3, 0.8) .. (4, 1.5) .. (5, 2.2);
            \node[right] at (5, 2.2) {\(T < T_{\mathrm{c}}\)};
            \draw[my blue, dotted, thick] (1.4, 0.52) -- (1.4, 0) node[below, black] {\(\rho_{\mathrlap{\mathrm{c}}}\)};
            \draw[my red, dotted, thick] (1, 0.29) -- (1, 0) node[below, black] {\(\rho_{\mathrlap{\mathrm{g}}}\)};
            \draw[my red, dotted, thick] (1.8, 0.29) -- (1.8, 0) node[below, black] {\(\rho_{\mathrlap{\mathrm{l}}}\)};
            \node[right, font=\scriptsize] at (1.8, 0.15) {Critical Region};
            \node[font=\scriptsize] (lin reg) at (1, -0.6) {Linear Region};
            \draw[thick, ->] ($(lin reg.north west) + (0.1, -0.1)$) to[bend left] (0.1, 0.1);
            \begin{scope}
                \clip (0, 0) rectangle (5, 5);
                \draw[dashed] (0, 0) circle [radius = 0.4];
                \draw[dashed] (1.4, 0) circle [radius = 0.52];
            \end{scope}
        \end{tikzpicture}
        \caption[Phase diagram showing the density]{A \((\rho, P)\) phase diagram showing three isotherms, one higher than the critical temperature, one lower, and one at the critical temperature. Notice the flat region for isotherms lower than the critical temperature where changing the density doesn't change the pressure, just the amount of the system in the solid and liquid phases which coexist in this region. For low densities we can approximate the isotherms as linear, which is the case for an ideal gas.}
        \label{fig:phase diagram density}
    \end{figure}
    
    \begin{figure}
        \tikzsetnextfilename{phase-diagram-density-temperature}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 5) node[left] {\(\rho\)} -- (0, 0) -- (5, 0) node[below] {\(T\)};
            \draw[ultra thick, highlight] (1.97, 2) -- (5, 2);
            \draw[ultra thick, highlight, dashed, domain=-2:0, xshift=2cm, yshift=2cm] plot (\x, {sqrt{-\x}});
            \draw[ultra thick, highlight, dashed, domain=-2:0, xshift=2cm, yshift=2cm] plot (\x, {-sqrt{-\x}});
            \draw[dashed, thick] (2, 2) -- (2, 0) node[below] {\(T_{\mathrm{c}}\)};
            \node at (1, 0.75) {\(\rho_{\mathrm{g}}\)};
            \node at (1, 3.25) {\(\rho_{\mathrm{l}}\)};
        \end{tikzpicture}
    \end{figure}
    
    \section{Magnet}
    Consider a magnetic solid made up of a collection of atoms with dipole moments.
    At high temperatures with no external field the dipole moments will be randomly aligned and the net magnetisation will be negligible.
    If an external field is applied then the dipole moments will align and the magnet will become magnetised.
    This is called the \defineindex{paramagnetic phase}.
    At low temperatures interactions between dipoles tend to align the dipoles.
    Below some critical temperature, \(T_{\mathrm{c}}\), the magnet will have a significant magnetisation, even if there is no external field applied.
    This is called the \defineindex{ferromagnetic phase}.
    
    In general if the external field, \(H\), is such that the magnetisation, \(M\), aligns with the external field then there will be a discontinuity for \(T < T_{\mathrm{c}}\) as the external field changes direction, the magnetisation will swap from positive to negative as the magnetisation goes from positive to negative.
    At the critical temperature we have \(M_{\mathrm{c}} = 0\) due to symmetry between the two ferromagnetic phases with positive and negative net magnetisations.
    The critical isotherm giving the magnets field, \(H\), as a function of the magnetisation, \(M\), will have zero slope at \(M = 0\), which corresponds to the response function diverging:
    \begin{equation}
        \chi = \diffp{M}{H}[H = 0] \to \infty.
    \end{equation}
    This means there will be large scale fluctuations in the magnetisation.
    
    \section{Binary Alloy}
    Consider an alloy of two types of atoms, \(A\) and \(B\), which occur in equal concentration arranged in a regular lattice.
    Again there will be some critical temperature, \(T_{\mathrm{c}}\), such that for \(T > T_{\mathrm{c}}\) we will have a disordered phase, where the atoms are arranged randomly on the sites, and for \(T < T_{\mathrm{c}}\) we will have an ordered state, where we get phase separation with atoms of type \(A\) and type \(B\) separating into two different regions.
    
    \section{Common Features}
    There are some features common to all phase transitions.
    The first is the coexistence curve, a line separating two phases which differ by macroscopic properties.
    Second is the critical point marking the end of the coexistence curve.
    Given a system we can define an order parameter, \(O\), which characterises the difference between the phases.
    For the example of the solid-liquid-gas system we can define \(O = \rho_{\mathrm{l}} - \rho_{\mathrm{g}}\).
    For the magnetic example we can define \(O = M\) to be the total magnetisation.
    
    At a phase transition we get a qualitative change in macroscopic properties as some parameter, usually the temperature, is varied.
    There are two broad classes of phase transition.
    A \defineindex{discontinuous phase transition}, or \define{first order phase transition}\index{first order phase transition|see{discontinuous phase transition}}, has a discontinuous jump in the order parameter upon crossing the coexistence curve.
    A \defineindex{continuous phase transition} has \(O\) change continuously across the coexistence curve.
    For a \defineindex{second order phase transition} the response function diverges, which is to say that the derivatives of \(O\) are discontinuous across the coexistence curve.
    For a third order phase transition the response function is continuous, but its derivative isn't and so on.
    We will focus on first and second order phase transitions.
    
    \chapter{Ising Model}
    We are interested in studying phase transitions.
    To do so we will use several techniques, including exact methods and approximations, such as mean field theories.
    In order to unify these approaches we shall apply them all to the same model, the Ising model, which is one of the most studied models in physics.
    On the surface this model appears to be about magnetisation, we can think of it as a basic version of the magnet described in the previous section, in which the dipoles can only align in one  of two directions.
    However, with slight modifications the Ising model can describe a lot of systems.
    We'll see some examples of other uses as we go.
    
    \section{The Model}
    The \defineindex{Ising model} consists of a lattice of \(N\) spins, which can be either up or down.
    That's it.
    We denote the spin at the \(i\)th site by \(S_i\), such that \(S_i = 1\) for spin up and \(S_i = -1\) for spin down.
    We also allow for some external field, \(h\), to be applied such that if the spins align with the field then the energy is reduced.
    We suppose that the spins couple with their nearest neighbours such that if they align the net energy is reduced.
    The total energy is then
    \begin{equation}
        E(\{S_i\}) = -h\sum_{i}S_i - J\sum_{\langle i, j \rangle} S_i S_j
    \end{equation}
    where \(\sum_{\langle i, j \rangle}\) is a sum over nearest neighbours.
    That means we sum over \(i\) from 1 to \(N\) and then \(j\) takes on whatever values are needed such that \(S_j\) is the spin of a spin which is nearest neighbours to \(S_i\).
    The \defineindex{coupling constant}, \(J\), is just a constant with dimensions of energy that tells us how much aligning two spins decreases the energy by, similarly \(h\) is just how much a single spin aligning with the external field decreases the energy.
    
    We define the \defineindex{coordination number}, \(z\), to be the number of nearest neighbours a given spin has.
    For example, a two-dimensional square lattice has \(z = 4\), a three-dimensional simple cubic lattice has \(z = 6\), and a face centred cubic lattice has \(z = 12\).
    For a system of \(N\) spins with coordination number \(z\) there are \(N z / 2\) nearest neighbour pairs, the factor of \(1/2\) stops us double counting since if we counted \(S_iS_j\) we don't want to also include \(S_jS_i\).
    
    The partition function for the Ising model is
    \begin{equation}
        \cpartition = \sum_{S_1 = \pm 1}\sum_{S_2 = \pm 1} \dotsb \sum_{S_N = \pm 1} \exp[-\beta E(\{S_i\})].
    \end{equation}
    
    The free energy of the system is
    \begin{equation}
        F = E - TS. 
    \end{equation}
    The equilibrium state is given by minimising \(F\).
    At low temperatures the \(TS\) term is negligible and \(F\) is minimised when \(E\) is.
    At higher temperatures the \(TS\) term dominates and \(F\) is minimised when \(TS\) is maximised.
    We therefore expect two different phases, one at low temperatures where the equilibrium state minimises energy, that is all spins align with each other and the external field, and one at high temperatures where the entropy dominates and the state is therefore disordered with essentially randomly aligned spins.
    
    The one-dimensional Ising model can be solved exactly and we will do so later.
    The two-dimensional Ising model can also be solved exactly but we won't do this.
    In three or more dimensions there is no exact solution to the Ising model.
    Instead we need to come up with approximations.
    We shall do this in the next section.
    
    \section{Mean Field Theory of the Ising Model}
    The mean field approximation starts from the configurational energy
    \begin{equation}
        E(\{S_i\}) = -h\sum_iS_i - J\sum_{\langle i, j \rangle} S_i S_j.
    \end{equation}
    Consider the contribution to the energy due to some particular spin, \(S_j\):
    \begin{equation}
        \varepsilon(S_j) = -hS_j - JS_j\sum_{k\text{ nn}} S_k
    \end{equation}
    where the sum is over the nearest neighbours (nn) of \(S_j\).
    The mean-field approximation, called the \defineindex{Weiss approximation}, is that the effect of the \(z\) individual nearest neighbours can be replaced by their average effect,
    \begin{equation}
        \varepsilon_{\mathrm{mf}}(S_j) = -hS_j - JS_j \sum_{k\text{ nn}} \expected{S_k} = -h_{\mathrm{mf}}S_j,
    \end{equation}
    where \(mf\) stands for mean field and 
    \begin{equation}
        h_{\mathrm{mf}} = h + Jzm.
    \end{equation}
    Essentially we can modify the external field to account for the effect of the neighbours through the mean magnetisation \(m = \expected{S_i} = \expected{M}/N\).
    
    Using this approximation the probability distribution for a single spin is
    \begin{align}
        P(S_j) &= \frac{\exp[-\beta\varepsilon_{\mathrm{mf}}(S_j)]}{Z(1)}\\
        &= \frac{\exp[-\beta\varepsilon_{\mathrm{mf}}(S_j)]}{\sum_{S_i=\pm 1} \exp[-\beta\varepsilon_{\mathrm{mf}}(S_i)]}\\
        &= \frac{\exp[\beta h_{\mathrm{mf}}S_]}{\exp[\beta h_{\mathrm{mf}}] + \exp[-\beta h_{\mathrm{mf}}]}.
    \end{align}
    
    There is still a \defineindex{consistence condition} to fulfil.
    The probability density depends on \(h_{\mathrm{mf}}\), which depends on the mean magnetisation per spin, \(m\), which depends on the probability distribution.
    We therefore need to find \(m\) in such a way that this all remains consistent.
    The value of \(m\) is
    \begin{align}
        m &= \sum_{S_j = \pm 1} S_jP(S_j)\\
        &= \frac{\exp[\beta h_{\mathrm{mf}}] - \exp[-\beta h_{\mathrm{mf}}]}{\exp[\beta h_{\mathrm{mf}}] + \exp[\beta h_{\mathrm{mf}}]} = \tanh(\beta h_{\mathrm{mf}}).
    \end{align}
    Therefore we need the mean magnetisation per spin to satisfy
    \begin{equation}
        m = \tanh(\beta h + \beta Jzm).
    \end{equation}
    
    It is possible to derive an alternate form of this relationship using
    \begin{align}
        m &= \frac{1}{\beta} \diffp{}{h_{\mathrm{mf}}} \ln Z(1)\\
        &= \frac{1}{\beta} \diffp{}{h_{\mathrm{mf}}} \ln\left[ \sum_{S_j = \pm 1} \exp[-\beta h_{\mathrm{mf}}S_j] \right]\\
        &= \frac{1}{\beta} \diffp{}{h_{\mathrm{mf}}} \ln( \exp[-\beta h_{\mathrm{mf}}] + \exp[\beta h_{\mathrm{mf}}] )\\
        &= \frac{1}{\beta} \diffp{}{h_{\mathrm{mf}}} \ln[2\cosh(\beta h_{\mathrm{mf}})].
    \end{align}
    Taking the derivative will get the same result we have, however, in some cases it may be easier to manipulate the function in this form before taking the derivative to get some desired result.
    
    \subsection{Solving the Consistency Condition}
    For simplicity we consider the case of zero applied field, so \(h = 0\).
    The consistency condition is then
    \begin{equation}
        m = \tanh(\beta Jzm).
    \end{equation}
    This is a transcendental equation, meaning that it has no elementary solution.
    Instead we consider the graphs \(y = \tanh(\beta Jzm)\) and \(y = m\).
    This is done in \cref{fig:ising model consistency solution} for two values of \(\beta Jz\).
    What we see is that there is either a single solution or three solutions depending on the value of \(\beta Jz\).
    
    \begin{figure}
        \tikzsetnextfilename{ising-model-consistency-solution}
        \begin{tikzpicture}[samples=100]
            \draw[very thick, ->] (-4, 0) -- (4, 0) node[below] {\(m\)};
            \draw[very thick, ->] (0, -2.5) -- (0, 2.5) node[left] {\(y\)};
            \draw[ultra thick, highlight, domain=-4:4] plot (\x, {2*tanh(\x)});
            \draw[ultra thick, my red, domain=-4:4] plot (\x, {2*tanh(\x/3)});
            \draw[ultra thick, my blue] (-2.5, -2.5) -- (2.5, 2.5);
            \draw[ultra thick, highlight] (0.5, -0.5) -- ++ (0.5, 0) node[right, black] {\(y = \tanh(\beta Jzm)\), \(\beta Jz = 1\)};
            \draw[ultra thick, my red] (0.5, -1) -- ++ (0.5, 0) node[right, black] {\(y = \tanh(\beta Jzm)\), \(\beta Jz = 1/3\)};
            \draw[ultra thick, my blue] (0.5, -1.5) -- ++ (0.5, 0) node[right, black] {\(y = m\)};
        \end{tikzpicture}
        \caption[Solving the consistency equation for the Ising model.]{Plot of the functions \(y = m\) and \(y = \tanh(\beta Jzm)\) for two values of \(\beta Jz = 1, 1/3\) showing that \(m = \tanh(\beta Jzm)\) has either one or three solutions depending on the value of \(\beta Jz\). The solutions are where the lines \(y = m\) and \(y = \tanh(\beta Jzm)\) cross.}
        \label{fig:ising model consistency solution}
    \end{figure}
    
    In the case where there is a single solution that solution is \(m = 0\).
    This means that the spins are essentially arranged randomly.
    This case corresponds to \(\beta J z\) being small, which is to say to high temperatures.
    The three solutions case also has \(m = 0\) as a solution, but it also has two solutions \(m = \pm m_0\) for some constant \(m_0\), which correspond to all of the spins being aligned either up or down depending on the sign.
    
    We are interested in phase transitions so we want to know when the Ising model goes from one to three solutions.
    We get three solutions when \(y = \tanh(\beta Jzm)\) is steeper than \(y = m\) at \(m = 0\), so when
    \begin{equation}
        \diff*{\tanh(\beta Jzm)}{m}[m = 0] > 1.
    \end{equation}
    There are two approaches we can take to solve this.
    
    Since this consistency equation is fairly simple we can just perform the derivative, let \(x = \beta Jzm\) and then
    \begin{equation}
        \diff*{\tanh(x)}{m} = \diff{x}{m}\diff*{\tanh x}{x} = \beta Jz \sech^2(\beta Jzm)
    \end{equation}
    evaluating at zero since \(\sech 0 = 1\) we see that the transition occurs when \(\beta Jzm = 1\).
    
    For a more complicated consistency equation it may be easier to first expand and then take the derivative.
    The Taylor expansion of \(\tanh\) starts
    \begin{equation}
        \tanh x = x - \frac{x^3}{3} + \dotsb.
    \end{equation}
    Since we will be evaluating at \(m = 0\), and hence \(x = 0\), we can neglect any constant terms, which vanish in the derivative, and any terms of order \(x^2\) or higher, which vanish when we evaluate at \(x = 0\), leaving us only with the linear term.
    Therefore we have
    \begin{equation}
        \diff*{\tanh x}{m} = \diff{x}{m}\diff*{\tanh x}{x} = \beta Jz \diff{x}{x} = \beta Jz,
    \end{equation}
    and so again we see the transition occurs when \(\beta Jz = 1\).
    
    We can now define a \defineindex{critical temperature},
    \begin{equation}
        T_{\mathrm{c}} \coloneqq \frac{Jz}{\boltzmann},
    \end{equation}
    which is the temperature at which \(\beta Jz = 1\) and the phase transition occurs.
    Note that \(\beta Jz = T_{\mathrm{c}} / T\).
    
    \subsection{Critical Behaviour}
    \subsubsection{Order Parameter}
    We will now consider the behaviour of the system near the critical temperature, \(T_{\mathrm{c}}\).
    The consistency condition can be expressed as
    \begin{equation}
        m = \tanh\left( m\frac{T_{\mathrm{c}}}{T} \right).
    \end{equation}
    Near the critical temperature \(T \approx T_{\mathrm{c}}\) and \(\abs{m} \ll 1\), since spins are likely to be fairly randomly arranged.
    Therefore we can expand the consistency condition to
    \begin{equation}
        m \approx m\frac{T_{\mathrm{c}}}{T} - \frac{m^3}{3} \left( \frac{T_{\mathrm{c}}}{T} \right)^3 + \order(m^5).
    \end{equation}
    This is a cubic equation in \(m\).
    The solutions are either that \(m = 0\) or we can divide by \(m\) and we get the quadratic
    \begin{equation}
        1 = \frac{T_{\mathrm{c}}}{T} - \frac{m^2}{3}\left( \frac{T_{\mathrm{c}}}{T} \right)^3 \implies m^2 = 3\left( \frac{T_{\mathrm{c}}}{T} \right)^3 \left( \frac{T_{\mathrm{c}}}{T} - 1 \right).
    \end{equation}
    Now define the \defineindex{reduced temperature}
    \begin{equation}
        t \coloneqq \frac{T - T_{\mathrm{c}}}{T_{\mathrm{c}}} = \frac{T}{T_{\mathrm{c}}} - 1,
    \end{equation}
    which we can think of as a measure of how far we are from the critical temperature.
    Importantly \(t = 0\) when \(T = T_{\mathrm{c}}\).
    Rearranging this definition we have
    \begin{equation}
        \frac{T}{T_{\mathrm{c}}} = 1 + t, \qqand \frac{T_{\mathrm{c}}}{T} = \frac{1}{1 + t}.
    \end{equation}
    We can then recast our equation for \(m^2\) above in terms of the reduced temperature:
    \begin{equation}
        m^2 = 3(1 + t)^3\left( \frac{1}{1 + t} - 1 \right) = -3t + \order(t^2).
    \end{equation}
    Notice that this means \(m\) is only real if \(t < 0\), and clearly \(m\) must be real for physical solutions.
    This corresponds to the fact that for \(t < 0\) we have \(T < T_{\mathrm{c}}\), which is the region where there are three (real) solutions to the consistency condition.
    
    We can therefore characterise the behaviour near the critical temperature.
    For \(T > T_{\mathrm{c}}\) we have \(m = 0\), corresponding to randomly aligned spins.
    For \(T < T_{\mathrm{c}}\) we have \(m \approx \pm 3\sqrt{\abs{t}}\).
    
    \subsubsection{Response Function}
    Now that we have the leading order behaviour in \(t\) for the order parameter, the magnetisation in this system, we move on to the behaviour response function, which for this system is the susceptibility:
    \begin{equation}
        \chi = \diffp{m}{h}\bigg\vert_{h = 0}.
    \end{equation}
    In order to find this we again expand the consistency condition now for nonzero external field, \(m = \tanh(\beta(h + Jzm))\).
    This gives
    \begin{equation}
        m = \beta h + m\frac{T_{\mathrm{c}}}{T} - \frac{m^3}{3}\left( \frac{T_{\mathrm{c}}}{T} \right)^3.
    \end{equation}
    We have neglected here terms of higher order than \(m^3\), but also a term proportional to \(hm^2\), since near the critical point \(m = 0\) or \(m \propto \sqrt{t}\), both of which are small, and since we take the derivative for \(h = 0\) we can also treat \(h\) as small.
    
    Taking the derivative we get
    \begin{equation}
        \chi = \beta + \chi \frac{T_{\mathrm{c}}}{T} - \chi m^2 \left( \frac{T_{\mathrm{c}}}{T} \right)^3,
    \end{equation}
    where we have used \(\diffp{m}/{h}|_{h = 0} = \chi\) to implicitly differentiate \(m\).
    Rearranging this we get
    \begin{equation}
        \chi = \beta\left[ 1 - \frac{T_{\mathrm{c}}}{T} + m^2\left( \frac{T_{\mathrm{c}}}{T} \right)^3 \right]^{-1}.
    \end{equation}
    For small \(t\) and using the appropriate value of \(m\), namely \(m = 0\) for \(T > T_{\mathrm{c}}\) and \(m = \pm 3\sqrt{\abs{t}}\) for \(T < T_{\mathrm{c}}\), for \(T > T_{\mathrm{c}}\) we get
    \begin{equation}
        \chi = \frac{\beta}{1 - T_{\mathrm{c}}/T} \sim \frac{\beta_{\mathrm{c}}}{t}
    \end{equation}
    where \(\beta_{\mathrm{c}} = 1/(\boltzmann T_{\mathrm{c}})\).
    For \(T < T_{\mathrm{c}}\) we get
    \begin{equation}
        \chi = \frac{\beta}{1 - T_{\mathrm{c}}/T + 3(T_{\mathrm{c}}/T - 1)} \sim \frac{\beta_{\mathrm{c}}}{2\abs{t}}.
    \end{equation}
    
    Notice that for \(t \to 0\) the response function diverges, indicating large fluctuations and a phase change.
    This corresponds to the graph of \(m\) being vertical at the critical temperature.
    
    Note that when we say \(a \sim t^{\gamma}\) what we mean is that
    \begin{equation}
        \lim_{t \to 0} \frac{\ln a}{\ln \abs{t}} = \gamma.
    \end{equation}
    We call \(\gamma\) a \defineindex{critical exponent} in this context.
    
    \subsubsection{Limitations}
    The mean field assumption is that we can neglect the correlations between spins.
    Mathematically this means that
    \begin{equation}
        \expected{S_iS_j} \approx \expected{S_i}\expected{S_j}
    \end{equation}
    for \(i \ne j\).
    We can write the energy of the Ising model as
    \begin{equation}
        E(\{S_i\}) = E_0 - h\sum_j S_j
    \end{equation}
    where \(E_0 = -J \sum_{\langle i, j \rangle} S_iS_j\).
    This makes the energy the same form as when we discussed fluctuations in \cref{sec:fluctuations general theory}.
    Hence we can identify the observable \(A = \sum_j S_j\) and conjugate field \(f = h\).
    We then know that the response function is
    \begin{equation}
        \chi_{AA} \coloneqq \diffp{\expected{A}}{f} = \beta[\expected{A^2} - \expected{A}^2].
    \end{equation}
    In the case of the Ising model we can identify \(\chi\) as the susceptibility per site given by
    \begin{equation}
        \chi = \diffp{m}{f} = \frac{1}{N}\chi_{AA} = \frac{\beta}{N} \sum_{j} \sum_{k} [\expected{S_jS_k} - \expected{S_j}\expected{S_k}].
    \end{equation}
    This equation is exact, but in the mean field equation all terms with \(j \ne k\) vanish and we are left with
    \begin{equation}
        \chi = \beta[1 - m^2].
    \end{equation}
    This doesn't diverge at \(T_{\mathrm{c}}\) and so we see that the mean field theory is inconsistent with regard to the susceptibility, \(\chi\).
    The problem is that the correlations become important as we approach the critical point.
    
    \subsection{Correlation Length}
    Define the \defineindex{connected correlation function} to be
    \begin{equation}
        G_{ij} \coloneqq \expected{S_iS_j} - \expected{S_i}\expected{S_j} = \expected{(S_i - \expected{S_i})(S_j - \expected{S_j})}.
    \end{equation}
    This measures the correlations between fluctuations, \(\Delta S_i = S_i - \expected{S_i}\).
    If \(G_{ij}\) is positive then this implies that the fluctuations of the spins are correlated.
    
    Defining \(R_{ij}\) as the distance between spins \(i\) and \(j\) for sufficiently large \(R_{ij}\) we expect that 
    \begin{equation}
        G_{ij} \approx C(R_{ij}) \exp[-R_{ij}/\xi],
    \end{equation}
    where \(C\) is some function which changes more slowly than the exponential and \(\xi\) is the \defineindex{correlation length}, which essentially cuts off the correlation function at distances greater than \(\xi\).
    
    When \(T > T_{\mathrm{c}}\) we have \(\expected{S_i} = 0\) and so a typical microstate consists of clusters of up and down spins, while the net magnetisation is zero.
    Intuitively we can think of \(\xi\) as being a measurement of the size of the largest cluster off correlated spins.
    As the temperature decreases to the critical temperature we expect \(\xi\) to diverge, as we start to get long range correlations.
    
    A refined mean field theory can be used to compute \(G_{ij}\), the two-point correlation function.
    Doing so we would find that
    \begin{equation}
        \xi \sim \abs{t}^{-1/2},
    \end{equation}
    which clearly diverges at \(t = 0\), which corresponds to \(T = T_{\mathrm{c}}\).
    
    \subsection{Summary of Mean Field Theory}
    In the absence of an external field the mean field theory predicts a critical temperature, \(T_{\mathrm{c}} = zJ/\beta\), at which there is a phase change.
    
    Near the critical temperature the behaviour is described by a few parameters which scale as \(\abs{t}\) to some critical exponent, where \(t = (T - T_{\mathrm{c}})/T_{\mathrm{c}}\).
    In particular for \(T > T_{\mathrm{c}}\) we have \(m = 0\), whereas for \(T < T_{\mathrm{c}}\) we have
    \begin{equation}
        T \sim m_{-}\abs{t}^{\tilde{\beta}} = 3\abs{t}^{1/2}.
    \end{equation}
    Note that traditionally this critical exponent is called \(\beta\), but we add a tilde to avoid confusion with \(\beta = 1/(\boltzmann T)\).
    The susceptibility near the critical temperature is given by
    \begin{equation}
        \chi \sim \chi_{\pm} \abs{t}^{-\gamma} = 
        \begin{cases}
            \beta_{\mathrm{c}}\abs{t}^{-1} & T > T_{\mathrm{c}},\\
            \frac{\beta_{\mathrm{c}}}{2}\abs{t}^{-1} & T < T_{\mathrm{c}}.
        \end{cases}
    \end{equation}
    The correlation length is given by
    \begin{equation}
        \chi \sim \xi_{\pm}\abs{t}^{-\nu} = \abs{t}^{-1/2}.
    \end{equation}
    
    As before when we say that, for example, \(m \sim \abs{t}^{\tilde{\beta}}\) what we mean is that
    \begin{equation}
        \lim_{\abs{t} \to 0} \frac{\ln m}{\ln \abs{t}} = \tilde{\beta}.
    \end{equation}
    
    We can define more critical exponents, for example the zero-field heat capacity is
    \begin{equation}
        C_{h} = \diffp{\mean{E}}{T}\bigg\vert_{h = 0} \sim \abs{t}^{-\alpha}.
    \end{equation}
    Just below \(T_{\mathrm{c}}\) we can characterise the discontinuity in the order parameter, \(m\), across the coexistence line by
    \begin{equation}
        h \sim \abs{m}^{\delta} \sgn(m).
    \end{equation}
    Using mean field theory we find that the values are \(\alpha = 0\) and \(\delta = 3\).
    
    The critical exponents, \(\alpha\), \(\tilde{\beta}\), \(\gamma\), \(\delta\), and \(\nu\) characterise the critical point.
    However, not all of these components are independent, it turns out only three are, which we can take to be \(\tilde{\beta}\), \(\gamma\), and \(\nu\).
    
    Experiments give slightly different values of the critical exponents, and these values depend on the number of dimensions.
    However, the Ising model can be modified to describe many different systems and, to the level of accuracy of the mean field theory, they mostly share the same values of the critical exponents, in three dimensions, \(\tilde{\beta} = 0.31\), \(\gamma = 1.25\), and \(\nu = 0.64\).
    This is called \defineindex{universality}.
    
    \section{Exact Results in One Dimension}
    It turns out that in one or two dimensions we can exactly solve the Ising model.
    We will do so here for one dimension, which is significantly easier than solving exactly in two dimensions.
    We will also assume no external field.
    
    In one dimension the Ising model is a row of spins, all of which can be up or down.
    We can give the state of the system in multiple ways.
    First, we can list the spins, which is what we've been doing so far.
    A more useful description for our current purposes is to define some new variable, \(n_i\), which is 1 if there is a domain wall after spin \(i\), and zero otherwise.
    That is it is 1 if \(S_i \ne S_{i + 1}\).
    This is related to the spin values by
    \begin{equation}
        n_i = \frac{1}{2}(1 - S_i S_{i + 1}) = 
        \begin{cases}
            1 & \text{if there is a domain wall after spin }i,\\
            0 & \text{if there isn't a domain wall after spin }i.
        \end{cases}
    \end{equation}
    Inverting this we have
    \begin{equation}
        S_i S_{i + 1} = 1 - 2n_i,
    \end{equation}
    and so we can write the energy of the system as
    \begin{equation}
        E = -J\sum_{\langle i, j \rangle}S_{i}S_{i + 1} = -J\sum_{i = 1}^{N}(1 - 2n_i) = -NJ + 2J\sum_{i = 1}^{N}n_i.
    \end{equation}
    The first term is just a constant, and as usual we can ignore constant terms added to the energy.
    We can then treat the system as a non-interacting assembly of domain walls with two states, \(n_i = 1\), which has energy \(\varepsilon = 2J\), and \(n_i = 0\), which has energy \(\varepsilon = 0\).
    Since the system is non-interacting we can write down the Boltzmann distribution for a particular value of \(i\):
    \begin{equation}
        P(n_i) = \frac{\e^{-\beta \varepsilon(n_i)}}{\e^{-\beta \varepsilon(0)} + \e^{-\beta \varepsilon(1)}} = \frac{\e^{-2\beta Jn_i}}{1 + \e^{-2\beta J}}.
    \end{equation}
    Now define \(p = P(1)\) as the probability that a domain wall is present at any given location.
    This has the high and low temperature behaviour
    \begin{equation}
        p \approx
        \begin{cases}
            \e^{-2\beta J} \to 0 & T \to 0,\\
            \displaystyle\frac{1}{2} - \frac{1}{2}\beta J \to \frac{1}{2} & T \to \infty.
        \end{cases}
    \end{equation}
    
    We see that this has the expected behaviour of no domain walls at \(T = 0\), since all of the spins align, and a disordered state with essentially randomly aligned spins at high temperatures, meaning that there is a fifty-fifty chance of two neighbouring spins being aligned, so the probability of a domain wall is \(1/2\).
    However, \(p\) goes smoothly between these two values, there is no phase transition.
    There is only one phase for \(T > 0\), and it is a paramagnetic phase.
    
    To better understand this we examine the two point correlation function and correlation length.
    Consider two particular sites, \(j\) and \(k\), separated by a distance of \(l\) units, that is \(l = \abs{j - k}\).
    Let \(m\) be the number of domain walls between \(j\) and \(k\).
    If \(m\) is even then there are an even number of alignment changes between \(j\) and \(k\) and so \(S_i = S_j\), similarly if \(m\) is odd \(j \ne k\).
    We then have that
    \begin{equation}
        S_j S_k = 
        \begin{cases}
            +1 & \text{if } m \text{ is even},\\
            -1 & \text{if } m \text{ is odd}.
        \end{cases}
    \end{equation}
    We can write this as \(S_jS_k = (-1)^m\).
    We then average to obtain
    \begin{equation}
        \expected{S_jS_k} = \sum_{m=0}^{l} p_m(-1)^m
    \end{equation}
    where \(p_m\) is the probability of having \(m\) domain walls between the two sites.
    The value of \(p_m\) is given by the binomial distribution for having \(m\) domain walls in the \(l\) possible locations between the two sites, so
    \begin{equation}
        p_m = \binom{l}{m}p^m(1 - p)^{l - m} \implies \sum_{m = 0}^{l} (p + 1 - p)^l = 1,
    \end{equation}
    where we recognise \(p_m\) as the terms appearing in the binomial expansion of \((p + 1 - p)^l\).
    We therefore have
    \begin{equation}
        \expected{S_jS_k} = \sum_{m = 0}^{l} \binom{l}{m} p^m (1 - p)^{l - m} (-1)^m = (-p + 1 - p)^l = (1 - 2p)^l.
    \end{equation}
    Rearranging this gives
    \begin{equation}
        \expected{S_jS_k} = (1 - 2p)^l = \exp[l\ln(1 - 2p)] = \e^{-l/\xi}.
    \end{equation}
    So we can identify the correlation length as
    \begin{equation}
        \xi = \frac{1}{\abs{\ln(1 - 2p)}} = 
        \begin{cases}
            \dfrac{1}{2}\e^{2\beta J} \to \infty & T \to 0,\\
            \dfrac{1}{\abs{\ln(\beta J)}} \to 0 & T \to \infty.
        \end{cases}
    \end{equation}
    From this we see that the correlation length decreases as temperature decreases, but it only diverges as \(T \to \infty\).
    This means the only \enquote{phase transition} is at \(T = 0\), which isn't really a phase transition since we can't get to the \enquote{phase} below \(T = 0\).
    We conclude that
    \begin{equation}
        \lim_{l \to \infty} \expected{S_j S_k} = 0
    \end{equation}
    for all positive temperatures.
    For a ferromagnetic phase, which doesn't exist in one dimension, there would be long range order, and so this quantity would be nonzero.
    
    \subsection{Absence of Long Range Order in One Dimension}
    Consider a one-dimensional system of particles with fixed position where each particle can be in one of two states, \(A\) or \(B\).
    This is a generalisation of the Ising model, which is the case where \(A = 1\) and \(B = -1\).
    We want to see if domain walls are favoured thermodynamically.
    
    Let \(\Delta E\) be the energy cost of introducing a single domain wall.
    In order for this to be finite the system must be limited to short range interactions.
    For example, in the Ising model we consider only nearest neighbour interactions and \(\Delta E = 2J\).
    We could introduce next nearest neighbour interactions if we desired.
    Importantly, \(\Delta E\) doesn't depend on \(N\), the size of the system.
    The entropy gain due to creating a domain wall can be computed using the Boltzmann entropy, \(\Delta S = \boltzmann \ln \Omega\).
    For the first domain wall \(\Omega = N\), since there are \(N\) possible locations for a domain wall, for the next there are \(N - 1\) choices and so on.
    For a system with \(m\) domain walls \(\Omega = \binom{N}{m}\).
    
    For sufficiently large \(N\) the free energy change by introducing a \(m\) domain walls,
    \begin{equation}
        \Delta F = \Delta E - T\Delta S = \Delta E - \boltzmann T\ln \binom{N}{m},
    \end{equation}
    will be dominated by entropy term for all \(T > 0\), and hence long range order cannot be maintained.
    
    \subsection{General Solution in One Dimension}
    We can solve the Ising model with an external field in one dimension.
    To do so we wish to calculate the partition function,
    \begin{equation}
        \cpartition = \sum_{\{S_i = \pm 1\}} \e^{-\beta E(\{S_i\})} = \sum_{S_i = \pm 1} \sum_{S_2 = \pm 1} \dotso \sum_{S_N = \pm 1} \e^{-\beta E(\{S_i\})}.
    \end{equation}
    We can write the energy as
    \begin{equation}
        E(\{S_i\}) = -\frac{h}{2} \sum_{i = 1}^{N}S_i - \frac{h}{2}\sum_{i = 1}^{N}S_{i + 1} - J\sum_{i} S_iS_{i + 1}.
    \end{equation}
    Here we are using periodic boundary conditions, so \(S_{N + 1} = S_1\).
    We then have
    \begin{equation}
        \e^{-\beta E} = \prod_{i = 1}^{N} \exp\left[ \frac{\beta h}{2}(S_i + S_{i + 1}) + \beta J_{S_iS_{i + 1}} \right].
    \end{equation}
    We then have
    \begin{equation}
        \cpartition = \sum_{\{S_i = \pm 1\}} \prod_{i = 1}^{N} T(S_i, S_{i + 1}),
    \end{equation}
    where
    \begin{equation}
        T(S_i, S_{i + 1}) \coloneqq \exp\left[ \frac{\beta h}{2} (S_i + S_{i + 1}) + \beta JS_iS_{i + 1} \right].
    \end{equation}
    We can write the values \(T(S_i, S_{i + 1})\) as a \(2 \times 2\) symmetric matrix, called the \defineindex{transfer matrix}, \(T\), where the rows correspond to \(S_i = \pm 1\) and the columns to \(S_{i + 1} = \pm 1\):
    \begin{equation}
        T = 
        \begin{blockarray}{lll}
            \text{\scriptsize\(S_{i+1} = +1\)} & \text{\scriptsize\(S_{i+1} = -1\)} & \\
            \begin{block}{(ll)l}
                \e^{\beta(J + h)} & \e^{-\beta J} & \text{\scriptsize\(S_i = +1\)}\\
                \e^{-\beta J} & \e^{\beta(J - h)} & \text{\scriptsize\(S_i = -1\)}\\
            \end{block}
        \end{blockarray}
        .
    \end{equation}
    We can then write the transition function with periodic boundary conditions as
    \begin{align}
        Z &= \sum_{\{S_i = \pm 1\}} T(S_1, S_2) T(S_2, S_3), \dotsm T(S_{N - 1}, S_N)T(S_N, S_1)\\
        &= \tr(T^N)\\
        &= \lambda_+^N + \lambda_-^N,
    \end{align}
    where \(\lambda_{\pm}\) are the eigenvalues of \(T\).
    We can calculate these to be
    \begin{equation}
        \lambda_{\pm} = \e^{\beta J} \cosh(\beta h) \pm \sqrt{\e^{2\beta J} \sinh^2(\beta h) + \e^{-2\beta J}}.
    \end{equation}
    The free energy per spin is
    \begin{equation}
        f = \frac{F}{N} = -\frac{\boltzmann T}{N}\ln \cpartition.
    \end{equation}
    For large \(N\) we find that
    \begin{equation}
        f \to -\boltzmann T \ln \lambda_+,
    \end{equation}
    since \(\lambda_{+} - \lambda_{-} > 0\) and so \(\lambda_-/\lambda_+ < 1\).
    We can then find various thermodynamic properties as derivatives of the free energy.
    
    \subsection{Phase Transitions in Two Dimensions}
    We now consider the case of a two-dimensional Ising model on a square lattice.
    This can also be solved exactly, although we won't do this.
    A domain wall is now an extended object, made of a chain of links separating two neighbouring sites.
    Take the length of the chain to be \(\tilde{N}\).
    The energy cost of the whole chain is then
    \begin{equation}
        \Delta E = 2J\tilde{N}.
    \end{equation}
    
    To evaluate the entropy gain due to this domain wall we have to estimate \(\Omega\), the number of possible paths that a domain wall of length \(\tilde{N}\) can take.
    Consider a lattice of \(N\) sites with side length \(L\), so \(N = L^2\).
    We are free to rotate the lattice without changing anything so there is noting special about which edge of the lattice the chain starts at.
    There are therefore \(L\) choices for where the chain can start.
    Not allowing the chain to double back on itself at any time there are three directions we could place the next link.
    We therefore approximate
    \begin{equation}
        \Omega \approx 3^{\tilde{N}}L.
    \end{equation}
    This is only an upper bound but its good enough.
    The entropy cost of this domain wall is then
    \begin{equation}
        \Delta S = \boltzmann \tilde{N} \ln 3 + \boltzmann \ln N \approx \boltzmann \tilde{N} \ln 3.
    \end{equation}
    The free energy cost of this domain wall is given by
    \begin{equation}
        \Delta F \approx \tilde{N}(2J - \boltzmann T \ln 3).
    \end{equation}
    For sufficiently low \(T\), in particular if
    \begin{equation}
        T < \frac{2J}{\ln 3},
    \end{equation}
    we have \(\Delta F > 0\) and a domain wall is not thermodynamically favourable.
    We therefore have, for sufficiently low \(T\), an ordered phase with long range order.
    
    We know that at large \(T\) the domain wall is favoured and we will have a disordered phase.
    This occurs at some critical temperature, \(T_{\mathrm{c}}\).
    Which it can be shown has our estimate of \(2J/\ln 3\) as a lower bound.
    
    The exact solution to the two-dimensional Ising model, called the \defineindex{Onsager solution}, gives
    \begin{equation}
        T_{\mathrm{c}} = \frac{2J}{\ln (\sqrt{2} + 1)},
    \end{equation}
    since \(\sqrt{2} + 1 \approx 2.41\) our lower bound is not too far off.
    
    \section{Lattice Gas}
    The Ising model can be modified to describe many systems.
    As an example we consider a \defineindex{lattice gas}, which is a lattice where each site is either occupied by a single particle, or empty.
    We can then assign to the \(i\)th lattice site a value
    \begin{equation}
        c_i = 
        \begin{cases}
            1 & \text{if site \(i\) is occupied},\\
            0 & \text{if site \(i\) is unoccupied}.
        \end{cases}
    \end{equation}
    The overall number concentration of the gas is
    \begin{equation}
        c = \frac{1}{N} \sum_{i = 1}^{N} c_i,
    \end{equation}
    which is simply the fraction of lattice sites with \(c_i = 1\).
    
    Notice that this model has built in hard core repulsion, since the lattice sites can have at most one occupant.
    We can introduce a short range potential as an energy \(-\varepsilon < 0\) associated with a pair of neighbouring particles, in which case
    \begin{equation}
        E = -\varepsilon \sum_{\langle i, j \rangle} c_ic_j.
    \end{equation}
    
    Allowing the total number of gas particles to change we work in the grand canonical ensemble.
    We then get the partition function
    \begin{equation}
        \gcpartition = \sum_{\{c_i = 0, 1\}} \exp\left[ -\beta(E = \mu \sum_{i = 1}^{N} c_i) \right] = \sum_{\{c_i = 0, 1\}} \exp\left[ \beta \varepsilon \sum_{\langle i, j \rangle} c_ic_j \beta\mu \sum_{i = 1}^{N} c_i \right].
    \end{equation}
    
    We can define the effective energy to be
    \begin{equation}
        E - \mu\sum_{i = 1}^{N} c_i,
    \end{equation}
    which we can map onto the Ising energy, up to a constant, by identifying
    \begin{equation}
        S_i = 2c_i -1 = \pm 1, \qquad J = \frac{\varepsilon}{4}, \qqand h = \frac{1}{4}(\varepsilon z + 2\mu).
    \end{equation}
    Recall that \(z\) is the coordination number.
    We therefore have
    \begin{equation}
        \gcpartition^{\mathrm{LG}} = \mathrm{const} \times \cpartition^{\mathrm{Ising}}.
    \end{equation}
    Taking logs of this equation we find that the grand potential of the lattice gas and the free energy of the Ising model are the same, up to an unimportant constant:
    \begin{equation}
        \Phi_{\mathrm{LG}}(T, \mu) = F_{\mathrm{Ising}}(T, h) + \mathrm{const}.
    \end{equation}
    
    Using the results from the Ising model we expect a discontinuous transition as \(\mu\) is decreased past \(\mu_{\mathrm{c}} = -\varepsilon z/2\) for \(T < T_{\mathrm{c}} = \varepsilon z/(4\boltzmann)\) we will have a \enquote{liquid} phase and a \enquote{gas} phase with concentrations
    \begin{equation}
        c_{\mathrm{l}} = \frac{1}{2}(1 + \abs{m}), \qqand c_{\mathrm{g}} = \frac{1}{2}(1 - \abs{m}),
    \end{equation}
    respectively.
    Here \(\abs{m}\) is the magnetisation of the Ising model.
    
    \chapter{Landau Theory}
    In this section we discuss \defineindex{Landau theory}.
    This is a very general method which we will explore by once again considering the Ising model.
    
    \section{Mean Field Theory of the Ising Model (Again)}
    The average energy of the Ising model is
    \begin{equation}
        \mean{E} = -h\sum_{i = 1}^{N}\expected{S_i} - J\sum_{\langle i, j \rangle} \expected{S_iS_j}.
    \end{equation}
    Making the approximation of uncorrelated spins this becomes
    \begin{equation}
        \mean{E} \approx -h\sum_{i = 1}^{N} \expected{S_i} = J\sum_{\langle i, j \rangle} \expected{S_i}\expected{S_j}.
    \end{equation}
    We now use \(\expected{S_i} = m\) and this becomes
    \begin{equation}
        \mean{E} \approx -hNm - \frac{1}{2}JzNm^2,
    \end{equation}
    where \(z\) is the coordination number of the lattice, that is the number of nearest neighbours that any lattice point has.
    Notice that the second sum in the mean energy is a sum over pairs of nearest neighbours, all contributing \(-Jm^2\) to the energy.
    There are \(Nz/2\) nearest neighbour pairs since there are \(N\) particles with \(z\) nearest neighbours but don't want to overcount by considering \(AB\) and \(BA\), so we divide by 2.
    
    Now consider the mean magnetisation per spin, \(m\), this is given by
    \begin{align}
        m &= \expected{S_i}\\
        &= \sum_{S_i = \pm 1}^{N} S_iP(S_i)\\
        &= P(1) - P(-1)\\
        &= c - (1 - c),
    \end{align}
    where \(c\) is the mean field probability of a spin being up.
    Rearranging this formula we see that
    \begin{equation}
        c = \frac{1}{2}(1 + m).
    \end{equation}
    The Gibbs entropy of the Ising model is then
    \begin{equation}
        S = -\boltzmann\sum_{\{S_i = \pm 1\}}P(S_i)\ln[P(S_i)] = -N\boltzmann[c\ln c + (1 - c)\ln(1 - c)].
    \end{equation}
    The Helmholtz free energy is defined as
    \begin{equation}
        F(m) = E(m) - TS(m).
    \end{equation}
    The Helmholtz free energy per spin is
    \begin{equation}
        f(m) \coloneqq \frac{F(m)}{N} = -hm - \frac{1}{2}Jzm^2 + \boltzmann T[c\ln c + (1 - c)\ln(1 - c)].
    \end{equation}
    
    In order to find the equilibrium state we should minimise \(f\) with respect to \(m\) at constant \(h\).
    Doing so we have
    \begin{align}
        \diffp{f}{m} &= -h - Jzm + \boltzmann T \diffp{c}{m}\diffp{}{c}[c\ln c + (1 - c)\ln(1 - c)]\\
        &= -h - Jzm + \frac{1}{2}\boltzmann T[\ln c + 1 - \ln(1 - c) - 1]\\
        &= -h - Jzm + \frac{1}{2}\boltzmann T[\ln c - \ln(1 - c)]\\
        &= -h - Jzm + \frac{1}{2}\boltzmann T\ln\left( \frac{c}{1 - c} \right)\\
        &= 0.
    \end{align}
    Rearranging this we get
    \begin{equation}
        \beta(h + Jzm) = \frac{1}{2}\ln\left( \frac{c}{1 - c} \right) = \frac{1}{2}\ln\left( \frac{1 + m}{1 - m} \right) = \artanh(m)
    \end{equation}
    from which we get
    \begin{equation}
        m = \tanh(\beta(h + Jzm)),
    \end{equation}
    which is the same result as we got previously.
    
    \subsection{Spontaneous Symmetry Breaking}
    Consider the free energy per spin, \(f(m)\), at different temperatures.
    For simplicity we first take \(h = 0\).
    We then have
    \begin{equation}
        \beta f(m) = -\frac{T_{\mathrm{c}}}{T}\frac{m^2}{2} + \left[ \frac{1 + m}{2} \ln\left( \frac{1 + m}{2} \right) + \frac{1 - m}{2} \ln\left( \frac{1 - m}{2} \right) \right] + \ln 2.
    \end{equation}
    Here we have chosen to define the free energy such that \(f(0) = 0\), hence the added constant \(\ln 2\), which is fine as the energy is only defined up to a constant.
    We now expand this for small \(m\) giving
    \begin{equation}
        \beta f(m) = -\frac{hm}{\boltzmann T} + \frac{T - T_{\mathrm{c}}}{T} \frac{m^2}{2} + \frac{m^4}{12} + \order(m^6).
    \end{equation}
    Here we include the factor of \(h\), for later use.
    Plotting this we get \cref{fig:landau free energy}.
    Notice that for \(T \ge T_{\mathrm{c}}\) there is a single minimum at \(m = 0\), corresponding to the single root to \(m = \tanh(\beta Jzm)\) at this temperature.
    For \(T < T_{\mathrm{c}}\) there are two minima, corresponding to the two equivalent phases of all spins up or all spins down.
    
    \begin{figure}
        \tikzsetnextfilename{landau-free-energy}
        \begin{tikzpicture}[samples=100]
            \draw[very thick, ->] (-5, 0) -- (5, 0) node[below] {\(m\)};
            \draw[very thick, ->] (0, -2) -- (0, 5) node[left] {\(f\)};
            \begin{scope}
                \clip (-5, -2) rectangle (5, 5);
                \draw[ultra thick, highlight, domain=-2:2] plot (2.5*\x, \x*\x*\x*\x);
                \draw[ultra thick, my blue, domain=-2:2] plot (2.5*\x, \x*\x*\x*\x + 2*\x*\x);
                \draw[ultra thick, my red, domain=-2:2] plot (2.5*\x, \x*\x*\x*\x - 2*\x*\x);
            \end{scope}
            \draw[ultra thick, my blue] (-3, -2.5) -- ++ (1, 0) node[right, black] {\(T > T_{\mathrm{c}}\)};
            \draw[ultra thick, highlight] (-0.5, -2.5) -- ++ (1, 0) node[right, black] {\(T = T_{\mathrm{c}}\)};
            \draw[ultra thick, my red] (2, -2.5) -- ++ (1, 0) node[right, black] {\(T < T_{\mathrm{c}}\)};
        \end{tikzpicture}
        \caption[Free energy of the Ising model.]{The free energy of the Ising model plotted for \(T < T_{\mathrm{c}}\), \(T = T_{\mathrm{c}}\), and \(T > T_{\mathrm{c}}\). In all cases \(h = 0\).}
        \label{fig:landau free energy}
    \end{figure}
    
    There are a few things to notice.
    First, \(f\) is symmetric, that is \(f(-m) = f(m)\).
    Second, at \(T = T_{\mathrm{c}}\) we have
    \begin{equation}
        \diffp[2]{f}{m}[m = 0] = 0.
    \end{equation}
    For \(T < T_{\mathrm{c}}\) we say that the symmetry \(m \to -m\) is spontaneously broken since the system must select one of the minima as its equilibrium state.
    
    If we now allow \(h\) to take a positive value then for \(T < T_{\mathrm{c}}\) we instead get \cref{fig:landau free energy positive external field}.
    There are still two minima with the value of \(h\) chosen.
    For larger values of \(h\) the minimum for negative \(m\) vanishes.
    The fact that there is only one global minimum reflects the fact that when there is an external field it is energetically favourable for the spins to align with it.
    For a negative external field the result would be the same but the negative minima would be the one that is selected for.
    
    \begin{figure}
        \tikzsetnextfilename{landau-free-energy-nonzer-external-field}
        \begin{tikzpicture}[samples=100]
            \draw[very thick, ->] (-5, 0) -- (5, 0) node[below] {\(m\)};
            \draw[very thick, ->] (0, -2) -- (0, 5) node[left] {\(f\)};
            \begin{scope}
                \clip (-5, -2) rectangle (5, 5);
                \draw[ultra thick, highlight, domain=-2:2] plot (2.5*\x, \x*\x*\x*\x - 2*\x*\x - 0.5*\x);
            \end{scope}
        \end{tikzpicture}
        \caption[Free energy of the Ising model with positive external field.]{The free energy of the Ising model with positive external field. Note that there are still too minima, but only one is a global minimum now, corresponding to the fact that the external field lowers the energy if the spins align with it.}
        \label{fig:landau free energy positive external field}
    \end{figure}
    
    \section{Landau Free Energy}
    The process of Landau theory is motivated by the previous example of the Ising model.
    We look for a way to write down an approximation of the free energy, called the \defineindex{Landau free energy}, without having to do detailed calculations.
    The idea is that we can often do this without considering microscopic details, instead we just consider the symmetries of the system.
    The general process is as follows:
    \begin{itemize}
        \item Identify the order parameter, for the Ising model it is \(m\).
        \item Analyse the behaviour of the system near the critical point, where the order parameter is small.
        \item Define the free energy density as a power series.
        \item Eliminate terms which don't respect the symmetry of the system.
        \item Truncate the power series at some suitable point once the physics is capture, for the Ising model this is after the \(m^4\) term.
    \end{itemize}
    
    For the order parameter the desirable properties is that it is zero in the high temperature disordered phase and nonzero in the ordered phase.
    For the case of the Ising model the important symmetry, in the absence of external fields, is that the system is invariant under \(m \to -m\).
    This means that the free energy contains only even terms:
    \begin{equation}
        f(m) = \mathrm{const} + am^2 + bm^4 + \order(m^6).
    \end{equation}
    The constant value isn't important and so we are free to take it as \(0\) so \(f(0) = 0\).
    We truncate at fourth order only because we know ahead of time that this is all that is needed to capture the physics.
    We expect \(a\) and \(b\) to be smooth functions of the temperature.
    
    Minimising the free eneergy we get
    \begin{equation}
        f'(m) = 2am + 4bm^3 = 0 \implies m^2 = -\frac{a}{2b}.
    \end{equation}
    Assuming that \(b\) is finite the transition must occur when \(a = 0\), so we identify \(a\) as being of the form
    \begin{equation}
        a(T) = (T - T_{\mathrm{c}})\times\mathrm{const} = ta_0
    \end{equation}
    where \(t = (T - T_{\mathrm{c}})/T_{\mathrm{c}}\).
    
    For a small applied field, \(h\), the symmetry is broken and we include a linear term:
    \begin{equation}
        f(m) = -hm + am^2 + bm^4,
    \end{equation}
    where we know from our previous work that the linear term should be \(-hm\).
    Notice that when \(h\) goes from positive to negative there is a discontinuous change of the global minima from positive to negative \(m\).
    
    \subsubsection{Critical Exponents}
    Once we have the Landau free energy we can analyse it to find the critical exponents.
    By minimising \(f(m)\) in the low temperature ordered phase we have
    \begin{equation}
        f'(m) = 2ta_{0} + 4bm^3 = 0 \implies m = \sqrt{\frac{a_0\abs{t}}{2b}}.
    \end{equation}
    Defining the critical exponent \(\tilde{\beta}\) by \(m \sim \pm \abs{t}^{\tilde{b}}\) we identify \(\tilde{\beta} = 1/2\).
    
    At \(T = T_{\mathrm{c}}\) we have \(t = 0\) and then
    \begin{equation}
        f(m) = -hm + bm^4.
    \end{equation}
    Minimising this we get
    \begin{equation}
        f'(m) = -h + 4bm^4 = 0 \implies h = 4bm^3.
    \end{equation}
    Defining the critical exponent \(\delta\) by \(h \sim \abs{m}^{\delta}\sgn(m)\) at \(t = 0\) we see that \(\delta = 3\).
    
    To find the behaviour of the susceptibility we minimise \(f(m)\) with respect to \(m\), which gives
    \begin{equation}
        f'(m) = -h + 2am + 4bm^3 = 0.
    \end{equation}
    Taking the derivative with respect to \(h\), and remembering that \(\diffp{m}{h}|_{h = 0} = \chi\) we have
    \begin{equation}
        -1 + 2a\chi + 12bm^2\chi = 0.
    \end{equation}
    From this we see that at \(h = 0\)
    \begin{equation}
        \chi \sim 
        \begin{cases}
            \dfrac{1}{2a_0t} & t < 0,\\
            \dfrac{1}{4a_0\abs{t}} & t > 0.
        \end{cases}
    \end{equation}
    Defining the critical exponent \(\gamma\) by \(\chi \sim \abs{t}^{-\gamma}\) we have \(\gamma = 1\).
    
    With some further work we can show that the heat capacity goes as \(c_h \sim \abs{t}^{-\alpha}\) at \(h = 0\) and \(\alpha = 0\).
    The correlation length goes as \(\xi \sim \abs{t}^{-\nu}\), with \(\nu = 1/2\).
    
    \subsection{Universality}
    The critical exponents at a continuous second order phase transition depend only on the symmetry of the order parameter, the range of interactions, and the dimension of space.
    Systems equivalent in this sense are said to be in the same \defineindex{universality class}.
    This sounds great, we can consider one model and then we will know the critical behaviour of all other models in the same class.
    
    The problem is that most of the critical exponents that we predict from Landau theory are wrong.
    For example, the following compares the values predicted by Landau theory, the exact values known in two dimensions, and the simulated values in three dimensions:
    \begin{equation}
        \begin{array}{r|cccc}
            & \tilde{\beta} & \delta & \alpha & \gamma\\\hline
            \text{Landau}\rule{0pt}{10pt} & 1/2 & 3 & 0 & 1\\
            \text{Ising 2D} & 1/8 & 15 & 0 & 7/4\\
            \text{Ising 3D} & 0.31 & 5.2 & 0.12 & 1.24
        \end{array}
    \end{equation}
    The reason that Landau theory fails is it is a mean field theory, and therefore neglects correlations near the critical point, which is when they become important as we have large fluctuations near the critical point, which correspond to high correlation.
    
    Landau theory can be modified to include the effect of spatial dimension, in which case we call it \defineindex{Landau--Ginzburg theory}.
    If we were to do this we would find that at a certain dimension, known as the \defineindex{upper critical dimension}, the fluctuations aren't important and the Landau exponents become exact.
    For the Ising model this happens for dimensions \(d \ge 4\).
    The explanation for why this is is that as the dimension increases the number off nearest neighbours increases and the approximation of replacing the neighbours with the mean field becomes better and better.
    
    \section{Other Landau Free Energies}
    There is no reason that the order parameter has to be a scalar.
    Consider for example the vector magnetic moment, \(\vv{m}\).
    Since the free energy is a scalar it must be constructed from scalar invariants of \(\vv{m}\), that is \(\abs{\vv{m}}\).
    A similar set up to the Ising model would then lead to a potential of the form
    \begin{equation}
        f = a_0t\abs{\vv{m}}^2 + b\abs{\vv{m}}^4.
    \end{equation}
    For the case where \(\vv{m}\) is constrained to a plane this gives the famous \defineindex{Sombrero potential} shown in \cref{fig:sombrero potential}.
    We see that the ground state has infinite degeneracy.
    If \(\vv{m} = m\vh{m}\) is in the minima then so is \(m\vh{n}\) for any unit vector \(\vh{n}\).
    These minima are also all connected and so the system can move between minima without any energetic cost.
    
    \begin{figure}
        \tikzsetnextfilename{sombrero-potential}
        \begin{tikzpicture}
            \begin{axis}[
                data cs=polar,
                samples=30,
                domain=-180:180,
                y domain=0:10,
                declare function={
                    higgspotential(\r)={-125*\r^2+\r^4};
                },
                xtick=\empty,
                ytick=\empty,
                ztick=\empty,
                xlabel = \(m_1\),
                ylabel = \(m_2\),
                zlabel = \rotatebox{-90}{\(f\)},
                colormap={CM}{color=(my green), color=(my blue), color=(highlight), color=(my red)}
                ]
                \addplot3 [surf, shader=flat, draw=black, z buffer=sort] {higgspotential(y)};
            \end{axis}
        \end{tikzpicture}
        \caption[Sombrero potential.]{The sombrero potential is the generalisation of the Landau free energy for the Ising model to a model with two-dimensional magnetic moment vectors.}
        \label{fig:sombrero potential}
    \end{figure}
    
    Why stop at vectors.
    There are also systems where we can define tensor order parameters.
    One example is in the study of liquid crystals, which are rod like molecules.
    The orientation is described by a vector, \(\vh{n}\), and since there is no preferred end of the rod we treat \(\vh{n}\) and \(-\vh{n}\) the same.
    One choice of order parameter is
    \begin{equation}
        Q_{ij} = \expected{n_in_j} - \frac{1}{3}\delta_{ij},
    \end{equation}
    where the \(-\delta_{ij}/3\) term simply normalises the system so that \(Q_{ij} = 0\) when the system is in the disordered phase, since \(n_in_i = 1\).
    At low temperatures there will be phases where the molecules line up, in this case \(Q_{ij}\) will be greater than zero.
    
    To construct the free energy we need to use scalar invariants of the tensor order parameter, one particular example that arises is the cubic term \(\tr(Q^3)\).
    This appears as it is a scalar invariant and there is no symmetry excluding it.
    
    As well as different types of order parameters Landau theory can also be used to model different types of phase transitions.
    For example the Landau free energy
    \begin{equation}
        f(m) = am^2 + cm^3 + bm^4
    \end{equation}
    displays a discontinuous phase transition, and such a phase transition occurs in liquid crystals.
    The Landau free energy
    \begin{equation}
        f(m) = am^2 + bm^4 + cm^6
    \end{equation}
    also displays a discontinuous phase transition if \(b < 0\).
    
    \chapter{Non Equilibrium Processes}
    \section{The Reversibility Paradox}
    Consider a box separated in two with a gas in one half of the box and vacuum in the other half.
    If we remove the separating barrier then the gas will expand to fill the box.
    If we filmed this and played the film in reverse it would be obvious that this had been done.
    We don't, in real life, see a gas spontaneously take up a smaller volume than it currently occupies.
    
    Macroscopically we can understand this system by considering the Boltzmann entropy and the second law of thermodynamics.
    The Boltzmann entropy is \(S = \boltzmann\ln\Omega\), where \(\Omega\) is the number of ways a system can be in a given macrostate.
    Correspondingly the Boltzmann energy increases with volume, since there are more ways to arrange the gas molecules in a larger volume.
    The second law says that entropy always increases (or stays the same), and hence a system containing a gas will always evolve to have that gas's volume maximised.
    This clearly gives a direction of time.
    
    The paradox occurs when we try to consider this system microscopically.
    There is nothing in the laws of motion of the individual gas molecules that isn't the same under time reversal.
    For example, if we model the gas as classical and apply Newton's equations we get
    \begin{equation}
        m\ddot{\vv{r}}_i = -\grad[i]U(\{\vv{r}\})
    \end{equation}
    for some potential, \(U\).
    This is second order in time derivatives and so when we reverse time, that is send \(t \to -t\), we have
    \begin{equation}
        \diff[2]{}{t} \to \diff[2]{}{(-t)} = \diff[2]{}{t}.
    \end{equation}
    So nothing changes in the equations of motion, except that all velocities reverse, which would send the system back to its original state, of all the gas in one half of the box.
    
    One may look for a solution to this in quantum mechanics, maybe there is something with all the probabilities and a loss of information that we neglect with this classical picture?
    The Schr√∂dinger equation is first order in time,
    \begin{equation}
        \operator{H}\psi = i\hbar\diffp{\psi}{t}.
    \end{equation}
    So under the transformation \(t \to -t\) we get
    \begin{equation}
        \operator{H}\psi = -i\hbar\diffp{\psi}{t}.
    \end{equation}
    However, by also transforming \(\psi \to \psi^*\) at the same time we just get
    \begin{equation}
        \operator{H}\psi^* = i\hbar\diffp{\psi^*}{t},
    \end{equation}
    and since \(\psi\) and \(\psi^*\) have the same magnitude the physics described by this equation is exactly the same.
    More formally time evolution in quantum mechanics is unitary, and hence we can reverse it.
    
    So, how does time reversibility fail between the microscopic and macroscopic descriptions?
    The resolution to the paradox is that we are considering only a single microstate in the microscopic case.
    The second law is simply a statistical statement, that systems tend to evolve to increase entropy, states which don't evolve in this way are just incredibly rare.
    So rare that in the lifetime of the universe they've probably never occurred.
    
    Slight complications to this resolution occur when we consider the Gibbs entropy.
    In order to study these the best tool for the job is Hamiltonian dynamics, which we will give a short synopsis of here.
    
    \section{Hamiltonian Dynamics}
    \begin{rmk}
        For more details on Hamiltonian dynamics see the Lagrangian dynamics course.
    \end{rmk}
    \vspace{5pt}
    Hamiltonian dynamics starts with the \defineindex{Hamiltonian}, which for all cases we care about can be written as
    \begin{equation}
        \hamiltonian = \sum_{i = 1}^{3N} \frac{p_i^2}{2m} + U(\{q_i\}),
    \end{equation}
    where \(q_i\) are generalised coordinates and \(p_i\) are the conjugate momenta.
    Notice that this is of the form kinetic energy plus potential energy, and so is numerically equal to the total energy of the system.
    The dynamics of the system are then dictated by \defineindex{Hamilton's equations}:
    \begin{equation}
        \dot{q}_i = \diffp{\hamiltonian}{p_i}, \qqand \dot{p}_i = -\diffp{\hamiltonian}{q_i}.
    \end{equation}
    Inserting the form of the Hamiltonian above we find that
    \begin{equation}
        \dot{q}_i = \frac{p_i}{m}, \qqand \dot{p}_i = -\diffp{U}{q_i}.
    \end{equation}
    We can interpret the first equation as simply the normal definition of momentum, \(p_i = m\dot{q_i}\) corresponds to \(\vv{p} = m\vv{v}\) if we take \(q_i\) as Cartesian coordinates.
    The second is Newton's second law with force due to a potential, \(U\).
    
    An important concept is that of \defineindex{phase space}, \(\Gamma\).
    This is a \(6N\) dimensional space spanned by \(\{q_i, p_i\}\).
    A point in phase space corresponds to a microstate of the system.
    We can denote a \defineindex{phase-space point} by
    \begin{equation}
        \vv{X} = (q_1, \dotsc, q_{3N}, p_1, \dotsc, p_{3N}).
    \end{equation}
    We will be interested in a probability distribution, \(\rho(\vv{X}, t)\), which is defined such that \(\rho(\vv{X}, t) \dd{\Gamma}\) is the probability that the system is in a state corresponding to a point in phase space in the small region \(\dl{\Gamma}\) centred on \(\vv{X}\) at time \(t\).
    The region is given by
    \begin{equation}
        \dl{\Gamma} \coloneqq \prod_{i = 1}^{3N} \dl{q_i}\dd{p_i} = \dl{^{3N}q}\dl{^{3N}p}.
    \end{equation}
    
    Recall that we think of an ensemble as a large fixed number, \(M\), of non-interacting systems all obeying the same equations of motion.
    Each system in the ensemble can be represented by a point in phase space.
    We can then view the ensemble as this collection of phase-space points.
    These points move around and \(\rho\) is the density of these \defineindex{representative points}.
    Since the total number of representative points, \(M\), is fixed we have
    \begin{equation}
        M = \int \rho(\vv{X}, t) \dd{\Gamma} = \mathrm{const}.
    \end{equation}
    We can therefore think of the collection of representative points, which is to say the ensemble, as evolving like an fluid in phase space, with constant \enquote{mass}, \(M\), and \enquote{mass} density \(\rho(\vv{X}, t)\).
    
    \subsection{Liouville's Theorem}
    Liouville's theorem states that if we think of an ensemble as a phase space fluid that fluid is incompressible.
    To prove this consider some fixed region of phase space, \(R\), and its boundary, \(\partial R\).
    We define the \defineindex{phase-space velocity} of the point \(\vv{X}\) to be
    \begin{equation}
        \vv{V} \coloneqq \dot{\vv{X}} = (\dot{q}_1, \dotsc, \dot{q}_{3N}, \dot{p}_1, \dotsc, \dot{p}_{3N}).
    \end{equation}
    The rate at which the number of phase-space points in the region \(R\) decreases is given by the flux of these points crossing \(\partial R\):
    \begin{equation}
        -\diff{}{t} \int_R \rho \dd{\Gamma} = \int_{\partial R} \rho \vv{V} \cdot \dl{\vv{S}},
    \end{equation}
    where \(\dl{\vv{S}} = \vh{n}\dd{S}\) is an area element normal to \(\partial S\) and pointing outwards.
    Note that we are assuming representative points aren't created or destroyed.
    We can then interpret this as a continuity equation for the fluid.
    Using the divergence theorem the right hand side can be written as
    \begin{equation}
        \int_{\partial R} \rho \vv{V}\cdot\dl{\vv{S}} = \int_{R} \div (\rho\vv{V}) \dd{\Gamma}
    \end{equation}
    where
    \begin{equation}
        \div \vv{A} = \sum_{i = 1}^{3N} \left( \diffp{A}{q_i} + \diffp{A}{p_i} \right).
    \end{equation}
    For a constant region, \(R\), we then have
    \begin{equation}
        \int_{R} \left[ \diffp{\rho}{t} + \div(\rho\vv{V}) \right] \dd{\Gamma} = 0.
    \end{equation}
    Since this must hold for an arbitrary region we have
    \begin{equation}
        \diffp{\rho}{t} + \div(\rho\vv{V}) = 0.
    \end{equation}
    
    Inserting the definitions of \(\vv{V}\) and the divergence we have
    \begin{equation}
        \diffp{\rho}{t} = - \sum_{i = 1}^{3N}\left[ \diffp{}{q_i}(\dot{q}_i\rho) + \diffp{}{p_i}(\dot{p}_i\rho) \right],
    \end{equation}
    where we have used the fact that any two generalised coordinates or momenta are independent.
    We can also use the identity \(\div(\varphi\vv{A}) = \vv{A}\cdot\grad\varphi + \varphi\div\vv{A}\) to write this as
    \begin{align}
        \diffp{\rho}{t} &= -\vv{V}\cdot\grad\rho - \rho\div\vv{V}\\
        &= -\sum_{i = 1}^{3N} \left[ \dot{q}_i\diffp{\rho}{q_i} + \dot{p}+i\diffp{\rho}{p_i} \right] - \rho\sum_{i = 1}^{3N} \left[ \diffp{\dot{q}_i}{q_i} + \diffp{\dot{p}_i}{p_i} \right].
    \end{align}
    
    Using Hamilton's equations we have
    \begin{equation}
        \div\vv{V} = \sum_{i = 1}^{3N} \left[ \diffp{}{q_i}\left( \diffp{\hamiltonian}{p_i} \right) + \diffp{}{p_i} \left( -\diffp{\hamiltonian}{q_i} \right) \right] = 0.
    \end{equation}
    Hence we have
    \begin{equation}
        \diffp{\rho}{t} = -\vv{V} \cdot \grad \rho = \sum_{i = 1}^{3N} \left[ \dot{q}_i\diffp{\rho}{q_i} + \dot{p}_i\diffp{\rho}{p_i} \right].
    \end{equation}
    
    The total time derivative of \(\rho\) is given by the chain rule:
    \begin{equation}
        \diff{\rho}{t} \coloneqq \diffp{\rho}{t} + \sum_{i = 1}^{3N}\left[ \diffp{\rho}{q_i}\dot{q}_i + \diffp{\rho}{p_i}\dot{p}_i \right] = \diffp{\rho}{t} + \vv{V} \cdot \grad \rho.
    \end{equation}
    Note that since \(q_i\) and \(p_i\) are themselves, in general, functions of time this is the time derivative which moves with the fluid, this is called the material or Lagrangian description of the fluid\footnote{See the notes from the continuum mechanics part of the vectors, tensors, and continuum mechanics part of the methods of theoretical physics course for more details.}.
    We therefore have
    \begin{equation}
        \diff{\rho}{t} = \diffp{\rho}{t} + \vv{V}\cdot\grad\rho = 0.
    \end{equation}
    This is \defineindex{Liouville's equation}, and it states that along any trajectory in phase space the density of representative points is constant.
    This means that the representative points evolve as an incompressible fluid.
    
    Any (differentiable) function of the canonical variables satisfies
    \begin{align}
        \diff{u}{t} &= \diffp{u}{t} + \sum_{i}\left[ \diffp{u}{q_i}\diffp{q_i}{t} + \diffp{u}{p_i}\diffp{p_i}{t} \right]\\
        &= \diffp{u}{t} + \sum_{i}\left[ \diffp{u}{q_i}\diffp{\hamiltonian}{p_i} - \diffp{u}{p_i}\diffp{\hamiltonian}{q_i} \right]\\
        &= \diffp{u}{t} + \poissonbracket{u}{\hamiltonian},
    \end{align}
    where we define the \defineindex{Poisson bracket}, \(\poissonbracket{-}{-}\) to be
    \begin{equation}
        \poissonbracket{u}{v} \coloneqq \sum_i\left[ \diffp{u}{q_i}\diffp{v}{p_i} - \diffp{u}{p_i}\diffp{v}{q_i} \right].
    \end{equation}
    
    We can then write Liouville's equation as
    \begin{equation}
        \diff{\rho}{t} = \diffp{\rho}{t} + \poissonbracket{\rho}{\hamiltonian} = 0.
    \end{equation}
    For an equilibrium solution we expect the distribution of representative points to be stationary, meaning that \(\diffp{\rho}/{t} = 0\).
    This then implies that \(\poissonbracket{\rho}{\hamiltonian} = 0\).
    If we consider the case of \(\rho\) being constant then this clearly holds and we will recover the microcanonical ensemble.
    If instead \(\rho\) is a function of only the Hamiltonian then this holds again and we recover the canonical ensemble.
    
    \section{Entropy}
    We now go back to the case where \(\rho\) is the probability density for a single system.
    It still obeys Liouville's equation, and evolves as an incompressible fluid.
    The Gibbs entropy for a discrete system is defined as
    \begin{equation}
        S_{\mathrm{G}} \coloneqq -\boltzmann\sum_{i} p_i \ln p_i.
    \end{equation}
    In the classical continuum limit this becomes
    \begin{equation}
        S_{\mathrm{G}} \coloneqq -\boltzmann\int \rho\ln\rho \dd{\Gamma} + \mathrm{const}.
    \end{equation}
    The constant is, as usual, unimportant and we are free to choose it to be zero.
    In general it will contain terms like \(-3N\ln h\), which come from taking the classical limit, recall from \cref{sec:classical stat mech} that \(h^{3}\) is the volume occupied by a single state in phase space.
    
    The problem with this definition is that
    \begin{equation}
        \diff{S_{\mathrm{G}}}{t} = -\boltzmann\diff{}{t} \int \rho\ln\rho \dd{\Gamma} = 0,
    \end{equation}
    suggesting entropy is constant.
    We would expect entropy to be increasing in most circumstances.
    
    Consider a classical system with localised probability density in phase space.
    Intuitively we would expect the probability density to spread out over time, which would cause an increase in entropy.
    However, Liouville's theorem forbids this from happening, which is why we seemingly get entropy as a constant.
    What actually happens is that the probability density spreads into some complicated shape according to the dynamics of the system.
    
    As an analogy suppose we create an emulsion of oil in water.
    Any sufficiently small region will contain only oil or only water, since one will form droplets in the other, so we see that the density of oil is constant.
    However, if we take larger volumes they will start to contain water as well as oil, making it seem like the density of oil is lower.
    
    The same process can be thought of as occurring in phase space.
    We can replace the probability density with the \define{coarse grained}\index{coarse graining} density, \(\mean{\rho}\), and redefine the entropy as
    \begin{equation}
        \mean{S} = -\boltzmann\int \mean{\rho} \ln \mean{\rho} \dd{\Gamma}.
    \end{equation}
    We do so by locally averaging \(\rho\) over some fixed local scale, \(\Lambda\), in order to obtain \(\mean{\rho}\).
    This tends to smooth out the spread out probability density, seemingly increasing its volume, and so \(\mean{\rho}\) will generally be less than \(\rho\).
    The result is that entropy increases in this process.
    This is because
    \begin{equation}
        s(\rho) = -\boltzmann \rho\ln \rho
    \end{equation}
    defines a concave function and for a concave function we have the general fact that the average of the function at two points is less than the function at the average of those two points.
    This means that
    \begin{equation}
        \frac{1}{2}[s(\rho_1) + s(\rho_2)] \le s\left( \frac{\rho_1 + \rho_2}{2} \right).
    \end{equation}
    This generalises to
    \begin{equation}
        \lambda s(\rho_1) + (1 - \lambda)s(\rho_2) \le s(\lambda \rho_1 + (1 - \lambda)\rho_2)
    \end{equation}
    for \(\lambda \in [1, 0]\).
    If we then consider two adjacent regions of phase space, both with volume \(V\), and local densities \(\rho_1\) and \(\rho_2\), we can merge them into a single region with volume \(2V\), and density \((\rho_1 + \rho_2)/2\).
    The entropy after this merging will be
    \begin{equation}
        \mean{s} = 2Vs\left( \frac{\rho_1 + \rho_2}{2} \right) \ge 2V\frac{1}{2}[s(\rho_1) + s(\rho_2)].
    \end{equation}
    The result of this is we recover the second law
    \begin{equation}
        \diff{\mean{S}}{t} \ge 0.
    \end{equation}
    
    This coarse graining process may seem to be just made up to fix this problem, and it is, but remember that the second law is just a statistical law, and therefore we should expect that it arises in averaging processes.
    This necessarily causes a loss of information, which corresponds to an increase in entropy.
    This gets to a fundamental point.
    Entropy is based on \emph{our} knowledge of the system, not on the information available in a perfect world.
    After all in a perfect world we could solve every system exactly and do away with statistical mechanics.
    
    Another example of coarse graining leading to irreversibility is friction.
    Newton's second law with a viscous term can be written as
    \begin{equation}
        m\ddot{\vv{r}}_i + \kappa\dot{\vv{r}}_i = - \grad[i] U(\{\vv{r}_i\}).
    \end{equation}
    This is not time reversible, due to the presence of the \(\dot{\vv{r}}_i\) first derivative term.
    However, in a perfect world we could get rid of the viscous term by including the interactions causing the friction in the potential, restoring time reversibility.
    The viscous term simply approximates the effect of these interactions, and hence gives rise to an arrow of time, much like increasing entropy.
    
    \chapter{The Master Equation}
    \section{Entropy}
    Consider a system evolving under the Hamiltonian \(\operator{H}\).
    We can choose the microstates to simply be the eigenstates, \(\{\ket{i}\}\), of the Hamiltonian.
    These evolve according to the Schr√∂dinger equation, meaning if the state of the system is
    \begin{equation}
        \ket{\Psi} = \sum_{i} c_i\ket{i}
    \end{equation}
    then the evolution of the system obeys
    \begin{equation}
        i\hbar\diffp{}{t}\ket{\Psi} = \operator{H}\ket{\Psi} = \sum_i c_iE_i\ket{i}.
    \end{equation}
    Here \(E_i\) is the eigenvalue associated with the eigenstate \(\ket{i}\).
    The solution to this is for the system to evolve according to
    \begin{equation}
        \ket{\Psi, t} = \sum_i c_i\exp[-iE_it/\hbar]\ket{i}.
    \end{equation}
    This means that \(c_i\) is constant and the state evolves by changing phase.
    The result is that evolving according to Schr√∂dinger's equation cannot change the classical weights, \(p_i\), associated with the system starting in a given microstate.
    This means that the entropy is
    \begin{equation}
        S_{\mathrm{G}} = -\boltzmann\sum_i \abs{c_i}^2 \ln\abs{c_i}^2,
    \end{equation}
    which is a constant, meaning
    \begin{equation}
        \diff{S_{\mathrm{G}}}{t} = 0,
    \end{equation}
    just as we had in classical mechanics.
    
    Suppose instead that we start with mixed states,
    \begin{equation}
        \rho = \sum_i p_i\ket{\Psi_i}\bra{\Psi_i},
    \end{equation}
    where \(\ket{\Psi_i}\) are states, which aren't necessarily eigenstates of \(\operator{H}\).
    The Gibbs entropy is then
    \begin{equation}
        S_{\mathrm{G}} = -\boltzmann\tr(\rho\ln\rho).
    \end{equation}
    Considering the Schr√∂dinger equation and its adjoint,
    \begin{equation}
        -i\hbar\diffp{}{t}\bra{\Psi} = \bra{\Psi}\operator{H},
    \end{equation}
    it can be shown that \(\rho\) satisfies
    \begin{equation}
        \diffp{\rho}{t} = -\frac{i}{\hbar}\commutator{\operator{H}}{\rho}.
    \end{equation}
    This is the \defineindex{von Neumann equation}.
    Under this we can show that the time evolution of \(\ket{\Psi_i}\) and \(\rho\) satisfies
    \begin{equation}
        \ket{\Psi_i, t} = \exp[-i\operator{H}t/\hbar]\ket{\Psi_i, 0}
    \end{equation}
    where \(\ket{\psi_i, 0} = \ket{\Psi_i}\), and
    \begin{equation}
        \rho(t) = \exp[-i\operator{H}t/\hbar]\rho(0)\exp[-i\operator{H}t/\hbar].
    \end{equation}
    We again find that
    \begin{equation}
        \diff{S_{\mathrm{G}}}{t} = 0.
    \end{equation}
    
    So, how do we get increasing entropy from quantum mechanics?
    
    \subsection{Fermi's Master Equation}
    In reality for most systems, certainly those of interest in statistical mechanics, we cannot write down the Hamiltonian exactly containing all microscopic detail.
    Instead we can use an approximate Hamiltonian, \(\operator{H}_0\), and describe the system using eigenstates of \(\operator{H}_0\).
    The true Hamiltonian is then
    \begin{equation}
        \operator{H} = \operator{H}_0 + \operator{h},
    \end{equation}
    where \(\operator{h}\) can be thought of as some small perturbation.
    The states \(\ket{i}\) are then approximate energy eigenstates of \(\operator{H}\).
    The matrix elements of \(\operator{h}\) in the eigenbasis of \(\operator{H}_0\) are
    \begin{equation}
        h_{ij} = \bra{i}\operator{h}\ket{j} = h_{ji}^*.
    \end{equation}
    This perturbation causes the system to move between the approximate states.
    We treat the states in a non-unitary way.
    To each state we assign a changing classical probability, \(p_i\), rather than keeping track of the amplitudes associated with the different states.
    We can view this as a form of coarse graining where quantum coherence between states is wiped out.
    
    Using time dependent perturbation theory it can be shown\footnote{See either the principles of quantum mechanics notes or quantum theory notes.} that if the system can jump from an initial state, \(\ket{i}\), with energy \(E_i\), into a narrow band of final states, \(\ket{f}\), with energy within \(\delta E\) of \(E_i\), then the probability of this jump is
    \begin{equation}
        \lambda_{if} = \frac{2\pi}{\hbar} \abs{h_{if}}^2\rho_f,
    \end{equation}
    where \(\rho_f\) is the density of final states.
    This is called \defineindex{Fermi's golden rule} for the one-to-many state transition rate.
    We can convert this to a one-to-one transition rate of going from some initial state, \(\ket{i}\), to a single final state, \(\ket{j}\), by dividing by the number of final states in the range \([E_{i} - \delta E, E_i + \delta E]\), which is just \(\rho_f\delta E\), giving the transition rate
    \begin{equation}
        \nu_{ij} = \frac{2\pi}{\hbar \delta E} \abs{h_{ij}}^2.
    \end{equation}
    
    Notice that \(\nu_{ij} \ge 0\) in this case and since \(h_{ij} = h_{ji}^*\) the jump rates are symmetric, so \(\nu_{ij} = \nu_{ji}\), this is often the case but not always.
    
    The interpretation of \(\nu_{ij}\) is that in the time \(t\) to \(t + \dl{t}\) the probability of transition from state \(\ket{i}\) to state \(\ket{j}\) is \(\nu_{ij}\dd{t}\).
    Note that here we assume \(\dl{t}\) is small enough that multiple transitions are negligible.
    Now consider some specific state \(i\).
    The flux of states transitioning into this state in time \(\dl{t}\) is
    \begin{equation}
        \dl{p}_i = -p_i\sum_{j\ne i} \nu_{ij}\dl{t} + \sum_{j\ne i} p_j\nu_{ji}\dl{t}.
    \end{equation}
    The first term accounts for the flux of states out of state \(i\) and the second for the flux of states into \(i\).
    We then have
    \begin{equation}
        \diff{p_i}{t} = -p_i\sum_{j \ne i} \nu_{ij} + \sum_{j\ne i} p_j\nu_{ji}.
    \end{equation}
    This is called the \defineindex{master equation}.
    Notice that it is first order in time and doesn't posses time reversal symmetry.
    
    If \(\nu_{ij} = \nu_{ji}\) then we have
    \begin{equation}
        \diff{p_i}{t} = \sum_{j\ne i} \nu_{ij}(p_j - p_i).
    \end{equation}
    
    Now consider only jumps from state 1 to state 2 and back.
    We have
    \begin{equation}
        \diff{}{t}(p_1 + p_2) = 0 \implies \diff{p_1}{t} = -\diff{p_2}{t}.
    \end{equation}
    This is because we have restricted the system to only transitions between states 1 and 2 and hence \(p_1 + p_2 = 1\).
    We also have
    \begin{equation}
        \diff{p_1}{t} = -p_1\nu_{12} + p_2\nu_{21} = -\diff{p_2}{t}.
    \end{equation}
    For the case of \(\nu_{12} = \nu_{21}\) we have
    \begin{equation}
        \diff{}{t}(p_1 - p_2) = -2\nu_{12}(p_1 - p_2).
    \end{equation}
    We can solve this to find that
    \begin{equation}
        p_1(t) - p_2(t) = [p_1(0) - p_2(0)]\exp[-2\nu_{12}t].
    \end{equation}
    The entropy is then
    \begin{equation}
        S = -\boltzmann p_1\ln p_1 = \boltzmann p_2\ln p_2,
    \end{equation}
    and it can be shown that this is increasing.
    
    \section{Random Walks}
    Consider a one-dimensional random walk.
    We can model this as a one-dimensional lattice of possible positions, labelled \(i = -L, \dotsc, L\), and take \(L \to \infty\) to avoid having to worry about boundary conditions.
    We then think of state \(i\) as \enquote{the particle is at position \(i\)}.
    If we assume that each step length is the same, and is the distance between nearest neighbours, then we find that \(\nu_{ij} = 0\) if \(j \ne i \pm 1\).
    Assuming an isotropic random walk we have that \(\nu_{i,i+1} = \nu_{i,i-1}\).
    Assuming a homogeneous random walk we have that \(\nu_{i,i\pm 1} = \nu\) for all \(i\).
    
    Now consider the master equation.
    With the above information we can evaluate the sums and rearrange giving
    \begin{align}
        \dot{p}_i &= -2p_i\nu + \nu(p_{i + 1} + p_{i - 1})\\
        &= \nu(p_{i - 1} - p_i) - \nu(p_i - p_{i + 1}).
    \end{align}
    We write it in this way since we can interpret the first term as the net probability flux from state \(i - 1\) to state \(i\), and the second term as the net probability flux from \(i\) to \(i + 1\).
    
    There is an exact solution to this recurrence relation, and its given by the modified Bessel functions of the first kind, \(I_n\), namely \(p_i(t) = \e^{-t}I_i(t)\).
    However, it is more useful to consider the limit of the step size going to zero.
    Let \(a\) be the step size.
    We can define the probability density \(p(x, t) \dd{x}\) to be the probability that the particle is found in the region \([x, x + \dl{x}]\) where \(x = ia\).
    We then replace \(p_i(t)\) with \(p(x, t)\) and \(p_{i\pm 1}(t)\) with \(p(x\pm a)\).
    Taylor expanding we have
    \begin{equation}
        p(x \pm a, t) = p(x, t) \pm a\diffp{p}{x} + \frac{a^2}{2} \diffp[2]{p}{x} + \dotsb.
    \end{equation}
    Inserting this into our equations we have
    \begin{align}
        \diffp{p}{t} &= \nu\left[ p(x, t) - a\diffp{p}{x} + \frac{a^2}{2} \diffp[2]{p}{x} - p(x, t) \right] - \nu\left[ p(x, t) - p(x, t) - a\diffp{p}{x} - \frac{a^2}{2} \diffp[2]{p}{x} \right]\\
        &= a^2\nu\diffp[2]{p}{x}.
    \end{align}
    We can then identify this as the diffusion equation,
    \begin{equation}
        \diffp{p}{t} = D\diffp[2]{p}{x},
    \end{equation}
    with \(D = a\nu^2\).
    
    For the initial condition where we know for certain the position of the particle is \(x = 0\), that is \(p(x, t = 0) = \delta(x)\), we can show that
    \begin{equation}
        p(x, t) = \frac{1}{\sqrt{4\pi Dt}}\exp\left[ -\frac{x^2}{4Dt} \right]
    \end{equation}
    is the solution.
    What this shows is that the probability density spreads out, while remaining centred on the original position.
    As time goes on it becomes increasingly likely to find the particle at larger distances, as one would expect.
    
    For an initial condition of the particle being at \(x_0\) we simply replace \(x\) with \(x - x_0\) in both the initial condition and solution.
    We can identify
    \begin{equation}
        \mean{x - x_0} = \int (x - x_0) p(x, t) \dd{x} = \int (x - x_0)\delta(x - x_0) \dd{x} = 0,
    \end{equation}
    since we can choose to evaluate this at any time, so choose \(t = 0\).
    We can also show that
    \begin{equation}
        \mean{(x - x_0)^2} = 2Dt = \Var(x),
    \end{equation}
    and so the standard deviation is \(\sigma = \sqrt{2Dt}\), meaning that the width of the distribution goes as \(t^{1/2}\).
    
    We can also consider a system of \(M\) non-interacting particles, in which case we simply replace \(p\) with \(\rho\) and everything is the same but instead of
    \begin{equation}
        \int p(x, t) \dd{x} = 1
    \end{equation}
    as normalisation we have
    \begin{equation}
        \int \rho(x, t) \dd{x} = M.
    \end{equation}
    
    \section{Detailed Balance}
    \subsection{Isolated System}
    Consider an isolated system, that is a microcanonical ensemble.
    In equilibrium the principle of equal \textit{a priori} probabilities holds and we have \(p_i^{\mathrm{eq}} = p^{\mathrm{eq}} = \mathrm{const}\).
    We also have transition rate symmetry, meaning \(\nu_{ij} = \nu_{ji}\).
    From this we have
    \begin{equation}
        p_i^{\mathrm{eq}}\nu_{ij} = p_j^{\mathrm{eq}}\nu_{ji}.
    \end{equation}
    This condition, called \defineindex{detailed balance}, is that the flux from \(i\) to \(j\) is equal to the flux from \(j\) to \(i\).
    This is a stronger statement than the master equation, which only requires an overall balance for a stationary equilibrium state, that is an equilibrium state with \(\dot{p}_i = 0\).
    
    Suppose we are looking for a system to be in stationary equilibrium.
    Then we have
    \begin{equation}
        \diffp{p_i}{t} = \sum_{j \ne i} (-p_i\nu_{ij} + p_j\nu_{ji}) = 0.
    \end{equation}
    If the detailed balance condition is met then clearly each term vanishes identically and we have a stationary equilibrium state, however, it is possible that rather than each term vanishing a system could just arrange the terms such that their sum vanishes, without meeting the detailed balance condition.
    We see that by imposing a system follows the detailed balance condition we are guaranteed to find an equilibrium stationary state.
    
    We can also apply the detailed balance condition to a grouping of states.
    Consider a system with two groups of states, \(A\) and \(B\).
    Summing the detailed balance condition over the state \(i\) in \(A\) and the state \(j\) in \(B\) we get
    \begin{equation}
        \sum_{i \in A} \sum_{j \in B} p_i^{\mathrm{eq}} \nu_{ij} = \sum_{i \in A} \sum_{j \in B} p_j^{\mathrm{eq}} \nu_{ji}.
    \end{equation}
    Suppose we want to know the mean rate of transition from \(A\) to \(B\).
    This is given by
    \begin{equation}
        p_A^{\mathrm{eq}}\nu_{AB} = \sum_{i \in A} p_i^{\mathrm{eq}} \sum_{j \in B} \nu_{ij}.
    \end{equation}
    We can identify the total probability of being in group \(A\) as
    \begin{equation}
        p_A^{\mathrm{eq}} = \sum_{i \in A} p_i^{\mathrm{eq}} = p^{\mathrm{eq}}\Omega_A,
    \end{equation}
    where \(\Omega_A\) is the number of microstates in \(A\).
    The mean rate of transition from \(A\) to \(B\) \emph{given that the system is in group \(A\)} is
    \begin{equation}
        \nu_{AB} = \frac{\sum_{i\in A} p_i^{\mathrm{eq}} \sum_{j\in B}\nu_{ij}}{\sum_{i\in A}p_i^{\mathrm{eq}}} = \frac{1}{\Omega_A} \sum_{i \in A} \sum_{j \in B} \nu_{ij}.
    \end{equation}
    Doing the same for transitions from \(B\) to \(A\) we find that
    \begin{equation}
        p_A^{\mathrm{eq}}\nu_{AB} = p_B^{\mathrm{eq}}\nu_{BA}.
    \end{equation}
    This is the detailed balance condition for groups of states.
    Importantly this holds even if the sizes of the two groups aren't equal, in which case we expect that \(p_A \ne p_B\) and \(\nu_{AB} \ne \nu_{BA}\).
    
    \subsection{Canonical Ensemble}
    One view of a canonical ensemble is an otherwise isolated system submerged in a heat bath, which allows for the total energy of the system to change without the number of particles changing.
    We then treat the combined system as an isolated system.
    We can specify a microstate of the system, but in general this will correspond to many possible states of the heat bath.
    This means that transitions between single states of the system correspond to transitions between groups of states of the composite system.
    Therefore these obey the detailed balance condition for group transitions in an isolated system.
    That is for states \(\alpha\) and \(\beta\) we have
    \begin{equation}
        p_\alpha^{\mathrm{eq}}\nu_{\alpha\beta} = p_\beta^{\mathrm{eq}} \nu_{\beta\alpha} \implies \frac{\nu_{\alpha\beta}}{\nu_{\beta\alpha}} = \frac{p_{\beta}^{\mathrm{eq}}}{p_{\alpha}^{\mathrm{eq}}}.
    \end{equation}
    
    In a canonical ensemble we know that \(p_\alpha^{\mathrm{eq}} \propto \exp[-\beta E_\alpha]\), where \(E_\alpha\) is the energy of state \(\alpha\) and the constant of proportionality, \(1/\cpartition\), is constant for a given system.
    We then find that
    \begin{equation}
        \frac{\nu_{\alpha\beta}}{\nu_{\beta\alpha}} = \frac{\exp[-\beta E_\beta]}{\exp[-\beta E_\alpha]} = \exp[\beta(E_\alpha - E_\beta)].
    \end{equation}
    This shows that, in general, the transition rates are asymmetric, and their ratio is a function of the energy difference.
    
    \subsubsection{Currents}
    In a system obeying detailed balance there are no microscopic probability currents, and as a consequence there are no macroscopic currents.
    As an example consider the continuity equation
    \begin{equation}
        \diffp{\rho}{t} + \diffp{J}{x} = 0.
    \end{equation}
    We can identify this as a diffusion equation with
    \begin{equation}
        J = -D\diffp{\rho}{x}
    \end{equation}
    as the diffusive particle current.
    A stationary solution has \(\diffp{\rho}/{t} = 0\), which requires that \(J\) is constant in space.
    An equilibrium solution is then the special case where \(J^{\mathrm{eq}} = 0\).
    This means that \(\rho^{\mathrm{eq}}\) is spatially constant as well.
    
    For a non-equilibrium stationary solution we will have \(J \ne 0\), in which case we would have
    \begin{equation}
        \rho(x) = \mathrm{const} - \frac{J}{D}x.
    \end{equation}
    In order to maintain this linear density profile in a stationary state we would require an external reservoir for diffusive particles, in which case the composite system is not in an equilibrium state.
    
    \subsubsection{Computer Simulations}
    One of the main uses of detailed balance is in computer simulations.
    The general idea is that a computer simulation will give accurate results for measurements, if not accurate dynamics, if we can get the system to evolve to a steady state.
    This can be done, at least most of the time, by requiring that the simulation obeys detailed balance.
    We will then find in general that the probability of being in a given state will be given by
    \begin{equation}
        p_\alpha = \frac{1}{\cpartition} \exp[-\beta E_\alpha].
    \end{equation}
    
    The most common way to do this is with a Monte-Carlo simulation using the Metropolis algorithm.
    In this simulation we randomly select states, \(\alpha\) and \(\beta\), and simulate the system moving between these states by setting the jump rate to be
    \begin{equation}
        \nu_{\alpha\beta} = 
        \begin{cases}
            \nu_0 & E_\beta < E_\alpha,\\
            \nu_0\exp[\beta(E_\alpha - E_\beta)] & E_\beta \ge E_\alpha,
        \end{cases}
        = \min \{ 1, \exp[\beta(E_\alpha - E_\beta)] \}.
    \end{equation}
    Here \(\nu_0\) is some constant, the value of which is not important.
    
    This will satisfy detailed balance, since
    \begin{align}
        p_\alpha^{\mathrm{eq}}\nu_{\alpha\beta} &= \frac{1}{\cpartition} \exp[-\beta E_\alpha]\nu_0\min\{1, \exp[\beta(E_\alpha - E_\beta)]\}\\
        &= \frac{1}{\cpartition} \nu_0 \min\{ \exp[-\beta E_\alpha], \exp[-\beta E_\beta] \}\\
        &= \frac{1}{\cpartition} \exp[-\beta E_\beta] \nu_0 \min\{ 1, \exp[\beta(E_\beta - E_\alpha)] \}\\
        &= p_\beta^{\mathrm{eq}}\nu_{\beta\alpha}.
   \end{align}
    
    We can then use this process to find various averages, such as \(\expected{O} = \sum_\alpha O_\alpha p_\alpha^{\mathrm{eq}}\), where \(O_\alpha\) is the value measured in state \(\alpha\).
    Note here that we don't sum over all states, rather we sum over all states that the system reaches over some period of time.
    
    \begin{cde}{Monte Carlo and Metropolis}{}
        The following pseudocode simulates the Monte Carlo method with the Metropolis algorithm running for \(N\) sweeps choosing units such that \(\boltzmann T = 1\).
        \begin{lstlisting}[gobble=12]
            measurements = []
            repeat N times:
                alpha = current state
                beta = randomly chosen state
                energy_change = energy(alpha) - energy(beta)
                if energy_change <= 0 
                or random_between_0_1() < exp(-energy_change):
                    set state to beta
                measurements.append(measurement of desired
                                    quantity in current state)
            expected_value = mean(measurements)
        \end{lstlisting}
    \end{cde}
    
    \chapter{Langevin Approach}
    In this section we will lay out the Langevin approach to non-equilibrium processes.
    We will start with the familiar case of a random walk, and then discuss Brownian motion, the original motivation for this approach.
    
    \section{Random Walk}
    Consider a one-dimensional random walk.
    We have seen that in the continuous limit the probability density follows a diffusion equation,
    \begin{equation}
        \diffp{p}{t} = D\diffp[2]{p}{x}.
    \end{equation}
    We saw that the diffusion coefficient is given by \(D = \nu a^2\), where \(\nu\) is the transition rate between two neighbouring sites and \(a\) is the distance between sites.
    
    Now consider a small time step, \(\Delta t\).
    In time \(t\) the position changes by \(\Delta x(t)\) from its position at time \(t\), that is
    \begin{equation}
        x(t + \Delta t) = x(t) + \Delta x(t).
    \end{equation}
    We can treat \(\Delta x(t)\) as a random variable.
    For sufficiently short periods of time, such that the probability of two transition occurring in the time period is negligible, we expect that
    \begin{equation}
        \Delta x(t) = 
        \begin{cases}
            \hphantom{-a}\mathllap{a} & \text{with probability } \nu \Delta t,\\
            -a & \text{with probability } \nu \Delta t,\\
            \hphantom{-a}\mathllap{a} & \text{with probability } 1 - 2\nu \Delta t.
        \end{cases}
    \end{equation}
    That is, the particle moves in the positive direction with probability \(\nu \Delta t\), and in the negative direction with the same probability.
    The particle then doesn't change position with probability \(1 - 2\nu\Delta t\).
    
    Since the particle is equally likely to move left or right we have that \(\expected{\Delta x(t)} = 0\).
    The variance is \(\expected{\Delta x(t)^2} = 2a^2\nu \Delta t = 2D\Delta t\).
    At different times \(\Delta x(t)\) are uncorrelated, and so we have \(\expected{\Delta x(t) \, \Delta x(t')} = \expected{\Delta x(t)}\expected{\Delta x(t')} = 0\) for \(t \ne t'\).
    
    We again want to take the continuum limit, so have \(\Delta t\) and \(\delta x\) go to zero.
    In order to do so in a way that makes everything converge we should scale the lattice spacing as
    \begin{equation}
        a \propto \sqrt{\Delta t}
    \end{equation}
    in the limit of \(\Delta t\to 0\).
    
    Doing this we can define
    \begin{equation}
        \eta(t) \coloneqq \diff{x}{t}.
    \end{equation}
    Then \(\eta(t)\) is a random variable satisfying
    \begin{equation}
        \expected{\eta(t)} = 0, \qqand \expected{\eta(t)\eta(t')} = \Gamma \delta(t - t'),
    \end{equation}
    for some constant \(\Gamma\).
    We call a random variable with statistics satisfying these requirements \defineindex{white noise}, since it averages out to zero and is uncorrelated at different times, so adding \(\eta(t)\) to a signal replicates a signal with background noise.
    
    This specification of \(\eta\) specifies a stochastic process, \(x(t)\), which is related to \(\eta\) by the \defineindex{Langevin equation}
    \begin{equation}
        \diff{x}{t} = \eta(t).
    \end{equation}
    We can integrate this equation giving
    \begin{equation}
        x(t) - x_0 = \int_0^t \eta(t') \dd{t'}
    \end{equation}
    where \(x_0 = x(0)\).
    Notice that we need to introduce a dummy variable, \(t'\), in the integral.
    
    Since all we know about \(\eta\) are various averages we can take averages of this result to extract information.
    Notice that these are thermodynamic averages, over whatever probability distribution \(\eta\) is drawn from, they aren't averages over time.
    As such we have that the average of the integral is simply the integral of the average, and the average of a constant is just the constant.
    Using this we have
    \begin{equation}
        \expected{x(t) - x_0} = \expected*{\int_0^t \eta(t) \dd{t'}} = \int_0^t \expected{\eta(t)} \dd{t'} = \int_0^t 0 \dd{t'} = 0.
    \end{equation}
    We can also compute the correlation.
    Doing so we have to be careful and introduce different dummy variables when we square the integral:
    \begin{align}
        \expected{(x(t) - x_0)^2} &= \expected*{\int_0^t \eta(t') \dd{t'} \int_0^t \eta(t'') \dd{t''}}\\
        &= \int_0^t \dl{t'} \int_0^t \dl{t''} \expected{\eta(t') \eta(t'')}\\
        &= \Gamma \int_0^t \dl{t'} \int_0^t \dl{t''} \delta(t' - t'')\\
        &= \Gamma \int_0^t \dl{t'}\\
        &= \Gamma t.
    \end{align}
    Comparing this to the diffusion equation we can identify \(\Gamma = 2D\), since we expect \(\expected{\Delta x^2} = 2D\Delta t\).
    
    It can be shown that for an appropriate choice of \(\Gamma\) the Langevin approach is equivalent to the diffusion approach.
    Proof of this is beyond the scope of this course, but as motivation consider the fact that a sum of random variables gives a Gaussian distribution, by the central limit theorem, and a Gaussian is indeed a solution to the diffusion equation.
    
    \section{Brownian Motion}
    \defineindex{Brownian motion} is the seemingly random motion of a colloidal particle suspended in a fluid.
    This was first discovered with pollen floating in water.
    The motion is due to collisions with molecules of the liquid.
    However, these molecules are too small and numerous to track individually.
    Instead we can treat the fluid molecules as a source of a random force.
    
    For simplicity we consider Brownian motion in one dimension.
    Consider a particle of mass \(m\) suspended in a fluid.
    Newton's second law for this particle can be written as
    \begin{equation}
        m\diff[2]{x}{t} = -\gamma\diff{x}{t} + f(t)
    \end{equation}
    where \(f(t)\) is the random force produced as the net action of the fluid molecules, and the \(\gamma\) term represents viscous friction.
    Note that viscous friction is itself a form of coarse graining, since on the molecular level it is just a result of more collisions and intermolecular forces.
    It is common to talk of the \defineindex{mobility}, \(\mu \coloneqq 1/\gamma\), instead of \(\gamma\).
    
    The random force is equally likely to positive and negative and so we expect that
    \begin{equation}
        \expected{f(t)} = 0.
    \end{equation}
    In principle the random forces are correlated, since molecular collisions take a nonzero amount of time to occur.
    However, we can assume that the the correlation is of the form
    \begin{equation}
        \expected{f(t)f(t')} = g(t - t'),
    \end{equation}
    for some decreasing function, \(g\).
    We assume that the correlation falls of rapidly such that for times greater than some correlation time, \(t_{\mathrm{c}}\), we can approximate \(g(t) = \Gamma\delta(t)\), which is to say
    \begin{equation}
        \expected{f(t)f(t')} = \Gamma \delta(t - t').
    \end{equation}
    So \(f\) is a white noise random variable.
    
    For simplicity we now take \(m = 1\).
    We also write \(v(t) = \diff{x}/{t}\), which allows us to write Newton's second law as
    \begin{equation}
        m\dot{v}(t) = -\gamma v(t) + f(t).
    \end{equation}
    We can solve this equation using an integrating factor.
    Recall that the integrating factor is given by the exponential of the integral of the zeroth order derivative coefficient in a first order linear ordinary differential equation.
    In this case the integrating factor is \(\exp[\gamma t]\).
    We then have
    \begin{equation}
        \diff{}{t} [v(t)\e^{\gamma t}] = \dot{v}(t)\e^{\gamma t} + \gamma v(t)\e^{\gamma t} = \e^{\gamma t}f(t).
    \end{equation}
    We can then integrate this giving
    \begin{equation}
        \int_0^t \diff{}{t'} [v(t')\e^{\gamma t'}] \dd{t'} = [v(t')\e^{\gamma t'}]_{t'=0}^{t'=t} = v(t)\e^{\gamma t} - v_0 = \int_0^t \e^{\gamma t'}f(t') \dd{t'},
    \end{equation}
    where \(v_0 = v(0)\).
    Notice we introduce a dummy variable, \(t'\), for the integration.
    From this we get
    \begin{equation}
        v(t) = v_0\e^{-\gamma t} + \int_0^t \e^{-\gamma(t - t')}f(t') \dd{t'}.
    \end{equation}

    In order to extract information we take various averages.
    First we have that the average velocity is
    \begin{align}
        \expected{v(t)} &= \expected*{v_0\e^{-\gamma t} + \int_0^t \e^{-\gamma(t - t')}f(t') \dd{t'}}\\
        &= \expected{v_0\e^{-\gamma t}} + \expected*{\int_0^t \e^{-\gamma(t - t')} f(t') \dd{t'}}\\
        &= v_0\e^{-\gamma t} + \int_0^t \e^{-\gamma(t - t')} \expected{f(t')} \dd{t'}\\
        &= v_0\e^{-\gamma t},
    \end{align}
    where we have used \(\expected{f(t')} = 0\).
    
    Consider the short time limit, so \(\gamma t \ll 1\).
    In this case we have \(\expected{v(t)} \approx v_0\), so friction has not yet had an effect.
    In the long time limit, \(\gamma t \gg 1\), we have \(\expected{v} \to 0\).
    So over long time scales the particle isn't really moving.
    
    We now consider the mean squared velocity:
    \begin{align}
        \expected{v(t)^2} &= \bigg\langle \left( v_0\e^{-\gamma t} + \int_0^t \e^{-\gamma(t - t')}f(t') \dd{t'} \right)\notag\\
        &\qquad \times \left( v_0\e^{-\gamma t} + \int_0^t \e^{-\gamma(t - t'')}f(t'') \dd{t''} \right)\bigg\rangle\\
        &= v_0^2\e^{-2\gamma t} + v_0\e^{-\gamma t}\int_0^t \e^{-\gamma(t - t')} \underbrace{\expected{f(t')}}_{=0} \dd{t'}\notag\\
        &\qquad + v_0\e^{-\gamma t}\int_0^t \e^{-\gamma(t - t'')} \underbrace{\expected{f(t'')}}_{=0} \dd{t''}\notag\\
        &\qquad + \int_0^t \dl{t'} \int_0^t \dl{t''} \, \e^{-\gamma(t - t')} \e^{\gamma(t - t'')} \expected{f(t')f(t'')}\\
        &= v_0^2\e^{-2\gamma t} + \int_0^t \dl{t'} \int_0^t \dl{t''} \e^{-\gamma(t - t')}\e^{-\gamma(t - t'')} \Gamma\delta(t' - t'')\\
        &= v_0^2\e^{-2\gamma t} + \Gamma \int_0^t \e^{-\gamma(t - t')}\e^{-\gamma(t - t')} \dl{t'}\\
        &= v_0^2\e^{-2\gamma t} + \frac{\Gamma}{2\gamma} [\e^{-\gamma(t - t')}]_{t' = 0}^{t' = t}\\
        &= v_0^2\e^{-2\gamma t} + \frac{\Gamma}{2\gamma} (1 - \e^{-2\gamma t}).
    \end{align}
    In the short time limit, \(\gamma t \ll 1\), we have \(\expected{v(t)^2} \approx v_0^2\).
    In the long time limit, \(\gamma t \gg 1\), we have \(\expected{v(t)^2} \to \Gamma / (2\gamma)\).
    What this means is that after a sufficiently long amount of time the particle \enquote{forgets} its initial velocity and comes into thermal equilibrium with the fluid.
    By equipartition a particle in equilibrium, with mass \(m = 1\), has kinetic energy
    \begin{equation}
        \frac{1}{2} \expected{v(t)^2} = \frac{1}{2}\boltzmann T \implies \Gamma = 2\gamma \boltzmann T.
    \end{equation}
    This relates the magnitude of the noise fluctuations, \(\Gamma\), to the viscous dissipative term, \(\gamma\).
    This shouldn't be a surprise since both are ultimately due to the same molecular dynamics.
    
    We can integrate our expression for \(v(t)\) again to get the position.
    We have to be careful and change \(t\) to the dummy variable \(t'\), and we then have to change the \(t'\) dummy variable in the integral to \(t''\).
    Note that this means changing the \(t\) in the limit of the integral to \(t'\).
    Doing so we get
    \begin{align}
        \int_0^t v(t') \dd{t'} &= x(t) - x_0\\
        &= \int_0^t v_0\e^{-\gamma t'} \dd{t'} + \int_0^t \dd{t'} \int_0^{t'} \dd{t''} \e^{-\gamma(t' - t'')} f(t'')\\
        &= \frac{v_0}{\gamma}(1 - \e^{-\gamma t}) + \int_0^t \dd{t'} \int_0^{t'} \dd{t''} \e^{-\gamma(t' - t'')} f(t'').
    \end{align}
    
    We can then find the average position:
    \begin{align}
        \expected{x(t)} = x_0 + \frac{v_0}{\gamma}(1 - \e^{-\gamma t}),
    \end{align}
    where we have used the fact that \(\expected{f(t'')} = 0\).
    The short time limit is \(1 - \e^{-\gamma t} \approx \gamma t\), and so we have \(\expected{x(t)} \approx x_0 + v_0 t\).
    This is ballistic motion.
    The particle moves as if there were no drag effects, at constant velocity, \(v_0\).
    The long time limit is \(\e^{-\gamma t} \approx 0\) giving \(\expected{x(t)} \approx x_0 + v_0/\gamma\).
    This means that the particle essentially stops moving.
    
    We can also calculate the fluctuations in position:
    \begin{equation*}
        \expected{(x(t) - x_0)^2} = \frac{v_0}{\gamma^2} + \int_0^t \!\! \dd{t_1}\int_0^{t_1} \!\! \dd{t_2} \, \e^{-\gamma(t_1 - t_2)} \int_0^t \!\! \dd{t_3}\int_0^{t_3} \!\! \dd{t_4} \, \e^{-\gamma(t_3 - t_4)} \expected{f(t_2)f(t_4)}.
    \end{equation*}
    Here we have used the fact that the cross terms vanish since \(\expected{f(t)} = 0\).
    We again have to be careful about introducing dummy variables for the integrals which don't clash.
    Consider just the integral term here, call it \(I\).
    We substitute in \(\expected{f(t_2)f(t_4)} = \Gamma \delta (t_2 - t_4)\).
    This gives
    \begin{equation}
        I = \Gamma \int_0^t \!\! \dd{t_1}\int_0^{t_1} \!\! \dd{t_2} \, \e^{-\gamma(t_1 - t_2)} \int_0^t \!\! \dd{t_3}\int_0^{t_3} \!\! \dd{t_4} \, \e^{-\gamma(t_3 - t_4)} \delta(t_2 - t_4).
    \end{equation}
    Now we perform the integral over \(t_4\).
    In order for the \(\delta(t_2 - t_4)\) term to not give zero we need to have \(t_2\) being between \(0\) and \(t_3\).
    This means we change the limits on the \(t_3\) integral to go from \(t_2\) to \(t\), so \(t_3 \ge t_2\).
    Doing so we get
    \begin{equation}
        I = \Gamma \int_0^t \!\! \dd{t_1} \int_0^{t_1} \!\! \dd{t_2} \, \e^{-\gamma(t_1 - t_2)} \int_{t_2}^{t} \!\! \dd{t_3} \, \e^{-\gamma(t_3 - t_2)}.
    \end{equation}
    Performing the \(t_3\) integral we get
    \begin{equation}
        I = \frac{\Gamma}{\gamma} \int_0^t \!\! \dd{t'} \int_0^{t_1} \!\! \dd{t_2} \e^{-\gamma(t_1 - t_2)}(1 - \e^{-\gamma(t - t_2)}).
    \end{equation}
    The \(t_2\) integral then gives
    \begin{align}
        I &= \frac{\Gamma}{\gamma^2} \int_0^t \!\! \dd{t_1} \left( 1 - \e^{-\gamma t_1} - \frac{1}{2}[\e^{-\gamma(t - t_1)} - \e^{-\gamma(t + t_1)}] \right)\\
        &= \frac{\Gamma t}{\gamma^2} - \frac{\Gamma}{\gamma^3}(1 - \e^{-\gamma t}) - \frac{\Gamma}{2\gamma^3}(1 - \e^{-\gamma t})^2.
    \end{align}
    
    The short time limit is \(\expected{(x(t) - x_0)^2} \approx v_0^2t^2\).
    In the long time limit \(\gamma t \gg 1\) so \(t/\gamma^2 \gg 1/\gamma^3\), which means that we have \(\expected{(x(t) - x_0)^2} \approx \Gamma t/\gamma^2\).
    Putting in \(\Gamma = 2\gamma \boltzmann T\) we get \(\expected{(x(t) - x_0)^2} \approx 2\boltzmann Tt/\gamma\).
    Identifying \(\expected{(x(t) - x_0)^2} = 2Dt\) where \(D\) is the diffusion constant we get
    \begin{equation}
        D = \frac{\boltzmann T}{\gamma}.
    \end{equation}
    This is called the \defineindex{Einstein relation}.
    It relates the diffusion constant, which is a measure of dissipation, to \(\Gamma\), which measures fluctuations.
    
    \section{General Theory}
    Consider some variable, \(X\), with mean zero, which fluctuates in time.
    We are interested in fluctuations in \(X\) about the mean, which we measure as correlations in equilibrium.
    For example, if \(X\) is positive at time \(\tau\) then it is more likely to be positive at time \(\tau + t\), for some small \(t\), than it is to be negative, assuming that \(X\) varies relatively slowly over this time scale.
    
    In equilibrium we expect the correlation function to be independent of the initial time, and depends only on the difference in time, that is
    \begin{equation}
        \expected{X(\tau)X(\tau + t)} = M_{XX}(\tau)
    \end{equation}
    for some function \(M_{XX} \colon \reals \to \reals\).
    For typical use cases we expect that there is some correlation time, \(t_{\mathrm{c}}\), such that \(M_{XX}\) rapidly falls of for \(t > t_{\mathrm{c}}\).
    This typically appears in the form of an asymptotic decay:
    \begin{equation}
        M_{XX}(t) \sim \e^{-t/t_{\mathrm{c}}}.
    \end{equation}
    
    It is possible to generalise this to compare two different fluctuating variables, for example, we may consider the magnetisation at two different, but nearby, places, which we expect to be correlated.
    We then define
    \begin{equation}
        \expected{X(\tau)Y(\tau + t)} = M_{XY}(\tau).
    \end{equation}
    We call \(M_{XY}\) the \defineindex{dynamic correlation matrix}, and we can view the case of \(M_{XX}\) as a special case.
    
    The principle of detailed balance implies that there are no currents in equilibrium.
    This means that once equilibrium is reached there is no direction of time, since there is no flowing current to distinguish forwards and backwards in time.
    We therefore have that fluctuations arising in equilibrium are symmetric:
    \begin{equation}
        M_{XY}(t) = M_{XY}(-t).
    \end{equation}
    Further we have
    \begin{alignat}{3}
        M_{YX}(t) &= \expected{Y(\tau) X(\tau + t)}\\
        &= \expected{X(\tau + t)Y(\tau)} \qquad & \tau \to \tau - t\\
        &= \expected{X(\tau)Y(\tau - t)}\\
        &= M_{XY}(-t)\\
        &= M_{XY}(t),
    \end{alignat}
    so \(M_{XY}\) is also symmetric in \(X\) and \(Y\).
    
    \subsection{Linear Response Theory}
    Now consider a small perturbation appliied to the system.
    We can apply a small \enquote{thermodynamic force}, \(f_X\), such as an external field, \(h\), applied if \(X\) is the local magnetisation, \(m\).
    
    There are various ways to experimentally investigate these perturbations.
    One way is to consider the perturbation over a long time, say starting at \(t = -\infty\) (or some large, negative approximation of \(-\infty\)), and then turning it off at \(t = 0\).
    For \(t > 0\) the resulting average fluctuation, \(Y\), decays away.
    
    Another possibility is to consider the perturbation applied as a sharp kick at \(t = 0\), such a kick can be modelled as a delta distribution and the result is a Green's function.
    
    In either case we write
    \begin{equation}
        \expected{Y(t)}_{f_X} = R_{YX}(t) f_X,
    \end{equation}
    where \(Y\) is the fluctuation due to the thermal force, \(f_X\), and the average is over \(f_X\).
    The \defineindex{response function matrix}, \(R_{YX}(t)\), generalises the susceptibility.
    
    For a sufficiently small perturbation the response will be so weak that it could have arisen as a natural, spontaneous fluctuation.
    In that case the fluctuation, \(Y(t)\), will on average decay in time as if it were such a fluctuation which just happened to occur at \(t = 0\).
    
    The \defineindex{fluctuation-dissipation theorem} makes this idea, that a sufficiently small perturbation can be treated as a spontaneous fluctuation, more rigorous.
    It states that
    \begin{equation}
        M_{YX}(t) = \boltzmann TR_{YX}(t).
    \end{equation}
    This says that the response function, \(R_{YX}\), which corresponds to the dissipation of the fluctuation, contains exactly the same information as the correlation function, \(M_{XY}\), which corresponds to the fluctuation.
    
    The factor of \(\boltzmann T\), can be motivated dimensionally, since \(f_XX\) has units of energy, and so does \(\boltzmann T\).
    This means that \(X\) has units of \([R][f_X] = [R][\boltzmann][T]/[X]\), and \(M\) has units of \([X]^2 = [R][\boltzmann][T]\).
    A full proof of the fluctuation-dissipation theorem is beyond the scope of the course as it either requires more classical mechanics, such as Poisson brackets, or more quantum mechanics through the formalism of the density matrix.
    
    Combining previous results we find that the fluctuation-dissipation theorem and symmetry of \(M_{XY}\) implies symmetry of the correlation matrix, that is
    \begin{equation}
        R_{XY}(t) = R_{YX}(t).
    \end{equation}
    This is \defineindex{Onsager's theorem}, which states that the mean response of \(X\) due to a thermodynamic force, \(f_Y\), is equivalent (in information content) to the mean response in \(Y\) due to \(f_X\).
    Of course, to use this for computations we have to carefully define \(f_X\) for a given system such that this theorem holds.
    
    Onsager's theorem results in the time reversibility of microscopic physics constraining the time irreversible relaxation of a macroscopic system.
    
    \subsubsection{Thermoelectricity}
    An application of the above theory is thermoelectricity.
    Suppose we have two blocks of metal in contact.
    If the left block is hotter than the right one then heat will flow from the left block to the right.
    Since metal is conductive we also have to allow for electrons to flow between the blocks.
    All of this will happen to achieve equilibrium, meaning that electrons are more likely to flow from the hot block to the cool block.
    This is called the \defineindex{thermoelectric effect}.
    
    Now consider a different experiment where the temperatures are the same, but the chemical potentials of the electrons are not.
    This is easiest to achieve by applying a voltage across the two blocks.
    Onsager's theorem states that not only will electrons flow across the blocks but also heat energy will flow between them.
    This is not easy to see from first principles but the symmetry of fluctuations and dissipations makes it so.
    This is called the \defineindex{Peltier effect}.
    
    \subsection{Over Damped Brownian Motion}
    Consider Brownian motion of a colloidal particle suspended in a fluid with an external force, \(F_{\mathrm{ext}}\).
    We can also add a restoring force, \(-\kappa x\), which keeps the particle position bounded as \(t \to \infty\).
    The Langevin equation for the system is then
    \begin{equation}
        m\diff[2]{x}{t} = -\gamma\diff{x}{t} + F_{\mathrm{ext}} - \kappa x + f(t).
    \end{equation}
    Here \(f\) is again a white noise random force, so \(\expected{f(t)} = 0\) and \(\expected{f(t)f(t')} = \Gamma \delta(t - t')\).
    
    Consider the case where the external force is switched on before \(t = 0\) and then turned off.
    As a simplification we consider the case of an over damped system.
    In this case accelerations are negligible, so the left hand side of the Langevin equation vanishes, and we get
    \begin{equation}
        \diff{x}{t} = \mu[F_{\mathrm{ext}} - \kappa x + f(t)]
    \end{equation}
    where \(\mu = 1/\gamma\).
    This is a valid approximation for a viscous fluid, with \(\gamma\) large.
    
    Using an integrating factor we have
    \begin{equation}
        \diff{}{t} [x\e^{\mu\kappa t}] = \diff{x}{t}\e^{\mu\kappa t} + \mu\kappa x\e^{\mu\kappa t} = \mu\e^{\mu\kappa t}[F_{\mathrm{ext}} + f(t)].
    \end{equation}
    Integrating this we get
    \begin{equation}
        x(t) = \mu_{-\infty}^{0} \e^{-\mu\kappa(t - t')}[F_{\mathrm{ext}} + f(t)] \dd{t'} + \mu \int_0^t \e^{-\mu\kappa(t - t')}f(t').
    \end{equation}
    The average displacement is then
    \begin{equation}
        \expected{x(t)}_{F_{\mathrm{ext}}} = \frac{1}{\kappa} \e^{-\mu\kappa t}F_{\mathrm{ext}}.
    \end{equation}
    For \(\mu\kappa t \ll 1\) we have \(\expected{x(t)}_{F_{\mathrm{ext}}} = F_{\mathrm{ext}}/\kappa - F_{\mathrm{ext}}\mu t\).
    This means the particle starts displaced by about \(F_{\mathrm{ext}}/\kappa\) and returns to zero displacement once the external force is removed.
    
    Comparing this with \(\expected{x}_f = Rf\) we have
    \begin{equation}
        R_{xx} = \frac{1}{\kappa}\e^{-\mu\kappa t}.
    \end{equation}
    Now considering the correlation function
    \begin{equation}
        M_xx(t) = \expected{x(\tau)x(\tau + t)}
    \end{equation}
    in the absence of external forces we have
    \begin{equation}
        x(t) = \mu \int_{-\infty}^t \e^{-\mu \kappa(t - t')} f(t') \dd{t'}.
    \end{equation}
    This allows us to compute the correlation function:
    \begin{align}
        M_{xx}(t) &= \mu^2\int_{-\infty}^{\tau} \dl{t'} \, \e^{-\mu\kappa(\tau - t')} \int_{-\infty}^{\tau + t} \dl{t''} \, \e^{-\mu\kappa(\tau + t - t'')} \expected{f(t')f(t'')}\\
        &= \mu^2 \Gamma \int_{-\infty}^{\tau} \dl{t'} \, \e^{-\mu\kappa(t + 2\tau - 2t')}\\
        &= \frac{\mu\Gamma}{2\kappa} \e^{-\mu\kappa t}.
    \end{align}
    The fluctuation dissipation theorem, \(\boltzmann T R_{xx} = M_{xx}\), then gives
    \begin{equation}
        \Gamma = \frac{2\boltzmann T}{\mu} = 2\gamma\boltzmann T,
    \end{equation}
    which is the same result we got previously.
    
%    %   Appdendix
%    \appendixpage
%    \begin{appendices}
%        \include{}
%    \end{appendices}
    
    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}
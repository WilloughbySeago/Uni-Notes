\documentclass[fleqn]{NotesClass}

%% Packages
\usepackage{csquotes}
\usepackage{etoolbox}
\usepackage{siunitx}
\usepackage{cancel}

% Tikz stuff
\usepackage{tikz}
\tikzset{>=latex}
% External
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
%\tikzexternaldisable
\usetikzlibrary{hobby}

% References, should be last things loaded
\usepackage{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor, breaklinks=true}
\gappto\UrlSpecials{\do\|{\newline}}
%\gappto\UrlBreaks{\UrlOrds}

\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths}


% Title page info
\title{Methods of Mathematical Physics}
\author{Willoughby Seago}
\date{September 20, 2021}
% \subtitle{}
% \subsubtitle{}

% Highlight colour
\definecolor{highlight}{HTML}{10cfd9}
\definecolor{darker}{HTML}{00acb6}
\definecolor{dark}{HTML}{03386c}
\definecolor{highlightalt}{HTML}{cde37a}

% Commands
% Maths
\newcommand*{\e}{\mathrm{e}}
\DeclareMathOperator{\Res}{Res}
\let\Re\relax
\let\Im\relax
\DeclareMathOperator{\Re}{Re}
\DeclareMathOperator{\Im}{Im}
\newcommand*{\order}{\mathcal{O}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\erfi}{erfi}
\DeclareMathOperator{\Ai}{Ai}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\linop}{\mathcal{L}}
\DeclarePairedDelimiterX{\innerprod}[2]{\langle}{\rangle}{#1 , #2}
\newcommand*{\laplaceTransform}{\mathcal{L}}
\newcommand*{\inverseLaplaceTransform}{\laplaceTransform^{-1}}
\newcommand*{\hermit}{\dagger}

% Include
\includeonly{}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/dirac-delta-lorentzian-limit.pdf}
    \tableofcontents
    \mainmatter
    
    \chapter{Introduction}
    \epigraph{The first lecture is like the first pancake}{Kristel Torokoff, shortly before an online lecture full of technical difficulties.}
    This is a course on mathematical methods.
    Specifically on analytic methods.
    Analytic here referring to analysis, with our focus on complex analysis.
    This course goes hand in hand with the symmetries of quantum mechanics course which teaches algebraic mathematical methods.
    Algebra here referring to abstract algebra, as in groups.
    
    \part{Complex Analysis}
    
    \chapter{Infinite Series}
    \epigraph{\enquote{Infinite}, that means an awful lot}{Kristel Torokoff}
    Infinite series are common in physics, for example approximating functions with Taylor series, doing perturbation theory, expanding integrals, the list goes on.
    
    A general \defineindex{infinite series} is a formal sum of the form
    \begin{equation}
        \sum_{n=0}^{\infty} a_n
    \end{equation}
    where \(a_n\) are the terms of the series, usually their values are specified by some rule.
    We can think of \(a_n\) as a function from \(\naturals\)\footnote{\(\naturals \coloneqq \{0, 1, 2, \dotsc\}\)} to whatever set of values \(a_n\) may take, that is \((a_n)\) is a sequence.
    Note that starting at \(n = 0\) is just since we need a start point, we could equally well start at \(n = 1\), \(n = 57\), or \(n = -\infty\).
    We are mostly interested in series which converge, which is something we should define properly.
    
    \section{Convergence}
    \begin{dfn}{Sequence Convergence}{dfn:sequence convergence}\index{sequence convergence}
        The sequence \((a_n)\) converges to the value \(l\) if for all \(\varepsilon>0\) there exists \(n_0\in\naturals\) such that \(\abs{a_n - l} < \varepsilon\) for all \(n > n_0\).
        In this case we write
        \begin{equation}
            \lim_{n\to\infty} a_n = l, \qqor a_n \to l\text{ as } n\to\infty.
        \end{equation}
    \end{dfn}
    What this means intuitively is that by taking terms sufficiently far along we can make the gap between \(a_n\) and \(l\) arbitrarily small.
    
    \begin{dfn}{Partial Sum}{dfn:partial sum}
        Given an infinite series
        \begin{equation}
            \sum_{n=0}^{\infty} a_n
        \end{equation}
        we define the \(N\)th \defineindex{partial sum} to be
        \begin{equation}
            \sum_{n=0}^{N}a_n.
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Series Convergence}{}\index{series convergence}
        The series
        \begin{equation}
            \sum_{n=0}^{\infty} a_n
        \end{equation}
        converges if the sequence of its partial sums converges.
        If this is the case the series and sequence of partial sums converge to the same value, \(S\).
        
        Combining \cref{dfn:sequence convergence,dfn:partial sum} we have that the series converges to \(S\) if for all \(\varepsilon>0\) there exists \(N_0\in\naturals\) such that
        \begin{equation}
            \abs{S - \sum_{n=0}^{N}a_n} < \varepsilon
        \end{equation}
        for all \(N > N_0\).
    \end{dfn}
    Intuitively this definition means that \(\sum a_n \to S\) if we can make the partial sums arbitrarily close to \(S\) by considering sufficiently many terms.
    
    \begin{dfn}{Absolute Convergence}{}
        A series,
        \begin{equation}
            \sum_{n=0}^{\infty} a_n
        \end{equation}
        is \define{absolutely convergent}\index{absolute convergence} if
        \begin{equation}
            \sum_{n=0}^{\infty} \abs{a_n}
        \end{equation}
        is convergent.
    \end{dfn}
    It turns out that absolute convergence is a significantly stronger condition than convergence.
    That is, all absolutely convergent series are also convergent, but not the other way round.
    
    If a series or sequence doesn't converge to a finite value we say it diverges.
    We may also abuse this term to say that, for example, an oscillating sequence, such as \((-1)^{n}\), diverges, when what we mean is it doesn't converge, which isn't quite the same but is often close enough.
    
    \begin{exm}{Geometric Series}{}
        The series
        \begin{equation}
            \sum_{n=0}^{\infty} x^n = 1 + x + x^2 + \dotsb
        \end{equation}
        is called the \defineindex{geometric series}.
        It is absolutely convergent whenever \(\abs{x}<1\) and divergent otherwise.
        
        The geometric sequence converges absolutely to \(1/(1-\abs{x})\).
        To show this we consider the partial sums:
        \begin{equation}
            G_N = \sum_{n=0}^{n}x^n
        \end{equation}
        which we can prove by induction are
        \begin{equation}
            G_N = \frac{1 - x^{N+1}}{1 - x}.
        \end{equation}
        In the case of \(N = 0\), our basis case, we have \(G_n = 0\), as the empty sum is by definition 0.
        Now suppose that
        \begin{equation}
            G_k = \frac{1 - x^{k+1}}{1 - x}
        \end{equation}
        for some \(k \in \integers\).
        It then follows that
        \begin{align}
            G_{k+1} &= x^{k+1} + G_{k}\\
            &= x^{k+1} \frac{1 - x^{k+1}}{x}\\
            &= \frac{x^{k+1}(1 - x) + 1 - x^{k+1}}{1 - x}\\
            &= \frac{x^{k+1} - x^{k+2} + 1 - x^{k+1}}{1 - x}\\
            &= \frac{1 - x^{k+2}}{1 - x}
        \end{align}
        as required.
        So, by the principle of induction, we have
        \begin{equation}
            G_N = \frac{1 - x^{N+1}}{1 - x}
        \end{equation}
        for all \(N \in \naturals\).
        
        It remains to prove that \(G_N \to 1/(1 - x)\) as \(N \to \infty\).
        Given some \(\varepsilon > 0\) we have to show that
        \begin{equation}
            \abs{\frac{1}{1 - x} - \frac{1 - x^{N+1}}{1 - x}} < \varepsilon
        \end{equation}
        for some sufficiently large \(N\).
        We start by combining the fractions to get
        \begin{equation}
            \abs{\frac{1}{1 - x} - \frac{1 - x^{N+1}}{1 - x}} = \abs{\frac{x^{N+1}}{1 - x}}
        \end{equation}
        For the specific value of \(\varepsilon\) under consideration take
        \begin{equation}
            N_0 = \left\lceil \frac{\log[\varepsilon(1 - x)]}{\log x} - 1 \right\rceil.
        \end{equation}
        It then follows that for \(N > N_0\) we have
        \begin{align}
            &N + 1 < \frac{\log[\varepsilon(1 - x)]}{\log x}\\
            \implies &(N + 1)\log x <  \log[\varepsilon(1-x)]\\
            \implies &x^{N+1} < \varepsilon(1 - x)\\
            \implies &\frac{x^{N+1}}{1 - x} < \varepsilon
        \end{align}
        which proves our claim.
        
        The proof for absolute convergence is identical but replace \(x\) with \(\abs{x}\).
    \end{exm}
    
    \section{Convergence Tests}
    It is a pain to prove convergence directly from the definition.
    For this reason many convergence tests have been invented.
    Most of these can tell us only whether or not a series converges but this is often enough.
    
    We start with a simple necessary, but not sufficient, condition for convergence which is immediately obvious.
    For \(\sum a_n\) to converge we must have \(a_n \to 0\) as \(n \to \infty\).
    
    Some tests apply only to certain series, for example, alternating series:
    \begin{dfn}{Alternating Series}{}
        An \defineindex{alternating series} is a series where the terms alternate between positive and negative.
        Such a series can be written as
        \begin{equation}
            \sum_{n=1}^{\infty} (-1)^{n-1}a_n
        \end{equation}
        where \(a_n\) are either all positive or all negative, depending on the sign of the first term.
    \end{dfn}
    
    \begin{lma}{Leibniz Criterion}{}
        The \defineindex{Leibniz criterion}, also known as the \defineindex{alternating series test} states that the series
        \begin{equation}
            \sum_{n=1}^{\infty} (-1)^{n-1} a_n
        \end{equation}
        converges if
        \begin{enumerate}
            \item \(a_n\) decrease monotonically\footnote{A sequence is monotonically decreasing (increasing) if \(a_{n+1} \le a_{n}\) (\(a_{n+1} \ge a_n\)) for all \(n\), if we can make the inequality \(<\) (\(>\)) instead we say the sequence is strictly monotonically decreasing (increasing).} for all but a finite number\footnote{A property holds for all but a finite number of terms of a sequence if there exists \(n_0\) such that the property holds for all \(a_n\) with \(n > n_0\).} of \(a_n\).
            \item \(a_n \to 0\) as \(n \to \infty\).
        \end{enumerate}
        \begin{proof}
            The proof consists of considering the odd and even partial sums,
            \begin{equation}
                S_{2m+1} = \sum_{n=1}^{2m+1} (-1)^{n-1}a_n, \qqand S_{2m} = \sum_{n=1}^{2m} (-1)^{n-1}a_n.
            \end{equation}
            If both converge to the same limit, \(L\), then the series converges to \(L\).
            
            The odd partial sums decrease monotonically:
            \begin{equation}
                S_{2(m+1)+1} = S_{2m + 1} - a_{2m+2} + a_{2m+3} \le S_{2m+1}
            \end{equation}
            where we use the fact that \(a_n\) is monotonically decreasing for sufficiently large \(n\) and so we may assume \(a_{2m+3} \le a_{2m + 1}\) and hence \(a_{2m+3}-a_{a_{2m+2}} < 0\).
            Similarly the even partial sums increase monotonically:
            \begin{equation}
                S_{2(m+1)} = S_{2m} + a_{2m+1} - a_{2m+2}\ge S_{2m}.
            \end{equation}
            Since \(a_n\) decrease monotonically and \(a_n \to 0\) we have that \(a_n \ge 0\) for all \(n\).
            Therefore \(S_{2m+1} - S_{2m} = a_{2m+1} \ge 0\).
            
            From this we have
            \begin{equation}
                a_1 - a_2 = S_2 \le S_{2m} \le S_{2m+1} \le S_1 = a_1.
            \end{equation}
            This shows that both odd and even partial sums are bounded.
            Since the odd partial sums decrease monotonically the bound below implies convergence.
            Since the even partial sums increase monotonically the bound above implies convergence.
            
            It remains to show that both sequences of partial sums converge to the same value.
            This is simple since
            \begin{equation}
                \lim_{m\to\infty} (S_{2m+1} - S_{2m}) = \lim_{m\to\infty} a_{2m+1} = 0
            \end{equation}
            by our original requirements, and also
            \begin{equation}
                \lim_{m\to\infty} (S_{2m+1} - S_{2m}) = \lim_{m\to\infty} S_{2m+1} - \lim_{m\to\infty} S_{2m} \implies \lim_{m\to\infty} S_{2m+1} = \lim_{m\to\infty} S_{2m}.
            \end{equation}
            Hence \(S_{2m+1}\) and \(S_{2m}\) must tend to the same value, \(L\), and therefore the original sequence converges.
        \end{proof}
    \end{lma}
    
    A more general convergence test, upon which many other convergence tests are built, is the ratio test.
    
    \begin{lma}{Ratio Test}{}\index{ratio test}
        Let
        \begin{equation}
            L = \lim_{n\to\infty} \, \abs*{\frac{a_{n+1}}{a_n}}.
        \end{equation}
        If \(L < 1\) then \(\sum a_n\) converges absolutely, if \(L > 1\) then it diverges, and if \(L = 1\) the test is inconclusive.
        
        \begin{proof}
            For any given \(\varepsilon < 1 - L\) we can find \(n_9\) such that
            \begin{equation}
                \abs{\frac{a_{n+1}}{a_n}} \le L + \varepsilon < 1
            \end{equation}
            for all \(n > n_0\).
            It then follows that if we fix \(N > n_0\) we have
            \begin{align}
                \abs{\sum_{n=N}^{\infty} a_n} &\le \sum_{n=N}^{\infty} \abs{a_n}\\
                &= \abs{a_N} + \abs{a_{N+1}} + \abs{a_{N+2}} + \dotsb\\
                &= \abs{a_N}\left[ 1 + \abs{\frac{a_{N+1}}{a_N}} + \abs{\frac{a_{N+2}}{a_{N}}} + \dotsb \right]\\
                &= \abs{a_N} \left[ 1 + \abs{\frac{a_{N+1}}{a_N}} + \abs{\frac{a_{N+2}}{a_{N+1}}}\abs{\frac{a_{N+1}}{a_N}} + \dotsb \right]\\
                &\le \abs{a_N} \sum_{n=0}^{\infty} (K + \varepsilon)^{n}
            \end{align}
            and this converges for \(L + \varepsilon < 1\) (which is indeed the case) as it is a geometric series.
            Note that the first inequality comes from noticing that if the absolute values are around the entire sum there is a chance that some terms cancel but if we take the absolute value of each term then they must add together.
            
            It then follows that the sum
            \begin{equation}
                \sum_{n=1}^{\infty} \abs{a_n} = \sum_{n=1}^{N-1} + \sum_{n=N}^{\infty} \abs{a_n}
            \end{equation}
            converges since the first sum contains only a finite number of terms and the second converges by our earlier work.
            This proves the case of \(L < 1\).
            
            Clearly if \(L > 1\) then the sequence fails to converge as the terms increase in size.
            If \(L = 1\) then we can't make a claim either way.
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Harmonic Series}{}
        An example of an inconclusive test is the \defineindex{harmonic series}:
        \begin{equation}
            \sum_{n=1}^{\infty} \frac{1}{n} = 1 + \frac{1}{2} + \frac{1}{3} + \dotsb
        \end{equation}
        and it famously diverges.
        However, the ratio test is inconclusive since
        \begin{equation}
            \lim_{n\to\infty} \abs{\frac{a_{n+1}}{a_n}} = \lim_{n\to\infty} \frac{1/(n+1)}{1/n} = \lim_{n\to\infty} \frac{n}{n+1} = 1.
        \end{equation}
    \end{dfn}
    
    \begin{lma}{Cauchy or Maclaurin Integral Test}{}
        The \define{Cauchy}, or \define{Maclaurin integral test}\index{Cauchy!integral test}\index{Maclaurin integral test} states that if \(f\) is a continuous monotonic decreasing function\footnote{A function is monotonically decreasing (increasing) if \(f(x) \le f(y)\) (\(f(x) \ge f(y)\)) for all \(x > y\), if we can make the inequality \(<\) (\(>\)) instead we say the sequence is strictly monotonically decreasing (increasing). That is a monotonically increasing function preserves the order and a monotonically decreasing function reverses it.} in agreement with \(a_n\), that is \(f(n) = a_n\) for all \(n\in I\) where \(I\) is the indexing set for the sequence \((a_n)\), then 
        \begin{equation}
            \sum_{n=b}^{\infty} a_n
        \end{equation}
        converges if
        \begin{equation}
            \int_{b-1}^{\infty} f(x)\dd{x}
        \end{equation}
        converges and diverges if
        \begin{equation}
            \int_b^{\infty} f(x)\dd{x}
        \end{equation}
        diverges.
        \begin{proof}
            For simplicity in the proof we will take \(b = 1\), but we can change this with a simple translation of the axes.
            The proof relies on the definition of the integral as the area under \(f(x)\).
            
            Suppose that \(f\) is the function plotted in \cref{fig:integral test}.
            This may not be its exact form but this works for any monotonically decreasing function.
            On the left hand side of this diagram starting at \(x = 1\) we have drawn blocks of width 1 which have the height \(f(x)\) on their left edge.
            Since these blocks are drawn at integer positions, \(n\), this height is \(a_n\).
            Since the width of the blocks is 1 the area of the blocks is \(a_n\) also.
            Clearly these blocks cover a larger area than the area under the curve and hence
            \begin{equation}
                \int_{1}^{\infty} f(x) \dd{x} \le \sum_{n=1}^{\infty} a_n.
            \end{equation}
            Now consider the right hand side of the diagram.
            Notice that these are the same blocks, but shifted left by 1.
            Clearly the area under the curve is greater than the area covered by the blocks so
            \begin{equation}
                \int_{0}^{\infty} f(x) \dd{x} \ge \sum_{n=1}^{\infty} a_n.
            \end{equation}
            Combining these we have
            \begin{equation}
                \int_{1}^{\infty} f(x) \dd{x} \le \sum_{n=1}^{\infty} a_n \le \int_{0}^{\infty} f(x) \dd{x}.
            \end{equation}
            Clearly from this we can conclude that if 
            \begin{equation}
                \int_{0}^{\infty} f(x) \dd{x}
            \end{equation}
            converges to some finite value then \(\sum a_n\) must as well since it is bounded above.
            Similarly if
            \begin{equation}
                \int_1^\infty f(x)\dd{x}
            \end{equation}
            diverges then there is no way that \(\sum a_n\) can converge.
        \end{proof}
    \end{lma}
    
    \begin{figure}
        \tikzsetnextfilename{integral-test}
        \begin{tikzpicture}
            \tikzset{axis/.style={very thick, <->}}
            \tikzset{box/.style={highlight, very thick, fill opacity=0.5, fill=highlight}}
            \foreach \i in {0,...,5} \node[below] at (\i, 0) {\i};
            \node[right] at (5, 0) {\(x\)};
            \draw[very thick, domain=0:5, samples=100, dark] plot (\x, {4/(\x+1)});
            \draw[box] (1, 0) rectangle (2, 2);
            \draw[box] (2, 0) rectangle (3, 4/3);
            \draw[box] (3, 0) rectangle (4, 1);
            \draw[box] (4, 0) rectangle (5, 4/5);
            \draw[axis] (0, 5) -- (0, 0) -- (5, 0);
            \begin{scope}[xshift=6cm]
                \foreach \i in {0,...,5} \node[below] at (\i, 0) {\i};
                \node[right] at (5, 0) {\(x\)};
                \draw[very thick, domain=0:5, samples=100, dark] plot (\x, {4/(\x+1)});
                \draw[box] (0, 0) rectangle (1, 2);
                \draw[box] (1, 0) rectangle (2, 4/3);
                \draw[box] (2, 0) rectangle (3, 1);
                \draw[box] (3, 0) rectangle (4, 4/5);
                \draw[axis] (0, 5) -- (0, 0) -- (5, 0);
            \end{scope}
        \end{tikzpicture}
        \caption{Comparison of leading and lagging blocks with the area under an integral, used for proving Cauchy's integral test.}
        \label{fig:integral test}
    \end{figure}

    \begin{exm}{Riemann Zeta Function}{}
        The \defineindex{Riemann zeta function}, or simply the \defineindex{zeta function}, \(\zeta\)\index{\(\zeta\)|see{zeta function}}, is defined by
        \begin{equation}
            \zeta(s) = \sum_{n=1}^{\infty} \frac{1}{n^s}.
        \end{equation}
        For now we consider only \(s\in\reals\).
        
        For a given value of \(s\) the ratio test gives us
        \begin{equation}
            \lim_{n\to\infty} \abs{\frac{1/(n+1)^s}{1/n^s}} = \lim_{n\to\infty} \abs{\frac{n}{n+1}}^s = 1,
        \end{equation}
        so it is inconclusive.
        Instead we try to apply the Cauchy integral test.
        Define
        \begin{equation}
            I_n = \int_1^{n} \frac{\dl{x}}{x^s} = \left[ \frac{x^{-s+1}}{1-s} \right]^n_1 = \frac{n^{1-s}}{1-s} = \frac{1}{1-s}\frac{1}{n^{s-1}}.
        \end{equation}
        Clearly if \(s > 1\) then \(I_n \to 0\) as \(n \to \infty\).
        The Cauchy integral test then allows us to conclude that the Riemann zeta function converges for all integer values greater than 1.
    \end{exm}

    \chapter{Asymptotic Series}
    \epigraph{If your choice is to commit an unlawful act, or be stuck, just commit the unlawful act.}{Kristel Torokoff}
    In this chapter we introduce asymptotic series.
    We will do so with a motivating example saving a formal definition until the end.
    
    \section{Motivating Example}\label{sec:asymptotic series:motivating example}
    Consider the integral
    \begin{equation}
        G(x) = \int_0^{\infty} \frac{\e^{-xt}}{1 + t} \dd{t}.
    \end{equation}
    It can be shown that for \(x > 0\) this is convergent.
    
    For large \(x\) this integral is dominated by small \(t\) since the \(\e^{-xt}\) term dominates the \(1/(1 + t)\) term.
    For this reason we may choose to expand the integrand about \(t = 0\).
    We can do so easily by noticing that if we rewrite the integrand as \(\e^{-xt}/[1-(-t)]\) then we can recognise a geometric series and so we have for \(\abs{t} \le 1\)
    \begin{equation}
        \frac{\e^{-xt}}{1 + t} = \e^{-xt} \sum_{m=0}^{\infty} (-t)^{m}.
    \end{equation}
    This is the alternating geometric series.
    
    Hence our integral is 
    \begin{equation}
        G(x) = \int_{0}^{\infty} \e^{-xt} \sum_{m=0} (-t)^{m}\dd{t}.
    \end{equation}
    Making a change of variables we define \(t' = xt\) and so \(x\dl{t} = \dl{t'}\) and the limits are unchanged, giving us
    \begin{equation}
        G(x) = \int_0^\infty \e^{-t'} \frac{1}{x^{m+1}}\sum_{m=0} (-t')^{m}\dd{t'} \mathrel{\text{\enquote{=}}} \sum_{m=0}^{\infty} \frac{(-1)^m}{x^{m+1}} \int_0^{\infty} \e^{-t'} (t')^m \dd{t'}.
    \end{equation}
    We put the equals in quotation marks as what we have done here is not legal.
    The sum doesn't converge uniformly and we cannot swap the order of the sum and integral.
    But what if we do anyway?
    After all, as physicists we are only interested in maths in so far as it gives us useful answers.
    Can we extract a useful answer anyway?
    Lets give it a go.
    
    If we define
    \begin{equation}
        I_m \coloneqq \int_0^\infty (t')^m \e^{-t'}\dd{t'}
    \end{equation}
    then integrating by parts we use
    \begin{align}
        u &= (t')^m & v &= -\e^{-t'}\\
        u' &= m(t')^{m-1} & v' &= \e^{-t'}
    \end{align}
    which gives us
    \begin{equation}
        I_m = [-(t')^m\e^{-t'}]_0^\infty + \int_0^\infty m(t')^{m-1}\e^{-t'}\dd{t'} = mI_{m-1}.
    \end{equation}
    From this it follows that
    \begin{equation}
        I_m = m!I_0
    \end{equation}
    We then have
    \begin{equation}
        I_0 = \int_0^{\infty} \e^{-t'} \dd{t'} = [-\e^{-t'}]_{0}^{\infty} = 1.
    \end{equation}
    Hence,
    \begin{equation}
        I_m = m!.
    \end{equation}

    Hence our integral is
    \begin{equation}
        G(x) \mathrel{\text{\enquote{=}}} \sum_{m=0}^{\infty} \frac{(-1)^mm!}{x^{m+1}}.
    \end{equation}
    We have an infinite series.
    We should check convergence, let's try the ratio test:
    \begin{equation}
        \lim_{m\to\infty} \abs*{\frac{a_{m+1}}{a_m}} = \lim_{m\to\infty} \frac{(m+1)!}{x^{m+2}}\frac{x^{m+1}}{m!} = \lim_{m\to\infty} \frac{m+1}{x} = \infty.
    \end{equation}
    So the sum diverges!
    
    However, if we evaluate the first few terms then we find that actually the series aligns very closely with numerical calculations of \(G\).
    Define
    \begin{equation}
        g_n(x) \coloneqq \sum_{m=0}^{n-1} \frac{(-1)^nn!}{x^{n+1}}.
    \end{equation}
    \Cref{fig:asymptotic expansion} shows \(g_n(10)\) plotted for \(n = 1, \dotsc, 26\) and the numerical value of \(G(10)\).
    We can see that around \(n=23\) \(g_n\) starts to diverge away from \(G(10)\), but before this \(g_n\) is a fairly good approximation of \(G\).
    This is very different to our usual expectation of series which is that more terms means a more accurate approximation.
    
    \begin{figure}
        \tikzsetnextfilename{asymptotic-expansion}
        \begin{tikzpicture}
            \tikzset{axis/.style={thick, ->}}
            \draw[axis] (0, 0) -- (9.5, 0) node[right] {\(n\)};
            \draw[axis] (0, -0.3) -- (0, 4) node[above] {\(g_n(10)\)};
            \draw[highlight, very thick] (0, 0.915633) -- (9.5, 0.915633);
            \node[left] at (0, 0.915) {\(G(10)\)};
            \foreach \pos in {(0.667,1.00), (1.00,0.900), (1.33,0.920), (1.67,0.914), (2.00,0.916), (2.33,0.915), (2.67,0.916), (3.00,0.915), (3.33,0.916), (3.67,0.915), (4.00,0.916), (4.33,0.915), (4.67,0.916), (5.00,0.915), (5.33,0.916), (5.67,0.915), (6.00,0.917), (6.33,0.913), (6.67,0.920), (7.00,0.908), (7.33,0.932), (7.67,0.881), (8.00,0.993), (8.33,0.735), (8.67,1.36), (9.00,-0.196), (9.33,3.84)} {
                \fill[dark] \pos circle [radius = 0.075];
            }
        \end{tikzpicture}
        \caption{The function \(g_n\) is defined by taking the first \(n-1\) terms of the series for \(G\). Here we have arbitrarily chosen to evaluate both at \(x = 10\).}
        \label{fig:asymptotic expansion}
    \end{figure}
    
    To aid in our analysis of this behaviour we define
    \begin{equation}
        R_n(x) = G(x) - g_n(x),
    \end{equation}
    which we can think of as the remainder of the series, that is all terms not included in \(g_n\).
    We can now work backwards to find an integral form for \(g_n\):
    \begin{equation}
        g_n(x) = \int_0^{\infty} \e^{-xt}\sum_{m=0}^{n-1} (-t)^{m}\dd{t}.
    \end{equation}
    We can recognise this sum as a finite geometric series and hence
    \begin{equation}
        g_n(x) = \int_0^{\infty} \e^{-xt} \frac{1 - (-1)^n}{1 + t}\dd{t}.
    \end{equation}
    Combing the integrals forms of \(G\) and \(g_n\) we get
    \begin{equation}
        R_n(x) = \int_0^{\infty} \e^{-xt} \frac{(-t)^n}{1 + t}\dd{t}.
    \end{equation}
    We can bound this:
    \begin{align}
        \abs{R_n(x)} = \abs*{\int_0^{\infty} \e^{-xt} \frac{(-t)^n}{1 + t}\dd{t}}\\
        &\le \int_0^{\infty} \abs{\e^{-xt}t^n}\\
        &= I_n\\
        &= \frac{n!}{x^{n+1}}
    \end{align}
    Here we have used the fact that \(1/(1 + t)\) is largest when \(t = 0\), and the integral of an absolute value is always at least as big as the absolute value of the integral.
    Also, \(I_n\) is the same integral as before, but now with \(\e^{-xt}\) instead of \(\e^{-t'}\).
    
    Consider how the last term of \(g_n\) grows in comparison to \(R_n\) as functions of \(x\).
    We can do this by considering their ratio as \(x \to \infty\):
    \begin{equation}
        \lim_{x\to\infty} \abs{\frac{R_n}{a_{m-1}}} = \lim_{x\to\infty} \frac{n!}{x^{n+1}}\frac{x^n}{(n-1)!} = \lim_{x\to\infty} \frac{n}{x} = 0.
    \end{equation}
    We see also that
    \begin{equation}
        \lim_{x\to\infty} x^nR_n(x) = \lim_{x\to\infty} \frac{n!}{x} = 0.
    \end{equation}
    What both of these show is that \(R_n(x)\) goes to zero faster than the last included term of the truncated series, \(g_n\).
    Therefore \(g_n(x)\) approaches \(G(x)\) but only for fixed \(n\) and large \(x\).
    This leads us to the definition of an asymptotic series.
    
    \begin{dfn}{Asymptotic Series}{}
        Consider the function defined by
        \begin{equation}
            f(z) = a_0 + \frac{a_1}{z} + \frac{a_2}{z^2} + \dotsb + \frac{a_n}{z^n} + R_n(z)
        \end{equation}
        for some function \(R_n\).
        Then
        \begin{equation}
            S_n(z) = \sum_{s=0}^{n}a_sz^{-s}
        \end{equation}
        is an \defineindex{asymptotic} series for \(f\) as \(z \to \infty\) if the \defineindex{remainder}, \(R_n\), satisfies
        \begin{equation}
            \lim_{z\to\infty} z^nR_n(z) = \lim_{z\to\infty} z^n
            [f(z) -  S_n(z)] = \lim_{z\to\infty} z^n\left[ f(z) - \sum_{s=0} \frac{a_s}{z^s} \right] = 0.
        \end{equation}
    \end{dfn}
    
    \begin{ntn}{Asymptotic Notation}{}
        If \(S_n(z)\) is an asymptotic expansion of \(f(z)\) then we denote this by \(S_n(z) \sim f(z)\).\index{\(\sim\)|see{asymptotic}}
        Note that this notation is often abused to say \enquote{\(f\) and \(S_n\) have similar behaviour as \(z \to \infty\)} but the precise mathematical meaning is in the sense described above.
        
        Another way of writing the condition on \(R_n\) is using \defineindex{little \(o\) notation}, in which the function \(g(x) = o(h(x))\) if \(g(x)/h(x) \to 0\) as \(x \to \infty\).
        In the case of asymptotic expansions we require that \(R_n(z) = o(z^{-n})\).
    \end{ntn}

    Intuitively a series is an asymptotic expansion for a function if the remainder goes to zero faster than the last included term as \(z \to \infty\).
    
    It should be noted that asymptotic expansions don't need to be convergent.
    In fact most of the interesting asymptotic expansions don't converge.
    
    The best place to truncate the series is where the remainder's absolute value is minimised.
    Usually this depends on the value at which we are evaluating the sum.
    
    Asymptotic series are usually alternating series in which the partial sums first approach the function but then start to increase in size and oscillate wildly.
    
    We can actually define an asymptotic series where instead of \(a_nz^{-n}\) the terms are of the form \(\varphi_n(z)\) where \(\varphi_n\) are functions such that \(\varphi_{n+1}(z) = o(\varphi_n(z))\), that is \(\varphi_{n+1}(z)/\varphi_n(z) \to 0\) as \(z \to \infty\).
    
    It is important to make sure you understand the difference between asymptotic and convergent series.
    \begin{important}
        A convergent series for \(f\) will approach \(f(z)\) as \(n \to \infty\) for a given value of \(z\).
        
        An asymptotic series for \(f\) will approach \(f(z)\) as \(z \to \infty\) for a given value of \(n\).
    \end{important}
    
    \chapter{Complex Analysis}
    \epigraph{Complex analysis, just a small subject for a Monday afternoon.}{Kristel Torokoff}
    This chapter is a recap of complex analysis.
    It will consist mostly of definitions, theorems, and examples.
    For proofs refer to the complex analysis part of the methods of theoretical physics notes: \url{https://github.com/WilloughbySeago/Uni-Notes/blob/main/year-3/Methods-of-Theoretical-Physics/Complex-Analysis/|Complex-Analysis-Notes.pdf}.
    
    \section{Basics}
    \begin{dfn}{Differentiable}{}
        The function \(f \colon S \subseteq \complex \to \complex\) is \defineindex{differentiable} at the point \(z \in S\) if
        \begin{equation}
            \lim_{\Delta z \to 0} \frac{f(z + \Delta z) - f(z)}{\Delta z}
        \end{equation}
        exists and is independent of the direction we approach 0.
        We define this limit, when it exists, to be the derivative of \(f\).
    \end{dfn}
    
    \begin{thm}{Cauchy--Riemann Relations}{}
        Let \(f\) be a complex function, which we can always write as
        \begin{equation}
            f(z) = f(x + iy) = u(x, y) + iv(x, y)
        \end{equation}
        for some \(u, v\colon \reals^2 \to \reals\).
        Then \(f\) is differentiable if and only if \(u\) and \(v\) satisfy the \defineindex{Cauchy--Riemann relations}:
        \begin{equation}
            \diffp{u}{x} = \diffp{v}{y}, \qqand \diffp{u}{y} = -\diffp{v}{x}.
        \end{equation}
        Or in an alternative notation
        \begin{equation}
            u_x = v_y, \qqand u_y = - v_x.
        \end{equation}
    \end{thm}
    
    \begin{dfn}{Analytic}{}
        The function \(f\) is \defineindex{analytic} on the domain \(D\) if \(f\) is differentiable at all points in \(D\).
    \end{dfn}
    
    For example, the function defined by \(f(z) = 1/z\) is analytic on \(\complex\setminus\{0\} \eqqcolon \complex^{\times}\), however at 0 it is not only not analytic but not defined.
    
    \begin{thm}{Cauchy's Theorem}{}
        \index{Cauchy!theorem}
        If \(f\) is an analytic on and within a closed contour, \(C\), then
        \begin{equation}
            \oint_C f(z) \dd{z} = 0.
        \end{equation}
    \end{thm}
    
    A consequence of Cauchy's theorem is that integrals of analytic functions are independent of the path between the start and end points as long as the paths can be continuously deformed into each other without crossing a singularity (i.e. the paths are homotopic).
    This follows from integrating one way along one path and back along another, forming an integral around a loop which is necessarily zero.
    
    \begin{thm}{Cauchy's Integral Formula}{thm:cauchy integral formula}
        \index{Cauchy!integral formula}
        If \(f\) is analytic in a neighbourhood of \(z_0\) and \(C\) is a closed contour contained in this neighbourhood with \(z_0\) in the interior of \(C\) then
        \begin{equation}
            f(z_0) = \frac{1}{2\pi i} \oint_C \frac{f(z)}{z - z_0}\dd{z}.
        \end{equation}
        It follows that
        \begin{equation}
            f^{(n)}(z_0) = \frac{n!}{2\pi i} \oint_C \frac{f(z)}{(z - z_0)^{n+1}}\dd{z}.
        \end{equation}
        It follows that if a function is analytic in a neighbourhood it is infinitely differentiable and all its derivatives are analytic on this neighbourhood.
    \end{thm}
    
    \begin{dfn}{Entire Functions}{}
        A function that is analytic for all finite values is called \defineindex{entire}.
    \end{dfn}
    
    For example, \(\e^z\), \(\sin z\), and \(\cos z\) are all entire functions.
    
    \begin{thm}{Liouville's Theorem}{}
        \index{Liouville's theorem}
        If \(f\) is analytic for all \(z\) and bounded then \(f\) is a constant function.
    \end{thm}

    This means that all functions of interest must have some point at which they are not analytic, or at which they are not bounded.
    For example, \(\e^z\), \(\sin z\), and \(\cos z\) all blow up at infinity.
    
    \section{Series Expansions}
    \begin{thm}{Taylor's Theorem}{}
        \index{Taylor's theorem}
        An analytic function has a convergent expansion about any point, \(z\), within its domain of analyticity.\footnote{Often analyticity is \emph{defined} as having a convergent expansion at a point. In this case we call functions satisfying the differentiable definition of an analytic function a holomorphic function. For complex functions the two are equivalent so we don't bother.}
        \begin{proof}
            We consider a series expansion at the point \(z\).
            From Cauchy's integral theorem (\cref{thm:cauchy integral formula}) we have
            \begin{equation}
                f(z) = \frac{1}{2\pi i} \oint_C \frac{f(z')}{z' - z}\dd{z'}
            \end{equation}
            for a circular contour, \(C\), centred at some point \(z_0\) and for \(z\) within \(c\).
            This then implies that
            \begin{equation}
                \abs*{\frac{z - z_0}{z' - z_0}} < 1,
            \end{equation}
            since \(z'\) are on the circle and so are further from the centre than \(z\) which is inside the circle.
            We then have
            \begin{align}
                \frac{1}{z' - z} &= \frac{1}{z' - z_0}\frac{1}{1 - \frac{z - z_0}{z' - z_0}}\\
                &= \frac{1}{z' - z_0} \sum_{n=0}^{\infty} \left( \frac{z - z_0}{z' - z_0} \right)^n
            \end{align}
            where we have used the geometric series, which is convergent since the absolute ratio of terms is less than one.
            We then find that
            \begin{align}
                f(z) = \frac{1}{2\pi i} \oint_C \sum_{n=0}^{\infty} \frac{f(z')(z - z_0)^n}{(z' - z_0)^{n+1}} \dd{z'}\\
                &= \sum_{n=0}^{\infty} \frac{(z-z_0)^n}{n!}f^{(n)}(z_0).
            \end{align}
            For an absolute ratio of terms less than one the geometric series converges uniformly and so exchanging the integral and sum is valid.
            This last equality then gives us the \defineindex{Taylor series} of \(f\) at \(z\).
        \end{proof}
    \end{thm}
    
    \begin{dfn}{Isolated Singularity}{}
        An \defineindex{isolated singularity} is a singularity, that is a point where a function is undefined, which has a neighbourhood which contains no other singularities.
    \end{dfn}
    
    \begin{thm}{Laurent Series}{}
        If \(f\) has an isolated singularity at \(z_0\) then we can expand \(f\) in a \defineindex{Laurent series}:
        \begin{equation}
            f(z) = \sum_{n=0}^{\infty} a_n(z - z_0)^n + \sum_{n=1}^{\infty} \frac{b_n}{(z - z_0)^n}.
        \end{equation}
        We call the first sum the \defineindex{analytic part} and the second the \defineindex{principle part}.
    \end{thm}
    
    \begin{dfn}{Pole Types}{}
        Let \(b_n\) be the coefficients of the principle part of a Laurent series expansion of \(f\) at some singularity \(z_0\).
        Then
        \begin{enumerate}
            \item If \(b_n = 0\) for \(n > 1\) and \(b_1 \ne 0\) we call \(z_0\) a \defineindex{simple pole}.
            \item If \(b_n = 0\) for \(n > N\) and \(b_N \ne 0\) we call \(z_0\) a \define{pole of order \(\bm{N}\)}\index{pole of order \(N\)}.
            \item If \(b_n \ne 0\) for infinitely many values of \(n\) then we call \(z_0\) an essential singularity.
        \end{enumerate}
    \end{dfn}
    
    \begin{dfn}{Residue}{}
        We call the coefficient \(b_1\) of a Laurent series for \(f\) the residue at \(z_0\), denoted \(\Res(f, z_0)\).
    \end{dfn}
    
    \section{Residue Theorem}
    \begin{thm}{Residue Theorem}{thm:residue}
        Let \(C\) be some closed contour, which we traverse in an anticlockwise direction.
        Let \(z_\alpha\) be poles inside \(C\).
        Then
        \begin{equation}
            \oint_C f(z) \dd{z} = 2\pi i\sum_\alpha \Res(f, z_\alpha).
        \end{equation}
    \end{thm}
    
    \begin{exm}{}{exm:integral 0 to inf 1/1+x^2}
        Consider the integral
        \begin{equation}
            I = \int_0^{\infty} \frac{1}{1 + x^2}\dd{x}.
        \end{equation}
        We can compute this with an integral in the complex plane running along the real axis from \(-R\) to \(R\) and then around a semicircle in the upper half plane.
        Along the large semicircle, \(C\), let \(z = R\e^{i\vartheta}\) for \(0 < \vartheta < \pi\).
        Then,
        \begin{equation}
            \int_C \frac{1}{1 + z^2} \dd{z} = iR\int_0^\pi \frac{\e^{i\vartheta}}{1 + R^2\e^{2i\vartheta}}.
        \end{equation}
        The magnitude of this is bounded by \(\pi R/(R^2 - 1)\) and hence goes to zero as \(R \to \infty\).
        Therefore the integral along this whole contour is equal to the integral along the real line as \(R \to \infty\), since we are interested in the integral from 0 to \(\infty\) this means the contour integral is equal to \(2I\).
        By noticing that
        \begin{equation}
            \frac{1}{1 + z^2} = \frac{1}{(z + i)(z - i)}
        \end{equation}
        we see that the integrand has simple poles at \(z = \pm i\), of these only \(z = i\) is inside the contour.
        The residue at this point is \(1/2i\) and hence
        \begin{equation}
            2I = 2\pi i\frac{1}{2i} = \pi \implies I = \frac{\pi}{2}.
        \end{equation}
    \end{exm}
    
    \begin{exm}{Generating Function}{}
        For some set of polynomials, \(f_n\), such as the Legendre polynomials\index{Legendre polynomials}, \(P_n\), we define a \defineindex{generating function} to be
        \begin{equation}
            \mathcal{F}(x, z) = \sum_{n=-\infty}^{\infty} f_n(x)z^n.
        \end{equation}
        Sometimes we can find a closed, analytical expression for \(\mathcal{F}\).
        In this case we can use the residue function to recover the polynomials:
        \begin{equation}
            \frac{1}{2\pi i} \oint_C \frac{\mathcal{F}(x, z)}{z^{n+1}}\dd{z} = f_n(x).
        \end{equation}
        Here \(C\) is some anticlockwise contour containing the origin.
        This gives us an integral representation for the polynomials.
        We will use this later.
    \end{exm}
    
    \begin{lma}{Jordan's Lemma}{lma:jordan's}
        \index{Jordan's lemma}
        Given an integral of the form 
        \begin{equation}
            I_R = \int_C \e^{ikz}f(z) \dd{z}
        \end{equation}
        for some real \(k > 0\) and function \(f\), which is analytic in the upper plane, except for at a finite number of poles, with \(C\) being the curved part of a semicircular contour between \(-R\) and \(R\) on the real axis with the curved part in the upper half plane.
        If
        \begin{equation}
            \lim_{\abs{z} \to\infty} f(z) = 0, \qquad\text{when}\qquad 0 \le \arg z \le \pi
        \end{equation}
        then
        \begin{equation}
            \lim_{R\to\infty} I_R = 0.
        \end{equation}
        
        An equivalent result holds for real \(k < 0\) with the condition now that \(\pi \le \arg z \le 2\pi\) and the contour is in the lower half plane.
    \end{lma}

    \begin{exm}{}{}
        Consider a resistance, \(R\), and inductance, \(L\), in series with a voltage\footnote{when we do Fourier transforms we will see that this is really a voltage impulse, \(A\delta(t)\) at time \(t = 0\)},
        \begin{equation}
            V(t) = \frac{A}{2\pi} \int_{-\infty}^{\infty} \e^{i\omega t}\dd{\omega}.
        \end{equation}
        The current due to an alternating voltage \(\e^{i\omega t}\) is \(\e^{i\omega t}/(R + i\omega L)\), which follows from Ohm's law with complex impedance \(R + i\omega L\).
        The current is then
        \begin{equation}
            I(t) = \frac{A}{2\pi} \int_{-\infty}^{\infty} \frac{\e^{i\omega t}}{R + i\omega L}\dd{\omega}.
        \end{equation}
        
        We can split this integral into two halves, one covering \(t < 0\) and the other \(t > 0\), since \(t\) plays the role of \(k\) in Jordan's lemma (\cref{lma:jordan's}).
        
        There is one pole at \(\omega = iR/L\) so we find that for \(t < 0\) we have a complete contour in the lower half plane containing no poles and so \(I(t) = 0\).
        
        For \(t > 0\) we have a complete contour in the upper half plane encircling a single pole.
        By the residue theorem (\cref{thm:residue}) the integral around the entire contour is \(A\e^{-Rt/L}\), and by Jordan's lemma (\cref{lma:jordan's}) the integral over the curved part of the contour is zero, leaving us with only the contribution from the real line, which means \(I(t) = A\e^{-Rt/L}\) for \(t > 0\).
        
        This is consistent with the physical picture where for \(t < 0\) there has never been a voltage and so there is no current.
        Then, at \(t = 0\), there is a voltage impulse, and after this there is an exponentially decaying current with time constant \(R/L\).
    \end{exm}
    
    \section{Multi-Valued Functions}
    When we extend functions to the complex plane they often become multi-valued.
    The canonical examples being \(\ln z\) and \(z^r\) for non-integer \(r\).
    We can see this most easily if we consider the polar form of \(z = Re^{i\vartheta}\):
    \begin{equation}
        \begin{array}{l}
            \ln z = \ln(R\e^{i\vartheta}) = \ln(R\e^{i\vartheta + 2\pi in}) = \ln R + i\vartheta + 2i\pi n,\\
            z^{1/p} = (R\e^{i\vartheta})^{1/p} = (R\e^{i\vartheta + 2\pi i n})^{1/p} = R^{1/p}\e^{i\vartheta/p + 2\pi in/p}
        \end{array}
        \qquad n \in \integers
    \end{equation}
    
    \begin{dfn}{Branch Point}{}
        A multi-valued function, \(f\), has a \defineindex{branch point} at \(z\) if \(f(z + \varepsilon) \ne f(z + \varepsilon\e^{2\pi i})\) in the limit \(\varepsilon \to 0\).
    \end{dfn}
    
    \begin{dfn}{Branch Cut}{}
        We can render a multi-valued function single valued by connecting branch points with \define{branch cuts}\index{branch cut}.
        For now we think of these as barriers which cannot be passed.
    \end{dfn}
    
    The choice of branch cut is arbitrary, as long as all branch points are connected to a branch cut the function will be single valued.
    Once we have chosen a branch cut however we must stick with it.
    Therefore we should take care to choose branch cuts to make the problem as simple as possible.
    For example, \(\ln\) has branch points at \(z = 0, \infty\), suppose we wish to integrate from \(0\) to \(\infty\) along the real axis.
    Then we may choose \([-\infty, 0]\) as a branch cut, handily keeping out of the way of our integral.
    
    Notice that we must consider branch points at infinity.
    Given a function, \(f\), if we know the position of all of its finite branch cuts then we can consider \(f(1/\lambda)\) and take \(\lambda \to 0\), if we end up evaluating at one of the finite branch points then infinity is also a branch point.
    
    A function is, by definition, not analytic at a branch point as if the function is not uniquely defined then the derivative isn't either.
    We can often integrate along a branch cut by integrating around it and taking the limit of getting closer to the branch cut.
    
    \begin{exm}{}{exm:keyhole contour}
        Consider the integral
        \begin{equation}
            I = \int_0^{\infty} \frac{1}{1 + x^3}\dd{x}.
        \end{equation}
        We cannot close the contour in the same way as in \cref{exm:integral 0 to inf 1/1+x^2}, and consider an integral along the real axis, since the integrand is not even.
        Instead we make a change of variables: \(u = x^3\) and hence \(\diff{x}/{u} = u^{-2/3}/3\) giving us
        \begin{equation}
            I = \int_0^{\infty} \frac{1}{3}\frac{u^{-2/3}}{1 + u}\dd{u}.
        \end{equation}
        This introduces a branch point at \(u = 0\).
        We take a branch cut from 0 to \(\infty\) along the positive real axis.
        
        See \cref{fig:keyhole contour} for the contour we will use.
        We can integrate just above the branch cut with \(u = r\e^{i\varepsilon}\) taking \(\varepsilon \to 0\).
        The result of this is \(I\).
        
        Similarly we can integrate just below the branch cut with \(u = r\e^{-i\varepsilon}\) taking \(\varepsilon \to 0\).
        The result of this is
        \begin{equation}
            \int_{\infty}^{0} \frac{r^{-2/3}}{3}\frac{\e^{-4\pi i/3}}{1 + r} \dd{r} - \e^{-4\pi i/3}I.
        \end{equation}
        
        We can close the contour with two arcs.
        We take the large arc to infinity so it includes the pole \(u = -1 = \e^{i\pi}\) and by the residue theorem (\cref{thm:residue}) the integral around the large arc is \(2\pi i \e^{2\pi i/3}/3\).
        We take the small arc around the origin to zero.
        The integral over this arc yields zero.
       
       Combining all of these results we have
       \begin{equation}
           (1 - \e^{4\pi i/3})I = \frac{2}{3}\pi i\e^{-2\pi i/3} \implies I = \frac{2\pi}{3\sqrt{3}}.
       \end{equation}
    \end{exm}
    
    \begin{figure}
        \tikzsetnextfilename{keyhole-contour-avoid-positive-real-axis}
        \begin{tikzpicture}
            \tikzset{axis/.style={very thick, ->}}
            \tikzset{contour/.style={ultra thick, highlight}}
            \draw[axis] (-4, 0) -- (4, 0);
            \draw[axis] (0, -4) -- (0, 4);
            \draw[contour] (0, 0.1) -- (4, 0.1) arc(1.43:358.57:4) -- (0, -0.1) arc(270:90:0.1);
        \end{tikzpicture}
        \caption{A contour avoiding the positive real axis, as used in \cref{exm:keyhole contour}.}
        \label{fig:keyhole contour}
    \end{figure}
    
    \section{Analytic Continuation}
    \begin{thm}{Identity Theorem}{thm:identity}
        If two functions are analytic in the region \(R\) and have the same values for all points in some subregion, or along some curve within \(R\), then the two functions are identical everywhere within \(R\).
    \end{thm}
    
    This result allows us to extend functions defined along the real axis to the complex plane.
    For example, if we define \(\e^x\) by its Taylor series for \(x \in \reals\) then we can uniquely extend this to define \(\e^z\) by the same Taylor series with \(z \in \complex\).
    
    Consider a power series about some point \(z_1\) with a finite radius of convergence, which extends to the nearest singularity.
    This power series represents a function, \(f_1\), which is analytic in the original domain of convergence.
    
    We can then expand this function about some new point, \(z_2\), within this domain of convergence.
    The resulting series may have a domain of convergence that extends beyond that of the original domain of convergence.
    We can use this to define a second function, \(f_2\), which is analytic on this new domain of convergence.
    
    Since \(z_2\) was chosen to be in the domain of convergence for \(f_1\) we know that there is a region where the domains of convergence overlap.
    The identity theorem (\cref{thm:identity}) then tells us that \(f_1\) and \(f_2\) must agree on this overlapping area.
    We call \(f_2\) the \defineindex{analytic continuation} of \(f_1\) into this new region.
    
    We can repeat this process as much as we like and eventually, assuming only isolated singularities, we can define the analytic continuation of \(f_1\) to the whole complex plane, minus the singularities.
    This procedure does potentially require an infinite number of steps so instead we usually resort to tricks and short cuts.
    
    \begin{exm}{}{}
        Consider the function \(f_1(z) = \sum_{n=0}^{\infty} z^n\).
        This is the geometric series and converges for \(\abs{z} < 1\) to \(f(z) = 1/(1 - z)\).
        \(f\) is then analytic everywhere, except at \(z = 1\).
        Hence \(f\) is the analytic continuation of \(f_1\) into \(\complex\setminus\{1\}\).
    \end{exm}
    
    \subsection{Caveats}
    Not all functions can be continued indefinitely.
    There may be a natural barrier of non-isolated singularities which cannot be passed.
    
    It is also possible that we will create a multi-valued function.
    For example \(f_2\) and the similarly defined \(f_3\) may overlap outside of the original domain of convergence and there is no requirement that they agree here.
    Similarly if we cross a branch cut we may get a result in the original domain of convergence that doesn't agree with \(f_1\).
    
    \chapter{Gamma Function}
    \epigraph{With just a little bit of hand waving, common sense, and bravery we have derived Stirling's formula.}{Kristel Torokoff}
    \begin{dfn}{Factorial}{}
        For a natural number, \(n\), the \defineindex{factorial}\index{"!|see{factorial}} is defined as
        \begin{equation}
            n! \coloneqq n(n - 1)(n - 2) \dotsm 1,
        \end{equation}
        with \(0! \coloneqq 1\).
        This can also be written recursively as
        \begin{equation}
            n! = n(n-1)!.
        \end{equation}
        
        For an even natural number the \defineindex{double factorial}\index{"!"!|see{double factorial}} is defined as
        \begin{equation}
            (2m)!! \coloneqq (2m)(2m-2)(2m-4) \dotsm 4\cdot 2
        \end{equation}
        with \(0!! \coloneqq 1\), and for an odd natural number
        \begin{equation}
            (2m + 1)!! \coloneqq (2m + 1)(2m - 1)(2m - 3) \dotsm 3\cdot 1.
        \end{equation}
        with \((-1)!! \coloneqq 1\).
    \end{dfn}
    In this section we are interested in the factorial and continuing it to non-integer values, and even to complex values.
    We mention the double factorial only because it appears in some expansions.
    Note that the double factor is usually smaller than the single factorial, despite the name.
    
    \section{Gamma Function}
    In \cref{sec:asymptotic series:motivating example} we met the integral
    \begin{equation}
        I_n = \int_0^{\infty} \e^{-t}t^{n} \dd{t}.
    \end{equation}
    We also showed that \(I_n = n!\).
    Changing up the notation we can view this as a function \(I\colon\naturals\to\naturals\) defined by \(I(n) = I_n = n!\).
    For this reason we call \(I\) an \defineindex{integral representation} of the factorial function.
    Integral representations are a handy tool and often allow us to generalise functions easily.
    
    Considering again the form of the integral we see that if we consider a similar function, \(I' \colon \{z\in\complex\mid\Re(z) > -1\} \to \complex\), then this integral still converges on the domain.
    We can think of this as generalising the factorial to complex values.
    This function, or rather its analytic continuation to the whole complex plane, which we will derive later, is so common that it gets its own name.
    \begin{dfn}{Gamma Function}{}
        The \defineindex{gamma function}\index{\(\Gamma\)|see{gamma function}}, \(\Gamma\colon\complex\to\complex\), is defined by
        \begin{equation}
            \Gamma(z) \coloneqq \int_{0}^{\infty} \e^{-t}t^{z-1}\dd{t}
        \end{equation}
        for \(\Re(z) > 0\).
    \end{dfn}
    
    The extra \({}-1\) term here is simply a historic accident in the definitions that has now stuck.
    For \(n \in \naturals\) we have
    \begin{equation}
        \Gamma(n + 1) = n!.
    \end{equation}
    
    In order to have the gamma function be defined on the entire complex plane we need to analytically continue it.
    There are two ways to do this.
    The first is simple, but requires an infinite amount of steps.
    The second is slightly more complex but requires only one step.
    
    We can apply integration by parts to the integral representation of the gamma function.
    Contrary to what we would normally do we take \(\e^{-t}\) as the term to differentiate, giving \(-\e^{-t}\), and \(t^{z - 1}\) as the term to integrate, giving \(t^{z}/z\)
    \begin{equation}
        \Gamma(z) = \left[ -\frac{t^{z}}{z} \e^{-t} \right]_{0}^{\infty} + \frac{1}{z}\int_{0}^{\infty} t^z\e^{-t} = \frac{1}{z}\Gamma(z + 1).
    \end{equation}
    This gives us \(\Gamma(z)\) in terms of \(\Gamma(z + 1)\).
    There is a simple pole at \(z = 0\), due to the factor of \(1/z\).
    Since the integral for \(\Gamma(z + 1)\) converges for \(\Re(z) > -1\) we have continued the gamma function slightly along the negative real axis into a strip of the complex plane.
    We can repeat this process and we get \(\Gamma(z)\) in terms of \(\Gamma(z + 2)\).
    This converges for \(\Re(z) > -2\), and has two simple poles, at \(z = 0, -1\).
    
    This process is of stripwise continuation will, if we carry it on to infinity, give us the gamma function defined on all of \(\complex\), with a set of simple poles at non-positive integers.
    This shows that the gamma function is meromorphic\footnote{a \defineindex{meromorphic} function is a function which is analytic except for at countably many isolated points.}.
    
    Unfortunately continuing on forever is not practical.
    Fortunately we don't actually need to as there is another way to analytically continue the gamma function.
    Notice that for non-integer \(z\) the \(t^{z-1}\) factor in the definition of the gamma function will give a branch point at \(t = 0\).
    We take a branch cut from zero to infinity along the positive real axis.
    We can then evaluate the gamma function by considering \defineindex{Hankel's contour}, \(C\), which is shown in \cref{fig:hankel contour}.
    Notice that this is \emph{not} a closed contour.
    We wish to compute
    \begin{equation}
        \int_C \e^{-t}t^{z-1} \dd{t}.
    \end{equation}
    
    \begin{figure}
        \tikzsetnextfilename{hankel-contour}
        \begin{tikzpicture}
            \tikzset{axis/.style={very thick, ->}}
            \draw[axis] (-2, 0) -- (5, 0) node[right] {\(\Re(t)\)};
            \draw[axis] (0, -2) -- (0, 2) node[above] {\(\Im(t)\)};
            \draw[red, very thick] (0, 0) -- (5, 0);
            \draw[highlight, ultra thick] (5, 0.2) -- (0.4, 0.2) arc(30:330:0.4) -- (5, -0.2);
            \draw[ultra thick, highlight, ->] (2.51, 0.2) -- (2.45, 0.2);
            \draw[ultra thick, highlight, ->] (2.49, -0.2) -- (2.55, -0.2);
            \node[above] at (2.5, 0.2) {I};
            \node[above left] at (135:0.25) {II};
            \node[below] at (2.5, -0.2) {III};
        \end{tikzpicture}
        \caption{The Hankel contour.}
        \label{fig:hankel contour}
    \end{figure}
    
    Along part I of the Hankel contour, being careful that this part of the contour goes from infinity to zero.
    We can parametrise this with \(t = r\e^{i\varepsilon}\), and we will take \(\varepsilon \to 0\), we then get
    \begin{equation}
        \int_{\mathrm{I}} \e^{-t}t^{z-1}\dd{t} = \int_{\infty}^{0} \e^{-r\e^{-i\varepsilon}}r^{z-1}\e^{i\varepsilon(z-1)} \e^{i\varepsilon} \dd{r} \to \int_{\infty}^{0} \e^{-r}r^{z-1} \dd{r} = -\Gamma(z).
    \end{equation}
    
    Similarly we can parametrise part III of the Hankel contour with \(t = r\e^{2\pi i - i\varepsilon}\), where, again, we take \(\varepsilon \to 0\).
    The contribution is therefore
    \begin{align}
        \int_{\mathrm{III}} \e^{-t}t^{z-1}\dd{t} &= \int_{0}^{\infty} \e^{-r\e^{2\pi i - i\varepsilon}}r^{z-1}\e^{(2\pi i - i\varepsilon)(z - 1)}\e^{2\pi i - i\varepsilon}\dd{r}\\
        &\to \e^{2\pi i(z - 1)}\int_{0}^{\infty} \e^{-r}r^{z-1} \dd{r}\\
        &= \e^{2\pi i(z - 1)}\Gamma(z).
    \end{align}
    
    Finally we need the contribution from the small arc that is part II of the Hankel contour.
    Notice that in the limit of the two straight lines getting closer and closer to the axis this arc becomes a full circle.
    We can parametrise this part using \(t = \rho\e^{i\vartheta}\), and we will take \(\rho \to 0\).
    \begin{align}
        \int_{\mathrm{II}} \e^{-t}t^{z - 1} &= \int_{0}^{2\pi} \e^{-\rho\e^{i\vartheta}}\rho^{z-1}\e^{i\vartheta(z - 1)}i\rho\e^{i\vartheta}\dd{\vartheta}\\
        &= i\rho^{z}\int_{0}^{2\pi}\e^{-\rho\e^{i\vartheta}}\e^{i\vartheta z}\dd{\vartheta}\\
        &\to 0.
    \end{align}
    
    Combing these we have
    \begin{equation}
        \int_C \e^{-t}t^{z-1} = (\e^{2\pi i(z - 1)} - 1)\Gamma(z) \implies \Gamma(z) = \frac{1}{\e^{2\pi i (z - 1)} - 1} \int_C \e^{-t}t^{z-1}\dd{t}.
    \end{equation}
    This is valid for all \(z \notin \integers\).
    Multiplying through by \(\e^{i\pi(z-1)}\), and recognising the exponential definition of the sine function we have
    \begin{equation}
        \Gamma(z) = \frac{1}{\e^{\pi i(z - 1)} - \e^{-\pi i(z-1)}}\int_C\e^{-t}t^{z-1}\dd{t} = \frac{1}{2i\sin[\pi(z - 1)]}\int_C\e^{-t}t^{z-1}\dd{t}.
    \end{equation}
    Which gives us a contour integral representation of the gamma function:
    \begin{equation}
        2 i\sin[\pi(z - 1)] \Gamma(z) = \int_C \e^{-t}t^{-z}.
    \end{equation} 
    
    As mentioned before this is valid for all \(z \notin \integers\).
    Now we consider \(z \in \integers\).
    There will be no branch point now, and hence no branch cut.
    We can therefore close up Hankel's contour at infinity.
    For \(z \ge 1\) there will be no poles, and hence the integral gives zero.
    The integrals along the two straight parts of the contour will cancel also and we arrive at \(0 = 0\).
    Instead we can just compute \(z!\) as a normal factorial.
    For \(z \le 1\) we have \(\sin[\pi(z - 1)] = 0\) and the right hand side gives an integral around a pole at \(t = 0\).
    This integral is non-zero and we conclude that we must have a pole at non-positive integers as this is the only way to justify having one side of the equation be zero.
    
    \section{Beta Function}
    Consider the product of two gamma functions, evaluated at some \(r, s > 0\):
    \begin{align}
        \Gamma(r)\Gamma(s) &= \int_{0}^{\infty} x^{r-1}e^{-x}\dd{x} \int_{0}^{\infty} y^{s-1}\e^{-y}\dd{x}\\
        &= \int_{0}^{\infty} \dd{x} \int_{0}^{\infty} \dd{y} x^{r - 1}(x + y - x)^{s - 1}\e^{-(x + y)}.
    \end{align}
    Now let \(u = x + y\) and we then have
    \begin{equation}
        \Gamma(r) \Gamma(s) = \int_{0}^{\infty} \dd{u} \e^{-u} \int_{0}^{u} \dd{x} (u - x)^{s - 1}.
    \end{equation}
    Let \(x = ut\), so \(\dl{x} = u\dd{t}\) and
    \begin{equation}
        \Gamma(r) \Gamma(s) = \int_{0}^{\infty} \e^{-u} u^{r+s-1} \dd{u} \int_0^1 t^{r -1}(1 - t)^{s-1} = \Gamma(r + s) B(r, s).
    \end{equation}
    
    \begin{dfn}{Beta Function}{}
        The \defineindex{beta function}\index{\(B\)|see{beta function}}, \(B\), is defined, for \(\Re(r), \Re(s) > 0\), as
        \begin{equation}
            B(r, s) \coloneqq \int_0^1 t^{r - 1}(1 - t)^{s - 1} = \frac{\Gamma(r)\Gamma(s)}{\Gamma(r + s)}.
        \end{equation}
    \end{dfn}
    
    Consider the case of \(r = z\) and \(s = 1 - z\), we have
    \begin{align}
        \Gamma(z)\Gamma(1 - z) &= \Gamma(z + 1 - z) B(z, 1 - z)\\
        &= \Gamma(1)B(z, 1 - z)\\
        &= B(z, 1 - z)\\
        &= \int_0^{1} t^{z-1}(1 - t)^{-z}\\
        &= \int_0^{\infty} \frac{x^{z-1}}{1 + x}\dd{x}
    \end{align}
    where we have made the change of variables to \(x = t/(1 - t)\).
    This final integral can be done by contour integration and we obtain the result
    \begin{equation}
        \Gamma(z)\Gamma(z-1) = \frac{\pi}{\sin(\pi z)}.
    \end{equation}
    This is called \defineindex{Euler's reflection formula}.
    We derived this for \(0 < \Re(z) < 1\) but since both sides of the equation can be continued to the rest of the plane, except at integers, this relation extends to the entire plane, again, except at integers.
    
    Setting \(z = 1/2\) in Euler's reflection formula gives we get the result
    \begin{equation}
        \Gamma(1/2) = \sqrt{\pi}.
    \end{equation}
    This can be a useful value to know.
    
    \section{Stirling's Formula}
    In this section we aim to find an approximation of the factorial for positive integers.
    This is desirable since computing factorials exactly for large numbers is very computationally expensive.
    Since we are interested in an approximation for large values it makes sense that we should look for an asymptotic expansion, and that is indeed what we shall do.
    
    \begin{figure}
        \tikzsetnextfilename{exp-vs-cubic}
        \begin{tikzpicture}
            \tikzset{axis/.style={very thick, ->}}
            \node (A) at (0, -0.1) {};  % For some reason externalise cuts off arrow edge so put point just to the left to expand picture size
            \draw[axis] (0, 0) -- (8, 0) node[right] {\(t\)};
            \draw[axis] (0, 0) -- (0, 5);
            \draw[highlight, very thick, domain=0:8, samples=300] plot (\x, {exp(-\x)});
            \draw[dark, very thick, domain=0:1.71, samples=300] plot (\x, \x^3);
            \draw[highlightalt, very thick, domain=0:8, samples=700] plot (\x, {\x^3 * exp(-\x)});
            \draw[very thick, highlight, text=black] (0, -0.5) -- ++ (1, 0) node[right] {\(\e^{-t}\)};
            \draw[very thick, dark, text=black] (3, -0.5) -- ++ (1, 0) node[right] {\(t^n\)};
            \draw[very thick, highlightalt, text=black] (6, -0.5) -- ++ (1, 0) node[right] {\(\e^{-t}t^n\)};
        \end{tikzpicture}
        \caption{Plot of \(\e^{-t}\), \(t^{n}\), and \(\e^{-t}t^{n}\).}
        \label{fig:plot for stirling}
    \end{figure}
    
    We know from our earlier work that
    \begin{equation}
        n! = \Gamma(n + 1) = \int_{0}^{\infty} \e^{-t}t^n \dd{t}.
    \end{equation}
    Consider the plot shown in \cref{fig:plot for stirling}.
    This shows the plot of the two factors in the integrand of the gamma function, as well as a plot of the entire integrand.
    We see that initially the increasing \(t^{n}\) wins and the product gets larger but eventually the exponential decay wins and the product goes to zero.
    This plot, for illustration purposes, corresponds to the case of \(n = 3\) but the shape is generally the same for all values of \(n\).
    With the integral being the area under the plot it is clear that the dominant contribution to the integral comes from an area around the maximum of the integrand.
    This becomes more and more true as \(n\) increases.
    We can rearrange the integrand to get
    \begin{equation}
        \e^{-t} t^{n} = \e^{-t}\e^{n\ln t} = \e^{-t + n\ln t}.
    \end{equation}
    Let \(f(t) = -t + n\ln t\).
    Clearly the integrand is maximised when \(f\) is maximised.
    Differentiating we have
    \begin{equation}
        0 = f'(t) = -1 + \frac{n}{t} \implies t = n
    \end{equation}
    so there is an extrema at \(t = n\).
    Taking the second derivative we have
    \begin{equation}
        f''(n) = -\frac{n}{t^2}\bigg\vert_{t=n} = -\frac{1}{n} < 0.
    \end{equation}
    
    We can expand \(f\) around this maximum:
    \begin{align}
        f(t) &= f(n) + f'(n)(t - n) + \frac{1}{2}f''(n)(t - n)^2 + \dotsb\\
        &= -n + n\ln n - \frac{1}{2}\frac{1}{n}(t - n)^2 + \dotsb.
    \end{align}
    Substituting this into the expression for our integral we have
    \begin{align}
        n! &= \int_{0}^{\infty} \e^{f(t)} \dd{t}\\
        &= \int_0^{\infty} \exp\left[ -n + n\ln n - \frac{(t - n)^2}{2n} + \dotsb \right]\\
        &= \e^{-n}\e^{n\ln n} \int_{0}^{\infty} \exp[-\frac{(t - n)^2}{2n}](1 + \dotsb)\dd{t}\\
        &= \e^{-n}n^n \int_{0}^{\infty} \exp[-\frac{(t - n)^2}{2n}](1 + \dotsb)\dd{t}.
    \end{align}
    At this point we notice that the integrand is small for \(t < 0\) and so if we extend the bounds of integration to include the entire real axis the error will be small as it is exponentially suppressed.
    We therefore have
    \begin{equation}
        n! \approx \e^{-n}n^{n} \int_{-\infty}^{\infty} \exp[\frac{(t - n)^2}{2n}](1 + \dotsb)\dd{t} = \e^{-n}n^n \sqrt{2\pi n}(1 + \dotsb)
    \end{equation}
    where we have recognised the Gaussian integral
    \begin{equation}
        \int_{-\infty}^{\infty} \e^{-a(x + b)^2} \dd{x} = \sqrt{\frac{\pi}{a}}.
    \end{equation}
    Hence we have found an approximation for the factorial.
    It can be shown that it is an asymptotic expansion.
    This particular approximation is called \defineindex{Stirling's approximation} and is particularly common in statistical mechanics where we often need to consider combinations of \(N \approx \num{e23}\) particles, which gives rise to \(N!\), which is simply impossible to compute exactly.
    Instead we use the approximation
    \begin{equation}
        n! \approx \e^{-n}n^n \sqrt{2\pi n},
    \end{equation}
    or, in its more common form taking logs of both sides and neglecting \(\ln(2\pi n)/2\) as the smallest term
    \begin{equation}
        \ln n! \approx n \ln n - n + \frac{1}{2}\ln(2\pi n) + \dotsb = n\ln n - n + \order(\ln n).
    \end{equation}
    
    \chapter{Asymptotic Expansion of Integrals}
    \epigraph{Doing integrals and having a wonderful time.}{Kristel Torokoff}
    \section{Laplace's Method for Real Integrals}
    \epigraph{We get away with murder because its exponentially suppressed.}{Kristel Torokoff}
    Inspired by our success with Stirling's approximation we discuss a more general method for finding asymptotic expansions of integrals.
    In particular consider the integral
    \begin{equation}
        I(x) = \int_a^b f(t) \e^{x\varphi(t)}\dd{t}.
    \end{equation}
    We wish to find an asymptotic expansion for large \(x\).
    We take \(f, \varphi \colon \reals\to\reals\) to be functions such that \(f(t)\) varies relatively slowly compared to \(\e^{x\varphi(t)}\).
    Note that if \(f(t)\) doesn't vary slowly we can always move it into the exponential in the way we moved \(t^n\) into the exponential with Stirling's approximation.
    
    The general argument is the same as for finding Stirling's approximation.
    We find the maximum of \(\varphi\), say at \(t = c\).
    We then expand about this point and substitute into the integral.
    We take the limits of the integral to infinity under the assumption that the error will be relatively small, and finally we integrate.
    
    In more detail \defineindex{Laplace's method for real integrals} is
    \begin{enumerate}
        \item Identify \(c = \argmax_{[a, b]}\varphi\), that is find \(c\) such that \(\varphi(c)\) is the maximum value value of \(\varphi\) on the interval \([a, b]\).
        If there is no maximum on this interval then one of the endpoints will be the largest point and we instead take \(c\) as that endpoint.
        If there are multiple maxima consider the one for which \(\varphi(c)\) is largest.
        
        \item Expand \(f(t)\) and \(\varphi(t)\) about \(t = c\).
        Since \(f\) is relatively slowly varying we can take it to be approximately constant, \(f(t) \approx f(c)\).
        For \(\varphi\) it varies faster and since \(c\) is a maximum \(\varphi'(c) = 0\).
        Therefore we need to go to (at least) second order:
        \begin{equation}
            \varphi(t) \approx \varphi(c) + \frac{1}{2}(t - c)^2\varphi''(c) + \dotsb.
        \end{equation}
        If \(c\) is instead a point of inflection then \(\varphi''(c)\) will be zero also and we will have to go to third order.
        Note that \(\varphi''(c) < 0\) assuming a maximum at \(c\).
        If \(c\) is an endpoint and not a maximum then the \(\varphi'(c)\) term won't vanish.
        
        
        \item Expand the integration range to \(\reals\) and compute the integral to the desired number of terms.
        Up to second order, assuming \(c\) is a maximum, we would have
        \begin{align}
            I(x) &\approx \int_{-\infty}^{\infty} f(c) \exp[x\varphi(c) - \frac{x}{2}\abs{\varphi''(c)}(t- c)^2] \dd{t}\\
            &= f(c)\exp[x\varphi(c)] \int_{-\infty}^{\infty} \e^{-\abs{\varphi''(x)}u^2/2}\dd{u}\\
            &= f(c)\exp[x\varphi(c)] \sqrt{\frac{2\pi}{x\abs{\varphi''(c)}}}\label{eqn:laplace method result}
        \end{align}
        where we have made a change of variables to \(u = t - c\).
        
        It can be shown that the error in increasing the integration range is proportional to the complementary error function, which we showed in a tutorial has asymptotic expansion
        \begin{equation}
            \erfc(x) \sim \frac{1}{\sqrt{\pi}} \frac{\e^{-x^2}}{x} \sum_{n=0}^{\infty} \frac{(-1)^n(2n - 1)!!}{x^{2n}2^n}.
        \end{equation}
        Hence, the error is exponentially suppressed.
    \end{enumerate}
    
    \begin{dfn}{Error Function}{}
        The \defineindex{error function} is defined as
        \begin{equation}
            \erf x = \frac{2}{\sqrt{\pi}} \int_0^x \e^{-t^2} \dd{t}.
        \end{equation}
        The \defineindex{complementary error function} is defined as
        \begin{equation}
            \erfc x = 1 - \erf x = \frac{2}{\sqrt{\pi}} \int_x^{\infty} \e^{-t^2}\dd{t}.
        \end{equation}
        The \defineindex{imaginary error function} is defined as
        \begin{equation}
            \erfi x = -i\erf(ix).
        \end{equation}
        The name error function comes from the relation to Gaussians and the distribution of statistical errors.
    \end{dfn}
    
    It may be necessary to manipulate an integral, say through a change of variables, to get it into a form where we can apply Laplace's method.
    If we wan more terms in the series we have to expand \(f\) and \(\varphi\) to higher orders.
    For example taking \(f\) to second order and \(\varphi\) to fourth we have
    \begin{multline}
        I(x) \approx \int_{-\infty}^{\infty} \left[ f(c) + uf'(c) + \frac{u^2}{2}f''(c)\right] \\
        \exp\left[ x\varphi(c) - \frac{x}{2}\abs{\varphi''(c)}u^2 + \frac{x}{3!}\varphi^{(3)}(c)u^3 + \frac{x}{4!}\varphi^{(4)}(c)u^{4} \right] \dd{u}
    \end{multline}
    where \(u = t - c\) again.
    We can then expand the exponential as
    \begin{multline}
        \exp\left[ x\varphi(c) - \frac{x}{2}\abs{\varphi''(c)}u^2 + \frac{x}{3!}\varphi^{(3)}(c)u^3 + \frac{x}{4!}\varphi^{(4)}(c)u^{4} \right] \\
        \approx 1 + \left( x\varphi(c) - \frac{x}{2}\abs{\varphi''(c)}u^2 + \frac{x}{3!}\varphi^{(3)}(c)u^3 + \frac{x}{4!}\varphi^{(4)}(c)u^{4} \right) +\\
        \frac{1}{2}\left( x\varphi(c) - \frac{x}{2}\abs{\varphi''(c)}u^2 + \frac{x}{3!}\varphi^{(3)}(c)u^3 + \frac{x}{4!}\varphi^{(4)}(c)u^{4} \right)^2.
    \end{multline}
    We end up with a collection of Gaussian integrals to compute.
    We can do so using the general formula
    \begin{equation}
        \int_{-\infty}^{\infty} u^n\e^{-au^2/2} = 
        \begin{cases}
            0, & \text{if \(n\) is odd},\\
            \dfrac{\sqrt{2\pi}}{a^{(n+1)/2}} n!!, &\text{if \(n\) is even}.
        \end{cases}
    \end{equation}
    
    We then simply collect terms of the same order in \(x\).
    For example the first correction to \cref{eqn:laplace method result} is
    \begin{equation}
        \exp[x\varphi] \sqrt{\frac{2\pi}{\abs{\varphi''}}}\frac{1}{x^{3/2}} \left[ -\frac{f^{(2)}}{2\varphi^{(2)}} + \frac{f\varphi^{(4)}}{8(\varphi^{(2)})^2} + \frac{f^{(1)\varphi^{(3)}}}{2(\varphi^{(2)})^2} - \frac{5f(\varphi^{(3)})^2}{24(\varphi^{(2)})^3} \right]
    \end{equation}
    where \(f\), \(\varphi\), and all of their derivatives are evaluated at \(c\).
    
    Clearly this procedure quickly becomes just a lot of unwieldy algebraic manipulation, but that's what computers are for.
    The important thing is that we develop an asymptotic expansion
    \begin{equation}
        I(x) \sim \exp[x\varphi(c)] \sqrt{\frac{2\pi}{x\abs{\varphi''(c)}}} \left[ f(c) + \frac{A}{x} + \frac{B}{x^2} + \dotsb \right]
    \end{equation}
    where \(A\), \(B\), and all other coefficients can, in principle, be calculated with basic algebra and known expansions.
    
    \subsection{Stirling's Formula Revisited}
    This more general formula allows us to quickly re-derive Stirling's formula.
    We start with
    \begin{equation}
        n! = \Gamma(n + 1) = \int_{0}^{\infty} \e^{-t}t^{n} \dd{t}.
    \end{equation}
    We make a change of variables to \(t = sn\) giving \(\dl{t} = n\dd{s}\) and hence
    \begin{align}
        \Gamma(n + 1) &= \int_{0}^{\infty} \e^{-sn}n^{n}s^{n} n\dd{s}\\
        &= n^{n+1}\int_0^\infty \e^{-sn} s^n\dd{s}\\
        &= n^{n+1} \int_0^{\infty} \exp[n(-s + \ln s)] \dd{s}.
    \end{align}
    Identifying this integrand as being of the required form \(f(s)\exp[n\varphi(s)]\) with \(\varphi(s) = -s + \ln s\) and \(f(s) = 1\) and noticing that \(c = 1\) is the maximum of \(\varphi\) we have
    \begin{equation}
        \Gamma(1 + n) \approx \exp[n\varphi(1)]\sqrt{\frac{2\pi}{n\abs{\varphi''(1)}}} = \e^{-n}n^{n}\sqrt{2\pi n}
    \end{equation}
    since \(\varphi(1) = -1 + \ln 1 = -1\), \(\varphi'(s) = -1 + 1/s\), and \(\varphi''(1) = -1/1^2 = -1\).
    Notice that we use \(n^{n+1}\sqrt{1/n} = n^{n}\sqrt{n^2/n} = n^n\sqrt{n}\).
    
    We can also analyse the error, and it turns out to be proportional to \(\erfc(\sqrt{n/2})\).
    For large \(n\) this is asymptotic to \(\e^{-x/2}\sqrt{2/(\pi n)}\).
    
    \section{Watson's Lemma}
    In this section we prove a lemma useful for Laplace-type integrals with \(\varphi(t) = -t\).
    
    \begin{lma}{Watson's Lemma}{}
        Consider the integral
        \begin{equation}
            I(x) = \int_0^b f(t)\e^{-xt} \dd{t}
        \end{equation}
        for \(b > 0\).
        Suppose \(f\) is continuous on \([0, b]\) and as \(t \to 0_+\) \(f\) has the asymptotic expansion
        \begin{equation}
            f(t) \sim t^{\alpha} \sum_{n=0}^{\infty} a_nt^{\beta n}
        \end{equation}
        where \(\alpha > -1\) and \(\beta < 0\), such that the integral is bounded near \(t \to 0_+\).
        Further as \(b \to \infty\) assume that \(f(t) = o(\e^{ct})\) as \(t \to \infty\) for some \(c > 0\) such that the integral is bounded for large \(t\).
        
        For large \(x > 0\) we then have
        \begin{equation}
            I(x) \sim \sum_{n=0}^{\infty} \frac{a_n}{x^{\alpha+\beta n + 1}}\Gamma(\alpha + \beta n + 1).
        \end{equation}
        
        \begin{proof}
            We prove this using Laplace's method with \(\varphi(t) = -t\).
            This has a global maximum at \(t = 0\), which is one of the limits of integration.
            Consider the integral
            \begin{equation}
                I(x; \varepsilon) = \int_0^\varepsilon f(t) \e^{-xt}\dd{t}
            \end{equation}
            for some small \(\varepsilon > 0\).
            
            Let \(N \in \naturals\) be such that for a given value of \(\varepsilon\) the first \(N\) terms of the asymptotic expansion of \(f\) are a good approximation, that is
            \begin{equation}
                \abs*{f(g) - t^\alpha \sum_{n=0}^{N} a_n t^{\beta n}} \le K t^{\alpha + \beta(N + 1)}
            \end{equation}
            where \(0 \le t \le b\) and \(K > 0\) is some constant.
            
            Substitute the first \(N\) terms of the asymptotic expansion for \(f\) into \(I(x; \varepsilon)\).
            This approximation has error
            \begin{align}
                \delta I &= \abs*{I(x; \varepsilon) - \sum_{n=0}^{N} a_n \int_0^{\varepsilon} t^{\alpha+\beta n}\e^{-xt} \dd{t}}\\
                &= \abs*{\int_0^{\varepsilon} \left[ f(t) - t^\alpha\sum_{n=0}^{N} a_nt^{\beta n} \right]\e^{-xt} \dd{t}}\\
                &\le \int_0^\varepsilon \abs*{f(t) - t^{\alpha} \sum_{n=0}^{N} a_nt^{\beta n}}\e^{-xt}\dd{t}\\
                &\le K \int_0^{\varepsilon} t^{\alpha + \beta(N + 1)}\e^{-xt} \dd{t}.
            \end{align}
        
            Now consider the integral
            \begin{equation}
                \int_0^{\infty} t^m\e^{-xt}.
            \end{equation}
            Making the substitution \(u = xt\) this becomes
            \begin{equation}
                \int_0^{\infty} \frac{u^m}{x^m} \e^{-u} \frac{\dd{u}}{x} = \frac{1}{x^{m+1}} \int_0^{\infty} u^m\e^{-u} \dd{u} = \frac{\Gamma(m + 1)}{x^{m+1}}.
            \end{equation}
        
            We therefore have
            \begin{equation}
                \delta I \le K \frac{\Gamma(\alpha + \beta + \beta N + 1)}{x^{\alpha + \beta + \beta N + 1}}.
            \end{equation}
            We now extend the range of integration to \([0, \infty)\) and we get
            \begin{align}
                I(x) &= \sum_{n=0}^{N} a_n \int_0^\infty t^{\alpha + \beta n} \e^{-xt} + o\left( \frac{1}{x^{\alpha + \beta N + 1}} \right)\\
                &= \sum_{n=0}^{N} a_n \frac{\Gamma(\alpha + \beta N + 1)}{x^{\alpha + \beta N + 1}} + o\left( \frac{1}{x^{\alpha + \beta N + 1}} \right).
            \end{align}
            This is true for all \(N\) as \(x \to \infty\).
        \end{proof}
    \end{lma}
    
    \section{Method of Stationary Phase}
    Consider an integral of the form
    \begin{equation}
        I(x) = \int_a^b f(t) \e^{ix\psi(t)} \dd{t},
    \end{equation}
    for large positive \(x\).
    This is similar to the form require for Laplace's method except that the exponent is purely imaginary.
    Here \(a\), \(b\), \(x\), \(f(t)\), and \(\psi(t)\) are all real for all \(t\).
    These integrals are often called \define{Fourier integrals}\index{Fourier integral} due to their similarity with the Fourier transform.
    
    The factor of \(i\) in the exponent means that when \(x\) is large the integrand will oscillate rapidly.
    We do not have the same exponential decay that we used to great effect with Laplace's method.
    Fortunately the rapid oscillations tend to cancel out in the integral.
    We make this idea rigorous with a lemma.
    
    \begin{lma}{Riemann--Lebesgue Lemma}{}
        If \(\abs{f(t)}\) is integrable and \(\psi(t)\) is continuously differentiable on \([a, b]\) and \(\psi(t)\) is \emph{not} constant over any subinterval of \([a, b]\) then as \(x \to \infty\)
        \begin{equation}
            I(x) = \int_a^b f(t)\e^{ix\psi(t)} \dd{t} \to 0.
        \end{equation}
    \end{lma}
    
    Notice that we explicitly state \(\psi(t)\) is not constant.
    So what happens if it is constant over some \((c, d) \subseteq [a, b]\)?
    In this case there will be no oscillations and therefore the contribution to the integral in \((c, d)\) will not cancel out.
    We can use this to our benefit.
    
    Consider a stationary point of \(\psi\), where \(\psi'(t) = 0\).
    The phase of the exponential is stationary here.
    Therefore the dominant contribution to the integral will come from this region near to where \(\psi'(t) = 0\).
    
    The \defineindex{method of stationary phase} is then as follows.
    For an integral of the form
    \begin{equation}
        I(x) = \int_a^b f(t)\e^{ix\psi(t)} \dd{t}.
    \end{equation}
    \begin{enumerate}
        \item Find \(c\) such that \(\psi'(c) = 0\).
        \item Expand \(f\) and \(\psi\) about \(t = c\):
        \begin{equation}
            f(t) \approx f(c), \qqand \psi(t) \approx \psi(c) + \frac{1}{2!}\psi''(c)(t - c)^2.
        \end{equation}
        \item Consider the integral over a small region about this point, that is take some \(\varepsilon > 0\) and we have
        \begin{equation}
            I(x) \sim \int_{-\infty}^{\infty} f(c)\exp\left[ ix\left[ \psi(c) + \frac{1}{2}\psi''(c)(t - c)^2 \right] \right]
        \end{equation}
        \item Extend the integral to \((-\infty, \infty)\).
        This introduces an error proportional to \(1/x\).
        Setting \(u = t - c\) we then have
        \begin{align}
            I(x) &\sim f(c) \e^{ix\psi(c)} \int_{-\infty}^{\infty} \e^{i\psi''(c)u^2/2}(1 + \dotsb) \dd{u}\\
            &= f(c)\e^{ix\psi(c)} \frac{\sqrt{2\pi}}{\sqrt{i\psi''(c)}}(1 + \dotsb)\\
            &= f(c) \e^{ix\psi(c) \pm i\pi/4} \sqrt{\frac{2\pi}{x\abs{\psi''(c)}}}\\
            &= f(c) \e^{ix\psi(c) + \sgn(\psi''(c))i\pi/4}\sqrt{\frac{2\pi}{x\abs{\psi''(c)}}}.
        \end{align}
        Note that for \(\pm\) we take \(+\) if \(\psi''(c) > 0\) and \(-\) if \(\psi''(c) < 0\).
        If \(\psi''(c) = 0\) then we need to consider a higher order expansion.
    \end{enumerate}
    
    If there are multiple stationary points then we must consider all of them and add the contributions together.
    We cannot take only the biggest as we did with Laplace's method since the \(1/x\) error is too large.
    
    If a stationary point is at the boundary of \([a, b]\) then only expand the integration limits on the opposite side of the integral.
    If considering a second order expansion for \(\psi\) then the result is an even function in \(u = t - c\) and so by halving the range of integration the value of the integral is halved.
    
    If the stationary point also is such that all derivatives of \(\psi\) up to the \((m - 1)\)th derivative are zero then use \(\psi(t) \approx \psi(c) + \psi^{(m)}(c)(t - c)^2/m!\) and the integral will behave like \(x^{-1/m}\) instead of \(x^{-1/2}\) as \(x \to \infty\).
    
    \begin{exm}{Airy Function}{}
        The \defineindex{Airy function}\index{\(\Ai\)|see{Airy function}} for large, negative arguments.
        This has the integral representation
        \begin{equation}
            \Ai(-x) = \frac{1}{\pi} \int_0^\infty \cos\left( \frac{\omega^3}{3} - x\omega \right) \dd{\omega}
        \end{equation}
        for large, positive \(x\).
        Notice that we can extend the integration range to \((-\infty, \infty)\) since the integrand is an even function:
        \begin{equation}
            \Ai(-x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \cos\left( \frac{\omega^3}{3} - x\omega \right) \dd{\omega}.
        \end{equation}
        Further, notice that
        \begin{equation}
            \exp\left[ i\left( \frac{\omega^3}{3} - x\omega \right) \right] = \cos\left( \frac{\omega^3}{3} - x\omega \right) + i\sin\left( \frac{\omega^3}{x} \right)
        \end{equation}
        is made of two parts, an even function, the same as our integrand, and an odd function, which vanishes when integrated over \((-\infty, \infty)\).
        Hence
        \begin{equation}
            \Ai(-x) = \int_{-\infty}^{\infty} \exp\left[ i\left( \frac{\omega^3}{3} - x\omega \right) \right] \dd{\omega}.
        \end{equation}
        Even if the imaginary part didn't vanish we could still consider the exponential and just discard the imaginary part at the end.
        
        In order to apply the method of stationary phase we want to be able to write the exponent in the form \(ix\psi\), where \(\psi\) is a real function of the integration variable.
        To do so we need to factor the \(x\) out, we can do this with a change of variables.
        After the change of variables we want both \(\omega^3\) and \(x\omega\) to have the same factor of \(x\).
        Suppose that we make a change of variables \(\omega = zx^p\).
        We therefore have \(\omega^3 = z^3x^{3p}\) and \(x\omega = zx^{p+1}\).
        So we need \(3p = p + 1\), that is \(p = 1/2\).
        So we choose \(\omega = z\sqrt{x}\).
        This gives us \(\dd{\omega} = \sqrt{x}\dd{z}\) and leaves our integration limits unchanged.
        We then have
        \begin{equation}
            \Ai(-x) = \frac{\sqrt{x}}{2\pi} \int_{-\infty}^{\infty} \exp\left[ ix^{3/2}\left( \frac{z^3}{3} - z \right) \right] \dd{z}.
        \end{equation}
        
        This is of the required form.
        Notice that what we called \(x\) above is \(x^{3/2}\) here.
        This doesn't matter since \(x^{3/2}\) is large and positive exactly when \(x\) is large and positive.
        We trivially have \(f(z) = 1\).
        Now take \(\psi(z) = z^3/3 - z\).
        We then have \(\psi'(z) = z^2 - 1\), which means we have stationary points at \(z = \pm 1\), and so \(\psi(\pm 1) = \mp 2/3\).
        We have \(\psi''(z) = 2z\) and so \(\psi''(\pm 1) = \pm 2\).
        
        Expanding about \(z = 1\) we have
        \begin{equation}
            \psi(z) \approx -\frac{2}{3} + (z - 1)^2
        \end{equation}
        and about \(z = -1\) we have
        \begin{equation}
            \psi(z) \approx \frac{2}{3} + (z + 1)^2.
        \end{equation}
        
        We then have
        \begin{align}
            \Ai(-x) &\approx \frac{\sqrt{x}}{2\pi} \bigg[ \int_{-\infty}^{\infty} \exp\left[ ix^{3/2}\left( -\frac{2}{3} + (z - 1)^2 \right) \right] \dd{z}\\
            &\qquad\qquad+ \int_{-\infty}^{\infty} \exp\left[ ix^{3/2}\left( \frac{2}{3} - (z + 1)^2 \right) \right] \dd{z} \bigg]\\
            &= \frac{\sqrt{x}}{2\pi} \bigg[ \exp\left[ \frac{2}{3}x^{3/2} - i\frac{\pi}{4} \right] \sqrt{\frac{2\pi}{2x^{3/2}}}\\
            &\qquad\qquad+ \exp\left[ -i\frac{2}{3}x^{3/2} + i\frac{\pi}{4} \right] \sqrt{\frac{2\pi}{2x^{3/2}}} \bigg]\\
            &= \frac{1}{x^{1/4}\sqrt{\pi}} \cos\left( \frac{2}{3}x^{3/2} - \frac{\pi}{4} \right).
        \end{align}
    \end{exm}
    
    \section{Saddle Point Method}
    \epigraph{The complex world is wacky. Abandon everything we have taught you so far, well, not everything.}{Kristel Torokoff}
    The \defineindex{saddle point method} generalises Laplace's method (for real exponents) and the method of stationary phase (for purely imaginary exponents) to contour integrals of the form
    \begin{equation}
        I(N) = \int_C g(z)\e^{Nf(z)}\dd{z}
    \end{equation}
    where \(N\) is large and positive, \(f, g \colon \complex \to \complex\) are analytic in a simply connected region on and around some given contour, \(C\), and \(g(z)\) varies slowly compared to \(\e^{f(z)}\).
    
    The value of \(I(N)\) is independent the contour by Cauchy's theorem.
    We can always write \(f(z) = u(z) + iv(z)\) for \(u, v \colon \complex \to \reals\), it's a basic fact of complex analysis, following immediately from the Cauchy--Riemann equations, that \(u\) and \(v\) are harmonic.
    We also know that \(f\) has no minima or maxima, only saddle points.
    The integral will be most sensitive to \(u(z)\).
    
    As with Laplace's method we expect that the integral will be dominated by a small region about the highest stationary point of \(f\).
    This follows since for a non-negligible contribution \(u\) must be maximised while \(v\) must be stationary, or the oscillations will cancel out.
    
    There is a theorem, known as Jensen's theorem, which states that \(\abs{F(z)}\) takes extremal points only at saddle points or at troughs where \(F(z) = 0\).
    We will consider \(F(z) = \exp(Nf(z))\).
    Considering \(\abs{\exp(Nf(z))}\) is then equivalent to considering \(\Re(f(z))\) since only the real part of the exponent contributes to the magnitude of the exponential.
    Since \(f(z) = u(z) + iv(z)\) and \(u\) is harmonic we have \(u_{xx} + u_{yy} = 0\), which implies that if \(u_{xx} > 0\) we must have \(u_{yy} < 0\), and vice versa.
    This in turn means that we don't have maxima or minima.
    In the rare case that \(u_{xx} = u_{yy} = 0\) the result still holds but showing it is more involved.
    
    To evaluate \(I(N)\) we deform the contour so that it passes through the saddle point.
    This doesn't change the value of the integral by Cauchy's theorem.
    We deform the contour in such a way that it remains in regions where \(\abs{\exp(Nf(z))}\) is small, and hence the contribution to the integrand is small, apart from at the saddle point.
    
    We then expand about the saddle point, \(z = z_0\), giving
    \begin{equation}
        g(z) \approx g(z_0), \qqand f(z) \approx f(z_0) + \frac{1}{2}f''(z_0)(z - z_0)^2.
    \end{equation}
    We then have
    \begin{equation}
        I(N) \approx g(z_0) \e^{Nf(z_0)} \int_{C'} \exp\left[ \frac{1}{2}Nf''(z_0)(z - z_0)^2 \right]\dd{z}
    \end{equation}
    where \(C'\) is our modified contour.
    
    Now set \(z - z_0 = r\e^{i\varphi}\) and write \(f''(z_0) = \abs{f''(z_0)}\e^{i\vartheta}\).
    To pick up the dominant contribution along the path of steepest descent we need to choose \(\varphi\), the angle at which the path passes through the saddle point, in such a way that the imaginary part of the exponential is zero and the real part is negative.
    Making these substitutions we have
    \begin{equation}
        I(N) \approx g(z_0) \e^{Nf(z_0)} \int_{C'} \exp\left[ \frac{1}{2}N\abs{f''(z_0)}\e^{i\vartheta}r^2\e^{2i\varphi} \right] \e^{i\varphi}\dd{r}.
    \end{equation}
    The phase of the exponent is \(\e^{i(\vartheta + 2\varphi)}\).
    We want this to be a multiple of \(\pi\), so that the exponential is real.
    We therefore choose \(\varphi = (m\pi - \vartheta)/2\) for \(m\in\integers\).
    We end up with four angles (restricting arguments to some interval of length \(2\pi\)).
    These map out four paths across the saddle point.
    These paths pair up with a path and its reverse.
    One pair will be along the path of steepest ascent and one along the path of steepest descent.
    It is this latter path that we are interested in since along this path the value of the integrand falls away quickly, which allows us to neglect contributions not close to the saddle point.
    What remains is to choose the value of \(\varphi\) from the pair which gives us the \enquote{correct sense}, that is the deformed contour should keep the same orientation of the original contour, or as close as possible.
    This will become clearer in the examples later.
    
    We can extend the limits of the integral to infinity and we get a Gaussian integral.
    Evaluating this with our chosen value of \(\varphi\) gives
    \begin{equation}
        I(N) \approx g(z_0) \e^{Nf(z_0)} \e^{i\varphi} \sqrt{\frac{2\pi}{N\abs{f''(z_0)}}}.
    \end{equation}
    This is the so called \defineindex{saddle-point approximation} to the integral \(I\).
    
    \subsection{Notes}
    If there are several saddle points along the contour then in the most general case you need to sum the contribution from each.
    Often however, one of the saddle points dominates and it is possible to argue your way out of considering the others.
    
    If there is no saddle point the the dominant contribution will come from somewhere on the boundary where it is possible that \(f'(z) \ne 0\).
    We still consider the path of steepest descent but this case must be handled slightly differently.
    It is still possible to find an asymptotic expansion.
    
    
    \subsection{Relation to Previous Methods}
    Laplace's method corresponds to a saddle point on the real axis with \(u_{xx} < 0\), that is a maximum as you travel along the real axis, and \(u_{yy} > 0\), that is a minimum along the imaginary axis.
    We go through the saddle point on the real axis so \(f''(z_0)\) is real and negative.
    We then have \(\vartheta = \pi\) and \(\varphi = 0\).
    
    The method of stationary phase corresponds to a saddle point with \(\vartheta + 2\varphi = \pi/2\) and so
    \begin{equation}
        \int \exp\left[ \frac{N}{2}f''(z_0)(z - z_0)^2 \right] \dd{z} = \e^{-i\vartheta/2 + i\pi/4} \int \exp\left[ \frac{iN}{2}\abs{f''(z_0)}r^2 \right]\dd{r}.
    \end{equation}
    
    \subsection{Examples}
    \epigraph{They're all related, its like the royal family}{Kristel Torokoff on Hankel and Bessel functions.}
    \begin{exm}{Binomial Coefficient}{}
        It can be shown that the following is an integral representation of the binomial coefficient:
        \begin{equation}
            \binom{N}{M} = \frac{1}{2\pi i} \oint_C \frac{(1 + z)^N}{z^{M + 1}} \dd{z}.
        \end{equation}
        Here \(C\) is a contour encircling the origin.
        We can understand this by expanding \((1 + z)^N\) with the binomial expansion, this gives us a series in \(z^n\).
        For \(n > M\) we end up with no pole and therefore that term doesn't contribute.
        For \(n < M\) the integrand is bounded by \(\abs{1/z^{n - m + 1}}\), and taking the contour to infinity this term vanishes.
        Finally for \(n = M\) we have \(\abs{1/z}\) which has a residue of \(1\) and so by the Residue theorem we get \(2\pi i\) times the coefficient of \(z^{M}\) in the expansion of \((1 + z)^{N}\), which is exactly what we define as \(\binom{N}{M}\).
        
        Now consider the case where \(M\) and \(N\) are large.
        We can always write \(M = Ny\) for some \(y\).
        Consider the integrand.
        It is possible to generate an exponential by using \(a = \e^{\ln a}\):
        \begin{align}
            \frac{(1 + z)^{N}}{z^{Ny + 1}} &= \frac{1}{z} \frac{(1 + z)^N}{z^{Ny}}\\
            &= \frac{1}{z}\exp\left[ \ln\left( \frac{(1 + z)^{N}}{z^{Ny}} \right) \right]\\
            &= \frac{1}{z}\exp[N\ln(1 + z) - Ny\ln z].
        \end{align}
        Hence
        \begin{equation}
            \binom{N}{M} = \frac{1}{2\pi i} \oint_C \frac{1}{z} \exp[N\ln(1 + z) - Ny\ln z] \dd{z}.
        \end{equation}
        Let \(f(z) = \ln(1 + z) - y\ln z\).
        Differentiating this we have
        \begin{equation}
            f'(z) = \frac{1}{1 + z} - \frac{y}{z} = 0 \implies z_0 = \frac{y}{1 - y}.
        \end{equation}
        The second derivative is then
        \begin{equation}
            f''(z) = -\frac{1}{(1 + z)^2} + \frac{y}{z^2} \implies f''(z_0) = \frac{(1 - y)^3}{y}.
        \end{equation}
        
        So the saddle point is on the real axis and at a minimum along the real axis but a maximum along the imaginary direction.
        The only singularity in the integrand is at \(z = 0\) and so we can deform the contour freely as long as we avoid the origin.
        We do so to pass through the saddle point in the imaginary direction.
        This is most easily done by just taking a large circle centred at the origin with radius \(z_0\), but can be done any way we like.
        We are choosing here to have \(\varphi = \pi/2\).
        
        Using the saddle-point approximation we then have
        \begin{align}
            \binom{N}{M} &\approx \frac{1}{2\pi i} \frac{\e^{i\varphi}}{z_0} \e^{Nf(z_0)} \sqrt{\frac{2\pi}{Nf''(z_0)}}\\
            &= \sqrt{\frac{1}{2\pi Ny(1 - y)}} \exp[-N(y\ln y + (1 - y)\ln(1 - y)].
        \end{align}
        The exponent here should be familiar from dealing with entropy and similar concepts in statistical mechanics, this arises due to the combinatorial uses of the binomial coefficients.
    \end{exm}
    
    \begin{dfn}{Bessel and Hankel Functions}{}
        A common differential equation is Bessel's differential equation,
        \begin{equation}
            x^2\diff[2]{y}{x} + x\diff{y}{x} + (x^2 - \nu^2)y = 0.
        \end{equation}
        This has two linearly independent solutions which are defined to be the \define{Bessel functions}\index{Bessel function}, in particular the Bessel function of the first kind, \(J_\nu\)\index{\(J_\nu\)|see{Bessel function (1st kind)}}, and second kind, \(Y_\nu\)\index{\(Y_\nu\)|see{Bessel function (2nd kind)}}.
        
        An alternative pair of linearly independent functions that solve Bessel's differential equation are the \define{Hankel functions}\index{Hankel function} of the first and second kind:
        \begin{equation}
            H_\nu^{(1)} \coloneqq J_\nu + iY_\nu, \qqand H_{\nu}^{(2)} \coloneqq J_\nu - iY_\nu.
        \end{equation}
        \index{\(H_\nu^{(1)}\)|see{Hankel function}}\index{\(H_\nu^{(2)}\)|see{Hankel function}}
        
        There are also the modified Bessel functions, which are rescaled Bessel functions, and spherical Bessel functions, which are solutions to Helmholtz's equation, \(\laplacian f = -k^2f\), in spherical coordinates, the radial component of which is of the form of Bessel's differential equation.
        Ultimately the spherical Bessel functions are also rescaled Bessel functions.
    \end{dfn}
    
    \begin{exm}{Hankel Function of the First Kind}{}
        It can be shown that the Hankel functions of the first kind have the integral representation
        \begin{equation}\label{eqn:hankel function first time integral}
            H_\nu^{(1)}(x) = \frac{1}{i\pi} \int_{0 + i\varepsilon}^{-\infty + i\varepsilon} \exp\left[ \frac{x}{2}\left( z - \frac{1}{z} \right) \right] \frac{1}{z^{\nu + 1}}\dd{z}
        \end{equation}
        with \(\varepsilon > 0\) and \(x\) large and positive.
        There is a branch cut along the negative real axis.
        The Hankel functions of the second kind have the same integral but with a contour that is just below the branch cut instead of above.
        
        Let \(f(z) = (z - 1/z)/2\).
        Then
        \begin{equation}
            f'(z) = 1 + \frac{1}{z^2} = 0 \implies z = \pm i.
        \end{equation}
        We then have \(f(\pm i) = \pm i\), as well as
        \begin{equation}
            f''(z) = -\frac{1}{z^3} \implies f''(\pm i) = \mp i.
        \end{equation}
        
        Expanding \(f\) near \(z = i\) we get
        \begin{align}
            f(z) &\approx f(i) + \frac{1}{2}f''(i)(z - i)^2\\
            &= i + \frac{r^2}{2}\exp\left[ -i\frac{\pi}{2} + i2\varphi \right].
        \end{align}
        We want the exponential to be purely real and negative.
        We therefore want \(\cos(2\varphi - \pi/2)  < 0\) and \(\sin(2\varphi - \pi/2) = 0\).
        This happens when \(2\varphi - \pi/2 = m\pi\) with \(m\in\integers\).
        In particular the unique values this gives are \(\varphi = \pi/4, 3\pi/4, 5\pi/4, 7\pi/4\).
        Of these \(3\pi/4\) and \(7\pi/4\) give the negative cosine.
        Of these it is \(3\pi/4\) that gives the \enquote{correct sense}.
        See \cref{fig:Hankel saddle point method}.
        
        We can do the same expansion about \(z = -i\) and we get the same set of four angles but this time it is \(\pi/4\) and \(-3\pi/4\) that give the steepest descent.
        
        It turns out that the contribution from the \(z = -i\) saddle point is much smaller than from the \(z = i\) saddle point.
        This is mostly because the initial contour is above the negative real axis, along which there is a branch cut.
        This means we cannot deform a contour to cross the \(z = -i\) saddle point without going back on ourselves to get out from under the real axis.
        But going back on ourselves simply cancels the contribution.
        As a result the net contribution from the \(z = -i\) saddle point is negligible.
        
        It then follows that by taking only the contribution from the \(z = i\) saddle point we have
        \begin{equation}
            H^{(1)}_\nu (x) \approx \exp\left[ i\left( x - \frac{\pi}{4} - \nu\frac{\pi}{2} \right) \right] \sqrt{\frac{2}{x\pi}}.
        \end{equation}
    \end{exm}
    
    \begin{figure}
        \tikzsetnextfilename{hankel-saddle-point-contour}
        \begin{tikzpicture}
            \draw[very thick, ->] (-4, 0)  -- (4, 0) node[right] {\(\Re(z)\)};
            \draw[very thick, ->] (0, -4)  -- (0, 4) node[above] {\(\Im(z)\)};
            \draw[ultra thick, red] (0, 0) -- (-4, 0);
            \fill[highlightalt] (0, -2) circle [radius = 0.075cm] node[left, black] {\(-i\)};
            \draw[very thick, highlight] (0, 0.2) -- (-4, 0.2);
            \draw[very thick, highlight, ->] (0, 0.2) --++ (-2.1, 0) node[above right, black] {\(C\)};
            \draw[darker] (0, 2) -- ++ (0.5, 0);
            \draw[darker] (0.3, 2) arc(0:130:0.3);
            \node at (0.4, 2.3) {\(\frac{3\pi}{4}\)};
            \draw[dark, use Hobby shortcut, very thick] (0, 0.2) .. (0.5, 0.2) .. (1, 0.5) .. (0.5, 1.5) .. (0, 2) .. (-0.5, 2.5) .. (-2, 1) .. (-2.3, 0.5) .. (-2.8, 0.2);
            \node at (1.3, 0.6) {\(C'\)};
            \draw[dark, very thick, ->] (0.6, 1.4) -- (0.5, 1.5);
            \draw[dark, very thick, ->] (-1.95, 1.1) -- (-2, 1);
            \fill[highlightalt] (0, 2) circle [radius = 0.075cm] node[below left, black] {\(i\)};
        \end{tikzpicture}
        \caption{The contour, \(C\), used in the integral representation of the Hankel function of the first kind, \(H_\nu^{(1)}\), and the modified contour, \(C'\), used to calculate it in the saddle point method. Notice that the contours are in the same \enquote{sense}.}
        \label{fig:Hankel saddle point method}
    \end{figure}
    
    \part{Ordinary Differential Equations}
        
    \chapter{Dirac Delta}
    \epigraph{What on Earth are we talking about?}{Kristel Torokoff}
    \section{Motivation}
    \epigraph{Words are obsolete.}{Kristel Torokoff}
    Consider an impulse, that is an instantaneous increase in momentum from 0 to \(mv\), applied at time \(t_0\).
    We can model this as the application of a force \(F(t)\) giving
    \begin{equation}
        mv = \int_{t_0-\tau}^{t_0 + \tau} F(t) \dd{t}.
    \end{equation}
    The force, \(F\), must be strongly peaked around \(t = t_0\) since at this point an instantaneous jump in the momentum means an infinite rate of change, and hence force.
    Obviously this is non-physical but still can be a useful concept.
    The exact details of \(F\) are not important, we can think of it as having any shape but as we make the duration of the impulse, \(2\tau\), smaller \(mv\) must be unchanged and so the peak must get higher so the area underneath is the same.
    In the limit \(\tau \to 0\) we write \(F(t) = mv\delta(t - t_0)\).
    
    Another example of a similar function is the mass density of a point mass, \(M\), which is concentrated at a point \(\vv{r_0}\) such that any integral over space containing that point gives \(M\) and any integral over space not containing the point gives zero.
    This again requires a strongly peaked function and in the limit of the volume of the integration region going to zero we write \(\rho(\vv{r}) = M\delta(\vv{r} - \vv{r_0})\).
    
    Both of these examples have functions which are peaked such that the peak gets smaller as the integration region gets larger in such a way that the integral stays constant, as long as it contains the distinguished point.
    We can therefore think of them as \enquote{functions} with a spike at that point and zero everywhere else.
    We make this idea rigorous in the following definition
    
    \begin{dfn}{Dirac Delta}{}
        The \defineindex{Dirac delta}, \(\delta\)\index{\(\delta\)|see{Dirac delta}}, is defined such that
        \begin{equation}
            \int_{-\infty}^{\infty} \delta(x) f(x) = \int_{-\varepsilon}^{\varepsilon} \delta(x)f(x) \dd{x} = f(0)
        \end{equation}
        for arbitrarily small \(\varepsilon > 0\) and sufficiently nice\footnote{Sufficiently nice in this case meaning differentiable, continuous and vanishing at infinity} test functions \(f\).
        This defines \(\delta\) only up to a constant.
        We further require that
        \begin{equation}
            \int_{-\infty}^{\infty} \delta(x) \dd{x} = 1.
        \end{equation}
    \end{dfn}
    
    The Dirac delta defined as above doesn't quite fit the requirements to be a function.
    Instead it is a \defineindex{distribution}, or \define{generalised function}\index{generalised function|see{distribution}}.
    A distribution is simply a mathematical object that we when integrated with a well behaved test function gives a second well behaved function.
    In the case of the Dirac delta this second function is the first function evaluated at 0.
    We won't worry about the details of what this means.
    For us the important thing is that the Dirac delta is fairly meaningless outside of an integral, although we can still formally manipulate it according to the rules we will derive shortly.
    
    \section{Dirac Delta as a Limit of a Function Sequence}
    One of the most useful ways to think about the Dirac delta is as the limit of a series of functions which have the key property of getting narrower and taller in such a way that the area remains constant, and is 1 as per our second normalisation requirement for \(\delta\), and also have the property that when integrated with a test function we get the test function evaluated at zero.
    This makes mathematicians a bit nervous as the limit of such sequences doesn't exist, since the Dirac delta is not a function.
    Fortunately we're doing physics and as such don't need to worry about trivial things like limits existing.
    
    \subsection{Top Hat}
    Define the \defineindex{top hat}\index{\(\Pi_n\)|see{top hat}} function
    \begin{equation}
        \Pi_n(x) \coloneqq
        \begin{cases}
            0, & \abs{x} > 1/n,\\
            n/2, & \abs{x} \le x.
        \end{cases}
    \end{equation}
    This gives a series of rectangular areas of width \(2/n\) and height \(n/2\), so area \(1\).
    In the limit \(n \to \infty\) they get narrower and taller, just as required for the Dirac delta.
    
    \begin{figure}
        \tikzsetnextfilename{dirac-delta-top-hat-limit}
        \begin{tikzpicture}
            \draw[very thick, ->] (-4, 0)  -- (4, 0) node[right] {\(x\)};
            \draw[very thick, ->] (0, 0)  -- (0, 8) node[above] {\(y\)};
            \foreach \i/\col in {0.3/20, 0.5/25, 1/30, 2/40, 3/50, 4/60, 5/70, 6/90, 7/100} {
                \draw[very thick, highlight!\col] (-4, 0) -- (-1/\i, 0) -- (-1/\i, \i) -- (1/\i, \i) -- (1/\i, 0) -- (4, 0);
                \node[right] at (1/\i, \i) {\i};
            }
        \end{tikzpicture}
        \caption{Top hat functions in a limit become Dirac deltas.}
    \end{figure}
    
    Consider now a test function, \(f\), which is continuous at \(0\).
    Then for all \(\varepsilon > 0\) there exists \(\eta\) such that for all \(\abs{x} < \eta\) \(\abs{f(0) - f(x)} < \varepsilon\), this is just the usual \(\varepsilon\)-\(\delta\) definition of continuity but avoiding reusing the symbol \(\delta\).
    It then follows that for all \(n > 1/\eta\)
    \begin{align}
        \abs*{f(0) - \int_{-\infty}^{\infty} \Pi_n(x)f(x) \dd{x}} &= \abs*{ \frac{n}{2} \int_{-1/n}^{1/n} (f(0) - f(x))\dd{x} }\\
        &\le \frac{n}{2} \int_{-1/n}^{1/n} \abs{f(0) - f(x)} \dd{x}\\
        &\le \varepsilon.
    \end{align}
    Therefore we have
    \begin{equation}
        \lim_{n\to\infty} \int_{-\infty}^{\infty} \Pi_n(x)f(x) \dd{x} = f(0)
    \end{equation}
    for any continuous function \(f\).
    This is the defining property of the Dirac delta.
    
    \subsection{Gaussian}
    Consider the Gaussian function
    \begin{equation}
        \mathcal{N}_n(x) = \frac{n}{\sqrt{\pi}} \e^{-n^2x^2}.
    \end{equation}
    This is peaked at \(x = 0\) with the area under the curve being 0.
    Further as \(n\) increases the peak becomes taller and the curve narrower, the obvious measure of curve width here being the standard distribution, \(1/n^2\), taking this to be a probability density function.
    
    \begin{figure}
        \tikzsetnextfilename{dirac-delta-gaussian-limit}
        \begin{tikzpicture}
            \draw[very thick, ->] (-4, 0)  -- (4, 0) node[right] {\(x\)};
            \draw[very thick, ->] (0, 0)  -- (0, 8) node[above] {\(y\)};
            \foreach \i/\col in {1/20, 2/25, 3/30, 4/40, 5/50, 6/60, 7/70, 8/80, 9/90, 10/100} {
                \draw[very thick, highlight!\col, domain=-4:4, samples=1000] plot (\x, {1.35 * \i * exp(-(\x)^2 * (\i)^2) * 0.564 });
            }
        \end{tikzpicture}
        \caption{Gaussian functions in a limit become Dirac deltas. Shown here are the Gaussians with standard deviations \(1/n^2\) with \(n = 1, \dotsc, 10\).}
    \end{figure}
    
    It can also be shown that in the limit of \(n \to \infty\) this has the defining property of the Dirac delta.
    
    \subsection{Lorentzian}
    Consider the Lorentzian defined by
    \begin{equation}
        L_n(x) = \frac{n}{\pi} \frac{1}{1 + n^2x^2}.
    \end{equation}
    This has unit area under the curve and is peaked at zero and becomes more peaked as \(n\) increases.
    The natural measure of width here being the full width at half maximum, \(2/n^2\).
    We can also show that it has the defining property of the Dirac delta.
    
    \begin{figure}
        \tikzsetnextfilename{dirac-delta-lorentzian-limit}
        \begin{tikzpicture}
            \draw[very thick, ->] (-4, 0)  -- (4, 0) node[right] {\(x\)};
            \draw[very thick, ->] (0, 0)  -- (0, 8) node[above] {\(y\)};
            \foreach \i/\col in {1/20, 2/25, 3/30, 4/40, 5/50, 6/60, 7/70, 8/80, 9/90, 10/100} {
                \draw[very thick, highlight!\col, domain=-4:4, samples=1000] plot (\x, {0.7 * \i * 1/(1 + (\i)^2*(\x)^2)});
            }
        \end{tikzpicture}
    \caption{Lorentzian functions in a limit become Dirac deltas. Shown here are the the Lorentzians with full width at half maximum \(2/n^2\) with \(n = 1, \dotsc, 10\).}
    \end{figure}
    
    \subsection{\texorpdfstring{\(\sinc\)}{sinc} Function}
    Consider the scaled \(\sinc\) function
    \begin{equation}
        f_n(x) = \frac{n}{\pi}\sinc^2(nx) = \frac{1}{n\pi} \frac{\sin^2(n x)}{x^2}.
    \end{equation}
    This is peaked at the origin, and the peak gets narrower as \(n\) increases.
    The natural width measurement here being the distance between the first minima, which occur at \(\pm \pi/n\), and so the width is \(2\pi/n\), which goes to zero as \(n\) increases.
    
    \begin{figure}
        \tikzsetnextfilename{dirac-delta-sinc-limit}
        \begin{tikzpicture}
            % https://tex.stackexchange.com/a/235009/180184
            \pgfmathdeclarefunction{sinc}{1}{%
                \pgfmathparse{abs(#1)<0.01 ? int(1) : int(0)}%
                \ifnum\pgfmathresult>0 \pgfmathparse{1}\else\pgfmathparse{sin(#1 r)/#1}\fi%
            }
            \draw[very thick, ->] (-4, 0)  -- (4, 0) node[right] {\(x\)};
            \draw[very thick, ->] (0, 0)  -- (0, 8) node[above] {\(y\)};
            \foreach \i/\col in {1/20, 2/25, 3/30, 4/40, 5/50, 6/60, 7/70, 8/80, 9/90, 10/100} {
                \draw[very thick, highlight!\col, domain=-4:4, samples=1000] plot (\x, {0.75 * \i * sinc(\i * \x)^2});
            }
        \end{tikzpicture}
        \caption{Scaled \(\sinc\) functions in a limit become Dirac deltas. Shown here are the the scaled \(\sinc\) functions with \(2\pi/n\) between first minima with \(n = 1, \dotsc, 10\).}
    \end{figure}
    
    \section{Integral Representation}
    Before we can get onto the integral representation of the Dirac delta we need a related function.
    
    \begin{dfn}{Heaviside Step Function}{}
        The \defineindex{Heaviside step function}\index{\(\theta\)|see{Heaviside step function}} is defined as
        \begin{equation}
            \theta(t) \coloneqq 
            \begin{cases}
                1, & t > 0,\\
                0, & t < 0.
            \end{cases}
        \end{equation}
        The notation \(H(t)\) is also used for this function.
        The value at zero is not important for our purposes, and there is no agreement on what it should be, common choices being 0, \(1/2\), and 1.
        We will only consider \(\theta\) as a distribution, inside an integral, where the value at a single point makes no difference.
    \end{dfn}
    
    For \(t \ne 0\) we have
    \begin{equation}
        \int_{-\infty}^t \delta(u) \dd{u} = \theta(t),
    \end{equation}
    since if \(t < 0\) the origin is not in the integration range and so the integral is zero, and if \(t > 0\) then the origin is in the integration range and the integral is 1.
    
    Now consider the following integral
    \begin{equation}
        I = \int_{-\infty}^{\infty} f(t) \theta'(t) \dd{t}
    \end{equation}
    where \(f\) is a well behaved test function, in particular it vanishes at infinity.
    Integrating by parts we have
    \begin{align}
        I &= [f(t)\theta(t)]_{-\infty}^{\infty} - \int_{-\infty}^{\infty} f'(t)\theta(t) \dd{t}\\
        &= f(\infty)\underbrace{\theta(\infty)}_{=1} - f(-\infty)\underbrace{\theta(-\infty)}_{=0} - \int_{0}^{\infty} f'(t) \dd{t}
    \end{align}
    at this point we use the requirement that \(f\) vanishes at infinity so this is
    \begin{equation}
        I = -\int_0^\infty f'(t) \dd{t} = -[f(t)]_{0}^{\infty} = -f(\infty) + f(0) = f(0),
    \end{equation}
    where again we have used the fact that \(f\) vanishes at infinity.
    Note that we can't just say \enquote{\(f(\infty) - f(\infty) = 0\)} here since infinity is, to use a technical term, weird, and formally these infinities arise in separate limiting procedures and we can't guarantee that they are actually the same point.
    From this we conclude that \(\theta'(t) = \delta(t)\).
    
    Consider the integral representation of the step function,
    \begin{equation}
        \theta(x) = \frac{1}{2\pi i} \int_{-\infty - i\gamma}^{\infty - i\gamma} \frac{\e^{ikx}}{k} \dd{k}
    \end{equation}
    where \(\gamma > 0\) is arbitrary.
    We will show this is in a tutorial but the general argument is that for \(x > 0\) the integral converges in the upper half plane.
    Closing the contour to form a semicircle we circle the origin, since the contour is just below the real axis.
    The integral over the curved part of the semicircle vanishes by Jordan's lemma (\cref{lma:jordan's}).
    Evaluating the residue at the origin gives us the value of the integral.
    For \(x < 0\) the integral converges in the lower half plane.
    Hence we can close the contour with a semicircle but now the contour contains no poles and so the integral is zero.
    
    From this we can easily derive an integral representation of the Dirac delta by noticing that
    \begin{equation}
        \diffp*{\e^{ikx}}{k} = \frac{\e^{ikx}}{k}.
    \end{equation}
    We then have
    \begin{equation}
        \delta(x) = \diff{\theta(x)}{x} = \frac{1}{2\pi} \int_{-\infty-i\gamma}^{\infty-i\gamma} \e^{ikx} \dd{k}.
    \end{equation}
    There is now no pole at the origin and so we are also free to take \(\gamma = 0\) giving us the integral representation of the Dirac delta:
    \begin{equation}
        \delta(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \e^{ikx} \dd{k}.
    \end{equation}
    This integral is divergent, like the Dirac delta it represents, and is only meaningful under another integral.
    
    Compare this result to the integral representation of the Kronecker delta that we derived in a tutorial:
    \begin{equation}
        \delta_{nm} = \frac{1}{2\pi i} \oint_{C} \frac{z^m}{z^{n+1}}
    \end{equation}
    where \(n, m \in \integers\) and \(C\) is an anticlockwise closed contour containing the origin.
    As a reminder if \(m > n\) then there are no poles and so this is zero.
    If \(m < n\) then the integral is bound by \(1/R^{m+n}\), where \(R\) is the radius of the contour, which we are free to take as a circle.
    Hence the integral vanishes as the contour goes to infinity.
    Only when \(m = n\) do we get the integral of \(1/z\) which gives \(2\pi i\) by the residue theorem.
    
    Now make a change of variables to \(z = \e^{ik}\) with \(k \in (-\pi, \pi)\) and we have
    \begin{equation}\label{eqn:integral representation kronecker delta}
        \delta_{nm} = \frac{1}{2\pi} \int_{-\pi}^{\pi} \e^{ik(m - n)} \dd{k}.
    \end{equation}
    Note the similarity to the integral representation of \(\delta\), in particular to the integral representation of \(\delta(x - x_0)\), which we will see in the next section.
    
    \section{Other Properties}
    \subsection{Shift}
    Consider what happens when we shift the Dirac delta by some amount \(x_0\).
    We then consider
    \begin{equation}
        \int_{-\infty}^{\infty} f(x)\delta(x - x_9) \dd{x}.
    \end{equation}
    A simple change of variables to \(u = x - x_0\) gives
    \begin{equation}
        \int_{-\infty}^{\infty} f(u + x_0)\delta(u) \dd{u} = f(0 + x_0) = f(x_0).
    \end{equation}
    
    From this we see that by shifting the Dirac delta we can pick out the value of a function at any point.
    This is often referred to as the \defineindex{sifting property} of the Dirac delta.
    Our previous ideas generalise to this shifted Dirac delta.
    In particular
    \begin{equation}\label{eqn:integral representation of dirac delta}
        \delta(x - x_0) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \e^{ik(x - x_0)} \dd{k}.
    \end{equation}
    
    \subsection{Derivative of the Dirac Delta}
    It is reasonable to think that the derivative of the Dirac delta is another distribution.
    As such we should consider it in an integral with a well behaved test function:
    \begin{equation}
        \int_{-\infty}^{\infty} f(x)\delta'(x) \dd{x}.
    \end{equation}
    Integrating by parts we get
    \begin{equation}
        \underbrace{[f(x)\delta(x)]_{-\infty}^{\infty}}_{=0\text{ as } f(\pm \infty) = 0} - \int_{-\infty}^{\infty} f'(x)\delta(x) \dd{x} = -f'(0).
    \end{equation}
    Repeated application of this process gives us
    \begin{equation}
        \int_{-\infty}^{\infty} f(x) \diff[n]{\delta(x)}{x} \dd{x} = (-1)^n \diff[n]{f}{x}[x=0].
    \end{equation}
    Note that in order for this to be well defined we require that \(f\) is \(n\) times differentiable.
    
    \subsection{Dirac Delta of Functions}
    Now consider \(\delta(ax)\) for some constant \(a\), and consider
    \begin{equation}
        \int_{-\infty}^{\infty} \delta(ax)f(x) \dd{x}.
    \end{equation}
    Suppose \(a > 0\).
    A change of variables to \(y = ax\) gives
    \begin{equation}
        \frac{1}{a}\int_{-\infty}^{\infty} \delta(y)f(y) \dd{y} = \frac{f(0)}{a}.
    \end{equation}
    If instead \(a < 0\) then the same change of variables gives
    \begin{align}
        \frac{1}{a}\int_{\infty}^{-\infty} \delta(y)f(y) \dd{y} &= -\frac{1}{\abs{a}}\int_{\infty}^{-\infty} \delta(y)f(y) \dd{y}\\
        &= \frac{1}{\abs{a}}\int_{-\infty}^{\infty} \delta(y)f(y) \dd{y}\\ &= \frac{f(0)}{\abs{a}}.
    \end{align}
    If \(a = 0\) the integral diverges.
    Hence
    \begin{equation}
        \delta(ax) = \frac{\delta(x)}{\abs{a}}.
    \end{equation}
    
    We can be a bit more general and consider the Dirac delta with some well behaved test function, \(g\), as its argument.
    In particular consider
    \begin{equation}
        \int_{-\infty}^{\infty} f(x)\delta(g(x)) \dd{x}.
    \end{equation}
    Let \(x_i\) be the zeros of \(g(x)\), since the argument of \(\delta\) being zero are the points where we pick out the value of the function.
    In particular between these points the integrand is zero and so this integral is equal to
    \begin{equation}
        \sum_{i}  \int_{x_i - \varepsilon}^{x_i + \varepsilon} f(x)\delta(g(x)) \dd{x}.
    \end{equation}
    Here \(\varepsilon > 0\) is arbitrarily small.
    In particular we take \(\varepsilon\) to be small enough that none of the integrals overlap.
    Note that this requires these be simple zeros, if they aren't then the result is undefined.
    
    A change of variables to \(y = g(x)\) gives \(g^{-1}(y) = 0\)\footnote{if \(g\) is not invertible we can restrict it to an arbitrarily small interval, \((x_i - \varepsilon, x_i + \varepsilon)\) over which it is invertible}, \(\dd{y} = g'(x)\dd{x}\), and \(x_i \pm \varepsilon = g(x_i \pm \varepsilon)\).
    We then have
    \begin{equation}
        \sum_{i} \int_{g(x_i - \varepsilon)}^{g(x_i + \varepsilon)} f(g^{-1}(y))\delta(y)\frac{1}{g'(g^{-1}(y))} \dd{y} = \sum_{i} \frac{f(x_i)}{\abs{g'(x_i)}}
    \end{equation}
    where the absolute value arises by the same argument as for the simple \(\delta(ax)\) case, if \(g'(x_i) > 0\) then nothing happens, if \(g'(x_i) < 0\) then the limits swap but we change them back by introducing a negative, which we then cancel with the negative of \(g'(x_i)\) to give \(\abs{g'(x_i)}\).
    
    From this we can identify
    \begin{equation}
        \delta(g(x)) = \sum_{i} \frac{\delta(x - x_i)}{\abs{g'(x_i)}}.
    \end{equation}
    Notice that this is consistent with the two cases \(g(x) = ax\) and \(g(x) = x - x_0\) that we considered earlier.
    
    \subsection{Higher Dimensions}
    While we mostly restrict ourselves to single variables in this course, be they real or complex, we can easily generalise the Dirac delta to arbitrary dimension, and this is common enough in physics that it warrants comment.
    The simplest way to do so is to define \(N\)-dimensional Dirac delta, \(\delta^{N}\), as a product of one-dimensional Dirac deltas.
    That is
    \begin{equation}
        \delta^N((x_1, x_2, \dotsc, x_N)) = \delta(x_1)\delta(x_2)\dotsm \delta(x_N).
    \end{equation}
    In particular in physics we usually care about the three-dimensional case.
    In particular when considering an effect concentrated at some point \(\vv{r_0}\) we often use
    \begin{equation}
        \delta^3(\vv{r} - \vv{r_0}) = \delta(x - x_0)\delta(y - y_0)\delta(z - z_0)
    \end{equation}
    where \(\vv{r} = (x, y, z)\) and \(\vv{r_0} = (x_0, y_0, z_0)\).
    
    Since the Dirac delta behaves almost identically in multiple dimensions we usually drop the superscript telling us the dimension and just write this as \(\delta(\vv{r} - \vv{r_0})\), letting the dimension of the argument inform us of the dimension we are considering.
    
    \chapter{Ordinary Differential Equations}
    \epigraph{Heeellllooo Count Wronski!}{Kristel Torokoff}
    \section{Types of ODEs}
    An \(n\)th order \defineindex{ordinary differential equation} (ODE)\glossary[acronym]{ODE}{ordinary differential equation} is a relation of the form
    \begin{equation}
        y^{(n)} = F(x, y(x), y'(x), \dotsc, y^{(n-1)}(x)).
    \end{equation}
    Here \(f\) is a function of \(x\), \(y\), and the \(n - 1\) first derivatives of \(y\).
    
    We say that the ODE is \defineindex{linear} if \(F\) is linear in \(y\) and its derivatives.
    Note that this does \emph{not} require \(F\) be linear in \(x\), that would lead only to particularly boring equations.
    
    An \(n\)th order ODE depends on \(n\) parameters, often called \define{constants of integration}\index{constant of integration}.
    These are usually set by either boundary conditions or initial values.
    We typically won't worry about this step since it isn't too difficult and is heavily dependent on the system we are studying.
    
    We can always write an \(n\)th order ODE as a system of \(n\) first order ODEs by defining the functions \(y_k\) as
    \begin{equation}
        y_k(x) \coloneqq \diff[k]{y}{x} = y^{(k)}(x).
    \end{equation}
    We then have
    \begin{equation}
        \diff{y_k}{x} = y_{k+1}(x), \text{for \(k = 0, \dotsc, n - 2\) and } \diff{y_{n-1}}{x} = F(x, y(x), y_1(x), \dotsc, y_{n-1}(x)).
    \end{equation}
    Notice that here we are treating \(y_k\) as functions, not derivatives of functions, and so \(\diff{y_k}/{x}\) is just a first order derivative.
    
    Sometimes first order ODEs are written in terms of differentials,
    \begin{equation}
        A(x, y) \dd{y} + B(x, y) \dd{x} = 0,
    \end{equation}
    which is to be understood as the result of multiplying through by \(\dl{x}\).
    Reversing this we get the same ODE in more standard notation:
    \begin{equation}
        A(x, y) \diff{y}{x} + B(x, y) = 0.
    \end{equation}
    
    We can always write a linear ODE in the form
    \begin{equation}
        \linop y(x) = f(x)
    \end{equation}
    where
    \begin{equation}
        \linop \coloneqq p_0(x) + p_1(x)\diff{}{x} + \dotsb + p_{n-1}(x) \diff[n - 1]{}{x} + \diff[n]{}{x}
    \end{equation}
    is a \defineindex{linear differential operator}.
    Here \(p_i\) are functions of \(x\) and we are free to choose them in conjunction with \(f\) such that the coefficient of \(\diff[n]{}/{x}\) is 1.
    
    If \(f(x) = 0\) for all \(x\) then we say that the ODE is \defineindex{homogeneous} and if \(f(x) \ne 0\) for some \(x\) then we say it is \defineindex{inhomogeneous}.
    
    \section{Linear Homogeneous ODEs}
    Consider the linear homogeneous ODE
    \begin{equation}
        y^{(n)} + p_{n-1}y^{(n-1)} + \dotsb + p_1(x)y' + p_0(x)y = 0.
    \end{equation}
    The general solution to this is of the form
    \begin{equation}
        y(x) = \sum_{i=1}^{n} C_iy_i(x)
    \end{equation}
    where \(C_i\) are constants of integration and \(\{y_i\}\) is a set of \(n\) linearly independent functions satisfying the ODE.
    Note that by linearly independent we mean the only solution to
    \begin{equation}
        \sum_{i} \alpha_i y_i(x) = 0
    \end{equation}
    is the trivial \(\alpha_i = 0 \forall i\) solution.
    
    Differentiating this linear dependence equation \(n - 1\) times we get \(n\) conditions that must be satisfied:
    \begin{equation}
        \sum_{i=1}^{n} \alpha_i y_i(x) = 0, \qquad \sum_{i=1}^n \alpha_i y_i'(x) = 0, \quad\dotsc\quad, \sum_{i=1}^{n} \alpha_iy_i^{(n-1)}(x) = 0.
    \end{equation}
    From these \(n\) equations we get a non-trivial solution if and only if the \defineindex{Wronskian}, defined as
    \begin{equation}
        W(x) = W[y_1(x), y_2(x), \dotsc, y_n(x)] \coloneqq
        \begin{vmatrix}
            y_1 & y_2 & \dots & y_n\\
            y_1' & y_2' & \dots & y_n'\\
            \vdots & \vdots & \ddots & \vdots\\
            y_1^{(n-1)} & y_2^{(n - 1)} & \dots & y_n^{(n-1)}
        \end{vmatrix}
        ,
    \end{equation}
    is non-vanishing for some \(x\).
    If this is the case then we can invert the system to get \(\alpha_i = 0\).
    If this is not the case then the vanishing of \(W(x)\) implies the linear dependence of some of the \(y_i\).
    Note that when we say vanish we mean vanishing for all \(x\), it is fine if the Wronskian vanishes for some specific values of \(x\).
    
    Consider a second order linear homogeneous ODE,
    \begin{equation}
        u''(x) + p_1(x)u'(x) + p_0(x)u(x) = 0.
    \end{equation}
    Let \(u_1\) and \(u_2\) be linearly independent solutions to this.
    We then have
    \begin{equation}
        W(x) = 
        \begin{vmatrix}
            u_1 & u_2\\
            u_1' & u_2'
        \end{vmatrix}
        = u_1u_2' - u_2u_1' \ne 0.
    \end{equation}
    Now consider the derivative of the Wronskian:
    \begin{align}
        W'(x) &= u_1'u_2' + u_1u_2'' - u_2'u_1' - u_2u_1''\\
        &= u_1u_2'' - u_2u_1''\\
        &= u_1[-p_1u_2' - p_0u_2] - u_2[-p_1u_1' - p_0u_1]\\
        &= p_1[-u_1u_2' + u_2u_1'] + p_0[-u_1u_2 + u_1u_2]\\
        &= p_1W(x).
    \end{align}
    So we have derived a first order differential equation for the Wronskian.
    This is separable and we can solve it by integration:
    \begin{equation}
        W' = -p_1W \implies \frac{W'}{W} = -p_1 \implies \ln W(x) = -\int_{x_0}^{x} p_1(z) \dd{z}.
    \end{equation}
    It then follows that
    \begin{equation}
        W(x) = W(x_0)\exp\left[ -\int_{x_0}^{x} p_1(z) \dd{z} \right].
    \end{equation}
    This is called \defineindex{Abel's formula}.
    It holds more generally for an \(n\)th order linear ODE we have
    \begin{equation}
        W'(x) = -p_{n-1}(x)W(x) \implies W(x) = W(x_0)\exp\left[ -\int_{x_0}^{x} p_1(z) \dd{z} \right].
    \end{equation}
    
    We can use Abel's formula to generate a second solution from a first solution for the second order case.
    Suppose we know \(u_1\) but not \(u_2\).
    Consider
    \begin{equation}
        \diff*{\left( \frac{u_2}{u_1} \right)}{x} = \frac{u_2'}{u_1} - \frac{u_2u_1'}{u_1^2} = \frac{u_1u_2' - u_2u_1'}{u_1^2} = \frac{W(x)}{u_1^2}.
    \end{equation}
    We then have
    \begin{equation}
        \frac{u_2}{u_1} = \int_{x_0}^{x} \frac{W(z)}{u_1(z)^2} \dd{z}.
    \end{equation}
    
    \begin{exm}{Laguerre's Equation}{}
        Consider the second order linear differential equation
        \begin{equation}
            xy'' + (1 - x)y' + ny = 0
        \end{equation}
        for some parameter \(n\).
        This is known as \defineindex{Laguerre's equation}.
        The polynomial solutions are the \define{Laguerre polynomials}\index{Laguerre polynomical}, \(L_n\)\index{\(L_n\)|see{Laguerre polynomial}}.
        Consider the case of \(n = 0\).
        We then have the simple case of \(L_0(n) = 1\).
        We can identify \(p_1(x) = (1 - x)/x\) and so the Wronskian is:
        \begin{equation}
            W(x) = \exp\left[ -\int_{x_0}^{x} \frac{1 - t}{t} \dd{t} \right] = A\exp[-\ln x + x]
        \end{equation}
        from which we see that a second linearly independent solution is
        \begin{equation}
            A\int_{x_0}^{x} \frac{\e^{-\ln u + u}}{(1 - u)/u}\dd{u} = \int_{x_0}^{x} \frac{\e^{u}}{1 - u} \dd{u}.
        \end{equation}
    \end{exm}
    
    \section{Sturm--Liouville Form}
    The \defineindex{Sturm--Liouville form}, or \defineindex{self-adjoint form}, of a second order linear inhomogeneous  ODE is
    \begin{equation}
        \linop(x)y(x) = f(x)
    \end{equation}
    where
    \begin{equation}
        \linop(x) = \diff{}{x}\left[ p(x) \diff{}{x} \right] + q(x).
    \end{equation}
    All second order ODEs can be written in this form.
    Say we start with
    \begin{equation}
        y''(x) + p_1(x)y'(x) + p_0(x)y(x) = g(x).
    \end{equation}
    We can then use \(W'(x) = -p_1(x)W(x)\) to write
    \begin{equation}
        \diff{}{x}\left[ \frac{1}{W}\diff{}{x} \right] = -\frac{W'}{W^2}\diff{}{x} + \frac{1}{W}\diff[2]{}{x} = \frac{1}{W}\diff[2]{}{x} + \frac{p_1}{W}\diff{}{x}.
    \end{equation}
    We then have
    \begin{equation}
        W(x)\left[ \diff{}{x}\left( \frac{1}{W(x)}\diff{}{x} \right) + \frac{p_0(x)}{W(x)} \right]y(x) = g(x).
    \end{equation}
    From this we can identify
    \begin{equation}
        p(x) = \frac{1}{W(x)}, \qquad q(x) = \frac{p_0(x)}{W(x)}, \qqand f(x) = \frac{g(x)}{W(x)}.
    \end{equation}
    Note that the Wronskian is only ever defined up to a multiplicative constant and hence these may all differ by a common constant factor and the solutions will be the same.
    
    \section{Green's Functions}
    Consider an inhomogeneous second order linear ODE,
    \begin{equation}
        \linop(x) u(x) = f(x)
    \end{equation}
    for \(x \in [a, b]\).
    This has as a solution
    \begin{equation}
        u(x) = u_{\mathrm{H}}(x) + u_{\mathrm{P}}(x).
    \end{equation}
    Here \(u_{\mathrm{H}}\) is the solution to the homogeneous equation, which we assume are known, and of the form
    \begin{equation}
        \sum_{i} \alpha_i \tilde{u}_i(x)
    \end{equation}
    where \(\tilde{u}_i\) each individually solve the homogeneous equation and are linearly independent.
    
    We wish to find the particular solution, \(u_{\mathrm{P}}\).
    We do so by constructing a \defineindex{Green's function}, \(G(x, x')\), such that it solves
    \begin{equation}
        \linop(x) G(x, x') = \delta(x - x').
    \end{equation}
    Notice that the right hand side is not a function and therefore the Green's \enquote{function} is also not really a function.
    We can think of \(G\) as an \enquote{inverse differential operator} which undoes \(\linop\) at a particular value of \(x = x'\).
    This allows us to solve the differential equation one point at a time to get the solution by integrating over all points, in particular
    \begin{equation}
        \int_{a}^{b} \linop(x) G(x,x') f(x') \dd{x'} = \int_{a}^{b} \delta(x - x') f(x') \dd{x'} = f(x).
    \end{equation}
    We can view the second integral as a superposition of perturbations, \(f(x')\), which come together to form the final perturbation \(f(x)\).
    Since \(\linop(x)\) acts only on \(x\) and we are integrating with respect to \(x'\) we are free to reverse the order of integration and \(\linop(x)\) so we have
    \begin{equation}
        \linop(x) \int_{a}^{b} G(x, x') f(x') \dd{x'} = f(x).
    \end{equation}
    Thinking about this what we have is some function of \(x\) (after we integrate over \(x'\)) to which we apply \(\linop(x)\) to get \(f(x)\).
    This is exactly what we require for \(u_{\mathrm{P}}(x)\) and so we identify
    \begin{equation}
        u_{\mathrm{P}}(x) = \int_{a}^{b} G(x, x') \dd{x'}.
    \end{equation}
    So, combining this with \(u_{\mathrm{H}}\) we have the solution, assuming of course that we can find \(G\).
    
    \subsection{Constructing the Green's Function}
    \subsubsection{Boundary Value Problem}
    For a boundary value problem we are given the values of \(u(a)\) and \(u(b)\).
    First start by considering \(x \ne x'\).
    We have
    \begin{equation}
        \linop(x) G(x, x') = \delta(x - x') = 0,
    \end{equation}
    this is the homogeneous solution to which we assume the solutions, \(\tilde{u}_i\), are known.
    We can then construct \(G(x, x')\) for \(x \ne x'\) from these solutions.
    In particular we define
    \begin{align}
        G(x, x') = 
        \begin{cases}
            C_1(x')u_1(x), & x < x',\\
            C_2(x')u_2(x), & x > x'.
        \end{cases}
    \end{align}
    Here \(C_i\) are functions to be determined and \(u_i\) are suitable linear combinations of \(\tilde{u}_i\) chosen to satisfy the boundary conditions.
    In particular \(u_1(x)\) satisfies the boundary conditions at \(a\) since \(a \le x'\) and \(u_2(x)\) satisfies the boundary conditions at \(b\) since \(b \ge x'\).
    Notice that the boundary conditions effect our choice of \(G\) so the same equation with different boundary conditions will have different Green's functions.
    
    We want \(G\) to be continuous so we choose \(G(x, x')\) to make this true, that is we demand
    \begin{equation}
        C_1(x)u_1(x) - C_2(x)u_2(x) = 0.
    \end{equation}
    
    Now consider now the integral
    \begin{equation}
        \int_{x' - \varepsilon}^{x' + \varepsilon} \linop(x) G(x, x') \dd{x'} = \int_{x' - \varepsilon}^{x' + \varepsilon} \delta(x - x') \dd{x} = 1
    \end{equation}
    for arbitrary \(\varepsilon > 0\).
    Using the Sturm--Liouville form we then have
    \begin{equation}
        \int_{x' - \varepsilon}^{x' + \varepsilon} \left[ \diff{}{x}\left[ p(x) \diff{G}{x} \right] + q(x)G(x, x') \right] \dd{x} = 1.
    \end{equation}
    Since \(qG\) is continuous at \(x = x'\) in the limit \(\varepsilon \to -0\) the contribution from this second term vanishes.
    We then have
    \begin{equation}
        \left[ p(x)\diff{G}{x} \right]_{x' - \varepsilon}^{x' + \varepsilon} = 1.
    \end{equation}
    Taking \(\varepsilon \to 0\) again we have a discontinuity of \(G'\) at \(x = x'\).
    Differentiating our earlier definition of \(G\) we have
    \begin{align}
        G'(x, x') = 
        \begin{cases}
            C_1(x')u_1'(x), & x < x',\\
            C_2(x')u_2'(x), & x > x'.
        \end{cases}
    \end{align}
    and so at \(x = x'\) we have
    \begin{equation}
        C_1(x)u_1'(x) - C_2(x)u_2'(x) = -\frac{1}{p(x)}.
    \end{equation}
    
    This gives us two equations in \(C_i\) which we can solve to get
    \begin{equation}
        C_1(x) = \frac{u_2}{p(x)W(x)} = Ku_2(x), \qqand C_2(x) = \frac{u_1(x)}{p(x)W(x)} = Ku_1(x)
    \end{equation}
    where the factors of \(K\) occur due to the arbitrary constant in the definition of the Wronskian and we have used \(p(x) = 1/W(x)\).
    
    We can write the definition of the Green's function now in one piece with Heaviside step functions:
    \begin{equation}
        G(x, x') = K[u_1(x)u_2(x')\theta(x' - x) + u_1(x')u_2(x)\theta(x - x')]
    \end{equation}
    where the first term corresponds to \(x < x'\) and the second to \(x > x'\).
    The particular solution is then the integral of this:
    \begin{equation}
        u_{\mathrm{P}}(x) = Ku_2(x) \int_a^x u_1(x') f(x') \dd{x'} + Ku_1(x)\int_x^b u_2(x') f(x') \dd{x'}.
    \end{equation}
    Notice that this automatically satisfies the boundary conditions at \(x = a, b\) by construction.
    We then have the full solution
    \begin{equation}
        u(x) = u_{\mathrm{P}}(x) + \sum_{i} \alpha_i \tilde{u}_i(x)
    \end{equation}
    where \(\alpha_i\) can be determined from the boundary conditions.
    
    \subsubsection{Initial Value Problem}
    The other type of differential equation problem we encounter is an initial value problem of the form
    \begin{equation}
        \ddot{u} + p_1(t)\dot{u} + p_0(t)u = f(t).
    \end{equation}
    We assume that \(f\) is some perturbation which only takes effect after \(t = 0\) and therefore for \(t < 0\) \(f(t) = 0\).
    We will consider the particular case of 
    \begin{equation}
        \ddot{u} + u = f(t)
    \end{equation}
    with the initial conditions \(u(0) = \dot{u}(0) = 0\).
    We can interpret this as a driven harmonic oscillator initially at rest at the origin to with the driving force \(f(t)\) applied starting at time \(t = 0\).
    The solution to the homogeneous equation is
    \begin{equation}
        u_{\mathrm{H}}(t) = A\sin t + B \cos t.
    \end{equation}

    Our Green's function, \(G(t, t')\) therefore represents the effects of the driving force at \(t = t'\).
    For \(t < t'\) we have \(G(t, t') = 0\), and also \(G'(t, t') = 0\), since the perturbation doesn't take effect until \(t = t'\).
    For \(t > t'\) we have
    \begin{equation}
        G(t, t') = C_1(t')u_1(t) + C_2(t')u_2(t)
    \end{equation}
    since we have no further conditions to satisfy so we choose the most general solution.
    We also have
    \begin{equation}
        \diff{G}{t}[t=t'] = \frac{1}{p(t)} \implies C_1u_1' + C_2u_2' = \frac{1}{p}.
    \end{equation}
    It follows that
    \begin{equation}
        C_1(t) = -\frac{u_2(t)}{p(t) W(t)}, \qqand C_2(t) = \frac{u_1(t)}{p(t)W(t)}.
    \end{equation}
    For our example we have \(u_1(t) = \sin t\) and \(u_2(t) = \cos t\), which gives \(W(t) = -1\) and so
    \begin{equation}
        C_1(t')= \cos t', \qqand C_2(t') = -\sin t'.
    \end{equation}
    Hence for \(t > t'\) our Green's function is
    \begin{equation}
        G(t, t') = \cos(t')\sin(t) - \sin(t')\cos(t) = \sin(t - t').
    \end{equation}
    Thus, the particular solution is
    \begin{align}
        u_{\mathrm{P}}(t) &= \int_{-\infty}^{\infty} G(t, t') f(t') \dd{t'}\\
        &= \int_{0}^{\infty} G(t, t')f(t') \dd{t'}\\
        &= \int_{0}^{t} \sin(t - t') f(t') \dd{t'}\label{eqn:driven shm solution}
    \end{align}
    which is as far as we can go without knowing \(f\).
    Note that the limits change from \((-\infty, \infty)\) to \((0, \infty)\) due to the fact that \(f(t') = 0\) for \(t' < 0\) and the second set of limits change from \(t' \in (0, \infty)\) to \(t' \in (0, t)\) since \(G(t - t') = 0\) for \(t < t'\).
    
    \subsection{Inhomogeneous Boundary Conditions}
    Consider the case of a second order linear differential equation, \(\linop y = f\), with boundary conditions given by
    \begin{equation}
        \alpha y(a) + \beta y'(a) = A, \qqand \gamma y(b) + \delta y'(b) = B.
    \end{equation}
    We can write the solution as
    \begin{equation}
        y(x) = y_{p}(x) + \tilde{y}(x)
    \end{equation}
    where \(y_p\) is the particular solution given by
    \begin{equation}
        y_p = \int_a^b G(x, x')f(x') \dd{x'},
    \end{equation}
    which satisfies \(\linop y_p = f\) with the homogeneous boundary conditions
    \begin{equation}
        \alpha y_p(x) + \beta y_p'(x) = 0, \qqand \gamma y_p(x) + \delta y_p'(x) = 0.
    \end{equation}
    We then have
    \begin{equation}
        f = \linop y = \linop (y_p + \tilde{y}) = f + \linop \tilde{y} \implies \linop \tilde{y} = 0.
    \end{equation}
    Similarly the boundary conditions are
    \begin{equation}
        \alpha \tilde{y}(a) + \beta \tilde{y}'(a) = A, \qqand \gamma \tilde{y}(b) + \delta \tilde{y}'(b) = B.
    \end{equation}
    So, \(\tilde{y}\) is the solution to the homogeneous equation with inhomogeneous boundary conditions.
    
    \begin{exm}{}{}
        Consider the forced harmonic oscillator described by
        \begin{equation}
            \diff[2]{y}{t} + y(t) = f(t)
        \end{equation}
        with boundary conditions
        \begin{equation}
            y(0) = A, \qqand y'(0) = B.
        \end{equation}
        The solution then follows from the work in this section and the result in \cref{eqn:driven shm solution}
        \begin{equation}
            y(t) = \int_{0}^{t} \sin(t - t') f(t') \dd{t'} + \tilde{y}(t)
        \end{equation}
        where
        \begin{equation}
            \tilde{y}(t) = A\cos t + B\sin t.
        \end{equation}
    \end{exm}
    
    \chapter{Series Solutions}
    We have seen how we can obtain a second solution to a homogeneous differential equation given the first solution.
    We then saw how Green's functions can be used to combine find solutions from this that work with inhomogeneous equations.
    In this chapter we look at how to find the first solution to the homogenous equation.
    
    \section{Classifying Points}
    \begin{dfn}{Classifying Points}{}
        Consider the second-order, linear, homogenous ordinary differential equation
        \begin{equation}
            u''(x) + p(x)u'(x) + q(x)u(x) = 0.
        \end{equation}
        Consider the point \(x = x_0\).
        Then
        \begin{itemize}
            \item if \(p\) and \(q\) are analytic at \(x = x_0\) we say \(x_0\) is an \defineindex{ordinary point}.
            \item else we say \(x = x_0\) is a \defineindex{singular point}.
        \end{itemize}
        Given a singular point, \(x_0\), we classify it further as
        \begin{itemize}
            \item \defineindex{regular}, if \((x - x_0)p(x)\) and \((x - x_0)^2q(x)\) are analytic.
            \item \defineindex{essential} or \define{irregular}\index{irregular|see{essential}} otherwise.
        \end{itemize}
    \end{dfn}
    
    For finite \(x_0\) we can determine the nature of the point by inspection of \(p\) and \(q\).
    For \(x_0 = \infty\) we can determine the nature of the point by considering \(z = 1/x\) studying the behaviour as \(z \to 0\).
    To do so we need to change variables in the whole equation, meaning that we need to change our derivatives according to
    \begin{equation}
        \diff{}{x} = \diff{z}{x}\diff{}{z} = -z^2\diff{}{z}
    \end{equation}
    and
    \begin{equation}
        \diff[2]{}{x} = \diff{z}{x}\diff{}{z}\left[ -z^2\diff{}{z} \right] = 2z^3\diff{}{z} + z^4\diff[2]{}{z}.
    \end{equation}
    Defining \(Y(z) = y(1/z)\) our differential equation becomes
    \begin{equation}
        Y''(z) + \left[ \frac{2}{z} - \frac{p(1/z)}{z^2} \right]Y'(z) + \frac{q(1/z)}{z^4}Y(z) = 0.
    \end{equation}
    
    Note that singularities in a differential equation don't necessarily imply singularities in the solution, for example
    \begin{equation}
        y''(x) - \frac{3}{x}y'(x) + \frac{3}{x^2}y(x) = 0
    \end{equation}
    has a singularity at \(x = 0\), but the solution, \(y(x) = Ax + Bx^3\) does not.
    
    \begin{exm}{}{}
        Consider the simple harmonic oscillator described by
        \begin{equation}
            y''(x) + \omega^2 y(x) = 0.
        \end{equation}
        We identify \(p(x) = 0\) and \(q(x) = \omega^2\)
        There are no singularities at finite \(x\).
        For the point at infinity our equation becomes
        \begin{equation}
            Y''(z) + \frac{2}{z}Y'(z) + \frac{\omega^2}{z^4}Y(z) = 0.
        \end{equation}
        This has an essential singularity at \(z = 0\) since \(z^2(\omega^2/z^4) = \omega^2/z^2\) is not analytic at zero.
    \end{exm}
    
    \begin{exm}{}{}
        Consider \defineindex{Legendre's equation}
        \begin{equation}
            (1 - x^2)y''(x) - 2xy'(x) + n(n + 1)y(x) = 0.
        \end{equation}
        This often occurs with \(x = \cos\vartheta\) in \defineindex{Laplace's equation}, \(\laplacian f = 0\), in spherical polar coordinates.
        We rewrite this in the conventional form dividing through by \(1 - x^2\) to give
        \begin{equation}
            y''(x) - \frac{2x}{1 - x^2}y'(x) + \frac{n(n + 1)}{1 - x^2}y(x) = 0.
        \end{equation}
        From this we identify \(p(x) = -2x/(1 - x^2)\) and \(q(x) = n(n+1)/(1 - x^2)\).
        Clearly this has singularities at \(x = \pm 1\) and since \((x \mp 1)p(x) = 2x/(x \pm 1)\) is analytic at \(x = \pm 1\), and similarly, \((x \mp 1)q(x)\) is analytic, this is a regular singular point.
        
        For the point at infinity our differential equation becomes
        \begin{equation}
            Y''(z) + \frac{2z}{z^2 - 1} Y'(z) + \frac{n(n + 1)}{z^2(z^2 - 1)} = 0
        \end{equation}
        which has a regular singular point at \(z = 0\), so \(x = \infty\) is a regular singular point.
    \end{exm}
    
    It can be shown that every ODE has at least one singular point.
    Further if an ODE has exactly one singular point at \(x = a\) then the ODE must be
    \begin{equation}
        y''(x) + \frac{2}{x - a} y'(x) = 0
    \end{equation}
    which has the solution
    \begin{equation}
        y(x) = A + \frac{B}{x - a}.
    \end{equation}
    
    \section{Ansatz (Taylor's Version)}\label{sec:Anzatz (Taylor's Version)}
    Consider the differential equation
    \begin{equation}
        u''(x) + p(x)u'(x) + q(x)u(x) = 0.
    \end{equation}
    Suppose \(x = x_0\) is a singular point.
    Then the ansatz
    \begin{equation}
        u(x) = \sum_{m=0}^{\infty} c_m(x - x_0)^{m}
    \end{equation}
    will converge in a region up to the nearest singular point.
    
    The general method is then to substitute this ansatz in and equate coefficients to get a recursion relation for \(c_m\).
    We then use the boundary conditions to get the first values of \(c_m\) and the others follow from the recurrence relation.
    This method is best demonstrated by an example.
    
    Consider Legendre's differential equation
    \begin{equation}
        (1 - x^2)u''(x) - 2xu'(x) + n(n + 1)u(x) = 0.
    \end{equation}
    We wish to solve this at \(x = 0\), which is an ordinary point.
    Suppose that
    \begin{equation}
        u(x) = \sum_{m=0}^{\infty} c_m(x) x^m.
    \end{equation}
    Then
    \begin{equation}
        u'(x) = \sum_{m=0}^{\infty} mc_mx^{m-1}, \qqand u''(x) = \sum_{m=0}^{\infty} m(m - 1)c_mx^{m - 2}.
    \end{equation}
    Substituting this into our differential equation we have
    \begin{align}
        0 &= (1 - x^2) \sum_{m=0}^{\infty} m(m - 1)c_mx^{m-2} - 2x\sum_{m=0}^{\infty} mc_mx^{m-1}\notag\\
        &\qquad+ n(n + 1) \sum_{m=0}^{\infty} c_mx^m\\
        &= \sum_{m=0}^{\infty} m(m - 1)c_mx^{m-2} - \sum_{m=0}^{\infty} m(m - 1)c_mx^{m} + \sum_{m=0}^{\infty}2mc_mx^{m}\notag\\
        &\qquad+ \sum_{m=0}^{\infty}n(n+1) c_mx^m.
    \end{align}
    Now consider the first sum, first notice that for \(m = 0, 1\) this term is zero, so we can change the limits on the sum to
    \begin{equation}
        \sum_{m=0}^{\infty} m(m - 1) c_mx^{m-2} = \sum_{m=2}^{\infty} m(m - 1) c_mx^{m-2}.
    \end{equation}
    We can then re-index to \(m' = m - 2\) giving
    \begin{equation}
        \sum_{m=2}^{\infty} m(m-1)c_mx^{m-2} = \sum_{m'=0}^{\infty} (m' + 2)(m' + 1)c_{m' + 2}x^{m'}.
    \end{equation}
    Now \(m'\) is just a dummy variable so we are free to rename it to \(m\) giving
    \begin{equation}
        \sum_{m=0}^{\infty} m(m - 1) c_{m}x^{m-2} = \sum_{m=0}^{\infty} (m + 1)(m - 1)c_{m+2}x^{m}.
    \end{equation}
    Putting this back in to the differential equation and combining the sums we get
    \begin{align}
        0 &= \sum_{m=0}^{\infty} [c_m(n(n + 1) - m - m^2) + c_{m+2}(m + 1)(m + 2)]x^m.
    \end{align}
    This must be true for all \(x\) (in the region of convergence), and therefore, since \(x^m\) and \(x^{m'}\) are linearly independent for \(m \ne m'\), we must have that the square bracket is zero.
    Hence
    \begin{equation}
        c_{m+2} = \frac{m(m + 1) - n(n + 1)}{(m + 1)(m + 2)} c_m = \frac{(m - n)(m + n + 1)}{(m + 2)(m + 1)}.
    \end{equation}
    This gives a recurrence relation for \(c_m\).
    Since it relates \(c_m\) and \(c_{m+2}\) we need two starting values, \(c_0\) and \(c_1\), to specify all \(c_m\).
    We get these values from the boundary conditions.
    Once we have found them we can use the recurrence relation to write the solution as
    \begin{align}
        u(x) &= \sum_{m=0}^{\infty} c_mx^m\\
        &= c_0\left[ 1 + \frac{c_2}{c_0}x^2 + \frac{c_4}{c_6}x^{4} + \dotsb \right] c_1\left[ x + \frac{c_3}{c_1}x^3 + \frac{c_5}{c_1}x^5 + \dotsb \right]\\
        &= c_0 \left[ 1 - n(n + 1)\frac{x^2}{2!} + n(n + 1)(n - 2)(n + 3)\frac{x^4}{4!} + \dotsb \right]\\
        &\times c_1\left[ x - (n - 1)(n + 2)\frac{x^3}{3!} + (n - 1)(n + 2)(n - 3)(n + 4)\frac{x^5}{5!} + \dotsb \right].
    \end{align}
    We separate into even and odd order terms giving two independent solutions.
    It can be shown that this converges for \(\abs{x} < 1\) using the ratio test.
    
    If \(n\) is an even (odd) positive integer then the series of even (odd) order terms terminates at \(x^{n}\).
    The solutions are then polynomials, and we call them the \define{Legendre polynomials}\index{Legendre polynomial}, denoting them \(P_n\).
    The first few Legendre polynomials are
    \begin{alignat}{3}
        P_0(x) &= 1, \qquad & P_2(x) &= \frac{1}{2}(3x^2 - 1),\\
        P_1(x) &= x, \qquad & P_3(x) &= \frac{1}{2}(5x^3 - 3x).
    \end{alignat}
    
    \section{Frobenius Expansion About a Regular Singular Point}
    \epigraph{What is the world coming to? Thank goodness we have maths to keep us sane}{Kristel Torokoff on the impending COP26 climate conference}
    For a regular singular point Taylor's ansatz fails.
    Instead we use the modified Frobenius series
    \begin{equation}
        y(x) = (x - x_0)^{\alpha} \sum_{m=0}^{\infty} c_m(x - x_0)^m
    \end{equation}
    with \(c_0 \ne 0\).
    
    Consider the ordinary differential equation
    \begin{equation}
        u''(x) + P(x)u'(x) + Q(x)u(x) = 0
    \end{equation}
    with an regular point at \(x_ = 0\), for simplicity, we can always do the same at any regular point.
    Since this is a regular point we know that \(xP(x)\) and \(x^2Q(x)\) are regular, meaning that we can expand \(P\) and \(Q\) in Laurent series starting from \(x^{-1}\) and \(x^{-2}\) respectively:
    \begin{equation}
        P(x) = \sum_{m=-1}^{\infty} P_mx^m, \qqand Q(x) = \sum_{m=-2}^{\infty} Q_mx^{m}.
    \end{equation}
    The Frobenius ansatz is then
    \begin{equation}
        u(x) = x^\alpha \sum_{n=0}^{\infty} a_nx^n
    \end{equation}
    with \(a_0 \ne 0\) and \(\alpha \in \reals\).
    We need to determine the values of \(\alpha\) and \(a_i\).
    We start by computing the derivatives
    \begin{align}
        u'(x) &= \alpha x^{\alpha-1}\sum_{n=0}^{\infty}  + x^\alpha \sum_{n=1}^{\infty} a_nnx^{n-1}\\
        u''(x) &= \alpha(\alpha - 1)x^{\alpha - 2} \sum_{n=0}^{\infty} a_nx^n + 2\alpha x^{\alpha - 1} \sum_{n=1}^{\infty} a_n nx^{n-1}\notag\\
        &\qquad+ x^\alpha \sum_{n=2}^{\infty} a_nn(n - 1) x^{n-2}.
    \end{align}
    Here we have written the sums such that the first term is nonzero.
    
    Substituting this into our differential equation, as well as the Laurent series for \(P\) and \(Q\) we get
    \begin{align}\label{eqn:sub in frobenius}
        0 &= \sum_{n=0}^{\infty} \bigg[ a_n(a + n)(\alpha + n - 1)x^{n + \alpha - 2} + \sum_{m=-1}^{\infty} P_m(\alpha + n)a_nx^{m + n + \alpha - 1} \notag\\
        &\qquad+ \sum_{m=-2}^{\infty} Q_ma_n x^{m + n + \alpha} \bigg].
    \end{align}
    From this we identify the coefficient of the \(x^{\alpha - 2}\) term, which must be zero as the coefficients of \(x^i\) must be zero in order for the whole sum to be zero.
    This gives us
    \begin{align}
        0 = a_0\alpha(\alpha - 1) + P_{-1}\alpha a_0 + Q_{-2}a_0.
    \end{align}
    Since \(a_0 \ne 0\) by our original assumption we therefore have
    \begin{equation}
        \alpha(\alpha - 1) + P_{-1}\alpha + Q_{-2} = 0.
    \end{equation}
    This is a quadratic equation for \(\alpha\), we call this the \defineindex{indicial equation}.
    Solving it we find two roots
    \begin{equation}
        r_{1,2} = -\frac{P_{-1} - 1}{2} \pm \sqrt{\frac{(P_{-1} - 1)^2}{4} - Q_{-2}}.
    \end{equation}
    From this we can write the indicial equation as
    \begin{equation}
        \alpha^2 + \alpha(P_{-1} - 1) + Q_{-2} = (\alpha - r_1)(\alpha - r_2) = \alpha^2 - \alpha(r_1+r_2) + r_1r_2.
    \end{equation}
    Which gives us
    \begin{equation}
        r_1 + r_2 = 1 - P_{-1}, \qqand r_1 r_2 = Q_{-2}.
    \end{equation}
    Without loss of generality we assume that \(\Re(r_1) \ge \Re(r_2)\).
    
    \subsection{Fuch's Theorem}
    \epigraph{I'll show you the proof, otherwise it looks like thermodynamics, just a postulate}{Kristel Torokoff}
    \epigraph{I don't think [Fuch] would survive a modern physics class, fortunately when he was around people where much more innocent}{Kristel Torokoff}
    
    \begin{thm}{Fuch's Theorem}{}
        In the notation of the last section if \(r_1 - r_2 \notin \integers\) then there will be two linearly independent solutions of the the form of Frobenius' ansatz.
        If \(r_1 - r_2 \in \integers\) then most of the time there will be one solution of the form of Frobenius' ansatz and the other solution will be of the form
        \begin{equation}
            u_2(x) = \ln(x - x_0)u_1(x) + C(x)(x - x_0)^\beta,
        \end{equation}
        although exceptionally there will still be two solutions of the form of Frobenius' ansatz.
        \begin{proof}
            Our starting point is \cref{eqn:sub in frobenius}:
            \begin{align}
                0 &= \sum_{n=0}^{\infty} \bigg[ a_nx^{n+\alpha-2}(\alpha + n)(\alpha + n - 1) + \sum_{m=-1}^{\infty} P_m(\alpha + n)a_nx^{m + n + \alpha - 1}\notag\\
                &\qquad+ \sum_{m=-2}^{\infty} Q_ma_nx^{m+n-\alpha} \bigg].
            \end{align}
            Now consider the coefficient of \(x^{N + \alpha - 2}\), where \(m\) and \(n\) can take any value so long as the resulting power of \(x\) is \(N + \alpha - 2\).
            As usual this coefficient must be zero.
            We then have
            \begin{align}
                0 &= a_N(\alpha + N)(\alpha + N - 1) + \sum_{n=0}^{N - 1}\sum_{m=-1}^{\infty} P_{N - n - 1}(\alpha + n)a_n\notag\\
                &\qquad+ P_{-1}(\alpha + N)a_N + \sum_{n=0}^{N - 1}\sum_{m=-2}^{\infty} Q_{m - 2 - n} a_n + Q_{-2}a_N.
            \end{align}
            Rearranging this to collect \(a_N\) and \(a_n\) we have
            \begin{multline}
                a_N\underbrace{[(N + \alpha)(N + \alpha - 1) + P_{-1}(N + \alpha) + Q_{-2}]}_{\xi}\\
                = -\sum_{n=0}^{N-1}[a_n(n + \alpha)P_{N - n - 1} + \alpha_nQ_{N - n - 2}].
            \end{multline}
            Now, supposing that the first term in square brackets, \(\xi\), is nonzero we get a recurrence relation for \(a_N\) in terms of \(a_0, \dotsc, a_{N - 1}\).
            To find the recurrence relation expand \(\xi\):
            \begin{align}
                \xi &= N^2 + 2N\alpha - N + \alpha^2 - \alpha + P_{-1}N + P_{-1}\alpha + Q_{-2}\\
                &= N^2 + 2N\alpha - N + NP_{-1}\\
                &= N(N + 2\alpha - (1 - P_{-1}1))\\
                &= N(N + 2\alpha - (r_1 + r_2))
            \end{align}
            where we have used \(\alpha^2 - \alpha + \alpha P_{-1} + Q_{-2} = 0\), which is the indicial equation, and the result \(r_1 + r_2 = 1 - P_{-1}\).
            
            If \(\alpha = r_1\), where we assume without loss of generality that \(\Re(r_1) \ge \Re(r_2)\), then we have
            \begin{equation}
                \xi = N(N + 2r_1 - r_1 - r_2) = N(N + r_1 - r_2)
            \end{equation}
            We then have \(\Re(\xi) > 0\), since \(N > 0\) and \(\Re(r_1 - r_2) \ge 0\).
            We therefore have \(\xi \ne 0\).
            If instead \(\alpha = r_2\) then
            \begin{equation}
                \xi = N(N + 2r_2 - r_1 - r_2) = N(N - (r_1 - r_2)),
            \end{equation}
            which will be zero when \(r_1 - r_2 = N \in \integers\).
            Therefore if \(r_1 - r_2 \ne N\) we can find a recurrence relation for \(a_n\).
            This must occur for all \(N\) in order to find a solution and therefore we require that \(r_1 - r_2 \notin \naturals\) to guarantee two solutions of the Frobenius form.
            
            Suppose then that \(r_1 - r_2 = N\), so the second solution is not of the form of Frobenius' ansatz.
            From Abel's formula we therefore have\footnote{we use the notation \(\int^x\) here to denote an integral with upper bound \(x\) and some arbitrary lower bound \(x_0 < x\) since all of this is defined only up to a constant.}
            \begin{equation}
                u_2 = u_1 \int^x \frac{W(t)}{u_1^2(t)} \dd{t}
            \end{equation}
            where \(W\) is the Wronskian given by
            \begin{equation}
                W(t) = \exp[-\int^{t} P(z) \dd{z}] = \exp[-\int^t \sum_{m=-1} P_mz^m \dd{z}].
            \end{equation}
            We can write \(u_1\) as
            \begin{equation}
                u_1(x) = x^{r_1} \sum_{n=0}^{\infty} a_nx^n.
            \end{equation}
            Using this we have
            \begin{align}
                \frac{u_2(x)}{u_1(x)} &= \int^x \frac{\exp[-\int^t [P_{-1}z^{-1} + P_0 + P_1z + \dotsb]]}{t^{2r_1}[a_0 + a_1 t + a_2 t^2 + \dotsb]^2} \dd{t}\\
                &= \int^x \frac{\exp[-(P_1\ln t + P_0t + P_1t^2/2 + \dotsb)]}{t^{2r_1}\left( \sum_{n=0}^{\infty} a_nt^n \right)^2} \dd{t}\\
                &= \int^x t^{-2r_1-P_{-1}} \sum_{n=0}^{\infty} b_nt^n \dd{t}
            \end{align}
            where we take everything apart from \(\exp[-P_{-1}\ln t]/t^{2r_1}\) and define a new series.
            Using \(r_1 + r_2 = 1 - P_{-1}\) and \(r_1 - r_2 = N\) (which is our current assumption) we can write \(2r_1 + P_{-1} = N + 1\), which gives us
            \begin{align}
                \frac{u_2(x)}{u_1(x)} &= \int^x t^{-1-N} \sum_{n=0}^{\infty} b_nt^n \dd{t}\\
                &= \int^x \sum_{n=0}^{\infty} b_n t^{n-N-1} \dd{t}\\
                &= \int^x b_Nt^{-1} + t^{-1-N} \sum_{\stackrel{n-0}{n\ne N}}^{\infty} b_nt^n \dd{t}\\
                &= b_N\ln x - x^{-N}\sum_{n-0}^{\infty} c_n x^n
            \end{align}
            where we have taken the last term of the integral and defined a new series.
            Defining \(C(x) = -\sum_{n=0}^{\infty} c_nx^n\) and \(\beta = -N\) this is of the desired form.
        \end{proof}
    \end{thm}
    
    \begin{exm}{}{}
        Consider Bessel's equation,
        \begin{equation}
            x^2y'' + xy' + (x^2 - n^2)y = 0.
        \end{equation}
        There is a regular singular point at \(x = 0\).
        Substituting in
        \begin{equation}
            y(x) = x^\alpha\sum_{m=0}^{\infty} c_mx^m
        \end{equation}
        we get
        \begin{multline}
            0 = \sum_{m=0}^{\infty} [(\alpha + m)(\alpha + m - 1)c_mx^{\alpha + m} + (\alpha + m)c_mx^{\alpha + m}\\
            - n^2 c_mx^{\alpha + m} + c_mx^{\alpha + m + 2}].
        \end{multline}
        The final sum is zero for \(m = 0, 1\).
        Setting the coefficients of \(x^\alpha\) and \(x^{\alpha + 1}\) to zero gives us the indicial equations
        \begin{align}
            c_0[\alpha^2 - n^2] &= 0 \implies c_0 = 0 \text{ or } \alpha = \pm n,\\
            c_1[(\alpha + 1)^2 - n^2] &= 0 \implies c_1 = 0 \text{ or } \alpha = -1 \pm n.
        \end{align}
        We defined the Frobenius ansatz such that \(c_0 \ne \) and therefore \(\alpha = \pm n \ne -1 \pm n\) which means \(c_1 = 0\).
        Alternatively we could demand that \(c_1 \ne 0\), and we would get the same series solution just shifted by 1.
        
        For \(m \ge 2\) we find the recurrence relation
        \begin{equation}
            c_m[(\alpha + m)^2 - n^2] + c_{m-2} = 0.
        \end{equation}
        Since \(c_1 = 0\)  this gives \(c_m = \) for all odd \(m\), for even \(m\) we have
        \begin{equation}
            c_{m+2} = -\frac{c_m}{(m + 2)(m + 2 + 2\alpha)}
        \end{equation}
        where \(\alpha = \pm n\).
        
        If \(n\) is not an integer then we get two independent series solutions corresponding to \(\alpha = \pm n\).
        If \(n\) is an integer then we take \(\alpha = \abs{n}\), otherwise the recurrence relation will diverge for \(m = -2 + 2\abs{n}\).
        We therefore only get one independent solution.
        
        Consider the case of \(n\) being a positive integer.
        We therefore take \(\alpha = n\) and we find that the first few terms in the series are
        \begin{align}
            c_2 &= - \frac{1}{2\cdot 2 (1 + n)}c_0,\\
            c_4 &= -\frac{1}{4\cdot 2(2 + n)}c_2 = \frac{1}{2^4\cdot 2(2 + n)(1 + n)}c_0\\
            c_6 &= -\frac{1}{6\cdot 2(3 + n)}c_4 = -\frac{1}{2^6\cdot 6 (3 + n)(2 + n)(1 + n)}c_0.
        \end{align}
        It doesn't take much to convince us from here that
        \begin{equation}
            c_{2p} = (-1)^p \frac{n!}{2^{2p}p!(n + p)!}c_0.
        \end{equation}
        We therefore have the solution
        \begin{equation}
            y_1(x) = J_n(x) = \sum_{p=0}^{\infty} \frac{(-1)^p}{p!(p + n)!} \left( \frac{x}{n} \right)^{n+2p}
        \end{equation}
        where we chose \(c_0 = 1/(2^nn!)\).
        This equation defines the \define{Bessel functions of the first kind}\index{Bessel function!first kind}.
        
        We can find a second solution, for simplicity we do so only for \(n = 0\).
        We then have
        \begin{equation}
            J_0(x) = 1 - \frac{x^2}{4} + \frac{x^4}{64} - \order(x^6).
        \end{equation}
        We have \(p(x) = 1/x\) and so the Wronskian is
        \begin{equation}
            W(x) = \exp\left[ -\int^x t^{-1} \dd{t} \right] = -\e^{-\ln x} = \frac{1}{x}.
        \end{equation}
        We therefore have
        \begin{align}
            y_2(x) &= y_1(x) \int^x \frac{1}{t}\frac{1}{y_1^2(t)} \dd{t}\\
            &= J_0(x) \int^x \frac{1}{t}\left[ 1 - \frac{t^2}{4} + \frac{t^4}{64} + \dotsb \right]^{-2}\dd{t}\\
            &= J_0(x) \int^x \frac{1}{t} \left[ 1 = \frac{t^2}{2} + \frac{5t^4}{32} + \dotsb \right] \dd{t}\\
            &= J_0(x) \left[ \ln x + \frac{x^2}{4} + \frac{5x^{4}}{128} + \dotsb \right].
        \end{align}
    \end{exm}
    
    \section{Expansion About an Irregular Singular Point}
    In general there is no exact solution about an irregular, or essential, singular point.
    There is, however, a common method for finding an approximate solution.
    
    \subsection{Normal Form}
    Consider a second order ODE of the form
    \begin{equation}
        u'' + p_1u' + p_0u = 0.
    \end{equation}
    Consider the change of variables \(u = vw\).
    We have \(u' = v'w + vw'\) and \(u'' = v''w + 2v'w' + vw''\).
    Now assuming that \(v \ne 0\), which is an acceptable assumption as \(v = 0\) implies \(u = 0\), which is not an interesting solution, we can write the differential equation as
    \begin{equation}
        w'' + \left( 2\frac{v'}{v} + p_1 \right)w' + \left( \frac{v''}{v} + p_1\frac{v'}{v} + p_0 \right)w = 0.
    \end{equation}
    
    Choosing
    \begin{equation}
        v(x) = \sqrt{W(x)} = \exp\left[ -\frac{1}{2}\int^x p_1(x')\dd{x'} \right]
    \end{equation}
    we see that \(2v'/v + p_1 = 0\) and so our differential equation becomes
    \begin{equation}
        w''(x) + Q(x)w(x) = 0
    \end{equation}
    with
    \begin{equation}
        Q(x) = p_0(x) - \frac{1}{2}p_1'(x) - \frac{1}{4}p_1^2(x).
    \end{equation}
    This is called the \defineindex{normal form} of the differential equation, also sometimes called the \define{Schr\"odinger form}\index{Schrodinger form@Schr\"odinger form} for the similarity with the time independent Schr\"odinger equation,
    \begin{equation}
        \psi'' + \frac{2m}{\hbar^2}(E - V(x))\psi = 0.
    \end{equation}
    
    \section{WKB Method}
    The Wentzel--Kramers--Brillouin method, commonly known as the WKB\glossary[acronym]{WKB}{Wentzel--Kramers--Brillouin} method, or alternatively as the JWKB or WKBJ method, with J standing for Jeffreys, or as the Liouville-Green method, is a way of approximating a solution for a differential equation about an irregular point.
    
    We start by writing the differential equation,
    \begin{equation}
        u''(z) + p_1(z)u'(z) + p_0(z)u(z) = 0
    \end{equation}
    in its normal form,
    \begin{equation}
        w''(z) + Q(z)w(z) = 0.
    \end{equation}
    where
    \begin{equation}
        u(z) = w(z)\exp\left[ -\frac{1}{2}\int^z p_1(t) \dd{t} \right],
    \end{equation}
    and
    \begin{equation}
        Q(z) = p_(z) - \frac{1}{2}p_1'(z) - \frac{1}{4}p_1^2(z).
    \end{equation}
    
    We look for a solution of the form
    \begin{equation}
        w(z) = a(z) \e^{i\Theta(z)}.
    \end{equation}
    
    Suppose that \(Q\) is constant.
    Then \(a\) will be constant and \(\Theta(z) = \pm \sqrt{Q}z\).
    
    Suppose instead that \(Q\) is slowly varying.
    Then \(a\) will also be slowly varying.
    We then have
    \begin{equation}
        w' = [a' + ia\Theta']\e^{i\Theta}, \qqand w'' = [a'' + 2ia'\Theta' + ia\Theta'' - a\Theta'^2]\e^{i\Theta}.
    \end{equation}
    Substituting this into our differential equation we have
    \begin{equation}
        [a'' + 2ia'\Theta' + ia\Theta'' + (Q - \Theta'^2)a]\e^{i\Theta} = 0.
    \end{equation}
    If we impose the conditions that \(Q - \Theta'^2 = 0\) and \(a'' + 2ia'\Theta' + ia\Theta'' = 0\) then for \(Q > 0\) we have
    \begin{equation}
        \Theta(z) = \pm \int^z \sqrt{Q(t)}\dd{t}.
    \end{equation}
    Since \(a\) is slowly varying we have \(a'' \approx 0\) and so we have
    \begin{equation}
        2a'\Theta' + a\Theta' = 0 \implies a^2\Theta' = \text{constant},
    \end{equation}
    which follows by differentiating \(a^2\Theta'\) and dividing through by \(a\).
    From this we have
    \begin{equation}
        a(z) \propto \frac{1}{\sqrt{\Theta'}} \propto Q^{1/4}.
    \end{equation}
    Since the integral of \(\sqrt{Q}\) will be proportional to \(1/\sqrt{Q}\).
    We therefore have
    \begin{equation}
        w(z) \approx Q^{-1/4} \left( A \exp\left[ i\int^z \sqrt{Q(t)} \dd{t} \right] + B \exp\left[ -i\int^z \sqrt{Q(t)} \dd{t} \right] \right)
    \end{equation}
    for constants \(A\) and \(B\).
    
    If \(Q < 0\) then we can derive a similar result using \(\e^{\Theta(z)}\) instead of \(\e^{i\Theta(z)}\).
    
    The WKB method only works if we have an essential singularity at infinity.
    In general as \(z \to \infty\) there are three cases of asymptotic behaviour.
    Suppose that \(Q \to k\) then if \(k = 0\) the solution goes as \(w(z) = Az + B\), since \(w'' = 0\) in this case.
    If \(k > 0\) then
    \begin{equation}
        w(z) = A\sin(\sqrt{k}z) + B\cos(\sqrt{k}z)
    \end{equation}
    which is oscillating.
    If \(k < 0\) then
    \begin{equation}
        w(z) = A\e^{\sqrt{\abs{k}}z} + B\e^{-\sqrt{\abs{k}}z}.
    \end{equation}
    
    \begin{exm}{}{}
        Consider Bessel's equation,
        \begin{equation}
            y''(x) + \frac{1}{x}y'(x) + \left( 1 - \frac{n^2}{x^2} \right)y(x) = 0.
        \end{equation}
        This has an essential singularity at infinity.
        It can be shown that by setting \(y = u/\sqrt{x}\) we get the normal form
        \begin{equation}
            w''(x) + \left[ 1 - \left( \frac{n^2}{x^2} - \frac{1}{4x^2} \right) \right]w(x) = 0.
        \end{equation}
        From which we see
        \begin{equation}
            Q(x) = 1 - \left( \frac{n^2}{x^2} - \frac{1}{4x^2} \right).
        \end{equation}
        We make the ansatz that \(w(x) = u(x)\e^{i\Theta(x)}\) with
        \begin{align}
            \Theta(x) &= \pm \int^x \sqrt{Q(t)}\dd{t}\\
            &= \pm \int^x \left( 1 + \frac{1/4 - n^2}{t^2} \right)^{1/2} \dd{t}.
        \end{align}
        We then have
        \begin{equation}
            u(x) = \frac{1}{\sqrt{x}} \left( 1 + \frac{1/4 - n^2}{x^2} \right)^{-1/4}(A\e^{ix} + B\e^{-ix})
        \end{equation}
        where we have used the fact that as \(x \to \infty\) we have \(Q \to 1 > 0\) and so we have oscillating behaviour.
        The asymptotic behaviour is then
        \begin{equation}
            u(x) \sim \frac{1}{\sqrt{x}} (A\e^{ix} + B\e^{-ix}).
        \end{equation}
    \end{exm}
    
    \chapter{Special Functions}
    \epigraph{\enquote{It can easily be shown}. LIARS! It's not easy at all.}{Kristel Torokoff}
    In this chapter we will discuss a few special functions that often appear, usually as the solution to various differential equations.
    We will start with Legendre's polynomials to develop our methods and then briefly cover other special functions.
    
    \section{Legendre Polynomials}
    \epigraph{For once there is logic.}{Kristel Torokoff}
    \epigraph{The first step includes magic.}{Kristel Torokoff}
    Legendre's differential equation is
    \begin{equation}
        (1 - x^2)u'' + 2xu' + n(n + 1)u = 0.
    \end{equation}
    We saw in \cref{sec:Anzatz (Taylor's Version)} that about the ordinary point \(x = 0\) the solution has a series expansion \(u(x) = \sum_{m=0}^{\infty} c_mx^m\) where
    \begin{equation}
        c_{m+2} = \frac{(m - n)(m + n - 1)}{(m + 2)(m + 1)}c_m.
    \end{equation}
    
    The Legendre polynomials are the polynomial solutions to Legendre's differential equation for positive integer \(n\).
    Recall that the series splits into two, odd and even series, and that if \(n\) is an even (odd) positive integer then the even (odd) series terminates at some finite power giving a polynomial.
    We denote the resulting polynomial by \(P_n\)\index{\(P_n\)|see{Legendre polynomial}}.
    
    It can be shown that
    \begin{equation}
        P_n(x) = \sum_{r=0}^{\lfloor n/2 \rfloor} \frac{(-1)^r}{2^nr!(n - r)!} \frac{(2n - 2r)!}{(n - 2r)!} x^{n - 2r}
    \end{equation}
    where \(\lfloor - \rfloor\) denotes the floor, or integer part, so, for example, \(\lfloor 3.5 \rfloor = \lfloor 3 \rfloor = \lfloor \pi \rfloor = 3\).
    
    \subsection{Rodriguez Formula}
    \epigraph{The only two tricks in mathematics, multiplying by 1 and adding 0.}{Kristel Torokoff}
    Consider the derivative
    \begin{align}
        \diff*[n]{x^{2n - 2r}}{x} &= \diff*[n-1]{(2n - 2r)x^{2n-2r-1}}{x}\\
        &= \diff*[n-2]{(2n - 2r - 1)(2n - 2r)x^{2n - 2r - 2}}{x}\\
        &\vdotswithin{=} \\
        &= (2n - 2r)(2n - 2r - 1) \dotsm (2n - 2r - (n - 1))x^{2n - 2r - n}\\
        &= \frac{(2n - 2r)!}{(n - 2r)!}x^{n - 2r}.
    \end{align}
    We can identify this as the last two factors in the definition of \(P_n\).
    Hence,
    \begin{equation}
        P_n(x) = \sum_{r=0}^{\lfloor n/2 \rfloor} \frac{(-1)^{r}}{2^nr!(n - r)!} \diff*[n]{x^{2n - 2r}}{x}.
    \end{equation}

    Suppose we consider some \(r > n/2\), then \(2r > n\) and hence \(2n - 2r > 2n - n = n\), hence
    \begin{equation}
        \diff*[n]{x^{2r - 2n}}{x} = 0
    \end{equation}
    for these values of \(r > n/2\), since at some point we differentiate away all powers of \(x\).
    This allows us to extend the sum upwards since this is just adding zero, thus
    \begin{equation}
        P_n(x) = \sum_{r=0}^{n} \frac{(-1)^r}{2^nr!(n - r)!} \diff*[n]{x^{2n - 2r}}{x}.
    \end{equation}
    Multiplying through by \(n!/n!\) and also moving the derivative and other terms not depending on \(r\) outside of the sum we have
    \begin{align}
        P_n(x) &= \frac{1}{2^nn!} \diff[n]{}{x} \sum_{r=0}^{n} \frac{n!}{r!(n - r)!} (-1)^r (x^2)^{n - r}\\
        &= \frac{1}{2n!} \diff[n]{}{x} \sum_{r=0}^{n} \binom{n}{r} (-1)^{r}(x^2)^{n - r}
    \end{align}
    which we can now identify as a binomial expansion of \((x^2 - 1)^n\), so
    \begin{equation}\label{eqn:rodrigues formula legendre polynomials}
        P_n(x) = \frac{1}{2^nn!} \diff[n]{}{x}(x^2 - 1)^n.
    \end{equation}
    This is \defineindex{Rodrigues Formula} for Legendre polynomials.
    Many other special functions have a formula also known as Rodrigues formula and derived in a similar way.
    
    \subsection{Integral Representations}
    Using Cauchy's integral formula,
    \begin{equation}
        \diff[n]{f}{z^n}[z=z_0] = \frac{n!}{2\pi i} \oint_C \frac{f(z)}{(z - z_0)^{n+1}} \dd{z}
    \end{equation}
    for some closed contour, \(C\), going once anticlockwise about \(z = z_0\), we can easily derive an integral representation of the Legendre polynomials by replacing the derivative in Rodrigues formula with an integral:
    \begin{equation}
        P_n(x) = \frac{1}{2^{n+1}\pi i} \oint_C \frac{(t^2 - 1)^n}{(t - x)^{n+1}} \dd{t}.
    \end{equation}
    This is called \define{Schl\"afli's integral representation}\index{Schlafli's integral representation@Schl\"afli's integral representation} of the Legendre polynomials.
    
    We showed from this in a tutorial that the large \(n\) behaviour of \(P_n\) is given by
    \begin{equation}
        P_n(\cos\alpha) \sim \sqrt{\frac{2}{\pi n\sin\alpha}} \sin\left( n\alpha + \frac{\alpha}{2} + \frac{\pi}{4} \right).
    \end{equation}
    
    We can obtain a second integral representation by taking the contour in Schl\"afli's integral representation to be a circle centred on \(z\) with radius \(\abs{\sqrt{z^2 - 1}}\).
    This can be parametrised as
    \begin{equation}
        t = z + \sqrt{z^2 - 1}\e^{i\varphi}, \qquad \text{with} \qquad \varphi \in [0, 2\pi].
    \end{equation}
    Using this we find the integral representation
    \begin{equation}
        P_n(z) = \frac{1}{2\pi} \int_{0}^{2\pi} (z + \sqrt{z^2 - 1}\cos\varphi)^n \dd{\varphi}.
    \end{equation}
    This is known as \defineindex{Laplace's integral representation} of the Legendre polynomials.
    
    In general there are many integral representations, which we choose to use depends on the problem we wish to solve.
    
    \subsection{Generating Function}
    In this section we introduce the notion of a generating function, which can be useful for finding recursion relations for special functions.
    Let
    \begin{equation}
        F(x, h) = \sum_{m=0}^{\infty} h^m P_m(x)
    \end{equation}
    where \(P_m\) are the Legendre polynomials.
    From this we see that we have
    \begin{equation}
        P_m(x) = \frac{1}{m!} \diffp*[m]{F(x, h)}{h}\bigg\vert_{h=0}
    \end{equation}
    since lower order terms are differentiated away and higher order terms have a factor of \(h\) still which is set to zero.
    We call \(F\) a \defineindex{generating function} for the Legendre polynomials.
    We can compute \(F\) using the integral representation of \(P_n\):
    \begin{align}
        F(x, h) &= \sum_{n=0}^{\infty} h^n \frac{1}{2^{n+1}\pi i} \oint_C \frac{(t^2 - 1)^n}{(t - x)^n} \dd{t}\\
        &= \frac{1}{2\pi i} \oint_C \sum_{m=0}^{\infty}  \left( \frac{h(t^2 - 1)}{2(t - x)} \right)^m \frac{1}{t - x} \dd{t}.
    \end{align}
    We can identify this as a geometric series with common ratio
    \begin{equation}
        \frac{h(t^2 - 1)}{2(t - x)}
    \end{equation}
    which means that the sum gives
    \begin{equation}
        \left( 1 - \frac{h(t^2 - 1)}{2(t - x)} \right)^{-1}.
    \end{equation}
    Simplifying this and substituting it back in we have
    \begin{align}
        F(x, h) &= -\frac{1}{i\pi h} \oint_C \frac{1}{t^2 - 2t/h - (1 - 2x/h)} \dd{t}\\
        &= -\frac{1}{i\pi h} \oint_C \frac{1}{(t - t_+)(t - t_-)} \dd{t}
    \end{align}
    where \(t_{\pm}\) are the roots of the quadratic in the denominator, that is
    \begin{equation}
        t_{\pm} = \frac{1}{h}\left( 1 \pm \sqrt{1 + h^2 - 2xh} \right).
    \end{equation}

    Notice that when \(h \to 0\) we have \(t_+ \to \infty\) and \(t_-\) becomes indeterminate.
    Applying L'H\^opital's rule we get \(t_- \to x\).
    Only \(t_-\) is within the contour then since we assume it is at some finite distance from the origin.
    Applying the residue theorem we can evaluate the integral as
    \begin{align}
        F(x, h) &= 2\pi i\Res\left( -\frac{1}{\pi h}\frac{1}{(t - t_+)(t - t_-)}, t_- \right)\\
        &= -\frac{2}{h} \frac{1}{t_{-} - t_{+}} = \frac{1}{\sqrt{1 - 2xh + h^2}}.\label{eqn:generating func closed form}
    \end{align}
    This gives us a closed form for \(F\), which we can differentiate to get \(P_m\).
    
    We can also use the closed form to derive a recurrence relation for Legendre polynomials.
    In particular notice that we have
    \begin{equation}
        \diffp{F}{h} = \frac{x - h}{(1 - 2xh + h^2)^{3/2}} = \frac{x - h}{1 - 2xh + h^2}F(x, h)
    \end{equation}
    so we can write
    \begin{equation}\label{eqn:deriving legendre polynomial recurrence relation}
        (1 - 2hx + h^2)\diffp{F}{h} = (x - h)F(x, h).
    \end{equation}
    We can then consider the derivative of the original definition of the generating function, which gives
    \begin{equation}
        \diffp{F}{h} = \sum_{m=1}^{\infty} P_m(x)mh^{m-1}.
    \end{equation}
    Substituting this into \cref{eqn:deriving legendre polynomial recurrence relation} and we get
    \begin{equation}
        \sum_{m=0}^{\infty} P_m m h^{m - 1} - 2x \sum_{m=0}^{\infty} P_mmh^m + \sum_{m=0}^{\infty} P_mmh^{m + 1} = x\sum_{m=0}^{\infty} P_mh^m - \sum_{m=0}^{\infty} P_mh^{m+1}.
    \end{equation}
    Now we re-index so that all \(h\) are to the same power.
    For the first term we re-index to \(m' = m - 1\), and for the third and last terms to \(m' = m + 1\).
    Dropping the primes this gives
    \begin{multline}
        \sum_{m=-1}^{\infty} P_{m+1}(m + 1) h^m - 2x \sum_{m=0}^{\infty} P_mmh^m + \sum_{m=1}^{\infty} P_{m-1} (m - 1)h^m\\
        = x\sum_{m=1}^{\infty} P_mh^m - \sum_{m=1}^{\infty} P_{m-1}h^m.
    \end{multline}
    Equating coefficients of \(h^m\) we see that
    \begin{equation}
        (m + 1)P_{m+1}(x) - x(2m + 1)P_m(x) + mP_{m-1}(x) = 0.
    \end{equation}
    This is true for positive integer \(m\).
    
    If instead we had considered the derivatives \(\diffp{F}/{x}\) then we would have found the recurrence relation
    \begin{equation}\label{eqn:legendre recursion formula}
        P_m(x) = P_{m+1}'(x) - 2xP_m'(x) + P_{m+2}'(x).
    \end{equation}

    \subsubsection{The Generating Function in Electromagnetism}
    The generating function for the Legendre polynomials appears in electromagnetism when we expand the Coulomb potential, \(V = Q/(4\pi\varepsilon_0r)\), where \(r = \abs{\vv{r_1} - \vv{r_2}}\) is the distance between the point at which we are calculating the field and the position of the charge, \(Q\).
    Without loss of generality assume \(r_1 > r_2\), also, let \(\vartheta\) be the angle between \(\vv{r_1}\) and \(\vv{r_2}\), and then
    \begin{align}
        \frac{1}{\abs{\vv{r_1} - \vv{r_2}}} &= [(\vv{r_1} - \vv{r_2}) \cdot (\vv{r_1} - \vv{r_2})]^{-1/2}\\
        &= (r_1^2 - 2\vv{r_1} \cdot \vv{r_2} + r_2^2)^{-1/2}\\
        &= (r_1^2 - 2r_1r_2\cos\vartheta + r_2^2)^{-1/2}\\
        &= \left[ r_1^2 \left( 1 - 2\frac{r_2}{r_1} + \frac{r_2^2}{r_1^2} \right) \right]^{-1/2}\\
        &= \frac{1}{r_1} \left[ 1 - 2\frac{r_2}{r_1}\cos\vartheta + \left( \frac{r_2}{r_1} \right)^2 \right]^{-1/2}\\
        &= \frac{1}{r_1} F\left( \cos\vartheta, \frac{r_2}{r_1} \right)\\
        &= \frac{1}{r_1} \sum_{n=0}^{\infty} P_n(\cos\vartheta) \left( \frac{r_2}{r_1} \right)^{n}.
    \end{align}
    Where in the penultimate step we have identified the closed form of the generating function from \cref{eqn:generating func closed form} with \(h = r_2/r_1\) and \(x = \cos\vartheta\).
    
    \subsection{Orthogonality and Completeness}
    \subsubsection{Orthogonality}
    The Legendre polynomials are \defineindex{orthogonal}.
    By this we mean that there is an inner product, \(\innerprod{-}{-}\), with respect to which
    \begin{equation}
        \innerprod{P_n}{P_m} \propto \delta_{nm}.
    \end{equation}
    In particular the inner product space is \(L^2([-1, 1])\), the space of square integrable functions on the interval \([-1, 1]\), and the inner product is
    \begin{equation}
        \innerprod{f}{g} \coloneqq \int_{-1}^{1} f(x)g(x) \dd{x}.
    \end{equation}
    
    To show that the Legendre polynomials are orthogonal with respect to this inner product we use the Sturm--Liouville form of the Legendre equation:
    \begin{equation}
        \diff{}{x} [(1 - x^2)P_{l}'(x)] + l(l + 1)P_l(x) = 0.
    \end{equation}
    Expanding this we get
    \begin{equation}
        -2xP_l'(x) + (1 - x^2)P_l''(x) + l(l + 1)P_l(x) = 0.
    \end{equation}
    This is the standard form of Legendre's equation, showing that this is the correct Sturm--Liouville form.
    Multiplying through by \(P_m\) we get
    \begin{equation}
        P_m(x)\diff{}{x} [(1 - x^2)P_l'(x)] + l(l + 1)P_l(x)P_m(x) = 0.
    \end{equation}
    Similarly, reversing the indices, we have
    \begin{equation}
        P_l(x)\diff{}{x} [(1 - x^2)P_m'(x)] + m(m + 1)P_m(x)P_l(x) = 0.
    \end{equation}
    Subtracting this from the previous equation we get
    \begin{multline}
        P_m\diff{}{x} [(1 - x^2)P_l'] - P_l\diff{}{x} [(1 - x^2)P_m'] + [l(l + 1) - m(m + 1)]P_mP_l\\
        = \diff{}{x} [(1 - x^2)(P_mP_l' - P_lP_m')] + [l(l + 1) - m(m + 1)]P_mP_l = 0.
    \end{multline}
    Integrating this equation from \(-1\) to \(1\) we get
    \begin{align}
        I &= \int_{-1}^{1} \diff{}{x} [(1 - x^2)(P_mP_l' - P_lP_m')] \dd{x}\notag\\
        &\qquad\qquad+ [l(l + 1) - m(m + 1)] \int_{-1}^{1} P_m P_l \dd{x}\\
        &= [(1 - x^2)(P_mP_l' - P_lP_m')]_{-1}^{1} + [l(l + 1) - m(m + 1)] \innerprod{P_m}{P_l}\\
        &= [l(l + 1) - m(m + 1)] \innerprod{P_m}{P_l},
    \end{align}
    since \(1 - x^2 = 0\) at both ends of the integration range.
    It then follows that
    \begin{equation}
        \innerprod{P_m}{P_l} = 0
    \end{equation}
    as long as \([l(l + 1) - m(m + 1)] \ne 0\), which means \(m \ne l\).
    
    Let \(f_m\) be a polynomial of degree \(m\), such that \(f_m(x) = a_0 + a_1x + \dotsb a_mx^m\).
    Then taking \(m < n\), integrating by parts \(m\) times, and using Rodrigues formula for the Legendre polynomials (\cref{eqn:rodrigues formula legendre polynomials}) we have
    \begin{align}\label{eqn:inner prod polynomial and legendre polynomial}
        \innerprod{f_m}{P_n} &= \int_{-1}^{1} f_m(x) P_n(x)\dd{x}\\
        &= \frac{1}{2^nn!} \int_{-1}^{1} f_m(x) \diff[n]{}{x}(x^2 - 1)^n \dd{x}\\
        &= \frac{1}{2^nn!} \overbrace{\left[ f_m(x) \diff[n-1]{}{x}(x^2 - 1)^n \right]_{-1}^{1}}^{=0}\\
        &\qquad\qquad- \frac{1}{2^nn!} \int_{-1}^{1} f_m'(x) \diff[n - 1]{}{x} (x^2 - 1)^n \dd{x}\\
        &\vdotswithin{=}\\
        &= (-1)^m \frac{a_mm!}{2^nn!} \int_{-1}^{1} \diff[n-m]{}{x}(x^2 - 1)^n \dd{x}\\
        &= (-1)^m \frac{a_mm!}{2^nn!} \left[ \diff[n-m-1]{}{x} (x^2 - 1)^n \right]_{-1}^{1}\\
        &= 0,
    \end{align}
    here we have used the fact that
    \begin{equation}
        \diff*[n - k]{(x^2 - 1)^n}{x}[n=\pm 1] = 0.
    \end{equation}
    
    If we consider again consider \cref{eqn:inner prod polynomial and legendre polynomial}, but now with \(m = n\), then \(n - m = 0\) and we instead have
    \begin{equation}
        \innerprod{f_n}{P_n} = (-1)^n \frac{a_nn!}{2^nn!} \int_{-1}^{1} (x^2 - 1)^n \dd{x}.
    \end{equation}
    We can compute this integral, first noticing that the integrand is even, and then we have
    \begin{equation}\label{eqn:inner prod fn Pn}
        \innerprod{f_n}{P_n} = 2(-1)^n \frac{a_n}{2^n} \int_{0}^1 (x^2 - 1) \dd{x} = (-1)^{n}\frac{a_n}{2^{n-1}} I_n.
    \end{equation}
    Here we define
    \begin{align}
        I_n &\coloneqq \int_0^1 (x^2 - 1)^n \dd{x}\\
        &\hphantom{:}= [x(x^2 - 1)^n]_{0}^{1} - 2n\int_0^1 x^2(x^2 - 1)^{n-1} \dd{x}\\
        &\hphantom{:}= -2n\int_{0}^{1} \left[ (x^2 - 1)^n + (x^2 - 1)^{n-1} \right] \dd{x}\\
        &\hphantom{:}= -2n(I_n - I_{n-1}).
    \end{align}
    To get this we integrated by parts, using
    \begin{alignat}{2}
        u &= (x^2 - 1)^{n-1},     \qquad && \hphantom{'}v= x,\\
        u' &= 2nx(x^2 - 1)^{n-1}, \qquad && v'= 1.
    \end{alignat}
    It follows from this that
    \begin{equation}
        I_n = -\frac{2n}{2n + 1} I_{n-1}.
    \end{equation}
    Using this, and the easily calculated \(I_0 = 1\), we get
    \begin{align}
        I_n &= -\frac{2n}{2n + 1}I_{n - 1}\\
        &= -\frac{2n}{2n + 1}\left[ -\frac{2(n-1)}{2(n - 1) + 1}I_{n-2} \right]\\
        &= \frac{2n\cdot 2(n-1)}{(2n+1)(2n-1)}I_{n-2}\\
        &= \frac{2n\cdot 2(n-1)}{(2n+1)(2n-1)}\left[ -\frac{2(n-2)}{2(n-2) + 1}I_{n-3} \right]\\
        &= -\frac{2n\cdot 2(n-1) \cdot 2(n-2)}{(2n+1)(2n-1)(2n-3)}I_{n-3}\\
        &= (-1)^{n} \frac{2n \cdot 2(n - 1) \cdot 2(n - 2) \dotsm 4 \dotsm 2}{(2n + 1)(2n - 1)(2n - 3) \dotsm 5\cdot 3}\\
        &= (-1)^n \frac{2 \cdot 4 \cdot 6 \dotsm 2n}{3 \cdot 5 \cdot 7 \dotsm (2n + 1)}\\
        &= (-1)^n \frac{[2 \cdot 4 \cdot 6 \dotsm 2n]\cancel{[2\cdot 4\cdot 6 \dotsm 2n]}}{\cancel{2}\cdot 3 \cdot \cancel{4} \cdot 5 \cdot \cancel{6} \cdot 7 \dotsm \cancel{2n} \cdot (2n + 1)}\\
        &= (-1)^n \frac{[2 \cdot 4 \cdot 6 \dotsm 2n]^2}{(2n + 1)!!}\\
        &= (-1)^n \frac{2^{2n}(n!)^2}{(2n+1)!!}.
    \end{align}
    Now, if \(f_n\), is also the Legendre polynomial \(P_n\) we have
    \begin{equation}
        a_n = \frac{(2n)!}{2^n(n!)^2}.
    \end{equation}
    This follows from Rodrigues formula (\cref{eqn:rodrigues formula legendre polynomials}), and the binomial expansion:
    \begin{align}
        P_n(x) &= \frac{1}{2^nn!} \diff[n]{}{x} (1 - x^2)^n\\
        &= \frac{1}{2^nn!} \diff[n]{}{x} \sum_{j=0}^{n} \binom{n}{j}x^{2(n - j)}(-1)^{j}\\
        &= \frac{1}{2^nn!} \diff[n-1]{}{x} \sum_{j=0}^{n} (2n - 2j)x^{2(n-j)-1}(-1)^{j}\\
        &\vdotswithin{=}\\
        &= \frac{1}{2^nn!} \sum_{j=0}^{n} \binom{n}{j} (2n - 2j)(2n - 2j - 1) \dotsm (2n - 2j - n + 1) x^{n - 2j}(-1)^j.
    \end{align}
    Considering the \(j = 0\) case we get the coefficient of \(x^n\) in \(P_n\), which is
    \begin{multline}
        \frac{1}{2^nn!} \binom{n}{0}2n(2n-1)(2n-2) \dotsm (2n-n+1)(-1)^0 \\
        = \frac{1}{2^nn!} \frac{n!}{(n-0)!0!} \frac{(2n)!}{n!}
        = \frac{(2n)!}{2^n(n!)^2}.
    \end{multline}
    
    Finally, putting this in to \cref{eqn:inner prod fn Pn} we get
    \begin{equation}
        \innerprod{P_n}{P_n} = \frac{(-1)^n}{2^{n-1}}a_nI_n = \frac{(-1)^n}{2^{n-1}} \frac{(2n)!}{2^n(n!)^2} (-1)^n \frac{2^{2n}n!}{(2n+1)!} = \frac{2}{2n+1}.
    \end{equation}
    Hence we have
    \begin{equation}
        \innerprod{P_n}{P_m} = \frac{2}{2n+1} \delta_{mn}.
    \end{equation}
    So, the Legendre polynomials are orthogonal.
    
    \subsubsection{Completeness}
    The Legendre polynomials are \define{complete}\index{completeness}.
    By this we mean that we can write any (sufficiently nice) function in \(L^2([-1, 1])\) as a series of Legendre polynomials, that is for \(f \in L^2([-1, 1])\) we have
    \begin{equation}
        f(x) = \sum_{l=0}^{\infty} c_lP_l(x)
    \end{equation}
    for some \(c_l \in \reals\).
    To determine \(c_l\) we simply consider the inner product
    \begin{align}
        \innerprod{f}{P_m} &= \int_{-1}^{1} f(x)P_m(x)\dd{x}\\
        &= \int_{-1}^{1} \sum_{l=0}^{\infty} c_lP_m(x)P_l(x)\dd{x}\\
        &= \sum_{l=0}^{\infty} c_l \int_{-1}^{1} P_m(x)P_l(x)\dd{x}\\
        &= \sum_{l=0}^{\infty} c_l\frac{2}{2l + 1}\delta_{ml}\\
        &= c_m\frac{2}{2m + 1}.
    \end{align}
    The ability to swap the sum and integral is where the \enquote{sufficiently nice} nature of \(f\) comes in.
    So,
    \begin{equation}
        c_m = \frac{2m + 1}{2}\innerprod{f}{P_m} = \left( m + \frac{1}{2} \right) \int_{-1}^{1} f(x)P_m(x) \dd{x},
    \end{equation}
    and
    \begin{equation}
        f(x) = \sum_{l=0}^{\infty} \left( l + \frac{1}{2} \right) \left( \int_{-1}^{1} f(t) P_m(t) \dd{t} \right) P_l(x).
    \end{equation}

    \section{Bessel Functions}
    Bessel's differential equation is
    \begin{equation}
        x^2u'' + xu' + (x^2 - \nu^2)u = 0
    \end{equation}
    for some \(\nu \in \reals\).
    This often appears as the radial part of Laplace's equation in cylindrical coordinates.
    The origin is a regular singular point for Bessel's differential equation.
    One solution to Bessel's differential equation are the \define{Bessel functions of the first kind}\index{Bessel function!first kind}, which are given by
    \begin{equation}
        J_\nu(x) = \sum_{n=0}^{\infty} \frac{(-1)^n}{n!\Gamma(n + 1 + \nu)} \left( \frac{x}{2} \right)^{2n + \nu}
    \end{equation}
    for \(\abs{\nu} \ne 1/2\).
    
    Bessel's differential equation is invariant under \(\nu \to -\nu\), and hence \(J_{-\nu}\) is also a solution, for \(\nu \notin \integers\).
    \begin{equation}
        J_{-\nu}(x) = \sum_{n = 0}^{\infty} \frac{(-1)^n}{n!\Gamma(n + 1 - \nu)} \left( \frac{x}{2} \right)^{2n - \nu}.
    \end{equation}
    These are linearly independent for non-integer \(\nu\).
    It can be shown that the Wronskian is
    \begin{equation}
        W[J_\nu, J_{-\nu}] = -\frac{2\sin(n\pi)}{\pi x}.
    \end{equation}
    
    For \(\nu = m \in \integers\) we have \(J_{-m} = (-1)^mJ_m\), and so clearly \(J_m\) and \(J_{-m}\) are linearly dependent.
    In this case we define \define{Bessel's functions of the second kind}\index{Bessel function!second kind}, also known as Neumann's functions, \(Y_n\).
    
    \subsection{Recursion Relation}
    Consider the following derivative
    \begin{align}
        \diff{}{x}(x^\nu J_\nu(x)) &= \diff{}{x} \left[ \sum_{n=0}^{\infty} \frac{(-1)^{n} x^{2\nu+2n}}{n!\Gamma(n + 1 + \nu)2^{2n+\nu}} \right]\\
        &= \sum_{n=0}^{\infty} \frac{(-1)^n(2\nu + 2n)x^{2n+2\nu-1}}{n!\Gamma(n+1+2)2^{2n+\nu}}\\
        &= \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n + 2\nu - 1}}{n! 2^{2n+\nu-1}\Gamma(n+\nu)}\\
        &= x^\nu J_{\nu-1}(x).
    \end{align}
    Here we have used
    \begin{equation}
        \frac{2n + 2\nu}{\Gamma(n + \nu + 1)} = 2\frac{n + \nu}{\Gamma(n + \nu + 1)} = 2\frac{n + \nu}{(n + \nu)\Gamma(n + \nu)} = \frac{2}{\Gamma(n + \nu)}
    \end{equation}
    which uses the recursive nature of the Gamma function: \(\Gamma(z + 1) = z\Gamma(z)\).
    
    We can also compute a similar derivative in a similar way:
    \begin{equation}
        \diff{}{x} (x^-\nu J_\nu(x)) = -x^{-\nu} J_{\nu+1}.
    \end{equation}
    
    We can also compute both of these integrals using the product rule to separate the \(x^{\pm \nu}\) and \(J_{\nu}\) terms.
    Doing so we get
    \begin{align}
        \diff{}{x} (x^{\nu} J_\nu) &= \nu x^{\nu - 1} J_\nu + x^\nu J_\nu',\\
        \diff{}{x} (x^{-\nu} J_\nu) &= -\nu x^{-\nu-1}J_\nu + x^{-\nu}J_\nu'.
    \end{align}
    
    Comparing the two ways of computing these derivatives we have
    \begin{align}
        x J_{\nu - 1} &= x J_\nu' + \nu J_\nu,\\
        -x J_{\nu + 1} &= x J_\nu' - \nu J_\nu.
    \end{align}
    Adding these and taking their difference gives us two recursion relations:
    \begin{align}
        J_{\nu-1}(x) + J_{\nu + 1}(x) &= \frac{2\nu}{x} J_\nu(x),\\
        J_{\nu-1}(x) - J_{\nu + 1}(x) &= 2J_\nu'(x).
    \end{align}
    
    \subsection{Generating Function}
    We can define a generating function by
    \begin{equation}
        F(x, h) \coloneqq \sum_{n = -\infty}^{\infty} J_n(x) h^n.
    \end{equation}
    Notice that we have to sum over all possible integer \(n\), hence we need to include negative integers as well.
    Now take the recurrence relation
    \begin{equation}
        J_{n-1} + J_{n+1} = \frac{2n}{x}J_n,
    \end{equation}
    multiply by \(h^n\), and then sum over \(n\) to get
    \begin{equation}\label{eqn:deriving bessel recurrence relation}
        \sum_{n}h^nJ_{n-1} + \sum_{n}h^nJ_{n+1} = \sum_{n}\frac{2n}{x}J_nh^n.
    \end{equation}
    Considering the first term we have
    \begin{equation}
        \sum_{n=-\infty}^{\infty} h^nJ_{n-1} = \sum_{n'=-\infty}^{\infty} h^{n'+1} J_{n} = h\sum_{n'=-\infty}^{\infty} h^{n'}J_{n'} = hF(x, h).
    \end{equation}
    Here we re-indexed to \(n' = n - 1\).
    Similarly for the second term we have
    \begin{equation}
        \sum_{n=-\infty}^{\infty} h^nJ_{n+1} = \frac{1}{h} \sum_{n'=-\infty}^{\infty} h^{n'}J_{n'} = \frac{1}{h}F(x, h).
    \end{equation}
    Finally for the third term we have
    \begin{equation}
        \diffp{F}{h} = \sum_n nh^{n-1}J_n.
    \end{equation}
    Therefore we can rewrite \cref{eqn:deriving bessel recurrence relation} as
    \begin{equation}
        hF + \frac{1}{h}F = \frac{2h}{x}\diffp{F}{h}.
    \end{equation}
    This is a separable differential equation since we can write it as
    \begin{equation}
        \frac{1}{F} \diffp{F}{h} = \frac{x}{2} \left( 1 + \frac{1}{h^2} \right).
    \end{equation}
    Integrating over \(h\) on the left hand side we get
    \begin{equation}
        \int \frac{1}{F} \diffp{F}{h} \dd{h} = \int \frac{1}{F} \dd{F} = \ln F.
    \end{equation}
    On the right hand side we get
    \begin{equation}
        \frac{x}{2} \int 1 + \frac{1}{h^2} \dd{h} = \frac{x}{2}\left( h - \frac{1}{h} \right) + \ln[\varphi(x)].
    \end{equation}
    Here \(\ln[\varphi(x)]\) is a \enquote{constant} of integration, which depends on the other variable, \(x\).
    Hence,
    \begin{equation}\label{eqn:bessel generating function derivation}
        F(x, h) = \varphi(x) \exp\left[ \frac{x}{2}\left( h - \frac{1}{h} \right) \right].
    \end{equation}
    
    We can do something similar with the second recurrence relation,
    \begin{equation}
        J_{n-1} - J_{n+1} = 2J_n'.
    \end{equation}
    Multiplying by \(h^n\) and summing over \(n\) we have
    \begin{align}
        \sum_n h^nJ_{n-1} - \sum_n h^n J_{n+1} &= \sum_n 2h^nJ_n'\\
        \implies hF - \frac{1}{F} = 2\diffp{F}{x}.
    \end{align}
    Here we have used
    \begin{equation}
        \diffp{F}{x} = \diffp{}{x}\sum_{n} h^nJ_n(x) = \sum_n h^n J_n'(x).
    \end{equation}
    Inserting \cref{eqn:bessel generating function derivation} into the right hand side we have
    \begin{align}
        2\diffp{F}{x} &= 2\diffp{}{x} \left[ \varphi(x) \exp\left[ \frac{x}{2}\left( h - \frac{1}{h} \right) \right] \right]\\
        &= 2\varphi'(x) \exp\left[ \frac{x}{2}\left( h - \frac{1}{h} \right) \right] + 2 \frac{1}{2}\left( h - \frac{1}{h} \right)\\
        &= \left( h - \frac{1}{h} \right)F + \varphi'(x) \exp\left[ \frac{x}{2}\left( h - \frac{1}{h} \right) \right].
    \end{align}
    For this to be equal to the left hand side, which is \((h - 1/h)F\), we must have \(\varphi'(x) = 0\), and so \(\varphi\) is just a constant.
    The choice of constant is just a choice of normalisation, and so we are free to choose, and take \(\varphi = 1\).
    We therefore have
    \begin{equation}
        F(x, h) = \exp\left[ \frac{x}{2} \left( h - \frac{1}{h} \right) \right] = \sum_{n = - \infty}^{\infty} J_n(h) h^n.
    \end{equation}
    
    Now that we have a closed form for the generating function, how can we get \(J_n\) from the generating function?
    We can't simply use the derivative like we did with Legendre polynomials, the doubly infinite sum makes this intractable.
    Instead consider what happens if we divide through by \(2\pi i h^{n+1}\) and then integrate over \(h\) around an anticlockwise contour, \(C\), which contains the origin.
    We then have
    \begin{equation}
        \frac{1}{2\pi i} \oint_C \frac{1}{h^{n+1}} F(x, h) \dd{h} = \frac{1}{2\pi i} \oint_C \sum_{m=-\infty}^{\infty} \frac{J_m(x)h^m}{h^{n+1}} \dd{h}.
    \end{equation}
    Careful consideration of the integral on the right hand side shows that it vanishes whenever \(n \ne m\).
    Clearly if \(m \ge n + 1\) there are no singularities and so that term of the integral vanishes.
    If \(m < n\) then we end up with an integrand that goes as \(1/h^{n - m}\), the extra power of \(h\) cancels with the Jacobian when we parametrise the contour, and then taking the contour to infinity the integral vanishes.
    Further if \(n = m\) we have an integral of \(1/h\), which by the residue theorem just gives \(2\pi i\), which cancels with the existing factor, leaving us with just \(J_n\).
    Hence,
    \begin{equation}
        J_{n}(x) = \frac{1}{2\pi i} \oint_C \frac{\exp\left[ \frac{x}{2}\left( h - \frac{1}{h} \right) \right]}{h^{n + 1}} \dd{h}.
    \end{equation}
    This is the Schl\"afli integral representation of the Bessel function of the first kind.
    Note the similarity to \cref{eqn:hankel function first time integral} for Hankel's function of the first kind, \(H_\nu^{(1)}\), which, along with Hankel's function of the second kind, \(H_\nu^{(2)}\) is closely related to the Bessel function by
    \begin{equation}
        H_\nu^{(1)} \coloneqq J_\nu + iY_\nu, \qqand H_\nu^{(2)} \coloneqq J_\nu - iY_\nu.
    \end{equation}

    We can derive another integral representation by taking \(h = \e^{i\vartheta}\).
    We then get
    \begin{align}
        J_n(x) &= \frac{1}{2\pi i} \int_{0}^{2\pi} \frac{\exp\left[ \frac{x}{2}(\e^{i\vartheta} - \e^{-i\vartheta}) \right]}{\e^{(n+1)i\vartheta}} i\e^{i\vartheta} \dd{\vartheta}\\
        &= \frac{1}{2\pi} \int_{0}^{2\pi} \exp\left[ \frac{x}{2}(\e^{i\vartheta} - \e^{-i\vartheta}) - in\vartheta \right] \dd{\vartheta}\\
        &= \frac{1}{2\pi} \int_{0}^{2\pi} \exp[ix\sin\vartheta - in\vartheta]\\
        &= \frac{1}{\pi} \int_{0}^{\pi} \cos[x\sin\vartheta - n\vartheta] \dd{\vartheta}.
    \end{align}
    Here we have used the fact that \(\sin\)  is odd about \(\pi\), the midpoint of the integration range, and so the \(\sin\) term vanishes.
    Similarly \(\cos\) is even over this range and so we can half the integration range and double the integral, this is exactly what we usually do with \(\sin\) and \(\cos\) in integrals, we've just shifted the origin slightly to the next point of symmetry.
    
    \section{Other Special Functions}
    Rodrigues' formula has a very general form from which we can define many other special functions.
    The \(n\)th order polynomials, \(R_n\), with constants \(e_n\) are defined by
    \begin{equation}
        R_n(x) \coloneqq \frac{1}{e_nw(x)} \diff[n]{}{x} [w(x) q^n(x)]
    \end{equation}
    where \(q\) is some polynomial and \(w\) is a weight function.
    With respect to this weight function and over the interval \((a, b)\), chosen such that \(q\) vanishes at \(a\) and \(b\), these polynomials are orthogonal, meaning
    \begin{equation}
        \int_a^b R_n(x) R_m(x) w(x) \dd{x} = N_n\delta_{mn}
    \end{equation}
    where \(\sqrt{N_n}\) is the norm of \(R_n\), defined by
    \begin{equation}
        N_n = \int_a^b R_n(x)R_n(x) w(x) \dd{x}.
    \end{equation}
    
    Choosing \(q(x) = x^2 - 1\) and the weight function \(w(x) = (1 - x)^\alpha(1 + x)^\beta\) with \(e_n = 2^nn!\) gives
    \begin{itemize}
        \item the Legendre polynomials for \(\alpha = \beta = 0\),
        \item the \define{Jacobi polynomials}\index{Jacobi polynomial} for \(\alpha, \beta > -1\),
        \item the \define{Gegenbauer polynomials}\index{Gegenbauer polynomial} for \(\alpha = \beta\), and
        \item the \define{Chebyshev polynomials}\index{Chebyshev polynomial} for \(\alpha = \beta = \pm 1/2\).
    \end{itemize}
    
    The choice of \(q(x) = 1\) and \(w(x) = \exp[-x^2]\) gives the \define{Hermite polynomials}\index{Hermite polynomial}, \(H_n\)\index{\(H_n\)|see{Hermite polynomial}}.
    These appear in the wave function of the harmonic oscillator.
    In particular, up to normalisation, the \(n\)th excited state of the harmonic oscillator of mass \(m\) and frequency \(\omega\) has the wave function \(H_n(x)\), where \(x = \sqrt{m\omega/\hbar}\tilde{x}\) is the dimensionless position, where \(\tilde{x}\) is the position with dimension.
    
    The choice of \(q(x) = x\) and \(w(x) = x^\alpha\e^{-x}\) gives the \define{generalised Laguerre polynomials}\index{generalised Laguerre polynomial}, \(L_n^{(\alpha)}\)\index{\(L_n^{(\alpha)}\)|see{generalised Laguerre polynomial}}. 
    These appear in the radial wave function of the non-relativistic hydrogen atom.
    In particular the state with principle quantum number \(n\), and angular momentum quantum number, \(l\), has wave function
    \begin{equation}
        \rho^lL_{n-l-1}^{2l+1}(\rho) \e^{-\rho/2}
    \end{equation}
    where
    \begin{equation}
        \rho = \frac{2r}{na_0}
    \end{equation}
    and
    \begin{equation}
        a_0 \coloneqq \frac{4\pi\varepsilon_{0}\hbar^2}{m_{\mathrm{e}}e^2}
    \end{equation}
    is the Bohr radius, defined in terms of the electric constant, \(\varepsilon_{0}\), reduced Planck's constant, \(\hbar\), electron mass, \(m_{\mathrm{e}}\), and elementary charge, \(e\).
    
    \part{Integral Transforms}
    \chapter{Fourier}
    \begin{rmk}
        For more details see the Fourier analysis part of the Fourier analysis and statistics course.
    \end{rmk}
    \section{Fourier Series}
    Given a function, \(f \colon [-L, L] \to \complex\) for some \(L \in \reals_{>0}\) we can, under mild conditions, write this function as a superposition of plane waves:
    \begin{equation}
        f(x) = \sum_{n=-\infty}^{\infty} C_n\exp\left[ \frac{in\pi x}{L} \right].
    \end{equation}
    We call this the \defineindex{Fourier series} of \(f\).
    It can be further extended to all \(\reals\) by taking this as a periodic function with period \(2L\).
    
    To find the value of the constants, \(C_n\), we use the identity from \cref{eqn:integral representation kronecker delta},
    \begin{equation}
        \delta_{mn} = \frac{1}{2\pi}\int_{-\pi}^{\pi} \exp[ik(m - n)] \dd{k}.
    \end{equation}
    Making a change of variables to \(y = kL/\pi\) we have
    \begin{equation}
        \delta_{mn} = \frac{1}{2L} \int_{-L}^{L} \exp\left[ \frac{i\pi(m - n)y}{L} \right] \dd{y}.
    \end{equation}
    We therefore have
    \begin{align}
        \frac{1}{2L} \int_{-L}^{L} f(x) \exp\left[ -\frac{i\pi nx}{L} \right] \dd{x} &= \frac{1}{2L}\int_{-L}^{L} \sum_{m=-\infty}^{\infty} C_m \exp\left[ \frac{i\pi(m - n)x}{L} \right]\\
        &= \sum_{m=-\infty}^{\infty} C_m\delta_{mn}\\
        &= C_n.
    \end{align}
    The ability to swap the integral and sum is guaranteed by the aforementioned \enquote{mild conditions}.
    
    If \(f\) is an even (odd) function then we can write the exponential in terms of \(\cos\) and \(\sin\) and the symmetry means that the \(\sin\) (\(\cos\)) terms will vanish.
    If \(f\) is instead defined on \([0, L]\) then we can choose either the odd or even extension to \([-L, L]\), and still get a Fourier series in the same way.
    If \(f(0) \ne 0\) then we typically choose the even extension as this avoids a discontinuity.
    
    It follows easily from the integral representation of \(\delta_{mn}\) above that
    \begin{align}
        \frac{1}{L}\int_{-L}^{L} \sin\left( \frac{\pi mx}{L} \right)\sin\left( \frac{\pi nx}{L} \right) \dd{x} = \delta_{mn},\\
        \frac{1}{L}\int_{-L}^{L} \sin\left( \frac{\pi mx}{L} \right)\sin\left( \frac{\pi nx}{L} \right) \dd{x} = \delta_{mn}.\\
    \end{align}
    These can be used to find the coefficients in \(\sin\) or \(\cos\) Fourier series.
    For example, if \(f\) is given by
    \begin{equation}
        f(x) = \sum_{n=1}^{\infty} A_n \sin\left( \frac{n\pi x}{L} \right)
    \end{equation}
    then
    \begin{equation}
        A_n = \frac{1}{L} \int_{-L}^{L} f(x) \sin\left( \frac{n\pi x}{L} \right) \dd{x} = \frac{2}{L} \int_0^L f(x) \sin\left( \frac{n \pi x}{L} \right) \dd{x}.
    \end{equation}
    
    \section{Fourier Transforms}
    \begin{dfn}{Fourier Transform}{}
        Given a function \(f\colon\reals\to\complex\) such that \(f(x) \to 0\) as \(\abs{x} \to \infty\) we define the \defineindex{Fourier transform} to be
        \begin{equation}
            \fourierTransform\{f(x)\} = \tilde{f}(k) \coloneqq \int_{-\infty}^{\infty} f(x) \e^{-ikx} \dd{x}.
        \end{equation}
        \index{\(\fourierTransform\)|see{Fourier transform}}
    \end{dfn}
    This is actually just one of a few conventions for defining Fourier transforms, they may differ in the sign of the exponent, as well as including constants either in front of the integral or in the exponential.
    There are also several notational conventions for denoting the Fourier transformed function, including \(\tilde{f}\), \(\hat{f}\), \(F\), or \(g\).
    
    \begin{thm}{Inversion Theorem}{}
        Given a function, \(f\), and its Fourier transform, \(\tilde{f}\), we have
        \begin{equation}
            f(x) = \inverseFourierTransform\{f(x)\} = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde{f}(k) \e^{ikx} \dd{k}.
        \end{equation}
        \begin{proof}
            Recall the integral representation of the Dirac delta given in \cref{eqn:integral representation of dirac delta}:
            \begin{equation}
                \delta(x - x') = \frac{1}{2\pi} \int_{-\infty}^{\infty} \e^{ik(x - x')} \dd{k}.
            \end{equation}
            We therefore have
            \begin{align}
                \frac{1}{2\pi} \int_{-\infty}^{\infty} \textcolor{highlight}{\tilde{f}(k)} \e^{ikx} \dd{k} &= \textcolor{dark}{\frac{1}{2\pi}\int_{-\infty}^{\infty} \dl{k}} \textcolor{highlight}{\int_{-\infty}^{\infty} \dl{x'} \, f(x') \e^{-ikx'}} \textcolor{dark}{\e^{ikx}}\\
                &= \int_{-\infty}^{\infty} f(x') \textcolor{dark}{\delta(x - x')} \dd{x'}\\
                &= f(x).
            \end{align}
        \end{proof}
    \end{thm}
    
    \subsection{Conventions}
    There is a lot of freedom in choosing the definition of the Fourier transform.
    Once it has been defined the inverse transform is automatically fixed, since we require that \(\inverseFourierTransform\{\fourierTransform\{f(x)\}\} = f(x)\).
    Important features shared between all conventions then mean that
    \begin{itemize}
        \item the sign in the exponent must differ between the forward and inverse transform, although which is positive and which is negative is just convention.
        \item there must be a factor of \(1/(2\pi)\) somewhere between the two transforms, to account for the \(1/(2\pi)\) in the integral representation of the delta distribution.
        Various conventions put a factor of \(1/(2\pi)\) with the inverse or the forward transform, or split it evenly between the two with factors of \(1/\sqrt{2\pi}\).
        Sometimes the \(2\pi\) is put in the exponent also.
    \end{itemize}
    
    A few common conventions, and their appropriate inverses, are given below
    \begin{alignat}{3}
        \tilde{f}(k) &= \int_{-\infty}^{\infty} f(x)\e^{-ikx} \dd{x}, \qquad && f(x) = \int_{-\infty}^{\infty} \tilde{f}(k)\e^{ikx} \dd{k},\\
        \tilde{f}(k) &= \frac{1}{2\pi}\int_{-\infty}^{\infty} f(x)\e^{ikx} \dd{x}, \qquad && f(x) = \int_{-\infty}^{\infty} \tilde{f}(k)\e^{-ikx} \dd{k},\\
        \tilde{f}(k) &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} f(x)\e^{-ikx} \dd{x}, \qquad && f(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \tilde{f}(k) \e^{ikx} \dd{k}.
    \end{alignat}
    These are all transformations between \(x\)-space and \(k\)-space, which we interpret as position and wave number.
    Alternatively we can restrict our integrals to \((0, \infty)\) and replace \(x\) with \(t\) and \(k\) with \(\omega\), which we can then interpret as time and angular frequency.
    If we include extra factors of \(\hbar\) then we can instead work with \(p = k\hbar\) and \(E = \omega\hbar\) as Fourier variables instead, which we interpret as momentum and energy:
    \begin{equation}
        \tilde{f}(p) = \int_{-\infty}^{\infty} f(x)\e^{-ipx/\hbar} \dd{x}, \text{ and } f(x) = \frac{1}{2\pi\hbar} \int_{-\infty}^{\infty} \tilde{f}(p)\e^{ipx/\hbar} \dd{p}.
    \end{equation}
    This is common in quantum mechanics\footnote{see quantum theory notes for more details} when moving between the position and momentum bases.
    
    \subsection{Higher Dimension}
    We can easily generalise the Fourier transform to higher dimensions.
    In particular given some function \(f \colon \reals^n \to \complex\) such that \(f(\vv{x}) \to 0\) as \(\abs{\vv{x}} \to 0\) we define the Fourier transform to be
    \begin{equation}
        \fourierTransform\{f(\vv{x})\} = \tilde{f}(\vv{k}) \coloneqq \int_{\reals^n} f(x) \e^{-i\vv{k} \cdot \vv{x}} \dd{^nk}.
    \end{equation}
    The inverse Fourier transform is then
    \begin{equation}
        \inverseFourierTransform\{\tilde{f}(\vv{k})\} = f(\vv{x}) = \frac{1}{(2\pi)^n} \int_{\reals^n} \tilde{f}(\vv{k}) \e^{i\vv{k} \cdot \vv{x}} \dd{x}.
    \end{equation}
    Where possible we choose \(\vv{k}\) to make \(\vv{k} \cdot \vv{x}\) as simple as possible, for example we may choose to have \(\vv{k}\) lie along one of the axes in Fourier-space.
    
    \subsubsection{Central Function}
    Consider the case where \(f\) is a central function, that is a function of distance from the origin, \(r\), in three dimensions.
    In particular we take \(f(r) = \e^{-r^2/a^2}\) for some constant \(a\), although this analysis should work for any reasonable central function.
    
    We work in spherical polar coordinates, so \(\dl[3]{x} = r^2\sin\vartheta\dd{r}\dd{\vartheta}\dd{\varphi}\).
    We use
    \begin{equation}
        \int_{0}^{2\pi} \dl{\varphi} = 2\pi,
    \end{equation}
    and
    \begin{equation}
        \int_{0}^{\pi} g(\vartheta)\sin\vartheta \dd{\vartheta} = \int_{-1}^{1} g(\vartheta) \dd{(\cos\vartheta)}.
    \end{equation}
    This is just the substitution \(u = \cos\vartheta\), and so \(\dl{u} = \dl{(\cos\vartheta)} = -\sin\vartheta\dd{\vartheta}\), and the limits go from \((0, \pi)\), to \((1, -1)\), which is then reversed by the minus sign from the measure.
    
    We choose our axes such that \(\vv{k}\) is parallel to the \(z\)-axis, and so \(\vv{k} \cdot \vv{r} = kz = kr\cos\vartheta\).
    We then have
    \begin{align}
        \fourierTransform\{f(r)\} &= \int_{\reals^3} f(r) \e^{-i\vv{k}\cdot\vv{r}} \dd{^3x}\\
        &= \int_{0}^{\infty} \dl{r} \int_{0}^{\pi} \dl{\vartheta} \int_{0}^{2\pi} \dl{\varphi} \, f(r) \e^{-ikr\cos\vartheta} r^2\sin\vartheta\\
        &= 2\pi \int_{0}^{\infty} \int_{-1}^{1} \dl{(\cos \vartheta)} r^2 f(r) \e^{-ikr\cos\vartheta}\\
        &= 2\pi \int_{0}^{\infty} r^2 f(r) \left[ -\frac{\e^{-ikr\cos\vartheta}}{ikr} \right]_{\cos\vartheta=-1}^{\cos\vartheta=1} \dd{r}\\
        &= -\frac{2\pi}{ik} \int_{0}^{\infty} r f(r) [-\e^{-ikr} + \e^{ikr}] \dd{r}\\
        &= \frac{4\pi}{ik} \int_{0}^{\infty} r f(r) \sin(kr) \dd{r}.
    \end{align}
    So far this holds for any central function \(f\), which vanishes at infinity.
    This is useful, for example, when computing the matrix elements of scattering from a central potential, which is defined to be proportional to the square of the Fourier transform of the scattering potential\footnote{for more details see the principles of quantum mechanics or quantum theory courses.}.
    For our case we are best going back a step to the exponentials, since our function is an exponential.
    We then have
    \begin{align}
        \fourierTransform\{f(r)\} &= \frac{2\pi}{ik} \int_{0}^{\infty} r[\e^{ikr} - \e^{-ikr}]\e^{-r^2/a^2} \dd{r}\\
        &= \frac{2\pi}{ik} \int_{0}^{\infty} r\e^{-r^2/a^2 + ikr} \dd{r} + \frac{2\pi}{ik} \int_{0}^{\infty} r\e^{-r^2/a^2 - ikr} \dd{r}.
    \end{align}
    Considering the second integral now we make the change of variables \(r' = -r\), so \(\dl{r'} = -\dl{r}\), and the limits change from \((0, \infty)\) to \((0, -\infty)\).
    With the extra minus sign from the measure the limits are therefore \((-\infty, 0)\).
    The integrand changes to \(\exp[-r'^2/a^2 + ikr']\), and so, dropping the primes, we have
    \begin{align}
        \fourierTransform\{f(r)\} &= \frac{2\pi}{ik} \int_{0}^{\infty} r\e^{-r^2/a^2 + ikr} \dd{r} + \frac{2\pi}{ik} \int_{-\infty}^{0} r\e^{-r^2/a^2 + ikr} \dd{r}\\
        &= \frac{2\pi}{ik} \int_{-\infty}^{\infty} r\e^{-r^2/a^2 + ikr} \dd{r}.
    \end{align}
    Completing the square we have
    \begin{equation}
        -\frac{r^2}{a^2} + ikr = \frac{1}{a^2}\left( r - \frac{1}{2}ia^2k \right)^2 -\frac{1}{4}a^2k^2.
    \end{equation}
    Defining
    \begin{equation}
        s = r - \frac{ia^2k}{2}
    \end{equation}
    we then have
    \begin{equation}
        \fourierTransform\{f(r)\} = \frac{2\pi}{ik} \int_{-\infty}^{\infty} \left( s - \frac{ia^2k}{2} \right) \e^{-s^2/a^2}\e^{-k^2a^2/4} \dd{s}.
    \end{equation}
    Splitting the bracketed term the first integral, with \(s\), gives zero, since the integrand is odd and the range of integration is symmetric about zero.
    Thus,
    \begin{align}
        \fourierTransform\{f(r)\} &= \frac{2\pi}{ik} \int_{-\infty}^{\infty} -\frac{ia^2k}{2} \e^{-s^2/a^2}\e^{-k^2a^2/4} \dd{s}\\
        &= \pi a^2\e^{-k^2a^2/4} \int_{-\infty}^{\infty} \e^{-s^2/a^2} \dd{s}\\
        &= \pi a^2 \sqrt{\pi a^2} \e^{-k^2a^2/4}.
    \end{align}
    Here we have used the standard Gaussian integral
    \begin{equation}
        \int_{-\infty}^{\infty} \e^{-\alpha x^2} \dd{x} = \sqrt{\frac{\pi}{\alpha}},
    \end{equation}
    which holds for all \(\alpha\) with \(\Re(\alpha) > 0\).
    
    Identifying our function as a Gaussian we can see that its width is given by \(\sigma_r = a/\sqrt{2}\), since we typically define a Gaussian as
    \begin{equation}
        \exp\left[ -\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2} \right]
    \end{equation}
    where \(\mu\) is the position of the peak and \(\sigma\) the width.
    Our Fourier transform is then another, scaled, Gaussian, now with width \(\sigma_k = \sqrt{2}/a\).
    The product of the widths is then
    \begin{equation}
        \sigma_r\sigma_k = \frac{a}{\sqrt{2}} \frac{\sqrt{2}}{a} = 1.
    \end{equation}
    This is a common feature of Fourier transforms, if we can define a width for a function then the transformed function will usually have a width that is inversely proportional to the width of the original function.
    Interpreting the width as a measure of uncertainty we see how this is related to the Heisenberg uncertainty principle.
    In particular if \(f\) is the unnormalised wave function in position space then we can define normalised wave functions in position and \(k\)-space by
    \begin{equation}
        \psi(x) = \left( \frac{2a}{\pi} \right)^{1/4}\e^{-ax^2}, \qqand \tilde{\psi}(k) = \left( \frac{1}{2a\pi} \right)^{1/4} \e^{-k^2/(4a)},
    \end{equation}
    We then have
    \begin{equation}
        \expected{x^2} = \left( \frac{2a}{\pi} \right)^{1/2} \int_{-\infty}^{\infty} x^2\e^{-2ax^2} \dd{x},
    \end{equation}
    which we can compute using the standard integral
    \begin{equation}
        \int_{-\infty}^{\infty} x^3\e^{-\alpha x^2} \dd{x} = \frac{1}{a}\sqrt{\frac{\pi}{4a}}.
    \end{equation}
    We find that
    \begin{equation}
        \expected{x^2} = \left( \frac{2a}{\pi} \right)^{1/2} \frac{1}{2a} \sqrt{\frac{\pi}{8a}} = \frac{1}{4a}.
    \end{equation}
    Similarly
    \begin{equation}
        \expected{p^2} = \left( \frac{1}{2\pi a} \right)^{1/2} \int_{-\infty}^{\infty} k^2\e^{-k^2/(2a)} \dd{k} = a.
    \end{equation}
    
    The uncertainty in position is
    \begin{equation}
        \Delta_x = \sqrt{\expected{x^2} - \expected{x}^2} = \sqrt{\expected{x^2}} = \frac{1}{2\sqrt{a}},
    \end{equation}
    and similarly for wave numer
    \begin{equation}
        \Delta_k = \sqrt{\expected{k^2} - \expected{k}^2} = \sqrt{\expected{k^2}} = \sqrt{a}.
    \end{equation}
    The momentum is related to the wave number by \(\vv{p} = \hbar\vv{k}\), so the uncertainty in momentum is
    \begin{equation}
        \Delta_p = \hbar\Delta_k = \hbar\sqrt{a}.
    \end{equation}
    Therefore we have
    \begin{equation}
        \Delta_x\Delta_p = \frac{1}{2\sqrt{a}} \hbar \sqrt{a} = \frac{\hbar}{2}.
    \end{equation}
    
    \section{Convolutions}
    \begin{dfn}{Convolution}{}
        Given two functions, \(f, g \colon \reals \to \complex\), we define their \defineindex{convolution}\index{\(\ast\)|see{convolution}} as
        \begin{equation}
            (f \ast g)(x) \coloneqq \int_{-\infty}^{\infty} f(y)f(x - y) \dd{y}
        \end{equation}
        for all \(x \in \reals\).
    \end{dfn}
    
    Convolutions arise naturally when combining probability density functions.
    We can think of them as shifting the functions by some amount and measuring the overlap.
    They are also common, although in a discrete form, in image processing, where we use them to apply filters.
    
    \begin{thm}{Convolution Theorem}{}
        Given two functions, \(f, g\colon \reals \to \complex\), whose Fourier transforms exist, the following holds
        \begin{equation}
            \fourierTransform\{ (f \ast g)(x) \} = \fourierTransform\{f(x)\}\fourierTransform\{g(x)\},
        \end{equation}
        and
        \begin{equation}
            \fourierTransform\{ f(x)g(x) \} = \frac{1}{2\pi} (\tilde{f} \ast \tilde{g})(x)
        \end{equation}
        where \(\tilde{f}(k) = \fourierTransform\{ f(x) \}\), and \(\tilde{g}(k) = \fourierTransform\{ g(x) \}\).
        
        \begin{proof}
            The proof follows easily from the definitions of the Fourier transform and convolution, first for the Fourier transform of a convolution we have
            \begin{align}
                \fourierTransform\{ (f \ast g)(x) \} &= \int_{-\infty}^{\infty} \dl{x} \, \e^{-ikx} (f \ast g)(x)\\
                &= \int_{-\infty}^{\infty} \dl{x} \, \e^{-ikx} \int_{-\infty}^{\infty} \dl{y} \, f(y)g(x - y).
            \end{align}
            Now make a change of variables, \(x' = x - y\), so \(x = x' + y\), \(\dl{x'} = \dl{x}\), and the limits are unchanged, since they are infinite.
            We then have
            \begin{align}
                \fourierTransform\{ (f \ast g)(x) \} &= \int_{-\infty}^{\infty} \dl{x'} \, \e^{-ik(x' + y)} \int_{-\infty}^{\infty} \dl{y} \, f(y)g(x')\\
                &= \int_{-\infty}^{\infty} f(y) \e^{-iky} \dd{y} \int_{-\infty}^{\infty} \e^{-ikx'} g(x') \dd{x'}\\
                &= \fourierTransform\{ f(y) \} \fourierTransform\{ g(x') \}\\
                &= \fourierTransform\{ f(x) \} \fourierTransform\{ g(x) \}
            \end{align}
            where in the last step we have simply renamed variables to bring the result in line with the hypothesis.
            
            For the second statement applying the inverse Fourier transform to both sides we get the equivalent statement
            \begin{equation}
                \inverseFourierTransform\{ \fourierTransform\{ f(x)g(x) \} \} = f(x)g(x) = \frac{1}{2\pi} \inverseFourierTransform\{ (\tilde{f} \ast \tilde{g})(k) \}.
            \end{equation}
            The proof then proceeds as for the first statement, but using the inverse Fourier transform:
            \begin{align}
                \inverseFourierTransform\{ (\tilde{f} \ast \tilde{g})(k) \} &= \frac{1}{2\pi}\int_{-\infty}^{\infty} \dl{k} \, \e^{ikx} (\tilde{f} \ast \tilde{g})(k)\\
                &= \frac{1}{2\pi}\int_{-\infty}^{\infty} \dl{k} \, \e^{ikx} \int_{-\infty}^{\infty} \dd{y} \, \tilde{f}(y)\tilde{g}(k - y)\\
                &= \frac{1}{2\pi}\int_{-\infty}^{\infty} \dl{k'} \, \e^{i(k' + y)x} \int_{-\infty}^{\infty} \dl{y} \tilde{f}(y)\tilde{g}(k')\\
                &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \e^{iyx} \tilde{f}(y) \dd{y} \int_{-\infty}^{\infty} \e^{ik'x} \tilde{g}(k') \dd{k'}\\
                &= 2\pi \inverseFourierTransform\{ \tilde{f}(y) \} \inverseFourierTransform\{ \tilde{g}(k) \}\\
                &= 2\pi f(x)g(x).
            \end{align}
            This proves the second statement.
        \end{proof}
    \end{thm}
    
    \begin{crl}{}{}
        The convolution operation is commutative, that is given two functions \(f, g \colon \reals \to \complex\) we have
        \begin{equation}
            f \ast g = g \ast f.
        \end{equation}
        \begin{proof}
            Consider the Fourier transform of the statement:
            \begin{equation}
                \fourierTransform\{ (f \ast g)(x) \} = \fourierTransform\{ f(x) \} \fourierTransform\{ g(x) \} = \fourierTransform\{ g(x) \} \fourierTransform\{ f(x) \} = \fourierTransform\{ (g \ast f)(x) \}.
            \end{equation}
            Taking the inverse Fourier transform we get the desired statement.
            Note that the middle equality is valid as it is just normal multiplication in \(\complex\), which is by definition commutative since \(\complex\) is a field.
        \end{proof}
    \end{crl}
    
    \section{Parseval's Theorem}
    \begin{thm}{Parseval's Theorem}{}
        For a function \(f \colon \reals \to \complex\) with Fourier transform \(\tilde{f}\)
        \begin{equation}
            \int_{-\infty}^{\infty} \abs{f(x)}^2 \dd{x} = \frac{1}{2\pi} \int_{-\infty}^{\infty} \abs{\tilde{f}(k)}^2 \dd{k}.
        \end{equation}
        \begin{proof}
            The proof follows from writing \(\abs{f(x)}^2 = f(x)f^*(x)\).
            We therefore have
            \begin{equation}
                f^*(x) = \inverseFourierTransform\{ \tilde{f}(k) \}^* = \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde{f}^*(k) \e^{-ikx} \dd{k}.
            \end{equation}
            Hence
            \begin{align}
                \int_{-\infty}^{\infty} \abs{f(x)}^2 \dd{x} &= \int_{-\infty}^{\infty} f(x)f^*(x) \dd{x}\\
                &= \int_{-\infty}^{\infty} \dl{x} \, f(x) \frac{1}{2\pi} \int_{-\infty}^{\infty} \dl{k} \tilde{f}^*(k) \e^{-ikx}\\
                &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \dl{k} \, \tilde{f}^*(k) \int_{-\infty}^{\infty} \dl{x} \, f(x) \e^{-ikx}\\
                &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \tilde{f}^*(k) \tilde{f}(k) \dd{k}\\
                &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \abs{\tilde{f}(k)}^2 \dd{k}.
            \end{align}
        \end{proof}
    \end{thm}
    
    \section{A Useful Result}
    Given some function, \(f\), and its Fourier transform, \(\tilde{f}\), what is the Fourier transform of \(xf(x)\)?
    This is fairly straight forward to compute:
    \begin{align}
        \fourierTransform\{ xf(x) \} &= \int_{-\infty}^{\infty} xf(x) \e^{-ikx} \dd{x}\\
        &= \int_{-\infty}^{\infty} \dl{x} \, \frac{1}{2\pi} \int_{-\infty}^{\infty} \dl{k} \, x\tilde{f}(k') \e^{-ikx}\e^{ik'x}\\
        &= \int_{-\infty}^{\infty} \dl{x} \, \frac{1}{2\pi} \int_{-\infty}^{\infty} \dl{k} \, x\tilde{f}(k) \e^{i(k' - k)x}\\
        &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \dl{x} \, x\bigg( \left[ \tilde{f}(k') \frac{\e^{ix(k' - k)}}{ix}  \right]_{k=-\infty}^{k=\infty}\notag\\
        &\hspace{4em}- \frac{1}{ix} \int_{-\infty}^{\infty} \tilde{f}'(k') \e^{ix(k' - k)} \dd{k'} \bigg)\\
        &= \frac{i}{2\pi} \int_{-\infty}^{\infty} \dl{x} \int_{-\infty}^{\infty} \dl{k'} \, \tilde{f}'(k') \e^{ix(k' - k)}\\
        &= i \int_{-\infty}^{\infty} \delta(k' - k)\tilde{f}'(k') \dd{k}\\
        &= i\tilde{f}'(k).
    \end{align}
    Here we have integrated by parts and then used the fact that \(\tilde{f}\) must vanish at infinity (in order for the inverse transform to exist).
    
    \section{Solving ODEs with Fourier Transforms}
    \subsection{Fourier Transform of Derivative}
    Given some function, \(f\), and it's Fourier transform, \(\tilde{f}\), it is easy to compute the Fourier transform of its derivative:
    \begin{align}
        \fourierTransform\{ f'(x) \} &= \int_{-\infty}^{\infty} f'(x) \e^{-ikx} \dd{x}\\
        &= [ f(x)\e^{-ikx} ]_{-\infty}^{\infty} + ik \int_{-\infty}^{\infty} f(x)\e^{-ikx} \dd{x}\\
        &= ik \tilde{f}(k).
    \end{align}
    Here we simply integrated by parts and then used the fact that \(f\) must vanish at infinity in order to have a Fourier transform.
    
    We can then compute Fourier transforms of second derivatives, or higher, recursively since
    \begin{equation}
        \fourierTransform\{f''(x)\} = ik\fourierTransform\{ f'(x) \} = (ik)^2\fourierTransform\{ f(x) \} = -k^2\tilde{f}(k).
    \end{equation}
    In general for the \(n\)th derivative we have
    \begin{equation}
        \fourierTransform\{ f^{(n)}(x) \} = (ik)^n \tilde{f}(k).
    \end{equation}
    
    \subsection{Differential Equations}
    Given some ordinary differential equation we can take the Fourier transform and this makes all of the derivatives into purely algebraic statements.
    We can then (hopefully) solve this to find the Fourier transform of our desired function.
    We can also transform the boundary or initial conditions to aid us in this step.
    We then \enquote{simply} invert the transformation to get the desired function.
    
    \subsection{Forced, Damped Harmonic Oscillator}
    The equation of motion for the forced, damped harmonic oscillator is
    \begin{equation}
        \ddot{y}(t) + 2\kappa \dot{y} + \Omega^2y(t) = f(t).
    \end{equation}
    Thinking of this as a physical system, as opposed to just an abstract differential equation, we take \(t\) as time and \(y\) as the position of the oscillator.
    We therefore take the Fourier variable to be \(\omega\).
    We define \(\tilde{y}(\omega) \coloneqq \fourierTransform\{ y(t) \}\) and \(\tilde{f}(\omega) \coloneqq \fourierTransform\{ f(t) \}\).
    Using the Fourier transform of derivatives from earlier we can take the Fourier transform of this differential equation, giving
    \begin{equation}
        (-\omega^2 + 2i\kappa\omega + \Omega^2)\tilde{y}(\omega) = \tilde{f}(\omega).
    \end{equation}
    This is now a purely algebraic equation, which we can solve easily for \(\tilde{y}(\omega)\):
    \begin{equation}
        \tilde{y}(\omega) = \frac{1}{\Omega^2 + 2i\kappa\omega - \omega^2} \tilde{f}(\omega).
    \end{equation}
    
    Now comes the tricky step, to find the solution we need to invert the Fourier transform.
    To do so we notice that the right hand side can be viewed as the product of two Fourier transforms, the fraction being one and \(\tilde{f}(\omega)\) the other.
    Therefore \(y\) is the convolution of some function, \(G\), which is the inverse Fourier transform of the fraction, and the function \(f\), which is some known function.
    We just need to find out what \(G\) is.
    We then have
    \begin{equation}
        y(t) = (G \ast f)(t) = \int_{-\infty}^{\infty} G(t - t')f(t') \dd{t'}
    \end{equation}
    in which \(G\) acts like a Green's function, explaining our choice of notation.
    So, what is \(G\)?
    \begin{equation}
        G(t) = \inverseFourierTransform\left\{ \frac{1}{\Omega^2 + 2i\kappa\omega - \omega^2} \right\} = \inverseFourierTransform\left\{ -\frac{1}{(\omega - \omega_{+})(\omega - \omega_{-})} \right\}
    \end{equation}
    here we have factorised the quadratic in the denominator, where
    \begin{equation}
        \omega_{\pm} = i\kappa \pm \sqrt{\Omega^2 - \kappa^2}.
    \end{equation}
    We therefore have
    \begin{equation}
        G(t) = \inverseFourierTransform\left\{ -\frac{1}{(\omega - \omega_{+})(\omega - \omega_{-})} \right\} = -\frac{1}{2\pi} \int_{-\infty}^{\infty} \frac{\e^{i\omega t}}{(\omega - \omega_{+})(\omega - \omega_{-})} \dd{\omega}.
    \end{equation}
    We assume that this inverse exists, which means that \(G\) must vanish at infinity.
    
    We can evaluate this integral by closing the contour and using the Residue theorem.
    For \(t > 0\) we close the integral in the upper half plane with a semicircle.
    In this case \(G(t)\) depends on the position of the poles, \(\omega_{\pm}\), since their imaginary parts are positive for all \(\Omega\) and \(\kappa\).
    We then have
    \begin{equation}
        G(t) = -2\pi i \sum_{\omega_{\pm}} \Res\left( \frac{1}{2\pi} \frac{\e^{i\omega t}}{(\omega - \omega_{+})(\omega - \omega_{-})}, \omega_{i} \right) = -i\left[ \frac{\e^{i\omega_{+}t}}{\omega_{+} - \omega_{-}} + \frac{\e^{i\omega_{-}t}}{\omega_{-} - \omega_{+}} \right].
    \end{equation}
    We use Jordan's lemma to argue that the contribution from the semicircle vanishes.
    
    For \(t < 0\) we instead close the contour in the lower half plane and therefore there are no residues.
    We then simply have \(G(t) = 0\) for \(t < 0\).
    We now consider the nature of \(G(t)\) for \(t > 0\) in three cases.
    
    \subsubsection{Under Damped}
    Suppose \(\Omega > \kappa > 0\).
    Then both poles have the same imaginary part, namely \(\kappa\), and
    \begin{equation}
        \omega_{+} - \omega_{-} = 2\sqrt{\Omega^2 - \kappa^2} > 0.
    \end{equation}
    We then have
    \begin{equation}
        G(t) = \theta(t) \e^{-\kappa t} \frac{\sin(\sqrt{\Omega^2 - \kappa^2} t)}{\sqrt{\Omega^2 - \kappa^2}}.
    \end{equation}
    
    \subsubsection{Over Damped}
    Suppose \(\kappa > \Omega > 0\).
    Then the poles lie on the imaginary axis, \(\omega_{\pm} = -i\kappa \pm i\sqrt{\kappa^2 - \Omega^2}\), and we have
    \begin{equation}
        G(t) = \theta(t) \frac{\sinh(\sqrt{\kappa^2 - \Omega^2} t)}{\sqrt{\kappa^2 - \Omega^2}}.
    \end{equation}
    
    \subsubsection{Critical Damping}
    Suppose \(\kappa = \Omega\).
    Then the two poles coincide to give a pole of order 2 at \(i\kappa\).
    For \(t > 0\) we then have
    \begin{equation}
        G(t) = 2\pi i\Res\left( -\frac{1}{2\pi}\frac{\e^{i\omega t}}{(\omega - i\kappa)^2}, i\kappa \right) = -i(it)\e^{-\kappa t} = t\e^{-\kappa t}.
    \end{equation}
    For all \(t\) we therefore have
    \begin{equation}
        G(t) = \theta(t) t\e^{-\kappa t}.
    \end{equation}
    
    \subsection{Boundary Conditions}
    In the last section we assumed that the Green's function vanished at infinity, as did a suitable number of derivatives in order for everything to converge and all of our integrals to exist.
    We call the resulting Green's function the \defineindex{fundamental Green's function}.
    It corresponds to the assumption of homogenous boundary conditions at infinity.
    
    In all three cases we find that \(G\) does indeed vanish at infinity, and therefore the resulting function \(y\) that we find by convolving \(f\) with \(G\) is actually a particular solution, rather than a general solution, as we have already applied our boundary conditions.
    We can construct the general solution by adding solutions to the homogenous equation:
    \begin{equation}
        y(t) = \int_{-\infty}^{\infty} G(t - t')f(t') \dd{t'} + A\e^{i\omega_{+}t} + B\e^{-i\omega_{-}t}.
    \end{equation}
    
    We can also derive the same general solution from the Fourier transform of the differential equation by noticing that when solving for \(\tilde{y}\) we risk division by zero when \(\omega = \omega_{\pm}\).
    We can solve this by adding in suitable delta distributions which gives
    \begin{equation}
        \tilde{y}(\omega) = \frac{1}{\Omega^2 + 2i\kappa\omega - \omega^2}\tilde{f}(\omega) + 2\pi A\delta(\omega - \omega_{+}) + 2\pi B \delta(\omega - \omega_{-}),
    \end{equation}
    for arbitrary constants \(A\) and \(B\)
    Inverting this then gives us the extra two terms, which is why we include the \(2\pi\) here, so that \(A\) and \(B\) agree with their previous use, if we hadn't already done this we could drop the \(2\pi\) and our solution would just have \(A/(2\pi)\), which is just a scaled arbitrary constant.
    
    It is probably easier to go the way we did however, and solve for the particular solution and then just add in the homogeneous solution.
    
    \subsection{Causality}
    Recall that we interpret
    \begin{equation}
        y_{\mathrm{P}}(t) = \int_{-\infty}^{\infty} G(t - t')f(t') \dd{t'}
    \end{equation}
    as the integral of a stimulus, \(f\), at time \(t'\) times the response, \(G\), at time \(t\).
    In our example \(G(t, t') = 0\) for all \(t < t'\), and so there is no response in the past at time \(t\) for some future stimulus at time \(t'\).
    We say the system is causal, the cause (stimulus) comes before the effect (response).
    
    In the limit of \(\kappa \to 0\), that is no damping, the poles, \(\omega_{\pm}\), approach the real axis from above.
    Therefore for any non-zero amount of damping the poles will be in teh upper half plane, and hence our analysis is valid.
    But when \(\kappa = 0\) the poles are on the real axis and we would have to treat the contour integral specially.
    We would not automatically have \(G(t, t') = 0\) for \(t < t'\), instead we would have to impose this as a condition.
    
    One fix for this is that we can use the integral representation
    \begin{equation}
        \delta(x) = \frac{1}{2\pi} \int_{-\infty - i\varepsilon}^{\infty + i\varepsilon} \e^{ikx} \dd{k}
    \end{equation}
    for \(\varepsilon > 0\).
    We then end up with a contour integral that passes \emph{just} below the real axis, and hence the poles are always included in the interior of the contour when we close in the upper half plane for \(t > 0\).
    
    \section{Sine and Cosine Transforms}
    Certain functions with a high level of symmetry have simplified transforms in terms of sine and cosine.
    In particular if a function is even, that is \(f(-x) = f(x)\), it has a cosine transformation given by
    \begin{equation}
        \tilde{f}(k) = 2\int_{0}^{\infty} f(x)\cos(kx) \dd{x}.
    \end{equation}
    Similarly if the function is odd, that is \(g(-x) = -g(x)\), then it  has a sine transformation given by
    \begin{equation}
        \tilde{g}(k) = 2\int_{0}^{\infty} g(x) \sin(kx) \dd{x}.
    \end{equation}
    The inverse transformations are then
    \begin{equation}
        f(x) = \frac{1}{\pi} \int_{0}^{\infty} \tilde{f}(k) \cos(kx) \dd{k}, \quad\text{and}\quad g(x) = \frac{1}{\pi} \int_{0}^{\infty}  \tilde{g}(k) \cos(kx) \dd{k},
    \end{equation}
    respectively.
    These are simply the Fourier transforms but the symmetry causes either the real or imaginary part to vanish.
    
    We can also use these transformations for functions defined on \([0, \infty)\), by choosing to extend the definition to \(\reals\) such that the function is either even or odd.
    The choice is usually obvious, for example we would extend \(f \colon [0, \infty) \to \reals\) such that \(f(x) = x^2\), to \(\reals\) by choosing \(f(x) = x^2\) for all \(x \in \reals\), which is an even extension.
    Note that if \(f(0) \ne 0\) then the odd extension will have a discontinuity at \(0\) as \(f(\varepsilon)\) jumps to \(f(-\varepsilon) = -f(\varepsilon)\) crossing from positive to negative arguments, for arbitrarily small \(\varepsilon > 0\).
    
    \chapter{Laplace}
    \section{Laplace Transform}
    \begin{dfn}{Laplace Transform}{}
        Given a function \(f \colon [0, \infty) \to \complex\) the \defineindex{Laplace transform} is defined as the following integral, if it exists,
        \begin{equation}
            \laplaceTransform\{ f(t) \} = F(s) \coloneqq \int_{0}^{\infty} f(t)\e^{-st} \dd{t}.
        \end{equation}
    \end{dfn}
    
    From the definition we can see that in order for the integral to converge \(\abs{\e^{-\alpha t}f(t)}\), for some \(\alpha\in\complex\) with \(\Re(\alpha) > 0\), must be bounded for large \(t\).
    Hence we must have \(\abs{f(t)} \le \e^{\alpha t}\).
    We require that
    \begin{equation}
        F(s) \le \int_{0}^{\infty} \e^{(\alpha - s)} \dd{t} < \infty,
    \end{equation}
    which means that \(F\) is valid for \(s\) satisfying \(\Re(s) > \Re(\alpha)\).
    
    \subsection{Laplace Transforms of Common Functions}
    The Laplace transform of \(f(t) = 1\) is
    \begin{equation}
        \laplaceTransform\{ 1 \} = \int_{0}^{\infty} \e^{-st} = \left[ \frac{\e^{-st}}{s} \right]_{0}^{\infty} = \frac{1}{s}.
    \end{equation}
    The Laplace transform of \(f(t) = \e^{\omega t}\) is
    \begin{equation}
        \laplaceTransform\{ \e^{\omega t} \} = \int_{0}^{\infty} \e^{(\omega - s)} \dd{t} = \frac{1}{s - \omega}, \quad s > \omega.
    \end{equation}
    The Laplace transform of \(\e^{i\omega}\) follows from the previous integral replacing \(\omega \to i\omega\) giving
    \begin{equation}
        \laplaceTransform\{ \e^{i\omega t} \} = \frac{1}{s - i\omega} = \frac{s + i\omega}{s^2 + \omega^2}.
    \end{equation}
    By the linearity of the integral we have \(\Re(\laplaceTransform\{f(t)\}) = \laplaceTransform\{\Re(f(t))\}\), and similarly for \(\Im\).
    It then follows that
    \begin{equation}
        \Re(\laplaceTransform\{ \e^{i\omega t} \}) = \laplaceTransform\{ \Re(\e^{i\omega t}) \} = \laplaceTransform\{ \cos(\omega t) \} = \Re\left( \frac{s + i\omega}{s^2 + \omega^2} \right) = \frac{s}{s^2 + \omega^2}.
    \end{equation}
    Similarly
    \begin{equation}
        \laplaceTransform\{ \sin(\omega t) \} = \frac{\omega}{s^2 + \omega^2}.
    \end{equation}
    These both hold for \(s > 0\).
    The Laplace transform of \(f(t) = \delta(t - a)\) for some \(a \in \reals\) is
    \begin{equation}
        \laplaceTransform\{ \delta(t - a) \} = \int_{0}^{\infty} \delta(t - a)\e^{-st} \dd{t} = \e^{-sa}, \qquad s > 0.
    \end{equation}
    The Laplace transform of \(f(t) = \theta(t - a)\) for some \(a \in \reals\) is
    \begin{align}
        \laplaceTransform\{ \theta(t - a) \} &= \int_{0}^{\infty} \theta(t - a)\e^{-st} \dd{t}\\
        &= \int_{a}^{\infty} \e^{-st} \dd{t}\\
        &= \left[ -\frac{\e^{-st}}{s} \right]_{a}^{\infty}\\
        &= \frac{\e^{-sa}}{s}, \qquad s > 0.
    \end{align}
    The Laplace transform of \(f(t) = t^n\) for some \(n \in \naturals\) is
    \begin{align}
        \laplaceTransform\{ t^n \} &= \int_{0}^{\infty} t^n \e^{-st} \dd{t}\\
        &= \underbrace{\left[ -\frac{t^n}{s}\e^{-st} \right]_{0}^{\infty}}_{=0} + \frac{1}{s} \int_{0}^{\infty} t^{n-1} \e^{-st} \dd{t}\\
        &\vdotswithin{=}\\
        &= \frac{n!}{s^{n+1}}.
    \end{align}
    The linearity of the Laplace transform, which follows from the linearity of the integral, then allows us to use this formula to find the Laplace transform of any polynomial or power series.
    Alternatively by analytic continuation we have
    \begin{equation}
        \laplaceTransform\{t^{\nu}\} = \frac{\Gamma(\nu + 1)}{s^{\nu + 1}},
    \end{equation}
    which holds for \(\nu > -1\).
    
    \subsection{Laplace Transforms of Derivatives}
    Given a function, \(f\), and its Laplace transform, \(F\), the Laplace transform of \(f'\) is
    \begin{align}
        \laplaceTransform\{ f'(t) \} &= \int_{0}^{\infty} f'(t) \e^{-st} \dd{t}\\
        &= [f(t) \e^{-st}]_{0}^{\infty} + s\int_{0}^{\infty} f(t) \e^{-st} \dd{t}\\
        &= -f(0^+) + sF(s)
    \end{align}
    where \(f(0^+)\) means the value of \(f(t)\) as \(t\) tends to zero from above.
    
    The Laplace transform of \(f''\) then follows by applying this twice:
    \begin{align}
        \laplaceTransform\{ f''(t) \} &= -f'(0^+) + s\laplaceTransform\{ f'(t) \}\\
        &= -f'(0^+) - sf(0^+) + s^2F(s).
    \end{align}
    Spotting a pattern we see that the Laplace transform of the \(n\)th derivative is
    \begin{equation}
        \laplaceTransform\{ f^{(n)}(t) \} = s^nF(s) - s^{n-1}f(0^+) - s^{n-1}f'(0^+) - s^{n-2}f''(0^+) - \dotsb - f^{(n-1)}(0^+).
    \end{equation}
    
    \subsection{Derivatives of Laplace Transforms}
    Given some function, \(f\), with Laplace transform \(F\) we have
    \begin{align}
        \diff{F}{s} &= \diff{}{s} \int_{0}^{\infty} f(t) \e^{-st} \dd{t}\\
        &= \int_{0}^{\infty} f(t) \diff{}{s} \e^{-st} \dd{t}\\
        &= \int_{0}^{\infty} f(t)(-t\e^{-st})\dd{t}\\
        &= -\laplaceTransform\{ tf(t) \}.
    \end{align}
    For the second derivative we then have
    \begin{equation}
        \diff[2]{F}{s} = \diff{}{s}\laplaceTransform\{ -tf(t) \} = \laplaceTransform\{ t^2f(t) \}.
    \end{equation}
    Again spotting a pattern we have
    \begin{equation}
        \diff[n]{F}{s} = \laplaceTransform\{ (-t)^nf(t) \}.
    \end{equation}
    
    \subsection{Laplace Transforms of Integrals}
    Given a function, \(f\), with Laplace transform, \(F\), we have
    \begin{equation}
        \laplaceTransform\left\{ \int_{0}^{t} f(w) \dd{w} \right\} = \int_{0}^{\infty} \e^{-st} \int_{0}^{\infty} f(w) \dd{w} \dd{t}.
    \end{equation}
    Integrating by parts using
    \begin{alignat}{3}
        u &= \int_{0}^{t}f(w)\dd{w}, \qquad & v &= -\frac{\e^{-st}}{s},\\
        u' &= f(t), \qquad & v' &= \e^{-st},
    \end{alignat}
    we have
    \begin{align}
        \laplaceTransform\left\{ \int_{0}^{t} f(w) \dd{w} \right\} &= \bigg[ -\underbrace{\vphantom{\int_{0}^{t}}\frac{\e^{-st}}{s}}_{\substack{\to 0\text{as}\\ t \to \infty}} \underbrace{\int_{0}^{t} f(w) \dd{w}}_{\to 0 \text{ as } t \to 0} \bigg]_{0}^{\infty} + \frac{1}{s} \int_{0}^{t} f(t) \e^{-st} \dd{t}\\
        &= \frac{1}{s}F(s).
    \end{align}
    
    \section{Inversion Theorem}
    Broadly speaking there are two methods for inverting Laplace transforms, the \enquote{engineering way}, looking the function up in a table of known Laplace transforms, and the \enquote{maths way}, using the inverse integral, which is what we are going to find in this section.
    
    By definition for a function, \(f\), with a Fourier transform we have
    \begin{equation}
        \inverseFourierTransform\{ \fourierTransform\{ f(t) \} \} = t(t).
    \end{equation}
    In particular consider this for some function \(F\) defined by
    \begin{equation}
        F(t) = \theta(t) f(t) \e^{-\gamma t}
    \end{equation}
    for some constant \(\gamma \in \reals\), which is such that \(\e^{-\gamma t}f(t)\) is bounded.
    We therefore have
    \begin{align}
        \theta(t)f(t) &= e^{\gamma t}\inverseFourierTransform\{ \fourierTransform\{ \e^{-\gamma u}f(t)\theta(t) \} \}\\
        &= \frac{\e^{\gamma t}}{2\pi} \int_{-\infty}^{\infty} \dl{\omega} \, \e^{i\omega t} \int_{-\infty}^{\infty} \dl{u} , \e^{-i\omega u} \e^{-\gamma u} f(u) \theta(u)\\
        &= \frac{e^{\gamma t}}{2\pi} \int_{-\infty}^{\infty} \dl{\omega} \, \e^{i\omega t} \int_{0}^{\infty} \dl{u} \, \e^{-i\omega u}\e^{-\gamma u} f(u)\\
        &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \dl{\omega} \, \e^{i(\omega - i\gamma)t} \int_{0}^{\infty} \dl{u} \, \e^{-iu(\omega - i\gamma)} f(u).
    \end{align}
    Now, defining \(s = \gamma + i\omega\) we have \(\dl{s} = i\dd{\omega}\), so \(\dl{\omega} = \dl{s}/i\), and the limits change from \(\pm \infty\) to \(\gamma \pm i\infty\), which gives
    \begin{align}
        \theta(t)f(t) &= \frac{1}{2\pi i} \int_{\gamma - i\infty}^{\gamma + i\infty} \dl{s} \, \e^{st} \int_{0}^{\infty} \dl{u} \, \e^{-st}f(u)\\
        &= \frac{1}{2\pi i} \int_{\gamma - i\infty}^{\gamma + i\infty} \dl{s} \, \e^{st} \laplaceTransform\{ f(u) \}.
    \end{align}
    For \(t > 0\) the left hand side is simply \(f(t)\) and we see that this integral is the inverse Laplace transform:
    \begin{equation}
        \inverseLaplaceTransform\{ F(s) \} = \frac{1}{2\pi i}\int_{\gamma - i\infty}^{\gamma + i\infty} \e^{st} F(s) \dd{s}.
    \end{equation}
    This is known as the \defineindex{Bromwich inversion formula}.
    
    Up to now we have been intentionally vague about what exactly \(\gamma\) is.
    This is because the value depends on the exact nature of \(F\).
    In particular we choose \(\gamma\) such that all \enquote{interesting} points are to the left in the complex plane.
    By interesting points we mean poles, branch points, and branch cuts.
    
    For large \(s\) and \(t > 0\) we require \(\Re(s) < 0\) for the integral to converge, since it has a factor of \(\e^{st}\).
    We therefore must close the contour on the left, entering the region of \enquote{interesting} points, the exact way we close the integral again depends on the nature of these interesting points, but ideally we would use a circular arc and if needed a key hole contour for a branch cut.
    
    For large \(s\) and \(t < 0\) we require \(\Re(s) > 0\) for the integral to converge.
    We therefore close the contour on the right, in the region free from \enquote{interesting} points.
    The integral is then trivially zero, which makes sense as we only define the Laplace transform for \(t > 0\).
    
    \begin{exm}{}{}
        Consider the function \(F(s) = 1/s\).
        This has a simple pole at \(s = 0\) and therefore for \(t > 0\) we have
        \begin{equation}
            f(t) = \frac{1}{2\pi i}\int_{\gamma - i\infty}^{\gamma + i\infty} \frac{\e^{st}}{s} \dd{s} = \Res\left( \frac{\e^{st}}{s}, 0 \right) = \e^{0} = 1.
        \end{equation}
        Therefore \(f(t) = \theta(t)\).
        
        Now consider \(F(s) = 1/s^{m+1}\),
        This has a pole of order \(m + 1\) at \(s = 0\) and therefore for \(t > 0\) we have
        \begin{equation}
            f(t) = \frac{1}{2\pi i}\int_{\gamma - i\infty}^{\gamma + i\infty} \frac{\e^{st}}{s} \dd{s} = \Res\left( \frac{\e^{st}}{s^{m+1}}, 0 \right) = \frac{t^m}{m!},
        \end{equation}
        where we find the residue by considering the \(m\)th order term in the Taylor expansion for \(\e^{st}\), which, when divided by \(s^{m+1}\), gives us a term proportional to \(1/s\), which gives our residue when evaluated at \(s = 0\).
        We then have \(f(t) = t^m\theta(t)/m!\).
        This is in agreement with \(\laplaceTransform\{t^\nu\} = \Gamma(\nu + 1) / s^{\nu + 1}\).
    \end{exm}
    
    \section{Convolution}
    The definition of convolution that we use in the context of Laplace transforms is slightly different.
    \begin{dfn}{Convolution}{}
        Given two functions, \(f, g \colon [0, \infty) \to \complex\), we define their \defineindex{convolution} as
        \begin{equation}
            (f \circ g)(t) \coloneqq \int_{0}^{t} f(t - y)g(y) \dd{y}.
        \end{equation}
    \end{dfn}
    
    The reason for restricting the limit is that we require the arguments of the functions, in particular \(t - y\), be positive.
    Taking the Laplace transform we get
    \begin{align}
        \laplaceTransform\{ (f \circ g)(t) \} &= \int_{0}^{\infty} \dl{t} \, \e^{-st}(f \circ g)(t)\\
        &= \int_{0}^{\infty} \dl{t} \, \e^{-st} \int_{0}^{t} \dl{y} \, f(t - y)g(y)\\
        &= \int_{0}^{\infty} \dl{y} \int_{y}^{\infty} \dl{t} \, \e^{-st} f(t - y)g(y)\\
        &= \int_{0}^{\infty} \dl{y} \int_{0}^{\infty} \dl{u} \, \e^{-s(u + z)} f(u)g(y)\\
        &= \int_{0}^{\infty} \dl{y} \e^{-su} f(u) \int_{0}^{\infty} \e^{sy}g(y)\\
        &= \laplaceTransform\{ f(t) \} \laplaceTransform\{ g(t) \}.
    \end{align}
    Here we made a substitution \(u = t - z\), we also used the fact that integrating for all \(t\) and \(y\) running from \(0\) to \(t\) is the same as integrating for all \(y\) and \(t\) running from \(y\) to \(\infty\).
    This can be seen by drawing the line \(y = t\) in the \((y,t)\)-plane, the region of integration is the triangle under this line and above the \(y\)-axis.
    Before we swap the integrals we going up to this line, and then along to infinity.
    After swapping the integrals we are going from this line to infinity and then down to zero.
    
    It can also be shown that
    \begin{equation}
        \laplaceTransform\{f(t)g(t)\} = \frac{1}{2\pi i} \int_{\gamma - i\infty}^{\gamma + i\infty} F(z)G(s - z) \dd{z}
    \end{equation}
    where \(F = \laplaceTransform\{f\}\) and \(G = \laplaceTransform\{g\}\).
    
    \section{Solving ODEs with Laplace Transforms}
    The general method for solving differential equations with the Laplace transform is the same as for the Fourier transform.
    Transform the differential equation and boundary conditions, solve the transformed equations, transform back.
    
    \subsection{Damped Harmonic Oscillators}
    We will solve the damped harmonic oscillator equation,
    \begin{equation}
        \ddot{u}(t) + 2\kappa\dot{u}(t) + \omega_0^2u(t) = 0,
    \end{equation}
    for the initial conditions \(u(0) = u_0\) and \(\dot{u}(0) = v_0\).
    Defining \(F(s) = \laplaceTransform\{u(t)\}\) we have
    \begin{align}
        \laplaceTransform\{\dot{u}(t)\} &= sF(s) - u(0) = sF(s) - u_0,\\
        \laplaceTransform\{\ddot{u}(t)\} &= sF(s) - su(0) - \dot{u}(0) = sF(s) - su_0 - v_0.
    \end{align}
    So, the transformed differential equation is
    \begin{equation}
        s^2F(s) - su_0 - v_0 + 2\kappa[sF(s) - u_0] + \omega_0^2 F(s) = 0.
    \end{equation}
    Solving for \(F(s)\) we have
    \begin{equation}
        F(s) = \frac{(s + 2\kappa)u_0 + v_0}{(s + \kappa)^2 + \omega^2}
    \end{equation}
    where we have defined \(\omega^2 \coloneqq \omega_0^2 - \kappa^2\).
    
    We now have to invert this to find \(u\).
    We can do so in two ways.
    First, using tables of Laplace transforms we find
    \begin{equation}
        \laplaceTransform\{\e^{-\kappa t}\cos(\omega t)\} = \frac{s + \kappa}{(s + \kappa)^2 + \omega^2}, \qqand \laplaceTransform\{\e^{-\kappa t}\sin(\omega t)\} = \frac{\omega}{(s + \kappa)^2 + \omega^2}.
    \end{equation}
    To use these we write \(F(s)\) as
    \begin{equation}
        F(s) = u_0\frac{s + \kappa}{(s + \kappa)^2 + \omega^2} + (u_0\kappa + v_0)\frac{1}{\omega} \frac{\omega}{(s + \kappa)^2 + \omega^2}.
    \end{equation}
    We the have
    \begin{equation}
        u(t) = u_0\e^{-\kappa t}\cos(\omega t) + (u_0\kappa + v_0)\frac{1}{\omega}\e^{-\kappa t}\sin(\omega t).
    \end{equation}
    
    Second, we can use the inversion theorem and evaluate
    \begin{equation}
        u(t) = \frac{1}{2\pi i} \int_{\gamma - i\infty}^{\gamma + i\infty} \frac{(s + 2\kappa)u_0 + v_0}{(s - s_{+})(s - s_{-})} \e^{st} \dd{s}
    \end{equation}
    where \(s_{\pm} = -\kappa \pm i\omega\) are the solutions to \((s + \kappa)^2 + \omega^2 = 0\), and \(\gamma > - \kappa\).
    For \(t > 0\) we close the contour to the left with a semi circle, surrounding both poles.
    We can argue by bounding that the contribution to the integral from the semicircle vanishes and so the integral around the closed contour is equal to the desired integral.
    We can then evaluate this with the residue theorem giving
    \begin{equation}
        u(t) = \frac{(s_{+} + 2\kappa)u_0 + v_0}{(s_{+} - s_{-})}\e^{s_{+}t} + \frac{(s_{-} + 2\kappa)u_0 + v_0}{s_{-} - s_{+}}\e^{s_{-}t}.
    \end{equation}
    Some algebra then shows that this reduces to the same solution.
    
    \subsection{Coupled Equations}
    Consider the decay process where particles of type 1 decay to particles of type 2, which decay to particles of type 3, which decay to a stable product.
    Let \(N_i(t)\) be the number of particles of type \(i\) at time \(t\).
    Let \(\lambda_i\) be the decay rate of type \(i\) decaying.
    As usual the rate at which \(N_i\) is taken to be proportional to the number of particles, so for type 1 we have the standard decay equation
    \begin{equation}
        \diff{N_1}{t} = -\lambda_1 N_1.
    \end{equation}
    For type 2 the number of particles decreases due to the decay, but also increases due to particles of type 1 decaying into type 2.
    More precisely
    \begin{equation}
        \diff{N_2}{t} = \lambda_1N_1 - \lambda_2N_2.
    \end{equation}
    Similarly for type 3
    \begin{equation}
        \diff{N_3}{t} = \lambda_2N_2 - \lambda_3N_3.
    \end{equation}
    
    We now have three coupled differential equations.
    Suppose we wish to solve the for a system which initially has \(N\) particles of type 1 and \(n\) particles of type 3, that is \(N_1(0) = N\), \(N_2(0) = 0\), and \(N_3(0) = n\).
    Let \(F_i(s) = \laplaceTransform\{N_i(t)\}\).
    Taking the Laplace transform of our equations we then have
    \begin{align}
        sF_1(s) - N &= -\lambda_1F_1(s),\\
        sF_2(s) &= \lambda_1F_1(s) - \lambda_2F_2(s),\\
        sF_3(s) - n &= \lambda_2F_2(s) - \lambda_3F_3(s).
    \end{align}
    We need to solve for \(F_i\).
    We can easily the first equation for \(F_1\) giving
    \begin{equation}
        F_1(s) = \frac{N}{s + \lambda_1}.
    \end{equation}
    Solving the second equation for \(F_2\) and then substituting in our solution for \(F_1\) above we get
    \begin{equation}
        F_2(s) = \frac{\lambda_1F_1(s)}{s + \lambda_2} = \frac{\lambda_1N}{(s + \lambda_1)(s + \lambda_2)}.
    \end{equation}
    Finally solving the third equation for \(F_3\) and substituting in our solution for \(F_2\) above we get
    \begin{equation}
        F_3(s) = \frac{n}{s + \lambda_3} + \lambda_2\frac{F_2(s)}{s + \lambda_3} = \frac{n}{s + \lambda_3} + \frac{\lambda_1\lambda_2N}{(s + \lambda_3)(s + \lambda_2)(s + \lambda_1)}.
    \end{equation}
    
    Suppose that the quantity we are interested in is \(N_3\).
    We can find this by computing
    \begin{equation}
        N_3(t) = \frac{1}{2\pi i} \int_{\gamma - i\infty}^{\gamma + i\infty} \left[ \frac{n}{s + \lambda_3} + \frac{\lambda_1\lambda_2N}{(s + \lambda_3)(s + \lambda_2)(s + \lambda_1)} \right] \e^{st} \dd{s}.
    \end{equation}
    We can close the contour and show that the extra part of the contour doesn't contribute and so we can compute this integral by the Rsidue theorem, if we do then we find that
    \begin{align}
        N_3(t) = \sum_{i = 1}^{3} \Res(-\lambda_i)\\
        &= \frac{\lambda_1\lambda_2N\e^{-\lambda_1t}}{(\lambda_2 - \lambda_1)(\lambda_3 - \lambda_1)} + \frac{\lambda_1\lambda_2N\e^{-\lambda_2t}}{(\lambda_1 - \lambda_2)(\lambda_3 - \lambda_2)} + n + \frac{\lambda_1\lambda_2N\e^{-\lambda_3t}}{(\lambda_1 - \lambda_3)(\lambda_2 - \lambda_3)}.
    \end{align}
    
    For large \(t\), since each term, apart from \(n\), is proportional to \(\e^{-\lambda_it}\), the value of \(N_3(t)\) is determined by the term where \(\lambda_i\) has the smallest real part, plus the constant \(n\).
    For example, if \(\lambda_3\) has the smallest real part then 
    \begin{equation}
        N_3(t) \sim \frac{\lambda_1\lambda_2N\e^{-\lambda_3t}}{(\lambda_1 - \lambda_3)(\lambda_2 - \lambda_3)} + n.
    \end{equation}
    For even larger times \(N_3(t) \approx n\) and we have a mostly steady-state where the number of particles decaying to type 3 is balanced by the number of particles of type 3 decaying.
    
    \subsubsection{Asymptotic Behaviour}
    We have seen here a general principle of Laplace transform inversion, which is that the asymptotic behaviour is given by the \enquote{singularity}\footnote{we use this word here in the physics sense, to mean poles, branch points, and branch cuts.} closest to the contour, that is the \enquote{singularity} with the largest real part.
    
    If the \enquote{singularity}, say \(s_0\), is an isolated pole then we calculate \(\Res(s_0)\) to find the asymptotic behaviour.
    If the \enquote{singularity} is instead a branch point then we expand around this point and find that
    \begin{equation}
        F(s) \sim (s - s_0)^\nu \sum_{n=0}^{\infty} a_n(s - s_0)^n.
    \end{equation}
    Note that this is a series around a branch point, so likely won't converge, but this is fine as we are looking at the asymptotic behaviour.
    
    \section{Integral Representations}
    We can use the inversion theorem to find integral representations for special functions.
    For example we have seen that \(\laplaceTransform\{t^\nu\} = \Gamma(\nu + 1)/s^{\nu + 1}\) for \(\nu > -1\).
    Therefore
    \begin{equation}
        \frac{t^{\nu}}{\Gamma(\nu + 1)} = \inverseLaplaceTransform\left\{ \frac{1}{s^{\nu + 1}} \right\} = \frac{1}{2\pi i} \int_{\gamma - i\infty}^{\gamma + i\infty} \frac{\e^{st}}{s^{\nu + 1}} \dd{s}.
    \end{equation}
    For this we require that \(\gamma > 0\) so that we are to the right of \(s = 0\), which is a branch point for non-integer \(\nu\).
    Take the branch cut to be along the negative real axis.
    We then Close this contour to the left of \(\gamma\).
    Doing so we have to avoid the branch cut, which we can do using a Hankel type contour, which looks like a semicircle, except that upon reaching the real axis it goes along the axis to the origin, then back underneath the axis in a way similar to \cref{fig:hankel contour}.
    
    The result is a contour containing no singularities.
    We can show that the integrals not along the branch cut don't contribute as the contour becomes infinitely large.
    Our inversion integral is then equal to the negative of the contour, \(C\), around the branch cut, which is now exactly like \cref{fig:hankel contour}, except reflected in the imaginary axis.
    We then have
    \begin{equation}
        \frac{1}{\Gamma(\nu + 1)} = t^{-\nu} \frac{1}{2\pi i} \int_{-C} \frac{\e^{st}}{s^{\nu + 1}} \dd{s} = \frac{1}{2\pi i} \int_{-C} \e^{z}z^{-\nu - 1} \dd{z}.
    \end{equation}
    Here we have defined \(z = st\).
    We have found an integral representation of \(1/\Gamma(\nu + 1)\).
    This can be verified by parametrising the integral above as \(z = \e^{i\pi u}\) and below as \(z = \e^{-i\pi u}\), and then identifying the integral for \(\Gamma(-\nu)\) and using Euler's reflection formula.
    
    \chapter{Generalised Transform Method}
    The Fourier transform and Laplace transform are very similar.
    In fact there are many transforms that are useful, these are just two of the most common.
    Another example may be the Mellin Transform, defined as
    \begin{equation}
        \varphi(z) = \int_{0}^{\infty} f(t) t^{z-1} \dd{t}, \qqand f(t) = \frac{1}{2\pi i} \int_{c - i\infty}^{c + i\infty} \varphi(z) t^{-z} \dd{z}.
    \end{equation}
    This is pretty much the Laplace transform replacing \(\e^{-st}\) with \(t^{z - 1}\).
    
    Another transform that one might consider is the Hilbert transform
    \begin{equation}
        g(y) = \frac{1}{\pi} \mathsf{P} \!\! \int_{-\infty}^{\infty}\nu\frac{f(x)}{x - y} \dd{x}, \qqand \frac{1}{\pi} \mathsf{P} \! \int_{-\infty}^{\infty} \frac{g(y)}{y - x} \dd{y}.
    \end{equation}
    Here \(\mathsf{P}\) denotes the Cauchy principle value of the integral, which is to say we split the integral at the troublesome point, \(x = y\), and compute the integrals either side as we take the limit of the bounds tending to that point.
    For more details see the complex analysis part of the methods of theoretical physics course.
    
    We can define a general integral transform of some function \(v\) into the function \(u\) as
    \begin{equation}
        u(z) = \int_{C} v(s) K(z, s) \dd{s}
    \end{equation}
    where \(C\) is some contour in \(\complex\), and \(K\) is the \defineindex{kernel}.
    Some common choices for the kernel include \(\e^{isz}\), like in the Fourier transform, \(\e^{sz}\), like in the Laplace transform, \(z^{-s}\), like in the Mellin transform, \(sJ_n(sz)\), the list goes on.
    Note that even if we use the Fourier-like kernel the transform is not necessarily the Fourier transform since we are considering some contour \(C\), and the Fourier transform is defined by an integral along the real axis.
    
    \section{Solving ODEs}
    Consider the homogenous linear ODE
    \begin{equation}
        \linop_{z}u(z) = 0.
    \end{equation}
    We make the ansatz that this has a solution of the form
    \begin{equation}
        u(z) = \int_C K(z, s) v(s) \dd{s}.
    \end{equation}
    Since \(\linop_z\) acts on \(z\) and the integral is with respect to \(s\) we can swap them giving
    \begin{equation}
        0 = \linop_{z}u(z) = \linop_{z} \int_C K(z, s) v(s) \dd{s} = \int_C K(z, s) v(s) \dd{s}.
    \end{equation}
    Since \(\linop_z\) acts only on \(z\) it has no effect on \(v(s)\).
    We can therefore consider \(\linop_z K(z, s)\).
    What we do is look for some linear operator on \(s\), say \(\mathcal{M}_s\) such that
    \begin{equation}
        \linop_{z}K(z, s) = \mathcal{M}_sK(z, s).
    \end{equation}
    Once we find a suitable operator we have
    \begin{equation}
        0 = \int_{C} [\mathcal{M}_sK(z, s)]v(s)\dd{s}.
    \end{equation}
    We can integrate this by parts giving
    \begin{equation}\label{eqn:laplace method goal}
        0 = [R(z, s)]_{C} - \int_C K(z, s)[\mathcal{M}_s^\hermit v(s)] \dd{s}.
    \end{equation}
    Here \(\mathcal{M}_s^\hermit\) is the adjoint operator of \(\mathcal{M}_s\).
    Don't worry too much about what this means, its just another operator and we never actually need to compute the adjoint, it appears naturally as we'll see in examples.
    It is defined such that
    \begin{equation}
        \mathcal{M}_s[K(z, s)v(s)] = [\mathcal{M}_sK(z, s)]v(s) + K(z, s)[\mathcal{M}_s^\hermit v(s)].
    \end{equation}
    The term \([R(z, s)]_C\) is some boundary term, and the integral is the transform of \(\mathcal{M}_s^\hermit v(s)\).
    We demand that both the boundary term and transform term vanish independently.
    Since we can only choose the boundary term to vanish by choosing \(C\) we must therefore have that \(\mathcal{M}_sv(s)\) vanishes in order for the integral to vanish.
    
    There are a few standard choices for \(C\):
    \begin{itemize}
        \item A closed contour enclosing a region in which \(R\) is analytic.
        \item An interval with \(R\) vanishing at the endpoints.
    \end{itemize}
    If we can continuously deform a contour into another contour without crossing poles, branch points, or branch cuts then the two contours will give the same solution.
    If we can't then the contours will give linearly independent solutions.
    
    \section{Confluent Hypergeometric Equation}
    The \defineindex{confluent hypergeometric equation} is
    \begin{equation}
        zu'' + (c - z)u' - au = 0
    \end{equation}
    for some parameters \(a\) and \(c\).
    We can solve this with a Laplace-like kernel, \(K(z, s) = \e^{sz}\).
    We make the ansatz that
    \begin{equation}
        u(z) = \int_C \e^{sz}v(s) \dd{s}
    \end{equation}
    for some function \(v\) and contour \(C\) to be determined.
    
    We can calculate the derivatives with respect to \(z\) easily since the integral is with respect to \(s\):
    \begin{align}
        u'(z) &= \int_C s\e^{sz}v(s)\dd{s},\\
        u''(z) &= \int_C s^2\e^{sz}v(s)\dd{s}.
    \end{align}
    Substituting these back into the differential equation we get
    \begin{equation}
        0 = \int_C [zs^2 + (c - z)s - a]\e^{sz}v(s) \dd{s}.
    \end{equation}
    We want to rewrite the term in the square brackets in such a way that separate out the \(z\) part and the non-\(z\) part, which can be done as follows
    \begin{equation}
        0 = \int_C [z(s^2 - s)]\e^{sz}v(s) \dd{s} + \int_C [cs - a]\e^{sz}v(s)\dd{z}.
    \end{equation}
    We leave the second term as it is, we integrate the first term by parts:
    \begin{alignat}{2}
        a &= z(s^2 - s)v(s), \qquad & v\hphantom{'} &= \frac{1}{z}\e^{sz},\\
        a' &= z(2s - 1)v(s) + z(s^2 - s)v'(s), \qquad & v' &= \e^{sz},
    \end{alignat}
    and so the first integral becomes
    \begin{multline}
        \left[ z(s^2 - s)v(s)\frac{1}{z}\e^{sz} \right]_C - \int_C \frac{1}{z}\e^{sz}[z(2s - 1)v(s) + z(s^2 - s)v'(s)]\\
        = [s(s - 1)v(s) \e^{sz}]_C - \int_C \e^{sz}[(2s - 1)v(s) + s(s - 1)v'(s)] \dd{s}.
    \end{multline}
    Putting this back with the second integral we have
    \begin{equation}
        0 = [s(s - 1)v(s)\e^{sz}]_C + \int_C [\{(cs - a) - (2s - 1)\}v(s) - s(s - 1)v'(s)]\e^{sz} \dd{s}.
    \end{equation}
    Which simplifies slightly to
    \begin{equation}
        0 = [s(s - 1)v(s)\e^{sz}]_C + \int_C [[s(c - 2) -a + 1]v(s) - s(s - 1)v'(s)] \e^{sz} \dd{s}.
    \end{equation}
    This is now in the same form as \cref{eqn:laplace method goal}.
    
    We require that the integrand vanishes for all \(s\), which must mean that
    \begin{equation}
        [s(c - 2) - a + 1]v(s) - s(s - 1)v'(s) = 0.
    \end{equation}
    This is a simple first order separable differential equation, rearranging we have
    \begin{equation}\label{eqn:hypergeometric transformed}
        \frac{v'(s)}{v(s)} = \frac{s(c - 2) - a + 1}{s(s - 1)}.
    \end{equation}
    A partial fractions decomposition gives
    \begin{equation}
        \frac{s(c - 2) - a + 1}{s(s - 1)} = \frac{a - 1}{s} + \frac{c - a + 1}{s - 1}.
    \end{equation}
    We can now integrate \cref{eqn:hypergeometric transformed} to get
    \begin{equation}
        \ln[v(s)] = (a - 1)\ln s + (c - a + 1)\ln(s - 1) + \mathcal{C}
    \end{equation}
    where \(\mathcal{C}\) is some constant of integration.
    We are free to choose this constant, so we take it to be 0 giving
    \begin{equation}
        v(s) = s^{a - 1}(s - 1)^{c - a + 1}.
    \end{equation}
    We now know that the solution is
    \begin{equation}
        u(z) = \int_C s^{a - 1}(s - 1)^{c - a - 1} \e^{sz} \dd{s}.
    \end{equation}

    We simply have to choose a contour, \(C\), such that
    \begin{equation}
        [s(s - 1)v(s)\e^{sz}]_C = [s^a(s - 1)^{c-a}\e^{sz}]_C = 0.
    \end{equation}
    The choice of this contour depends on the nature of \(a\) and \(c\), we will consider a few cases here.
    
    \subsection{\texorpdfstring{\(a, c \in \integers\) and \(\Re(z) > 0\)}{Integer a and c and Re(z) > 0}}
    \subsubsection{\texorpdfstring{\(a \le 0\) and \(c - a \le 0\)}{a and c - a Both Negative}}
    In this case we have poles at \(s = 0\) and \(s = 1\).
    Two independent contours that we can choose are closed contours encircling each pole.
    
    \subsubsection{\texorpdfstring{\(a > 0\) and \(c - a \le 0\)}{a Positive and c - a Less Than or Equal to Zero}}
    In this case  there is a pole at \(s = 1\), but no longer at \(s = 0\).
    We can still use a closed contour around the pole at \(s = 1\).
    For the second contour we instead choose the interval \((-\infty, 0)\), since \(\e^{sz}\) vanishes at \(s = -\infty\) and \(s^a\) vanishes at \(s = 0\), so the boundary term vanishes at both ends of the interval.
    
    \subsubsection{\texorpdfstring{\(a > 0\) and \(c - a > 0\)}{a and c - a Positive}}
    In this case there are no poles, and hence no closed contours that we can use to get a nontrivial answer.
    Instead we use two open contours.
    We can use the \((-\infty, 0)\) contour from the last section, and we can also use \((0, 1)\) since \(s^a\) vanishes at \(s = 0\) and \((s - a)^{c - a}\) vanishes at \(s = 1\).
    
    \subsection{\texorpdfstring{\(a \in \integers\), \(c \notin \integers\), and \(\Re(z) > 0\)}{Integer a, Noninteger c, and Positive Re(z)}}
    \subsubsection{\texorpdfstring{\(a \le 0\)}{a Less Than or Equal to 0}}
    In this case there is a pole at \(s = 0\).
    We can therefore choose one contour to be a closed contour about the origin.
    There is a branch point at \(s = 1\) since \(c - a \notin \integers\).
    There is a second branch point at infinity.
    The obvious choice of branch cut is along the positive real axis, starting at \(s = 1\).
    We can also use a contour from \(-\infty\) to \(s = 0\), since \(\e^{sz}s^a(s - 1)^{c - a}\) vanishes at these two points.
    
    \subsection{\texorpdfstring{\(a \notin \integers\), \(c \in \integers\) and \(\Re(z) > 0\)}{Noninteger a, Integer c, and Re(z) Positive}}
    There are branch points at \(s = 0\) and \(s = 1\), but not at infinity.
    We therefore consider a branch cut from 0 to 1.
    
    \subsubsection{\texorpdfstring{\(\Re(a), \Re(c) > 0\)}{Re(a) and Re(c) Positive}}
    Since \(s^{a}(s-1)^{c-a}\e^{sz}\) vanishes at \(s = 0, 1, -\infty\) there are 5 possible contours we can choose connecting two points in such a way as to avoid the branch cut, in particular we can choose 0 to \(-\infty\) along the negative real axis, 0 to 1 above the branch cut, 0 to 1 below the branch cut, 1 to \(-\infty\) above the branch cut, or 1 to \(-\infty\) below the branch cut.
    Any two linearly independent contours from this list should work, for example we may choose 0 to \(-\infty\) and 0 to 1 above the branch cut.
    We couldn't choose \(0\) to \(-\infty\) and \(1\) to \(-\infty\) since these contours overlap.
    
    \subsubsection{\texorpdfstring{\(\Re(a) < 0\) and \(\Re(c) > 0\)}{Re(a) Negative and Re(c) Positive}}
    One option is to choose one contour from \(0\) to \(-\infty\) and the other to be a loop around the branch cut.
    
    \section{Airy Equation}
    Airy's differential equation is
    \begin{equation}
        u''(z) - zu(z) = 0.
    \end{equation}
    We will look for a solution of the form
    \begin{equation}
        u(z) = \int_C \e^{sz}v(s)\dd{s}.
    \end{equation}
    We start by finding the derivatives:
    \begin{equation}
        u''(z) = \int_C s^2\e^{sz}v(s) \dd{s}.
    \end{equation}
    Substituting this into our differential equation we have
    \begin{equation}
        0 = \int_C \e^{sz}[s^2 - z]v(s) \dd{s}.
    \end{equation}
    Integrating the second term by parts
    \begin{alignat}{2}
        a &= zv(s), \qquad & v\hphantom{'} &= \frac{1}{z}\e^{sz},\\
        a' &= zv'(s), \qquad & v' &= \e^{sz},
    \end{alignat}
    we have
    \begin{equation}
        \int_{C} z\e^{sz}v(s) \dd{s} = \left[ \frac{1}{z}\e^{sz}zv(s) \right]_C - \int_C zv'(s)\frac{1}{z}\e^{sz}.
    \end{equation}
    Combining this back with the first term we have
    \begin{equation}
        -[v(s)\e^{sz}]_C + \int_C \e^{sz}[s^2v(s) + v'(s)]\dd{s} = 0.
    \end{equation}
    We therefore choose \(v\) such that
    \begin{equation}
        s^2v(s) - v'(s) = 0 \implies \frac{v'}{v} = -s^2 \implies \ln [v(s)] = A\e^{-s^3/3}
    \end{equation}
    for some constant of integration, \(A\).
    
    We therefore have the solution
    \begin{equation}
        u(z) = \int_C \e^{zs - s^3/3} \dd{s}.
    \end{equation}
    We choose \(C\) such that
    \begin{equation}
        [v(s)\e^{sz}]_C = [\e^{zs - s^3/3}]_C = 0.
    \end{equation}
    
    Now, the integrand for \(u(z)\) above has no singularities, this means that \(C\) cannot be a closed contour.
    The boundary term has no zeros for finite \(s\), so we must consider infinity.
    At \(s = \infty\) let \(s = R\e^{i\vartheta}\), and we'll take \(R \to \infty\).
    The dominating term is
    \begin{equation}
        \e^{-s^3/3} = \exp\left[ -\frac{R^3}{3}(\cos(3\vartheta) + i\sin(3\vartheta)) \right],
    \end{equation}
    except when \(\cos(3\vartheta) = 0\), in which case \(\e^{sz}\) dominates.
    We need to determine in what directions the boundary term vanishes as \(s \to \infty\).
    To do so we rewrite it as
    \begin{align}
        \e^{sz-s^3/3} &= \exp\left[ R(\cos\vartheta + i\sin\vartheta)z - \frac{R^3}{3}\{\cos(3\vartheta) + i\sin(3\vartheta)\} \right]\\
        \abs{\e^{sz - s^3/3}} &= \exp\left[ -\frac{R^3}{3}\cos(3\vartheta)\left\{ 1 - \frac{3z}{R^2}\frac{\cos\vartheta}{\cos(3\vartheta)} \right\} \right].
    \end{align}
    For \(\cos(3\vartheta) > 0\) and \(\Re(z) > 0\) this vanishes in the limit \(R \to \infty\).
    
    The first requirement, that \(\cos(3\vartheta)\), holds in three regions:
    \begin{alignat}{4}
        -\frac{\pi}{2} &< 3\vartheta < \frac{\pi}{2} \qquad\qquad & -\frac{\pi}{6} &< \vartheta < \frac{\pi}{6}\\
        -\frac{3\pi}{2} &< 3\vartheta < \frac{5\pi}{2} \qquad\qquad & -\frac{\pi}{2} &< \vartheta < \frac{5\pi}{6}\\
        -\frac{5\pi}{2} &< 3\vartheta < -\frac{3\pi}{2} \qquad\qquad & -\frac{5\pi}{6} &< \vartheta < \frac{\pi}{2}
    \end{alignat}
    These are depicted in \cref{fig:airy regions}.
    Outside of these regions \(\cos(3\vartheta) < 0\) and so for \(\Re(z) > 0\) the function diverges as \(R \to \infty\).
    The lines separating these regions are known as the \define{Stokes lines}\index{Stokes line}.
    
    If we have a contour entirely within one region then in the limit of \(R \to \infty\) the function will vanish everywhere on the contour and so the result is the trivial solution.
    For nontrivial results we instead require contours that pass between regions.
    Some possible contours are shown in \cref{fig:airy regions}.
    We can use these to complete a contour at infinity, by integrating along each contour.
    Since there are no poles this integral will vanish and the result is that we can determine the integral on one contour entirely from the integrals on the two other contours.
    This means that there are only two independent contours and hence two solutions.
    
    \begin{figure}
        \tikzsetnextfilename{airy-generalised-transform-regions}
        \begin{tikzpicture}
            \begin{scope}
                \clip (-4, -4) rectangle (4, 4);
                \fill[highlight] (0, 0) -- (-pi/6 r:5) -- (pi/6 r:5);
                \fill[highlight] (0, 0) -- (pi/2 r:6.5) -- (5*pi/6 r:6.5);
                \fill[highlight] (0, 0) -- (-5*pi/6 r:6.5) -- (-pi/2 r:6.5);
            \end{scope}
            \draw[dark, very thick] (-2, 4)  .. controls (0, 0) .. (4, 1) node[right] {\(C_1\)};
            \draw[dark, very thick] (4, -1) node[right] {\(C_2\)}  .. controls (0, 0) .. (-2, -4);
            \draw[dark, very thick] (-4, -3) .. controls (0, 0) .. (-4, 3) node[left] {\(C_3\)};
            
            \coordinate (label 1) at (3.05, 1.95);
            \node[highlight, rotate around={pi/6 r:(label 1)}, font=\footnotesize] at (label 1) {\(\vartheta \in [-\pi/6, \pi/6]\)};
            \coordinate (label 2) at (-3.1, 1.59);
            \node[highlight, rotate around={-pi/6 r:(label 2)}, font=\footnotesize] at (label 2) {\(\vartheta \in [\pi/2, 5\pi/6]\)};
            \coordinate (label 3) at (-3.05, -1.58);
            \node[highlight, rotate around={pi/6 r:(label 3)}, font=\footnotesize] at (label 3) {\(\vartheta \in [-5\pi/6, -\pi/2]\)};
            
            \draw[very thick, ->] (-4, 0) -- (4, 0) node[right] {\(\Re(s)\)};
            \draw[very thick, ->] (0, -4) -- (0, 4) node[above] {\(\Im(s)\)};
        \end{tikzpicture}
        \caption{The regions in which \(\e^{sz - s^3/3}\) vanishes as \(s \to \infty\).}
        \label{fig:airy regions}
    \end{figure}

    \section{Another Example}
    Consider the differential equation
    \begin{equation}
        zu'' + 2au' + zu = 0
    \end{equation}
    where \(\Re(a) > 0\).
    We will solve this with the ansatz
    \begin{equation}
        u(z) = \int_{\gamma} \e^{zs} v(s) \dd{s}.
    \end{equation}
    As usual the first step is to differentiate the ansatz,
    \begin{equation}
        u' = \int_{\gamma} s\e^{sz} \dd{s}, \qqand u'' = \int_{\gamma} s^2 \e^{sz} v(s) \dd{s},
    \end{equation}
    and substitute into the differential equation getting
    \begin{equation}
        0 = \int_{\gamma} [zs^2 + 2as + z] \e^{sz} v(s) \dd{s}.
    \end{equation}
    Rewriting the integral to separate out the \(z\) dependence of the term in square brackets we have
    \begin{equation}
        0 = \int_{\gamma} z(s^2 + 1)\e^{sz}v(s) \dd{s} + \int_{\gamma} 2as\e^{sz}v(s) \dd{s}.
    \end{equation}
    We then do the first integral by parts:
    \begin{alignat}{2}
        a &= z(1 + s^2)v(s), \qquad & v\hphantom{'} &= \frac{1}{z}\e^{sz},\\
        a' &= z(s^2 + 1)v'(s) + 2zsv(s), \qquad & v' &= \e^{sz},
    \end{alignat}
    we get
    \begin{equation}
        \left[ z(1 + s^2)v(s)\frac{1}{z}\e^{sz} \right]_{\gamma} - \int_{\gamma} [z(s^2 + 1)v'(s) + 2zsv(s)]\frac{1}{z}\e^{sz} \dd{s}.
    \end{equation}
    Putting this back with the other integral we have
    \begin{equation}
        0 = [(1 + s^2)v(s)\e^{sz}]_{\gamma} + \int_{\gamma} [2asv(s)-(s^2 + 1)v'(s) - 2sv(s)]\e^{sz}.
    \end{equation}
    The integrand should vanish giving
    \begin{equation}
        \frac{v'}{v} = \frac{2s(a - 1)}{s^2 + 1} \implies \ln v = (a - 1) \int \frac{2s}{s^2 + 1} \dd{s} = (a - 1) \ln (s^2 + 1).
    \end{equation}
    Which gives
    \begin{equation}
        v(s) = (s^2 + 1)^{a - 1}.
    \end{equation}
    Hence the solution is
    \begin{equation}
        u(z) = \int_{\gamma} (s^2 + 1)^{a - 1} \e^{sz} \dd{s}
    \end{equation}
    where we choose \(\gamma\) such that \([\e^{sz}(s^2 + 1)^a]_{\gamma} = 0\).
    We're dealing with a second order differential equation, so we expect two such contours.
    
    If \(a \notin \integers\) then we have a branch point at \(s = \pm i, \infty\), recall that we find the branch point at infinity by considering \(s \to 1/t\) and \(t \to 0\),
    \begin{equation}
        (s^2 + 1)^{a - 1} \to \left( \frac{1}{t^2} + 1 \right)^{a-1} = \left( \frac{1 + t^2}{t^2} \right)^{a - 1}.
    \end{equation}
    This still has a problem point at \(t = 0\), so we have a branch point at infinity.
    
    Considering the boundary term, \(\e^{sz}(s^2 + 1)^a\) we see this vanishes at \(s = \pm i\) and \(s = \infty\) for \(\Re(z) < 0\), or \(s = -\infty\) for \(\Re(z) > 0\).
    There are no poles since \(\Re(a) > 0\) and so we can't choose closed contours.
    Instead we choose contours joining \(\pm i\).
    We choose a branch cut going from \(-i\) to \(i\), and then another from \(i\) to \(\infty\), both along the imaginary axis.
    We choose one contour connecting \(i\) and \(-i\) to the left of the branch point, and one connecting \(i\) and \(-i\) on the right of the contour.
    
    Next consider the special case of \(z \in \reals\) and \(a = 1\), so \(a - 1 = 0\), meaning our solution is now of the form
    \begin{equation}
        u(z) = \int_{\gamma} (s^2 + 1)^{a - 1} \e^{sz} \dd{s} = \int_{\gamma} \e^{sz} \dd{s}.
    \end{equation}
    If \(a = 1\) then here are no branch points.
    This means that the two contours chosen before give the same value, and so we need a second independent contour.
    First we can evaluate the integral to find the solution if we take the contour to be the line segment from \(-i\) to \(i\):
    \begin{align}
        u_1(z) = \int_{-i}^{i} \e^{sz} \dd{s} = \left[ \frac{1}{z}\e^{sz} \right]_{-i}^{i} = C\frac{1}{z}(\e^{iz} - \e^{-iz}) = \frac{C}{z}\sin z
    \end{align}
    for some constant \(C\) to be fixed by the initial conditions.
    
    For a second solution we need a new contour.
    If \(z > 0\) then \(s = -\infty\) is a vanishing point for the boundary term.
    We can take contours from \(\pm i\) to \(-\infty\), in particular we can choose straight lines parallel to the real axis.
    We'll take \(-i\) to \(-\infty\).
    We then have
    \begin{equation}
        u_2(z_+) = \int_{-\infty - i}^{-i} \e^{sz} \dd{s} = \left[ \frac{1}{z}\e^{sz} \right]_{-\infty - i}^{-i} = -\frac{1}{z}\e^{-iz}.
    \end{equation}
    For \(z < 0\) we instead need \(s = \infty\) and we take \(i\) to \(\infty\) as our contour.
    We then have
    \begin{equation}
        u_2(z_-) = \int_{i}^{\infty + i} \e^{sz} \dd{s} = \left[ \frac{1}{z}\e^{sz} \right]_{i}^{\infty + i} = \frac{1}{z}\e^{iz} = -\frac{1}{\abs{z}}\e^{-i\abs{z}}.
    \end{equation}
    We can write these two solutions for positive and negative \(z\) as one, including now an arbitrary constant, which we absorb the overall minus sign with,
    \begin{equation}
        u_2(z) = \frac{\tilde{C}}{z}\e^{-i\abs{z}}.
    \end{equation}
    Requiring that our two solutions be independent we can expand the exponential using Euler's formula and discard the non-independent sine term giving
    \begin{equation}
        u_2(z) = \frac{\hat{C}}{z}\cos z,
    \end{equation}
    where \(\hat{C}\) is some constant to be determined from initial conditions.
    
    \part{Partial Differential Equations}
    \chapter{Examinable Partial Differential Equations}
    When we take the Laplace or Fourier transform of an ordinary differential equation we get an algebraic equation.
    When we take the Laplace of Fourier transform of a partial differential equation (PDE)\glossary[acronym]{PDE}{partial differential equation} we reduce the number of independent variables by one.
    This means that if we take the transform of a two independent variable PDE we reduce it to an ODE.
    
    \begin{rmk}
        Partial differential equations are examinable only if they can easily be reduced to ordinary differential equations in this way.
    \end{rmk}
    
    \section{Heat Equation}
    Consider a well insulated semi-infinite bar, extending from \(0\) to infinity, along the \(x\)-axis.
    Suppose that the bar is initially at \qty{0}{\degreeCelsius} everywhere, that is \(u(x, 0) = 0\), where \(u(x, t)\) is the temperature of the bar at \(x\) at time \(t\), in degrees celsius.
    At time \(t = 0\) the end of the bar at \(x = 0\) is heated immediately to \qty{100}{\degreeCelsius} and held there.
    
    The temperature in the bar evolves according to the heat equation
    \begin{equation}
        \diffp[2]{u}{x} = \frac{1}{\alpha^2}\diffp{u}{t}
    \end{equation}
    for some constant \(\alpha\).
    We can solve this by taking the Lorentz transform with respect to the time variable.
    Let
    \begin{equation}
        U(x, p) = \laplaceTransform_t\{u(x, t)\} = \int_{0}^{\infty} \e^{-pt}u(x, t)\dd{t}.
    \end{equation}
    We then have
    \begin{equation}
        \laplaceTransform_{t} \left\{ \diffp{u}{t} \right\} = pU(x, p) - u(x, 0) = pU(x, p)
    \end{equation}
    since \(u(x, 0) = 0\) by the initial conditions.
    We also have
    \begin{equation}
        \laplaceTransform_t\left\{ \diffp[2]{u}{x} \right\} = \diffp*[2]{\laplaceTransform_{t} \{u\}}{x} = \diffp*[2]{U(x, p)}{x}.
    \end{equation}
    
    Hence the transformed equation is
    \begin{equation}
        \diffp*[2]{U(x, p)}{x} = \frac{p}{\alpha^2} U(x, p).
    \end{equation}
    Defining \(\tilde{U}(x) = U(x, p)\) we see that this is really an ordinary differential equation in \(x\) for some fixed \(p\):
    \begin{equation}
        \diff*[2]{\tilde{U}(x)}{x} = \frac{p}{\alpha^2}\tilde{U}(x).
    \end{equation}
    Regardless we continue to work with \(U(x, p)\), and just mentally hold \(p\) constant.
    
    This second order ODE can be solved easily by the ansatz \(U(x, p) = \e^{\lambda x}\), which gives \(U''(x, p) = \lambda^2\e^{\lambda x}\) and so
    \begin{equation}
        \lambda^2 = \frac{p}{\alpha^2} \implies \lambda = \pm \frac{\sqrt{p}}{\alpha}.
    \end{equation}
    The full solution is then
    \begin{equation}
        U(x, p) = C_1 \e^{x\sqrt{p}/\alpha} + C_2 \e^{-x\sqrt{p}/\alpha}.
    \end{equation}
    
    Applying the boundary condition that \(u(0, t) = 100\) we need to take the transform of this, which is easy since
    \begin{equation}
        \laplaceTransform_{t}\{100\} = 100 \laplaceTransform_{t}\{1\} = 100 \int_{0}^{\infty} \e^{-pt} \dd{t} = 100\left[ \frac{1}{p}\e^{-pt} \right]_{0}^{\infty} = \frac{100}{p}.
    \end{equation}
    We also require that \(u \to 0\) as \(x \to \infty\).
    This is because we need a physical solution, so infinitely far away there can be no change in a finite time.
    This means that we have \(U \to 0\) as \(x \to 0\), and so the transformed function must also vanish as \(x \to \infty\).
    Using this we have
    \begin{equation}
        0 = \lim_{x\to\infty} U(x, p) = \lim_{x\to 0}\left[ C_1\e^{x\sqrt{p}/\alpha} + C_2\e^{-x\sqrt{p}/\alpha} \right].
    \end{equation}
    In order for this to hold we must have \(C_1 = 0\).
    We can then apply the first condition, that \(U(0, p) = 100/p\), and we have
    \begin{equation}
        U(0, p) = C_2 = \frac{100}{p}.
    \end{equation}
    Therefore we have
    \begin{equation}
        U(x, p) = \frac{100}{p}\e^{-x\sqrt{p}/\alpha}.
    \end{equation}
    The final step is to invert this, doing so gives the solution
    \begin{equation}
        u(x, t) = 100\erfc\left( \frac{x}{2\alpha\sqrt{t}} \right).
    \end{equation}
    
    \section{Wave Equation}
    Consider the wave equation
    \begin{equation}
        \diffp[2]{u}{x} = \frac{1}{c^2} \diffp[2]{u}{t}.
    \end{equation}
    Taking the Fourier transform with respect to \(x\) if we define \(\tilde{u}(k, t) = \fourierTransform_x\{u(x, t)\}\) we have
    \begin{equation}
        \fourierTransform_{x}\left\{ \diffp[n]{u}{x} \right\} = (ik)^n\tilde{u}(k, t),
    \end{equation}
    and
    \begin{equation}
        \fourierTransform_{x}\left\{ \diffp[2]{u}{t} \right\} = \diffp*[2]{\fourierTransform_{x}\{u\}}{t} = \diffp[2]{\tilde{u}}{t}.
    \end{equation}
    The transformed wave equation is therefore
    \begin{equation}
        -k^2\tilde{u}(k, t) = \frac{1}{c^2}\diffp[2]{\tilde{u}}{t}.
    \end{equation}
    This is now an ordinary differential equation in \(t\) for some fixed \(k\).
    We can solve it by a variety of methods discussed in this course for various initial and boundary conditions.
    
    %Appendicies
    %\appendixpage
    %\begin{appendices}
    %    \include{}
    %\end{appendices}
    
    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}

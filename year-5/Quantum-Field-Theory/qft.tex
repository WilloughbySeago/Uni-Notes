% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

%% Packages
\usepackage{tensor}
\usepackage{csquotes}
\usepackage{scalerel,stackengine}  % needed for d with slash
\usepackage{simpler-wick}
\usepackage{siunitx}
\usepackage{slashed}
\declareslashed{}{\not}{.1}{.5}{A}
\declareslashed{}{\not}{.05}{.5}{p}
\declareslashed{}{\not}{.1}{.5}{\partial}
\declareslashed{}{\not}{-.1}{.5}{a}

% Tikz stuff
\usepackage{tikz}
%\tikzset{>=latex}
% External
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
% Other libraries
\usetikzlibrary{calc}

\usepackage[compat=1.1.0]{tikz-feynman}

% References, should be last things loaded
\usepackage[pdfauthor={Willoughby Seago},pdftitle={Quantum Field Theory},pdfkeywords={quantum field theory, QFT, quantisation, perturbation theory, Feynman diagrams, QED, QCD, standard model, path integral, renormalisation},pdfsubject={Quantum Field Theory}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}

% Highlight colour
\definecolor{Yellow}{HTML}{F9C80E}
\definecolor{Orange}{HTML}{F86624}
\definecolor{Red}{HTML}{EA3546}
\definecolor{Purple}{HTML}{662E9B}
\definecolor{Blue}{HTML}{43BCCD}

\colorlet{highlight}{Red}

% Title page info
\title{Quantum Field Theory}
\author{Willoughby Seago}
\date{}
% \subtitle{}
% \subsubtitle{}

% Commands
% Particles
\newcommand{\Pe}{\ensuremath{\symrm{e}^{-}}}
\newcommand{\Pmu}{\ensuremath{\text{\normalfont μ}^{-}}}

\newcommand{\APe}{\ensuremath{\symrm{e}^{+}}}
\newcommand{\APmu}{\ensuremath{\text{\normalfont μ}^{-}}}

% Text
\newcommand*{\course}[1]{\textit{#1}}
% Maths
\newcommand{\minkowskiMetric}{\eta}
\newcommand{\dalembertian}{\partial^2}
\newcommand{\e}{\symrm{e}}
\newcommand{\lagrangian}{L}
\newcommand{\lagrangianDensity}{\symcal{L}}
\newcommand{\hamiltonianDensity}{\symcal{H}}
\DeclarePairedDelimiterX{\poissonBracket}[2]{\{}{\}}{#1, #2}
\newcommand{\parity}{\symcal{P}}
\newcommand{\chargeConjugation}{\symcal{C}}
\newcommand{\timeReversal}{\symcal{T}}
\newcommand{\hermit}{{\dagger}}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1, #2}
\newcommand{\ident}{\symbb{1}}
\newcommand{\dbar}[1][0.0]{\ThisStyle{\ensurestackMath{%
            \stackengine{#1\LMex}{\SavedStyle \symrm{d}}{\SavedStyle\overbar{}%
                \mkern2.25mu}{O}{r}{F}{F}{L}}}}
\newcommand{\invariantmeasure}[1]{%
    \mathchoice{\dbar #1}{\dbar #1}{\dbar[.05] #1}{\dbar[.06] #1}
}
\newcommand\bardelta{\ThisStyle{\ensurestackMath{%
            \stackengine{-.3\LMpt}{\SavedStyle \delta}{\SavedStyle\overbar{}%
                \mkern4.5mu}{O}{r}{F}{F}{L}}}}
\newcommand{\hilbertSpace}{\symbb{H}}
\newcommand{\normalordering}[1]{\mathopen{\vcentcolon}{#1}\mathclose{\vcentcolon}}
\AtBeginDocument{
    \let\Re\relax
    \let\Im\relax
    \DeclareMathOperator{\Re}{Re}
    \DeclareMathOperator{\Im}{Im}
}
\DeclareMathOperator{\Res}{Res}
\newcommand{\trans}{{\top}}
\newcommand{\EM}{\text{em}}
\newcommand{\interaction}{\symrm{I}}
\newcommand{\probability}{\symbb{P}}
\newcommand{\heaviside}{\theta}
\DeclareMathOperator{\timeOrdering}{T}
\newcommand{\feynman}{\symrm{F}}
\newcommand{\amplitude}{\symcal{M}}
\DeclarePairedDelimiterX{\anticommutator}[2]{\{}{\}}{#1, #2}
\DeclareMathOperator{\tr}{tr}
\newcommand{\order}{\symcal{O}}
\newcommand{\phaseSpaceMeasure}[1][n]{(\dl{\symrm{PS}})_{#1}}
\DeclarePairedDelimiterX{\innerproduct}[2]{\langle}{\rangle}{#1 , #2}
\newcommand{\DL}[1]{\symcal{D}#1}
\newcommand{\DD}[1]{\,\symcal{D}#1}
\newcommand{\diracadjoint}[1]{\overbar{#1}}
\newcommand{\phys}{\symup{phys}}

\includeonly{parts/prelim, parts/hamiltonian-calculation}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/energy-momentum-relation}
    \tableofcontents
    \listoffigures
    \mainmatter
    
    \chapter{Introduction}
    \section{Course Overview}
    \define{Quantum field theory (QFT)}\index{quantum field theory}\glossary[acronym]{QFT}{Quantum Field Theory} is combination of relativity and quantum mechanics.
    It will be our topic of study in this course.
    We will focus mostly on a single quantum field theory, namely \define{quantum electrodynamics (QED)}\index{quantum electrodynamics}\glossary[acronym]{QED}{Quantum Electrodynamics}, which is the result of combining electrodynamics, which is inherently relativistic, and quantum mechanics.
    
    There are various courses leading into this course, some of the important ones for which I have notes are
    \begin{itemize}
        \item \course{Principles of Quantum Mechanics};
        \item \course{Quantum Theory};
        \item \course{Classical Electrodynamics}; and
        \item \course{Symmetries of Quantum Mechanics}.
    \end{itemize}
    This course has a companion course, \course{Symmetries of Particles and Fields}, which focuses on the mathematical abstraction of symmetry.
    Ideas from this companion course, and the related course \course{Symmetries of Quantum Mechanics} will occur throughout this course.
    Another related course, focused more on the experimental side of this field, is \course{Particle Physics}.
    I have notes for both \course{Symmetries of Particles and Fields} and \course{Particle Physics}.
    
    This course, and its companion, lead naturally into the course \course{Gauge Theories in Particle Physics}, for which I also have notes.
    Many ideas in this course will be expanded upon in the gauge theories course.
    The gauge theories course will also treat other quantum field theories, such as \defineindex{electroweak} interactions (a combination of electromagnetism and the weak interactions), and \define{quantum chromodynamics (QCD)}\index{quantum chromodynamics}\glossary[acronym]{QCD}{Quantum Chromodynamics} (the quantum field theory of the strong force).
    The course also briefly touches on lattice gauge theory, a particular way of doing calculations in nonperturbative QCD.
    
    This course is roughly divided into four sections:
    \begin{itemize}
        \item Canonical QFT: Here we deal with operators and quantisation, familiar from traditional quantum mechanics, such as in \course{Principles of Quantum Mechanics}.
        \item QED: We will use this as an example, to allow us to work in a concrete setting rather than in the abstract.
        This is in many ways the simplest of quantum field theories, we study mostly interactions of photons and electrons, and since photons aren't charged the interactions are about as simple as they get.
        Mathematically this \enquote{simplicity} is due to the underlying \(\unitary(1)\) symmetry, which is an Abelian gauge group, and is significantly easier to work with than, say, the \(\specialUnitary(3)\) symmetry of QCD.
        \item Path integral formalism: This is a more abstract, but often more powerful, yet equivalent formulation of quantum field theory.
        This formalism leads itself to gauge theories, and so will be used heavily in \course{Gauge Theories in Particle Physics}.
        Nonrelativistic path integrals were treated in \course{Quantum Theory}.
        \item Renormalisation: When we do QFT calculations we often get infinite results.
        Renormalisation is the process of removing these infinities and extracting meaningful results.
        The renormalisation of QED will be treated in more detail in \course{Gauge Theories}.
    \end{itemize}
    There is some concurrent teaching of these topics, but in these notes I separate them out, so if something's not making sense maybe check to see if its been explained in more detail in a different section.
    
    \section{Conventions}
    Quantum field theory is full of conventions.
    One convention that almost everyone follows is that we work in natural units, where \(c = \hbar = 1\).
    This makes the formulas look much simpler and less cluttered, and we can put \(c\) and \(\hbar\) back in by dimensional analysis.
    
    The second convention is unfortunately much more varied, its the choice of metric.
    We will use the \(({+}{-}{-}{-})\) metric,
    \begin{equation}
        \minkowskiMetric_{\mu\nu} = 
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & -1 & 0 & 0\\
            0 & 0 & -1 & 0\\
            0 & 0 & 0 & -1
        \end{pmatrix}
        .
    \end{equation}
    The most common alternative to this is \(({-}{+}{+}{+})\), where the diagonal has a single negative 1 and the rest is made of positive 1s.
    There are some even rarer choices to use an imaginary metric, \((i{+}{+}{+})\), which still gives the same relative minus sign when we square each element in the inner product.
    
    Throughout the course, unless specified otherwise, we will use the Einstein summation convention.
    Specifically, if an index appears exactly twice in a term, once in a raised position and once in a lowered position, then we sum over all values of that index.
    We also follow the convention where Greek indices, such as \(\mu\) and \(\nu\), run from \(0\) to the number of dimensions minus one, most commonly meaning \(\mu = 0, 1, 2, 3\).
    On the other hand, Latin indices, such as \(i\) and \(j\), run from \(1\) to the number of dimensions minus one, most commonly, \(i = 1, 2, 3\).
    This means that Greek indices are summed over all components, whereas Latin indices are summed only over spatial components.
    
    We follow the convention that the electromagnetic field strength tensor is
    \begin{equation}
        F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu.
    \end{equation}
    An alternative convention is
    \begin{equation}
        F^{\mu\nu} = \partial^\nu A^\mu - \partial^\mu A^\nu,
    \end{equation}
    which differs from our choice by an overall minus sign.
    For example, this results in Maxwell's equations being written as \(\partial_\mu F^{\mu\nu} = -j^\nu\), instead of \(\partial_\mu F^{\mu\nu} = j^\nu\), which is what we'll be using.
    
    \part{Canonical Quantisation}
    \chapter{Classical Fields}
    \section{Relativity Basics}
    In order to understand quantum field theory we will need to quantise fields.
    In order to quantise fields we should be sure that we understand classical fields, so this is where we start the course.
    We will briefly recap ideas mostly from the \course{Classical Electrodynamics} and \course{Quantum Theory} courses, so check the notes for this course for more details.
    
    \subsection{Four-Vectors}
    We can consider a generic four-vector,
    \begin{equation}
        a^\mu = (a^0, \vv{a}) = (a^0, a^1, a^2, a^3).
    \end{equation}
    Using the metric, \(\minkowskiMetric_{\mu\nu} \coloneqq \diag(1,-1,-1,-1)\), we can lower the indices to get a covariant four-vector:
    \begin{equation}
        a_\nu \coloneqq \minkowskiMetric_{\mu\nu}a^\mu = (a^0, -\vv{a}) = (a^0, -a^1, -a^2, -a^3) = (a_0, a_1, a_2, a_3).
    \end{equation}

    Given two four vectors, \(a^\mu = (a^0, \vv{a})\) and \(b = (b^0, \vv{b})\), we can take the inner product,
    \begin{equation}
        a \cdot b \coloneqq a^\mu b_\mu = a_\mu b^\mu = \minkowskiMetric_{\mu\nu} a^\mu b^\nu = a^0b^0 - \vv{a} \cdot \vv{b}.
    \end{equation}
    Here \(\vv{a} \cdot \vv{b} = a^ib^i = a^1b^1 + a^2b^2 + a^3b^3\) is the standard dot product for three-vectors.
    
    For the specific case of the inner product of a four-vector with itself we use the shorthand
    \begin{equation}
        a^2 = a \cdot a = a^0 a^0 - \vv{a} \cdot \vv{a} = (a^0)^2 - \vv{a}^2 .
    \end{equation}
    
    Four vectors transform under Lorentz transformations.
    The broadest class of such transformations is the \defineindex{Lorentz group}, \(\orthogonal(1, 3)\).
    This includes all boosts and rotations, including those that invert the directions of space or time.
    In this course we only consider \define{proper orthochronous Lorentz transformations}\index{proper orthochronous Lorentz transformation}, that is Lorentz transformations preserving the orientation of space (proper) and time (orthochronous).
    These form the \defineindex{proper orthochronous Lorentz group} \(\specialOrthogonal^+(1, 3)\).
    From now on if we say Lorentz transformation we mean \emph{proper orthochronous} Lorentz transformation.
    
    \subsection{Specific-Four Vectors}
    The position is a four-vector, \(x^\mu = (t, \vv{x})\), where \(t\) is the time coordinate and \(\vv{x}\) is the spatial position.
    Note that in SI units this would be \(x^\mu = (ct, \vv{x})\), since the components of a four-vector must have the same dimensions.
    We can also construct a covariant position four-vector, \(x_\mu = (t, -\vv{x})\).
    
    The momentum is a four-vector, \(p^\mu = (E, \vv{p})\), where \(E\) is the energy, and \(\vv{p}\) is the relativistic three-momentum.
    Note that in SI units this would be \(p^\mu = (E/c, \vv{p})\).
    We can also construct a covariant momentum four-vector, \(p_\mu = (E, -\vv{p})\).
    
    For a (real\footnote{as in, not a virtual particle}) particle of mass \(m\) the four-momentum squares to the mass squared, that is
    \begin{equation}
        m^2 = p^2 = E^2 - \vv{p}^2.
    \end{equation}
    Rearranging this gives us
    \begin{equation}
        E^2 = \vv{p}^2 + m^2.
    \end{equation}
    This is the relativistic \defineindex{energy-momentum relation}, it's perhaps more familiar if we reinstate the factors of \(c\):
    \begin{equation}
        E^2 = \vv{p}^2c^2 + m^2c^4,
    \end{equation}
    and is most famous in the case where \(\vv{p} = \vv{0}\):
    \begin{equation}
        E = mc^2.
    \end{equation}
    
    We can construct a four-vector derivative by defining the derivative with respect to \(x^\mu\):
    \begin{equation}
        \partial_\mu \coloneqq \diffp{}{x^\mu} = \left( \diffp{}{t}, \grad \right).
    \end{equation}
    Note that this is a covariant vector, since the contravariant quantity, \(x^\mu\), appears in the denominator, so the derivative transforms in the opposite way to how it would if \(x^\mu\) were in the numerator.
    We can similarly define a contravariant operator,
    \begin{equation}
        \partial^\mu = \diffp{}{x_\mu} = \left( \diffp{}{t}, -\grad \right).
    \end{equation}
    The square of these operators is common enough to be given it's own name, it's called the \defineindex{d'Alembert operator},
    \begin{equation}
        \dalembertian = \partial^\mu \partial_\mu = \diffp[2]{}{t} - \laplacian.
    \end{equation}
    This same quantity is also denoted \(\square^2\), as a sort of four-dimensional (note four sides) Lorentzian-manifold analogue of the Laplacian, \(\laplacian\), and also confusingly sometimes denoted \(\square\), without the superscript 2.
    We also call this the wave operator, since if \(f\) is a field satisfying \(\dalembertian f = 0\) then expanding and rearranging this we have
    \begin{equation}
        \diffp[2]{f}{t} = \diffp[2]{f}{x},
    \end{equation}
    which is the equation of a wave travelling at the speed of light.
    
    \section{Nonrelativistic Particle and Wave}
    A nonrelativistic particle of mass \(m\) has the energy-momentum relation
    \begin{equation}
        E = \frac{\vv{p}^2}{2m} = \frac{1}{2}m\vv{v}^2.
    \end{equation}
    To get an equation for this particle we substitute in the usual operators,
    \begin{equation}
        E \to i\diffp{}{t}, \qand \vv{p} \to -i\grad.
    \end{equation}
    Acting on an arbitrary state, \(\psi\), we then get
    \begin{equation}
        i\diffp{\psi}{t} = \frac{(-i\grad)^2}{2m} \psi = -\frac{1}{2m}\laplacian\psi.
    \end{equation}
    This is exactly the \define{Schrödinger equation}\index{Schrödinger!equation} for a free particle.
    
    One solution to this is a plane wave,
    \begin{equation}
        \psi(x) = \e^{-iEt + i\vv{p} + \vv{x}} = \e^{-ip\cdot x}.
    \end{equation}
    This is only a solution if \(E^2 = \vv{p}^2/2m\).
    Notice that while we can write the exponent with the relativistic four-vectors this is still nonrelativistic since the energy-momentum is not relativistic.
    In particular, we have a single energy value for each possible momentum, as seen in \cref{fig:energy-momentum relation nonrelativistic}.
    
    \begin{figure}
        \tikzsetnextfilename{energy-momentum-nonrelativistic}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [below] {\(\vv{p}\)};
            \draw[thick, ->] (0, 0) -- (0, 3) node [left] {\(E\)};
            \draw[very thick, highlight, domain=-3:3, samples=500] plot (\x, \x*\x/3);
        \end{tikzpicture}
        \caption[Nonrelativistic energy-momentum relation.]{The nonrelativistic energy-momentum relation assigns a single energy to each three-momentum. Note that this is really just a slice through a four-dimensional plot, but all that really matters is the magnitude of the three-momentum.}
        \label{fig:energy-momentum relation nonrelativistic}
    \end{figure}
    
    We know that solutions to the Schrödinger equation can be interpreted as amplitudes, which we then take the modulus square of, \(\abs{\psi}^2\), to get a probability density function.
    We call a function with this property a \defineindex{wave function}.
    
    \section{Relativistic Particle and Wave}
    A relativistic particle of mass \(m\) has the energy-momentum relation
    \begin{equation}
        E^2 = \vv{p}^2 + m^2.
    \end{equation}
    Making the operator substitutions and acting on an arbitrary state, \(\varphi\), we get
    \begin{equation}
        \left( i\diffp{}{t} \right)^2\varphi = (-i\grad)^2\varphi + m^2\varphi.
    \end{equation}
    Rearranging this we get
    \begin{equation}
        (\dalembertian + m^2)\varphi = 0.
    \end{equation}
    This is the \defineindex{Klein--Gordon equation}.
    
    One solution to this is a plane wave,
    \begin{equation}
        \varphi(x) = \e^{-ip \cdot x}.
    \end{equation}
    This is a solution provided that \(p^2 = m^2\).
    If \(p^2 = m^2\) then we say that \(p^\mu\) is \defineindex{on-shell}.
    
    If \(e^{-ip\cdot x}\) is a solution then so is
    \begin{equation}
        \varphi^* = \e^{ip\cdot x}.
    \end{equation}
    We see that we get pairs of solutions.
    These correspond to the pairs of possible energy values, \(E = \pm \sqrt{\vv{p}^2 + m^2}\), as shown in \cref{fig:energy-momentum relation relativistic}.
    
    \begin{figure}
        \tikzsetnextfilename{energy-momentum-relation}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [below] {\(\vv{p}\)};
            \draw[thick, ->] (0, -3) -- (0, 3) node [left] {\(E\)};
            \clip (-3, -3) rectangle (3, 3);
            \draw[very thick, highlight, domain=-3:3, samples=500] plot ({sinh(\x)}, {cosh(\x)});
            \draw[very thick, highlight, domain=-3:3, samples=500] plot ({sinh(\x)}, {-cosh(\x)});
            \draw[dashed, thick, black!50] (-3, -3) -- (3, 3);
            \draw[dashed, thick, black!50] (-3, 3) -- (3, -3);
            \node[above left] at (0, 1) {\(m\)};
        \end{tikzpicture}
        \caption[Relativistic energy-momentum relation]{The relativistic energy momentum assigns two energies to a each three-momentum. The curve is a hyperbola. Notice that at zero three-momentum there we don't have zero energy, instead we have energy \(\pm m\), since we are counting the mass towards the energy in the relativistic formulation. The hyperbola here are the \enquote{shell} referred to in \enquote{on-shell}.}
        \label{fig:energy-momentum relation relativistic}
    \end{figure}
    
    The negative energy solutions to the Klein--Gordon equation pose a problem, in particular, it is possible for \(\varphi^* \varphi\) to be negative, which means there is no probabilistic interpretation of \(\varphi\), and so \(\varphi\) is \emph{not} a wave function.
    Instead, we just call \(\varphi\) a field.
    
    States of a system obeying the Klein--Gordon equation correspond to relativistic free particles (plural).
    
    \section{Maxwell Fields}
    The most familiar fields are electromagnetic fields.
    In this section we will briefly recap a relativistic treatment of these fields.
    For more details see the \course{Classical Electrodynamics} course.
    
    The electromagnetic \enquote{potential} is
    \begin{equation}
        A^\mu(x) = (\varphi(x), \vv{A}(x)),
    \end{equation}
    where \(\varphi\) is the electric potential and \(\vv{A}\) is the electromagnetic vector potential.
    The electric \enquote{field} is then
    \begin{equation}
        F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu.
    \end{equation}
    \begin{wrn}
        An alternative convention,
        \begin{equation}
            F^{\mu\nu} = \partial^\nu A^\mu - \partial^\mu A^\nu,
        \end{equation}
        differs by a sign.
    \end{wrn}
    The electromagnetic current density is
    \begin{equation}
        j^\mu = (\rho, \vv{j}).
    \end{equation}
    This current is conserved, mathematically this is expressed as its four-divergence vanishing,
    \begin{equation}
        \partial_\mu j^\mu = \diffp{\rho}{t} + \div\vv{j} = 0.
    \end{equation}
    This is a continuity equation.
    
    Maxwell's equation is
    \begin{equation}
        \partial_\mu F^{\mu\nu} = j^\nu.
    \end{equation}
    Writing \(F^{\mu\nu}\) in terms of \(A^\mu\) we get
    \begin{equation}
        \partial_\mu F^{\mu\nu} = \partial_\mu \partial^\mu A^\nu - \partial_\mu \partial^\nu A^\mu = \dalembertian A^\nu - \partial^\nu \partial_\mu A^\mu.
    \end{equation}
    The \defineindex{Lorenz gauge} condition is to choose \(A^\mu\) such that its four-divergence vanishes, \(\partial_\mu A^\mu\).
    Then we have \(\partial_\mu F^{\mu\nu} = \dalembertian A^\nu\), so we can write Maxwell's equation as
    \begin{equation}
        \dalembertian A^\nu = j^\nu.
    \end{equation}
    
    In QFT we call \(A^\mu\) the \defineindex{electromagnetic field} and we call \(F^{\mu\nu}\) the \defineindex{electromagnetic field strength}.
    This is partly an annoying historical artefact and partly because \(A^\mu\) is the field appearing in our equations in an analogous way to \(\varphi\) in the Klein--Gordon equation.
    
    In free space, that is when \(j^\mu = 0\), Maxwell's equation is
    \begin{equation}
        \dalembertian A^\mu = 0.
    \end{equation}
    Notice that this is similar to the Klein--Gordon equation with massless particles, \(m = 0\).
    This is good, because we would like electromagnetism to work with photons, which are massless.
    The one difference from the Klein--Gordon equation is that \(A^\mu\) is a four-vector, whereas \(\varphi\) is a scalar.
    The only change in the solutions is that we, in theory, get a different solution for each component of \(A^\mu\).
    The way that this manifests in the solutions is we get an extra vector out front called the \defineindex{polarisation vector}, \(\varepsilon^\mu\):
    \begin{equation}
        A^\mu = \varepsilon^\mu \e^{-ip\cdot x}.
    \end{equation}
    Assuming the momentum is on-shell, that is \(p^2 = 0\) in this massless case.
    This means the momentum is light-like, more good news for electromagnetism explaining photons.
    
    It can be shown that\footnote{See the \course{Classical Electrodynamics} course, note that there we work with \(k^\mu = \hbar p^\mu\).} \(\varepsilon_\mu p^\mu = 0\).
    We are also free to choose \(\varepsilon^\mu\) such that \(\varepsilon^0 = 0\), then the condition \(\varepsilon_\mu p^\mu = 0\) becomes \(\vv{\varepsilon} \cdot \vv{p} = 0\).
    This means that electromagnetic waves are \defineindex{transverse}, since their polarisation vector, \(\vv{\varepsilon}\), is perpendicular to their direction of travel, \(\vv{p}\).
    
    Since the mathematics of the vector field \(A^\mu\) is so similar to the mathematics of the scalar field \(\varphi\) we will mostly deal with scalar fields in this course to develop our theory, then work with other fields once we have a good understanding off the maths.
    Each type of field corresponds to a different spin.
    Scalar fields describe spin zero particles, like the Higgs boson, and vector fields describe spin one particles, like the photon.
    Spin \(1/2\) particles are described by spinors.
    
    \chapter{Lagrangians and Hamiltonians}
    \section{Lagrangian Dynamics of a Particle}
    \epigraph{That's all there is to a Lagrangian dynamics course, the rest is just examples.}{Richard Ball}
    \begin{rmk}
        See the \course{Lagrangian Dynamics} course for more details.
    \end{rmk}
    Consider a particle of mass \(m\) moving in one dimension in a conservative force field.
    We can define its position with a \define{generalised coordinate}\index{generalised!coordinate}, \(q\), which gives us a \define{generalised velocity}\index{generalised!velocity}, \(\dot{q} \coloneqq \diff{q}/{t}\).
    We can then define the \defineindex{Lagrangian}:
    \begin{equation}
        \lagrangian(q, \dot{q}) = T(\dot{q}) - V(q).
    \end{equation}
    Here \(T(\dot{q})\) is the kinetic energy of the particle, which we assume depends only on the velocity for simplicity, and \(V(q)\) is the potential energy of the particle, which must depend only on the position.
    The Lagrangian is a function of the generalised position and velocity.
    
    The \defineindex{action} of the particle between the times \(t_1\) and \(t_2\) is defined to be
    \begin{equation}
        S[q(t)] \coloneqq \int_{t_1}^{t_2} \dl{t} \, \lagrangian(q, \dot{q}).
    \end{equation}
    \defineindex{Hamilton's principle} states that the path the particle takes, \(q(t)\), is such that the action is extremised, that is the variation, \(\delta S\), given by varying \(q \to q + \delta q\) vanishes.
    We can compute the variation in a general action, \(S\), by computing the variation in the Lagrangian:
    \begin{equation}
        \delta \lagrangian = \diffp{\lagrangian}{q} \delta q + \diffp{\lagrangian}{\dot{q}} \delta \dot{q}.
    \end{equation}
    For a smooth variation, \(\delta q\), we have
    \begin{equation}
        \delta \dot{q} = \diffp{}{t}(\delta q),
    \end{equation}
    and so
    \begin{equation}
        \delta \lagrangian = \diffp{\lagrangian}{q} \delta q + \diffp{\lagrangian}{\dot{q}} \diff{}{t} (\delta q)
    \end{equation}
    Integrating this the variation in the action is
    \begin{equation}
        \delta S = \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} \delta q + \diffp{\lagrangian}{\dot{q}} \diff{}{t} (\delta q) \right].
    \end{equation}
    The product rule tells us that
    \begin{equation}
        \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \delta q \right) = \diffp{\lagrangian}{\dot{q}} \diff{}{t} (\delta \dot{q}) + \diff{}{t} \left( \diffp{\lagrangian}{\dot{q}} \right) \delta q.
    \end{equation}
    Rearranging this we can rewrite the second term in \(\delta S\) as a total derivative minus a term:
    \begin{align}
        \delta S &= \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} \delta q + \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \delta q \right) - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \delta q \right]\\
        &= \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \right] \delta q + \int_{t_1}^{t_2} \dl{t} \, \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \delta q \right)\\
        &= \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \right] \delta q + \left[ \diffp{\lagrangian}{\dot{q}} \delta q \right]_{t_1}^{t_2}.
    \end{align}
    Now suppose that \(\delta q\) vanishes at \(t_1\) and \(t_2\), so \(\delta q(t_1) = \delta q(t_2) = 0\), then the last term above vanishes and we have
    \begin{equation}
        \delta S = \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \right]\delta q.
    \end{equation}
    If \(\delta S\) is to vanish for all variations \(\delta q\) then we must have
    \begin{equation}
        \diffp{\lagrangian}{q} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) = 0.
    \end{equation}
    This is the \defineindex{Euler--Lagrange equation} for a single particle in one dimension.
    
    This generalises to \(n\)-dimensional space by replacing \(q\) with \(n\) coordinates \(q_i\) and \(\dot{q}\) with \(\dot{q}_i\).
    The Lagrangian is then
    \begin{equation}
        \lagrangian(q_i, \dot{q}_i) = T(\dot{q}_i) - V(q_i),
    \end{equation}
    and the action is
    \begin{equation}
        S[q_i(t)] = \int_{t_1}^{t_2} \dl{t} \, \lagrangian(q_i, \dot{q}_i).
    \end{equation}
    We then get \(n\) Euler--Lagrange equations:
    \begin{equation}
        \diffp{\lagrangian}{q_i} - \diff{}{t} \left( \diffp{\lagrangian}{\dot{q}_i} \right) = 0.
    \end{equation}
    
    \begin{exm}{}{}
        The Lagrangian for a simple harmonic oscillator is
        \begin{equation}
            \lagrangian = \frac{1}{2} m \dot{x}^2 - \frac{1}{2} m\omega^2 x^2.
        \end{equation}
        We have
        \begin{equation}
            \diffp{\lagrangian}{x} = -m\omega^2 x, \qand \diffp{\lagrangian}{\dot{x}} = m\dot{x} \implies \diff{}{t}\left( \diffp{\lagrangian}{\dot{x}} \right) = m\ddot{x}.
        \end{equation}
        Hence,
        \begin{equation}
            m\ddot{x} = -m\omega^2 x \implies \ddot{x} = -\omega^2 x
        \end{equation}
        is the equation of motion for the simple harmonic oscillator, which is exactly what we would expect.
    \end{exm}
    
    \section{Hamiltonian Dynamics of a Particle}
    Let's go back to the one-dimensional case.
    We can define the \defineindex{canonical momentum} of the particle as
    \begin{equation}
        p \coloneqq \diffp{\lagrangian}{\dot{q}}.
    \end{equation}
    Note that, in general, this is \emph{not} the normal momentum.
    
    We then define the \defineindex{Hamiltonian}, \(H\), as a function of the generalised position and canonical momentum:
    \begin{equation}
        H(q, p) \coloneqq p\dot{q} - \lagrangian(q, \dot{q}).
    \end{equation}
    Note that we must eliminate any \(\dot{q}\) remaining in the expression for the Hamiltonian, this can be done by solving the defining relation \(p = \diff{\lagrangian}/{\dot{q}}\) for \(\dot{q}\) and then substituting in the result.
    
    Now consider what happens when we vary \(H\).
    Varying the left hand side we get
    \begin{equation}
        \dl{H} = \diffp{H}{q} \dd{q} + \diffp{H}{p} \dd{p}.
    \end{equation}
    Varying the right hand side we have
    \begin{equation}
        \dl{H} = p \dd{q} + q \dd{p} - \diffp{\lagrangian}{q} \dd{q} - \diffp{\lagrangian}{\dot{q}} \dd{\dot{q}}.
    \end{equation}
    We can rewrite the last term in terms of the canonical momentum:
    \begin{equation}
        \dl{H} = p \dd{q} + q \dd{p} - \diffp{\lagrangian}{q} \dd{q} - p \dd{\dot{q}} =  q \dd{p} - \diffp{\lagrangian}{q} \dd{q}.
    \end{equation}
    Using the Euler--Lagrange equations the second term can be rewritten to give
    \begin{equation}
        \dl{H} =  q \dd{p} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \dd{q} = q \dd{p} - \dot{p} \dd{q}.
    \end{equation}
    Comparing this with the result for the left hand side variation we can read off
    \begin{equation}
        \dot{q} = \diffp{H}{p}, \qand \dot{p} = - \diffp{H}{q}.
    \end{equation}
    These are \defineindex{Hamilton's equations}.
    
    This all generalises to \(n\) dimensions.
    First replace \(q\) with \(q_i\), and \(p\) with \(p_i \coloneqq \diffp{\lagrangian}/{q_i}\).
    Then the Hamiltonian is
    \begin{equation}
        H(q_i, p_i) = \sum_i p_i \dot{q}_i - \lagrangian(q_i, \dot{q}_i).
    \end{equation}
    Hamilton's equations are
    \begin{equation}
        \dot{q}_i = \diffp{H}{p_i}, \qand \dot{p}_i = -\diffp{H}{q_i}.
    \end{equation}
    
    It can be useful when doing Hamiltonian mechanics to define the \defineindex{Poisson bracket} of two functions, \(A\) and \(B\), depending on position, \(q_i\), and momentum, \(p_i\):
    \begin{equation}
        \poissonBracket{A, B} \coloneqq \sum_i \left( \diffp{A}{p_i}\diffp{B}{q_i} - \diffp{A}{q_i}\diffp{B}{p_i} \right).
    \end{equation}
    This has the advantage of allowing us to express Hamilton's equations in the more symmetric form
    \begin{equation}\label{eqn:hamilton's equations with poisson brackets}
        \dot{q}_i = \poissonBracket{H}{q_i}, \qand \dot{p}_i = \poissonBracket{H}{p_i}.
    \end{equation}
    
    Note the similarity to Heisenberg's operator equation of motion for a time independent operator, \(A\):
    \begin{equation}
        \dot{A}(t) = i [H, A].
    \end{equation}
    Replacing Poisson brackets with \(i\) times the commutator can get us quite a long way in quantum mechanics.
    
    The Hamiltonian is conserved.
    To show this consider the time derivative:
    \begin{equation}
        \diff{H}{t} = \diffp{H}{p} \dot{p} + \diffp{H}{q} \dot{q} = \dot{q}\dot{p} - \dot{p}\dot{q} = 0.
    \end{equation}
    We can often identify the Hamiltonian with the total energy when this is also conserved.
    
    \begin{exm}{}{}
        The canonical momentum for a simple harmonic oscillator is
        \begin{equation}
            p = \diffp{\lagrangian}{\dot{x}} = m\dot{x},
        \end{equation}
        which is just the normal momentum in this case.
        The Hamiltonian is then
        \begin{equation}
            H = p\dot{q} - \lagrangian = m\dot{x}^2 - \frac{1}{2}m\dot{x}^2 + \frac{1}{2}m\omega^2x^2 = \frac{1}{2}m\dot{x}^2 + \frac{1}{2}m\omega^2x^2,
        \end{equation}
        which is exactly the energy of a simple harmonic oscillator.
    \end{exm}
    
    \section{Lagrangian Dynamics of a Field}
    Lagrangian dynamics doesn't change that much when we work with fields.
    We start by replacing the generalised coordinate \(q(t)\) with the field, \(\varphi(x)\), which now depends on a position in spacetime, rather than just a time.
    Instead of a Lagrangian we work with a \define{Lagrangian density}\index{Lagrangian!density}, \(\lagrangianDensity(\varphi, \partial_\mu \varphi)\), replacing the time derivative of the coordinate with a four-vector derivative.
    We then have to integrate over a region of spacetime to get the action:
    \begin{equation}
        S[\varphi(x)] \coloneqq \int \dl{^4x} \, \lagrangianDensity(\varphi, \partial_\mu \varphi).
    \end{equation}
    The integral should be taken over some arbitrary region of spacetime.
    Note that we can write this as
    \begin{equation}
        S[\varphi(x)] = \int \dl{t} \int \dl{^3\vv{x}} \, \lagrangianDensity(\varphi, \partial_\mu \varphi) = \int \dl{t} \, \lagrangian(\varphi, \partial_\mu \varphi)
    \end{equation}
    where
    \begin{equation}
        \lagrangian(\varphi, \partial_\mu \varphi) \coloneqq \int \dl{^3\vv{x}} \, \lagrangianDensity(\varphi, \partial_\mu \varphi).
    \end{equation}
    So, integrating \(\lagrangianDensity\) over space gives \(\lagrangian\), which we call the Lagrangian, which explains the interpretation of \(\lagrangianDensity\) as a Lagrangian \emph{density}.
    While this distinction is important the Lagrangian doesn't actually appear that much in QFT, so people often just call \(\lagrangianDensity\) the Lagrangian, leaving the density part implicit.
    
    The Euler--Lagrange equations for the Lagrangian density don't change much, and can be derived in a very similar way.
    We start by varying the field, \(\varphi \to \varphi + \delta \varphi\), and looking at the resulting variation in the Lagrangian density:
    \begin{equation}
        \delta\lagrangianDensity = \diffp{\lagrangianDensity}{\varphi} \delta \varphi + \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta (\partial_\mu \varphi).
    \end{equation}
    For a smooth variation we have
    \begin{equation}
        \delta (\partial_\mu \varphi) = \partial_\mu (\delta \varphi)
    \end{equation}
    and so
    \begin{equation}
        \delta\lagrangianDensity = \diffp{\lagrangianDensity}{\varphi} \delta \varphi + \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \partial_\mu (\delta \varphi).
    \end{equation}
    We can use the product rule,
    \begin{equation}
        \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right) = \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi + \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \partial_\mu (\delta \varphi),
    \end{equation}
    to write the last term as a total derivative minus a term:
    \begin{equation}
        \delta\lagrangianDensity = \diffp{\lagrangianDensity}{\varphi} \delta \varphi + \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right) - \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi.
    \end{equation}
    Integrating over some spacetime region, \(\Omega\), to get the variation in the action we have
    \begin{align}
        \delta S &= \int_\Omega \dl{^4x} \left[ \diffp{\lagrangianDensity}{\varphi} \delta \varphi + \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right) - \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi \right]\\
        &= \int_\Omega \dl{^4x} \left[ \diffp{\lagrangianDensity}{\varphi} \delta \varphi - \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi \right] + \int_\Omega \dl{^4x} \, \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right).
    \end{align}
    Applying the divergence theorem to the second integral we can replace it with an integral of \((\diffp{\lagrangianDensity}/{(\partial_\mu \varphi)}) \delta \varphi\) over the boundary, \(\partial \Omega\).
    Choosing a variation, \(\delta \varphi\), which vanishes on the boundary this term will vanish, and we will be left with
    \begin{equation}
        \delta S = \int_\Omega \dl{^4x} \left[ \diffp{\lagrangianDensity}{\varphi} \delta \varphi - \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi \right].
    \end{equation}
    For this to vanish for all variations of the field we require that
    \begin{equation}
        \diffp{\lagrangianDensity}{\varphi} - \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) = 0.
    \end{equation}
    These are the \define{Euler--Lagrange equations}\index{Euler--Lagrange equation} for a scalar field.
    
    \section{Hamiltonian Dynamics of a Field}
    Following the same logic as for a single particle we can define the \defineindex{canonical momentum},
    \begin{equation}
        \pi(x) \coloneqq \diffp{\lagrangianDensity}{\dot{\varphi}}.
    \end{equation}
    This is \emph{not} the same as the actual momentum.
    Notice that this definition treats time differently to position, this will result in the Hamiltonian formulation not being Lorentz invariant, although the results we get still are, the steps between are just frame dependent.
    This corresponds to the non-Lorentz invariance of the energy, the energy of a particle of mass \(m\) is \(\gamma m\).
    
    We then define the Hamiltonian, \(H\), to be
    \begin{equation}
        H \coloneqq \int \pi \dot{\varphi} \dd{^4x} - L = \int \dd{^4x} \, (\pi \dot{\varphi} - \lagrangianDensity) = \int \dl{^3\vv{x}} \, \hamiltonianDensity
    \end{equation}
    where
    \begin{equation}
        \hamiltonianDensity \coloneqq \pi \dot{\varphi} - \lagrangianDensity
    \end{equation}
    is the \defineindex{Hamiltonian density}.
    Both the Hamiltonian and Hamiltonian density must be functions of the field, \(\varphi\), and the canonical momentum, \(\pi\), which is just another field.
    
    The same derivation as before works without modification, so we won't repeat it here, the result is \defineindex{Hamilton's equations} for a field:
    \begin{equation}
        \dot{\varphi} = \diffp{\hamiltonianDensity}{\pi}, \qand \dot{\pi} = - \diffp{\hamiltonianDensity}{\varphi}.
    \end{equation}
    
    As before, the Hamiltonian density is conserved:
    \begin{align}
        \diff{\hamiltonianDensity}{t} &= \int \dl{^3\vv{x}} \left( \diffp{\hamiltonianDensity}{\varphi} \dot{\varphi} + \diffp{\hamiltonianDensity}{\pi} \dot{\pi} \right)\\
        &= \int \dl{^3\vv{x}} (-\dot{\pi} \dot{\varphi} + \dot{\varphi} \dot{\pi})\\
        &= 0.
    \end{align}
    Again, when energy is conserved we can often identify it with the Hamiltonian.
    
    \section{Klein--Gordon Equation}
    \epigraph{We have to learn to run before we walk.}{Richard Ball}
    The Lagrangian for a scalar field, \(\varphi\), is
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi)(\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2.
    \end{equation}
    Clearly we have
    \begin{equation}
        \diffp{\lagrangianDensity}{\varphi} = -m^2\varphi.
    \end{equation}
    We can also compute the derivative with respect to \(\partial_\mu \varphi\), note that \((\partial_\mu \varphi)(\partial^\mu \varphi)\) is just \((\partial \varphi)^2\), so we can treat this just like the derivative of a quantity squared, giving
    \begin{equation}
        \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} = \partial^\mu \varphi.
    \end{equation}
    We end with an upper index, since the left hand side has a lower index in the denominator, which is an upper index in the numerator.
    We then have
    \begin{equation}
        \partial_\mu \left( \diffp{\lagrangianDensity}{\varphi} \right) = \partial_\mu \partial^\mu \varphi = \dalembertian \varphi.
    \end{equation}
    Using the Euler--Lagrange equations we then have
    \begin{equation}
        \dalembertian \varphi + m^2 \varphi = 0,
    \end{equation}
    which is just the Klein--Gordon equation.
    
    The canonical momentum associated with this Lagrangian is
    \begin{equation}
        \pi = \diffp{\lagrangianDensity}{\dot{\varphi}} = \diffp{\lagrangianDensity}{(\partial_0 \varphi)}  = \partial^0 \varphi = \dot{\varphi}.
    \end{equation}
    The Hamiltonian is then
    \begin{equation}
        \hamiltonianDensity = \pi \dot{\varphi} - \lagrangianDensity = \frac{1}{2}(\pi^2 + (\grad \varphi)^2 + m^2\varphi^2).
    \end{equation}
    The \(\grad\varphi\) term comes from
    \begin{align}
        \pi^2 - \frac{1}{2} (\partial_\mu\varphi)(\partial^\mu\varphi) &= \pi^2 - \frac{1}{2} [(\partial_0 \varphi)(\partial^0 \varphi) - (\partial_i \varphi)(\partial^i \varphi)]\\
        &= \pi^2 - \frac{1}{2}[\dot{\varphi}^2 - (\grad\varphi)^2] = \frac{1}{2}[\pi^2 - (\grad\varphi)^2]
    \end{align}
    where we've used \(\dot{\varphi} = \pi\).
    
    Compare the Hamiltonian density for the Klein--Gordon equation with the Hamiltonian for the harmonic oscillator:
    \begin{equation}
        H = \frac{1}{2m} (p^2 + m^2\omega^2 x^2).
    \end{equation}
    Up to a factor of \(\omega^2\) and \(1/m\) this is pretty much the same as the Klein--Gordon Hamiltonian density, minus the \((\grad \varphi)^2\) term.
    Similarly the Klein--Gordon Lagrangian density is very similar to the harmonic oscillator Lagrangian.
    This suggests that we should study the harmonic oscillator, and indeed we shall soon.
    
    \begin{exm}{Electromagnetism}{}
        The Lagrangian density for the electromagnetic field, \(A^\mu\), is
        \begin{equation}
            \lagrangianDensity = -\frac{1}{4}F^{\mu\nu}F_{\mu\nu}.
        \end{equation}
        We can insert the definition of \(F^{\mu\nu}\) and expand this out to get
        \begin{align}
            \lagrangianDensity &= -\frac{1}{4}(\partial^\mu A^\nu - \partial^\nu A^\mu)(\partial_\mu A_\nu - \partial_\nu A_\mu)\\
            &= -\frac{1}{4}[(\partial^\mu A^\nu)(\partial_\mu A_\nu) - (\partial^\nu A^\mu)(\partial_\mu A_\nu)\\
            &\qquad- (\partial^\mu A^\nu)(\partial_\nu A_\mu) + (\partial^\nu A^\mu)(\partial_\nu A_\mu)]\\
            &= -\frac{1}{2}(\partial^\mu A^\nu)(\partial_\mu A_\nu) + \frac{1}{2}(\partial^\mu A^\nu)(\partial_\nu A_\mu).
        \end{align}
        
        We treat each component of \(A_\nu\) as an independent field.
        We then have
        \begin{equation}
            \diffp{\lagrangianDensity}{A_\nu} = 0, \qand \diffp{\lagrangianDensity}{(\partial_\mu A_\nu)} = -\partial^\mu A^\nu + \partial^\nu A^\mu = -F^{\mu\nu}.
        \end{equation}
        Hence, the Euler--Lagrange equations give
        \begin{equation}
            \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu A_\nu)} \right) = -\partial_\mu \partial^\mu A^\nu + \partial^\nu \partial_\mu A^\mu = 0.
        \end{equation}
        That is,
        \begin{equation}
            \dalembertian A^\nu - \partial^\nu \partial_\mu A^\mu = 0 \iff -\partial_\mu F^{\mu\nu} = 0
        \end{equation}
        This is Maxwell's equation in a vacuum in an arbitrary gauge.
        
        The canonical momentum is
        \begin{equation}
            \pi^\mu(x) = \diffp{\lagrangianDensity}{\dot{A}_\mu} = \diffp{\lagrangianDensity}{(\partial_0 A_\mu)} = -\partial^\mu A^0 + \partial^0 A^\mu = -F^{0\mu}.
        \end{equation}
        Antisymmetry of \(F^{\mu\nu}\) implies that \(\pi^\mu(x) = 0\), and so \(\pi^\mu(x) = (0, -\vv{E}(x))\), where \(\vv{E}(x)\) is the electric field.
        
        The fact that \(\pi^0(x) = 0\) will cause problems later when we try to quantise the electromagnetic field.
        
        In the presence of sources the Lagrangian density is instead
        \begin{equation}
            \lagrangianDensity = -\frac{1}{4}F^{\mu\nu}F_{\mu\nu} - J^\mu A_\mu.
        \end{equation}
        This doesn't change the term given by differentiating with respect to \(\partial_\mu A_\nu\), all that changes is we now have
        \begin{equation}
            \diffp{\lagrangianDensity}{A_\mu} = -J^\mu,
        \end{equation}
        and so the result of applying the Euler--Lagrange equations is now
        \begin{equation}
            \dalembertian A^\nu - \partial^\nu \partial_\mu A^\mu = J^\nu,
        \end{equation}
        which is Maxwell's equation in the presence of a source.
    \end{exm}
    
    \section{Symmetries}
    \subsection{Discrete Symmetries}
    There are three discrete symmetries of particular interest in quantum field theory, they are
    \begin{itemize}
        \item \defineindex{parity}: \(\parity \colon \vv{x} \mapsto -\vv{x}\), \(\parity \colon x^\mu = (x^0, x^i) \mapsto (x^0, -x^i) = (x_0, x_i) = x_\mu\), so parity acts to swap raised and lowered indices of positions;
        \item \defineindex{time reversal}: \(\timeReversal \colon t \mapsto t\), \(\timeReversal \colon x^\mu = (x^0, x^i) \mapsto (-x^0, x^i) = -(x^0, -x^i) = -(x_0, x_i) = -x_\mu\), so time reversal acts to swap raised and lowered indices and negate positions;
        \item \defineindex{charge conjugation}: \(\chargeConjugation \colon e \mapsto -e\), where \(e\) is the charge of the particle.
    \end{itemize}
    
    Consider, for example, the Lagrangian of the Klein--Gordon equation:
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial^\mu \varphi)(\partial_\mu\varphi) - \frac{1}{2}m^2 \varphi^2.
    \end{equation}
    Under parity the scalar field is unchanged, and \(\partial^\mu \leftrightarrow \partial^\mu\), so the Lagrangian is unchanged under parity.
    Under time reversal the scalar field is unchanged, and \(\partial^\mu \leftrightarrow -\partial_\mu\), so the Lagrangian is unchanged under time reversal.
    Since this Lagrangian has no charges involved it is trivially invariant under charge conjugation.
    
    Now consider the Electromagnetic Lagrangian,
    \begin{equation}
        \lagrangianDensity = -\frac{1}{4}F^{\mu\nu}F_{\mu\nu} - J^\mu A_\mu.
    \end{equation}
    Under parity transformations \(F^{\mu\nu} \leftrightarrow F_{\mu\nu}\), \(J^\mu \to J_\mu\), and \(A_\mu \to A^\mu\), so
    \begin{equation}
        \parity\lagrangianDensity = -\frac{1}{4}F_{\mu\nu}F^{\mu\nu} - J_\mu A^\mu = \lagrangianDensity,
    \end{equation}
    so the Lagrangian is unchanged by parity.
    Under time reversal transformations \(F^{\mu\nu} \leftrightarrow -F_{\mu\nu}\) since the derivatives in the definition of \(F^{\mu\nu}\) transform as \(\partial^\mu \to -\partial_\mu\) under time reversal.
    On the other hand, \(J^\mu \to J^\mu\), and \(A_\mu \to A_\mu\), since time reversal doesn't effect the current or potential.
    Hence,
    \begin{equation}
        \timeReversal\lagrangianDensity = -\frac{1}{4}(-F_{\mu\nu})(-F^{\mu\nu}) - J^\mu A_\mu = \lagrangianDensity,
    \end{equation}
    so the Lagrangian is unchanged under time reversal.
    Under charge conjugation \(F^{\mu\nu} \leftrightarrow -F_{\mu\nu}\), \(J^\mu \to -J_\mu\), and \(A^\mu \to -J_\mu\), so
    \begin{equation}
        \chargeConjugation\lagrangianDensity = -\frac{1}{4}(-F_{\mu\nu})(-F^{\mu\nu}) - (-J_\mu)(-A^\mu) = \lagrangianDensity,
    \end{equation}
    so the Lagrangian is invariant under charge conjugation.
    
    Invariance under each of these symmetries means that both of these Lagrangians are invariant under the combined \(\chargeConjugation\parity\timeReversal\) symmetry, given by \(x^\mu \mapsto -x^\mu\) and \(e \mapsto -e\).
    
    \subsection{Continuous Symmetries}
    \begin{rmk}
        For more details see the \course{Symmetries of Particles and Fields} course.
    \end{rmk}
    If the action is invariant under a group of continuous symmetries then we have a conserved quantity.
    This is the essence of \defineindex{Noether's theorem}.
    The most common examples being
    \begin{itemize}
        \item invariance under translations in time leading to energy conservation;
        \item invariance under spatial translations leading to momentum conservation;
        \item invariance under rotation leading to angular momentum conservation.
    \end{itemize}
    
    The simplest case is, perhaps, when the Lagrangian is invariant under translations, then the action is also necessarily translation invariant.
    Suppose we have a translation
    \begin{equation}
        \varphi(x) \to \varphi'(x) = \varphi(x) + \delta \varphi(x).
    \end{equation}
    Note that the translation is allowed to depend on \(x\).
    The variation in \(\lagrangianDensity\) is
    \begin{align}
        \delta \lagrangianDensity &= \diffp{\lagrangianDensity}{\varphi} \delta\varphi + \diffp{\lagrangianDensity}{(\partial_\mu)} \delta(\partial_\mu \varphi)\\
        &= \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu\varphi)} \right) \delta \varphi + \diffp{\lagrangianDensity}{(\partial_\mu\varphi)} \delta(\partial_\mu\varphi)\\
        &= \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right).
    \end{align}
    The second equality is simply an application of the Euler--Lagrange equations to rewrite the first term.
    The last equality is just recognising the product rule.
    
    Imposing that \(\lagrangianDensity\) is invariant under translations like this we have \(\delta \lagrangianDensity = 0\), which gives us the continuity equation
    \begin{equation}
        \partial_\mu J^\mu(x) = 0,
    \end{equation}
    where
    \begin{equation}
        J^\mu(x) \coloneqq \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi
    \end{equation}
    is the conserved current, a generalisation of the normal current in electrodynamics.
    There is also a corresponding conserved \enquote{charge}:
    \begin{equation}
        Q = \int\dl{^3\vv{x}} \, J^0(x).
    \end{equation}
    To see that this quantity is conserved consider
    \begin{equation}
        \dot{Q} = \int \dl{^3\vv{x}} \, \partial_0 J^0(x) = -\int \dl{^3\vv{x}} \, \partial_i J^i(x) = 0
    \end{equation}
    where in the last step we've used the divergence theorem to rewrite the last term as a surface integral, then take this surface to be at infinity, and made the usual assumption that fields vanish sufficiently quickly at infinity.
    
    \begin{exm}{Probability Current}{}
        Consider the Lagrangian
        \begin{equation}
            \lagrangianDensity = i\psi^* \partial_t \psi - \frac{1}{2m} (\grad\psi^*) \cdot (\grad\psi) - V(\vv{x}, t)\psi^* \psi.
        \end{equation}
        If we treat \(\psi\) and \(\psi^*\) as separate fields then varying one of them gives the Schrödinger equation.
        This Lagrangian is invariant under the transformation
        \begin{equation}
            \psi \to \e^{i\alpha}\psi, \qand \psi^* \to \e^{-i\alpha}\psi^*
        \end{equation}
        where \(\alpha\) is a constant.
        Linearising we get the transformation
        \begin{equation}
            \psi \to \psi(1 + i\alpha), \qand \psi^* \to \psi^*(1 - i\alpha).
        \end{equation}
        Since we have two fields, \(\psi\) and \(\psi^*\), the conserved current is a sum over field:
        \begin{equation}
            J^\mu = \diffp{\lagrangianDensity}{(\partial_\mu \psi)} \delta \psi + \diffp{\lagrangianDensity}{(\partial_\mu \psi^*)} \delta \psi^*.
        \end{equation}
        Computing the derivatives we have
        \begin{align}
            \diffp{\lagrangianDensity}{(\partial_t \psi)} &= i\psi^*, \qquad & \diffp{\lagrangianDensity}{(\partial_i \psi)} = -\frac{1}{2m}\partial^i \psi^*,\\
            \diffp{\lagrangianDensity}{(\partial_t \psi^*)} &= 0, \qquad & \diffp{\lagrangianDensity}{(\partial_i \psi^*)} = -\frac{1}{2m}\partial^i \psi.
        \end{align}
        Hence, using the variations
        \begin{equation}
            \delta \psi = i\alpha \psi, \qand \delta \psi^* = -i\alpha \psi^*
        \end{equation}
        we have the time component of the conserved current:
        \begin{equation}
            J^0 = \rho = -\alpha \psi^*\psi,
        \end{equation}
        and the position components:
        \begin{align}
            J^i &= \frac{i\alpha}{2m}(\psi^* \partial^i \psi - \psi \partial^i \psi^*)\\
            \vv{J} &= \frac{i\alpha}{2m}(\psi \grad \psi^* - \psi^* \grad \psi).
        \end{align}
        We can then identify \(J^\mu\) as, up to a constant, the conserved probability current of \cref{sec:continuity equation}.
    \end{exm}
    
    It can be shown that the Euler--Lagrange equations are invariant under the addition of a four-divergence, 
    \begin{equation}
        \lagrangianDensity \to \lagrangianDensity + \partial_\mu \Lambda^\mu
    \end{equation}
    for some four-vector field \(\Lambda^\mu\) with a sufficiently smooth derivative.
    The same derivation as before, but setting \(\delta\lagrangianDensity = \partial_\mu \Lambda^\mu\) instead of zero, then gives a conserved current
    \begin{equation}
        j^\mu = \diffp{\lagrangianDensity}{(\partial_\mu\varphi)} \delta\varphi - \Lambda^\mu.
    \end{equation}
    
    As well as changing the Lagrangian directly we can instead act on spacetime, for example, by translation:
    \begin{equation}
        x^\mu \to x'^\mu = x^\mu + a^\mu
    \end{equation}
    for some infinitesimal four-vector \(a^\mu\).
    This induces a change in the scalar field, given by Taylor expanding:
    \begin{equation}
        \varphi(x) \to \varphi(x + a) = \varphi(x) + a^\mu \partial_\mu \varphi(x).
    \end{equation}
    Since the Lagrangian density is also a scalar it must transform the same way:
    \begin{equation}
        \lagrangianDensity \to \lagrangianDensity + a^\mu \partial_\mu \lagrangianDensity = \lagrangianDensity + a^\nu \partial_\mu (\tensor{\delta}{^\mu_\nu} \lagrangianDensity).
    \end{equation}
    Comparing this to the transformation \(\lagrangianDensity \to \lagrangianDensity + \partial_\mu \Lambda^\mu\) we can identify
    \begin{equation}
        \partial_\mu \Lambda^\mu = a^\nu \partial_\mu (\partial^\mu_\nu \lagrangianDensity).
    \end{equation}
    We then have four conserved currents, one for each value of \(\nu\), such as
    \begin{equation}
        J^\mu(x) = \diffp{\lagrangianDensity}{(\partial_\mu \varphi)}a^0\partial_0\varphi - a^0 \tensor{\delta}{^\mu_0}\lagrangianDensity,
    \end{equation}
    which comes from setting \(\nu = 0\).
    We can combine these four currents into a single rank 2 tensor, scaling out the \(a^\nu\) parameter, to get
    \begin{equation}
        \tensor{T}{^\mu_\nu} = \diffp{\lagrangianDensity}{(\partial_\mu \lagrangianDensity)}\partial_\nu \varphi - \lagrangianDensity \tensor{\delta}{^\mu_\nu}.
    \end{equation}
    Raising the index we get
    \begin{equation}
        T^{\mu\nu} = \diffp{\lagrangianDensity}{(\partial_\mu \varphi)}\partial^\nu \varphi - \lagrangianDensity \minkowskiMetric^{\mu\nu}.
    \end{equation}
    This is the \defineindex{energy-momentum tensor}, also called the \define{stress-energy tensor}\index{stress-energy tensor|see{energy-momentum tensor}}\footnote{Strictly this is the energy-momentum density tensor, but very rarely does anyone bother to make that distinction.}.
    
    The conserved \enquote{charge} associated with the \(\nu = 0\) component is
    \begin{equation}
        \int \dl{^3\vv{x}} \, T^{00} = \int \left( \diffp{\lagrangianDensity}{(\partial_0\varphi)} \partial^0\varphi - \lagrangianDensity \right) = \int \dl{^3\vv{x}} \, \hamiltonianDensity = H,
    \end{equation}
    so this is an expression of energy conservation.
    This shouldn't be surprising as translations by \(a^0\) correspond to time translations.
    
    Similarly the conserved charges associated with the \(\nu = i\) components are
    \begin{equation}
        P^i = \int \dl{^3\vv{x}} \, T^{0i} = \int \dl{^3\vv{x}} \, \pi \partial^i \varphi.
    \end{equation}
    Since we expect these three components to form a four-vector with time component given by \(H\) we interpret them as the actual momentum of the particle, as opposed to the conjugate momentum, \(\pi\).
    We can then interpret \(\pi\partial_i\varphi\) as the momentum density.
    
    \chapter{Quantised Fields}
    Quantum field theory is a quantum theory of fields.
    We've seen classical fields, now we need some quantum mechanics before we can start quantising fields.
    So, in this section we'll give a quick recap of quantum mechanics.
    For more details see the \course{Principles of Quantum Mechanics} and \course{Quantum Theory} courses.
    
    \section{Quantum Mechanics}
    \epigraph{It's slightly disturbing that people believe it.}{Richard Ball, on quantum mechanics}
    \subsection{The Basics}
    In quantum mechanics we start with coordinates \(q_i\), and a Lagrangian, \(\lagrangian(q_i, \dot{q}_i)\).
    We then define canonical momenta,
    \begin{equation}
        p_i \coloneqq \diffp{\lagrangian}{q_i}.
    \end{equation}
    From this we can find a Hamiltonian,
    \begin{equation}
        H = \sum_i p+i\dot{q}_i - \lagrangian.
    \end{equation}

    So far this could all be classical.
    The quantum mechanics begins when we interpret \(q_i\), \(p_i\), and \(H\) as \define{Hermitian operators}\index{Hermitian!operator}\footnote{An operator, \(A\), is Hermitian if \(A^\hermit = A\), where \(A^\hermit\) is the \define{Hermitian conjugate}\index{Hermitian!conjugate}, defined such that \(\bra{\psi}A\ket{\varphi} = \bra{\varphi}A^\hermit\ket{\psi}^*\).} acting on a Hilbert space of states.
    The Hermitian requirement is so that these operators have real eigenvalues, which we can then interpret as the results of measurements.
    For example, the average energy is given by
    \begin{equation}
        E = \bra{\psi, t} H \ket{\psi, t},
    \end{equation}
    where \(\ket{\psi, t}\) is the state of the particle, that is a vector in the Hilbert space, and \(\bra{\psi, t} = \ket{\psi, t}^\hermit\) is the corresponding vector in the dual space.
    
    The next step is to realise that operators don't necessarily commute, and so we impose the \define{canonical commutation relation (CCR)}\index{canonical commutation relation}\glossary[acronym]{CCR}{Canonical Commutation Relation}:
    \begin{equation}
        \commutator{q_i}{p_i} \coloneqq i\delta_{ij},
    \end{equation}
    where \(\commutator{A}{B} \coloneqq AB - BA\) is the \defineindex{commutator}.
    As well as this we impose that the positions commute, and the momenta commute, so
    \begin{equation}
        \commutator{q_i}{q_j} = \commutator{p_i}{p_j} = 0.
    \end{equation}
    
    \subsection{Time Dependence}
    The time evolution of a state, \(\ket{\psi, t}\), under a system with Hamiltonian \(H\) is given by the \define{Schrödinger equation}\index{Schrödinger!equation}\index{time dependent Schrödinger equation}
    \begin{equation}
        i \diff{}{t} \ket{\psi, t} = H\ket{\psi, t}.
    \end{equation}
    This is a fairly simple differential equation, if we ignore the fact that \(H\) is an operator and \(\ket{\psi, t}\) is a vector, the solution is just
    \begin{equation}\label{eqn:time evolution of Schrödinger equation}
        \ket{\psi, t} = \e^{-iHt}\ket{\psi}
    \end{equation}
    where \(\ket{\psi} = \ket{\psi, 0}\) is the initial state of the system.
    Fortunately this is perfectly valid, even though \(H\) \emph{is} an operator and \(\ket{\psi, t}\) \emph{is} vector, we just have to interpret the exponential through its power series:
    \begin{equation}
        \e^{-iHt} \coloneqq \sum_{n = 0}^{\infty} \frac{1}{n!}(-iHt)^n.
    \end{equation}
    
    This scheme with which we have worked so far, where operators are time independent and states are time dependent, is called the \define{Schrödinger picture}\index{Schrödinger!picture}.
    It turns out that its actually easier to do QFT by interpreting operators to be time dependent and states to be time independent, called the \define{Heisenberg picture}\index{Heisenberg!picture}.
    We distinguish between these two pictures by either including \(t\) or not, as appropriate in our operators and states.
    A state, \(\ket{\psi}\), in the Heisenberg picture is related to the state \(\ket{\psi, t}\) in the Schrödinger picture by
    \begin{equation}
        \ket{\psi} = \e^{iHt}\ket{\psi, t},
    \end{equation}
    which is just what we get rearranging \cref{eqn:time evolution of Schrödinger equation}.
    An operator, \(A(t)\), in the Heisenberg picture is related to the operator \(A\) in the Schrödinger picture by
    \begin{equation}
        A(t) = \e^{iHt} A \e^{-iHt}.
    \end{equation}
    
    Importantly, expectation values are the same in both pictures, and so both pictures describe the same physics:
    \begin{align}
        \expected{A}_{\symrm{S}} &= \bra{\psi, t} A \ket{\psi, t}\\
        &= \bra{\psi} \e^{iHt} A \e^{-iHt}\\
        &= \bra{\psi} A(t) \ket{\psi}\\
        &= \expected{A}_{\symrm{H}},
    \end{align}
    where the subscripts refer to the picture in which we interpret the expectation value.
    
    This picture changing is the same for all operators, so in particular, the position and momentum in the Heisenberg picture are given by
    \begin{equation}
        q_i(t) = \e^{iHt}q_i\e^{-iHt}, \qqand p_i(t) = \e^{iHt}p_i\e^{-iHt}.
    \end{equation}
    We also have
    \begin{equation}
        H(t) = \e^{iHt} H \e^{-iHt} = \e^{iHt}\e^{-iHt} H = H,
    \end{equation}
    which works since \(H\) commutes with itself, and the exponentials are just power series in \(H\), so also commute with \(H\).
    This shows that the Hamiltonian is time independent in both pictures, which is a statement of energy conservation.
    
    Consider two operators, \(A(t)\) and \(B(t)\), in the Heisenberg picture.
    If we know their commutator, \(\commutator{A}{B}\), in the Schrödinger picture then we can also compute it in the Heisenberg picture:
    \begin{align}
        \commutator{A(t)}{B(t)} &= \braket{\e^{iHt}A\e^{-iHt}}{\e^{iHt}B\e^{-Ht}}\\
        &= \e^{iHt}A\e^{-iHt}\e^{iHt}B\e^{-iHt} - \e^{iHt}B\e^{-iHt}\e^{iHt}A\e^{iHt}\\
        &= \e^{iHt}AB\e^{-iHt} - \e^{iHt}BA\e^{-iHt}\\
        &= \e^{iHt}(AB - BA)\e^{-iHt}\\
        &= \e^{iHt}\commutator{A}{B}\e^{iHt}.
    \end{align}
    In particular,
    \begin{equation}
        \commutator{q_i(t)}{p_j(t)} = \e^{iHt}\commutator{q_i}{p_j}\e^{-iHt} = \e^{iHt}i\delta_{ij}\e^{-iHt} = i\delta_{ij}\e^{iHt}\e^{-iHt} = i\delta_{ij},
    \end{equation}
    so the canonical commutation relations hold if both operators are evaluated at the same time.
    Similarly,
    \begin{equation}
        \commutator{q_i(t)}{q_j(t)} = \commutator{p_i(t)}{p_j(t)} = 0.
    \end{equation}
    
    In the Heisenberg picture instead of the Schrödinger equation we have the \define{Heisenberg equation}\index{Heisenberg!equation}, which we can derive by considering the time derivative of a generic operator, \(A(t)\):
    \begin{align}
        \diff{}{t}A(t) &= \diff{}{t}(\e^{iHt}A\e^{-iHt})\\
        &= iH\e^{iHt}A\e^{-iHt} + \e^{iHt}A(-iH)\e^{-iHt}\\
        &= i\e^{iHt}HA\e^{-iHt} - i\e^{iHt}AH\e^{-iHt}\\
        &= i\e^{iHt}\commutator{H}{A}\e^{-iHt}\\
        &= i\commutator{H}{A(t)}.
    \end{align}
    That is,
    \begin{equation}
        \diff{}{t}A(t) = i\commutator{H}{A(t)}.
    \end{equation}
    
    The Heisenberg equation applied to the position and momentum gives
    \begin{equation}
        \diff{}{t}q(t) = i\commutator{H}{q(t)}, \qand \diff{}{t}p(t) = i\commutator{H}{p(t)}.
    \end{equation}
    Note the similarity to \cref{eqn:hamilton's equations with poisson brackets}.
    In general when we work in the Heisenberg picture things can look pretty similar to classical mechanics, but with \(i\) times the commutator in place of Poisson brackets.
    
    \begin{exm}{}{}
        Consider the Lagrangian for a particle of mass \(m\) in a position dependent potential, \(V\), in one dimension:
        \begin{equation}
            L = \frac{1}{2}m\dot{q}^2 - V(q).
        \end{equation}
        The Hamiltonian for this system is
        \begin{equation}
            H = \frac{p^2}{2m} + V(q).
        \end{equation}
        We then have
        \begin{equation}
            \dot{q} = i\commutator{H}{q(t)} = \frac{i}{2m}\commutator{p^2}{q}.
        \end{equation}
        Now, consider three operators, \(A\), \(B\), and \(C\).
        We have
        \begin{align}
            \commutator{AB}{C} &= ABC - CBA\\
            &= ABC - ACB + ACB - CBA\\
            &= A\commutator{B}{C} + \commutator{A}{C}B.
        \end{align}
        Using this with \(A = B = p\) and \(C = q\) gives
        \begin{equation}
            \commutator{p^2}{q} = p\commutator{p}{q} + \commutator{p}{q}p = -2ip,
        \end{equation}
        where the negative comes from the antisymmetry of the commutator, \(\commutator{p}{q} = -\commutator{q}{p} = -i\).
        Hence,
        \begin{equation}
            \dot{q} = \frac{p}{2m}.
        \end{equation}
        Classically we would have \(p = mv = m\dot{q}\), and this is just the quantum analogue of the classical nonrelativistic momentum.
        
        We also have
        \begin{equation}
            \dot{p} = i\commutator{H}{p(t)} = i\commutator{V(q)}{p}.
        \end{equation}
        We can expand \(V(q)\) as a power series in \(q\), we also have the identity
        \begin{equation}
            \commutator{q^n}{p} = inq^{n-1}.
        \end{equation}
        This can be proven with induction on \(n\).
        First, take \(n = 0\), then
        \begin{equation}
            \commutator{q^0}{p} = \commutator{\ident}{p} = 0.
        \end{equation}
        Now suppose that
        \begin{equation}
            \commutator{q^k}{p} = ikq^{k-1}
        \end{equation}
        for some nonnegative integer \(k\).
        Then
        \begin{multline}
            \commutator{q^{k+1}}{p} = \commutator{qq^k}{p} = q\commutator{q^k}{p} + \commutator{q}{p}q^k\\
            = q ikq^{k-1} + iq^k = i(k + 1)q^k,
        \end{multline}
        and so by induction the identity holds for all natural numbers, \(n\).
        
        Expanding \(V\) as a Taylor series we have
        \begin{equation}
            V(q) = \sum_{n = 0}^{\infty} \frac{q^n}{n!} \diffp[n]{V}{q}\bigg|_{q = 0}.
        \end{equation}
        Then we have
        \begin{align}
            \commutator{V(q)}{p} &= \sum_{n = 0}^{\infty} \frac{1}{n!} \diffp[n]{V}{q}\bigg|_{q = 0} \commutator{q^n}{p}\\
            &= \sum_{n = 0}^{\infty} \frac{1}{n!} \diffp[n]{V}{q}\bigg|_{q = 0} in q^{n - 1}\\
            &= \sum_{n = 1}^{\infty} \frac{iq^{n - 1}}{(n - 1)!} \diffp[n]{V}{q}\bigg|_{q = 0}.
        \end{align}
        To first order we then have
        \begin{equation}
            \commutator{V(q)}{p} = i \diffp[n]{V}{q}\bigg|_{q = 0}
        \end{equation}
        and so
        \begin{equation}
            \dot{p} = i\commutator{V(q)}{p} = -\diffp{V}{q}.
        \end{equation}
        Compare this to the classical nonrelativistic equation
        \begin{equation}
            F = -\diffp{V}{x},
        \end{equation}
        expressing the force due to a potential.
        
        Both of these two analogies, known as \defineindex{Ehrenfest's theorem}, between classical mechanics and quantum mechanics are really due to the similarity between the classical and quantum equations of motion in the Heisenberg picture.
    \end{exm}
    
    \section{Quantum Fields}
    \epigraph{Fundamentally silly}{Richard Ball}
    The procedure for quantising fields is almost identical to the procedure for quantising position and momentum.
    Instead of the position and momentum we have the fields \(\varphi\) and \(\pi\).
    We interpret these as operators in the Heisenberg picture.
    Since in field theory we deal with densities instead to be integrated over, such as \(\lagrangianDensity\) instead of \(\lagrangian\), we replace \(\delta_{ij}\) in the canonical commutation relations with \(\delta^3(\vv{x} - \vv{x}')\), note that this treats time differently to space.
    We postulate that the at some time, \(t\), we have
    \begin{equation}
        \commutator{\varphi(t, \vv{x})}{\pi(t, \vv{x}')} = i\delta^3(\vv{x} - \vv{x}')
    \end{equation}
    and the fields commute with themselves at all positions:
    \begin{equation}
        \commutator{\varphi(t, \vv{x})}{\varphi(t, \vv{x}')} = \commutator{\pi(t, \vv{x})}{\pi(t, \vv{x}')} = 0.
    \end{equation}
    This is called the \define{equal time commutation relation (ETCR)}\index{equal time commutation relation}\glossary[acronym]{ETCR}{Equal Time Commutation Relation}.
    
    In quantum mechanics as done in the previous section we treat \(t\) as a parameter and \(\vv{x}\) as an operator.
    This is clearly not Lorentz covariant.
    There are also other problems, such as having only a single \(\vv{x}\) operator which only allows us to talk of a single particle.
    In quantum field theory we interpret both \(t\) and \(\vv{x}\) as parameters and \(\varphi\) as an operator.
    We can then make this Lorentz covariant and treat systems with multiple particles.
    
    \subsection{Equations of Motion}
    The Heisenberg equation,
    \begin{equation}
        \dot{A}(t, \vv{x}) = i \commutator{H}{A(t, \vv{x})},
    \end{equation}
    applies to any operator, \(A\), and hence applies to \(\varphi\).
    We then have
    \begin{equation}
        \dot{\varphi}(t, \vv{x}) = i \commutator{H}{\varphi(t, \vv{x})} = i \int \dl{^3\vv{x}'} \, \commutator{\hamiltonianDensity(t, \vv{x}')}{\varphi(t, \vv{x})}
    \end{equation}
    We've replaced the Hamiltonian with an integral over the Hamiltonian density.
    We'll now introduce a shorthand notation where primes denote quantities evaluated at \((t, \vv{x}')\) and an absence of a prime is a quantity evaluated at \((t, \vv{x})\), so \(\varphi' = \varphi(t, \vv{x}')\) and \(\varphi = \varphi(t, \vv{x})\).
    The Hamiltonian density is
    \begin{equation}
        \hamiltonianDensity' = \frac{1}{2}(\pi'^2 + (\grad'\varphi')^2 + m^2\varphi'^).
    \end{equation}
    The commutator then becomes
    \begin{equation}
        \commutator{\hamiltonianDensity'}{\varphi} = \frac{1}{2}\left( \commutator{\pi'^2}{\varphi} + \commutator{(\grad'\varphi')^2}{\varphi} + m^2\commutator{\varphi'^2}{\varphi} \right).
    \end{equation}
    The last two terms vanish, since the fields commute with themselves at all positions.
    The first term isn't too bad if we use
    \begin{equation}
        \commutator{AB}{C} = A \commutator{B}{C} + \commutator{A}{C}B,
    \end{equation}
    we have
    \begin{align}
        \frac{1}{2}\commutator{\pi'^2}{\varphi} &= \frac{1}{2}\pi'\commutator{\pi'}{\varphi} + \frac{1}{2}\commutator{\pi'}{\varphi}\pi'\\
        &= -\frac{1}{2}\pi'\commutator{\varphi}{\pi'} - \frac{1}{2}\commutator{\varphi}{\pi'}\pi'\\
        &= -\frac{1}{2} \pi' i\delta^3(\vv{x} - \vv{x}') - \frac{1}{2} i\delta^3(\vv{x} - \vv{x}') \pi'\\
        &= -i\pi' \delta^3(\vv{x} - \vv{x}').
    \end{align}
    Hence, we have the equation of motion
    \begin{align}
        \dot{\varphi}(t, \vv{x}) &= i \int \dl{^3\vv{x}'} \commutator{\hamiltonianDensity(t, \vv{x}')}{\varphi(t, \vv{x})}\\
        &= i \int \dl{^3\vv{x}'} (-i\pi(t, \vv{x}') \delta^3(\vv{x} - \vv{x}'))\\
        &= \pi(t, \vv{x}).
    \end{align}
    So, \(\dot{\varphi} = \pi\).
    This should be no surprise since we found this same relationship for \(\varphi\) satisfying the Klein--Gordon equation before we started quantising.
    
    Similarly we can find the equation of motion for \(\pi\):
    \begin{equation}
        \dot{\pi}(t, \vv{x}) = i\commutator{H}{\pi(t, \vv{x})} = i\int \dl{^3\vv{x}'} \commutator{\hamiltonianDensity(t, \vv{x}')}{\pi(t, \vv{x})}.
    \end{equation}
    The commutator in this case is
    \begin{equation}
        \commutator{\hamiltonianDensity'}{\pi} = \frac{1}{2}(\commutator{\pi'^2}{\pi} + \commutator{(\grad'\varphi')^2}{\pi} + m^2\commutator{\varphi'^2}{\pi}).
    \end{equation}
    The first term vanishes this time.
    The last term can be computed easily:
    \begin{align}
        \frac{1}{2}m^2\commutator{\varphi'^2}{\pi} &= \frac{1}{2}m^2 \varphi'^2\commutator{\varphi'}{\pi} + \frac{1}{2}m^2\commutator{\varphi'}{\pi}\varphi'\\
        &= \frac{1}{2}m^2 i\delta^3(\vv{x} - \vv{x}') + \frac{1}{2}m^2 \varphi' i\delta^3(\vv{x} - \vv{x}')\\
        &= im^2 \delta^3(\vv{x} - \vv{x}').
    \end{align}
    The second term isn't too bad, we just have to notice that \(\grad'\) acts on \(\vv{x}'\) and not on \(\vv{x}\), allowing us to pull it outside of commutators like \(\commutator{\grad'\varphi'}{\pi} = \grad'\commutator{\varphi'}{\pi}\).
    Proceeding as before we then have
    \begin{align}
        \frac{1}{2}\commutator{(\grad'\varphi')^2}{\pi} &= \frac{1}{2} (\grad'\varphi') \cdot \commutator{\grad'\varphi'}{\pi} + \frac{1}{2} \commutator{\grad'\varphi'}{\pi} \cdot (\grad'\varphi')\\
        &= \frac{1}{2}(\grad'\varphi') \cdot (\grad'\commutator{\varphi'}{\pi}) + \frac{1}{2}(\grad'\commutator{\varphi'}{\pi}) \cdot (\grad'\varphi')\\
        &= \frac{1}{2}(\grad'\varphi') \cdot (\grad'i\delta^3(\vv{x} - \vv{x}')) + \frac{1}{2}(\grad'i\delta^3(\vv{x} - \vv{x}')) \cdot (\grad'\varphi')\notag\\
        &= i(\grad'\varphi') \cdot (\grad'\delta^3(\vv{x} - \vv{x}')).
    \end{align}
    We can deal with the gradient of the delta distribution by integrating by parts, and assuming that fields vanish sufficiently quickly at infinity:
    \begin{align}
        \int \dl{^3\vv{x}'} (\grad'\varphi) \cdot (\grad'\delta^3(\vv{x} - \vv{x}')) &= \int \dl{^3\vv{x}'} \, \grad' \cdot [\delta^3(\vv{x} - \vv{x}') (\grad'\varphi')]\\
        &\qquad- \int \dl{^3\vv{x}'} \, (\grad'^2\varphi') \delta^3(\vv{x} - \vv{x}')\\
        &= \int_{\partial V} [\delta^3(\vv{x} - \vv{x}') \grad'\varphi']\\
        &\qquad- \int \dl{^3\vv{x}'} \, (\grad'^2\varphi') \delta^3(\vv{x} - \vv{x}')\\
        &= - \int \dl{^3\vv{x}'} \, (\grad'^2\varphi') \delta^3(\vv{x} - \vv{x}')
    \end{align}
    where as usual the boundary term vanishes.
    This means that, as distributions,
    \begin{equation}
        i(\grad'\varphi') \cdot (\grad'\delta^3(\vv{x} - \vv{x}')) = -i(\grad'^2\varphi')\delta^3(\vv{x} - \vv{x}').
    \end{equation}
    Hence, we have
    \begin{align}
        \dot{\pi}(t, \vv{x}) &= i \int \dl{^3\vv{x}'} \commutator{\hamiltonianDensity(t, \vv{x}')}{\varphi(t, \vv{x})}\\
        &= i \int \dl{^3\vv{x}'} \left[ -i(\grad'^2\varphi') \delta^3(\vv{x} - \vv{x}') + im^2 \delta^3(\vv{x} - \vv{x}')\right]\\
        &= \laplacian\varphi - m^2\varphi.
    \end{align}
    
    Now, we have \(\pi = \dot{\varphi}\), so \(\dot{\pi} = \ddot{\varphi} = \partial_0\partial^0 \varphi\).
    Hence,
    \begin{equation}
        \partial_0\partial^0 \varphi = \laplacian\varphi - m^2\varphi = \partial_i\partial^i \varphi - m^2\varphi.
    \end{equation}
    Rearranging this we get
    \begin{equation}
        0 = (\partial_0\partial^0 - \partial_i\partial^i)\varphi + m^2\varphi = (\dalembertian + m^2)\varphi,
    \end{equation}
    so the quantised fields satisfy the Klein--Gordon equation.
    This means that the Heisenberg equations for field operators look a lost like classical field equations with operators replacing the fields and \(i\) times the commutator replacing Poisson brackets.
    
    Even though the Hamiltonian formulation and the equal time commutation relations aren't Lorentz covariant the final result is, and that's all that we care about.
    The steps in the derivation, however, are frame dependent.
    This is an inescapable problem with canonical quantisation.
    
    \chapter{Quantum Harmonic Oscillator}
    \epigraph{By understanding the harmonic oscillator you understand everything! Well, except from all the things we can't solve.}{Richard ball}
    \epigraph{Surely the hydrogen atom isn't a harmonic oscillator? Well it is!}{Richard Ball}
    The quantum harmonic oscillator is one of the most important systems in quantum mechanics.
    It's everywhere, we use it to model atoms and phonons, molecules, and pretty much anything else.
    Even other systems we study in quantum mechanics, such as the hydrogen atom or pairs of particles, can be interpreted as a harmonic oscillator by a change of coordinates.
    After all, any potential in physics can always be Taylor expanded to second order at a minimum to get a harmonic potential.
    Part of the reason why we study the harmonic oscillator is that its very nice.
    We can solve it exactly.
    The results are also nice, evenly spaced energy levels, bounded below.
    
    The harmonic oscillator is the system with the Hamiltonian
    \begin{equation}
        H = \frac{p^2}{2m} + \frac{1}{2}m\omega^2 q^2.
    \end{equation}
    We have previously remarked on the similarity of this and the Klein--Gordon Hamiltonian density, and suggested that study of the harmonic oscillator could be enlightening for the Klein--Gordon equation.
    In this section we'll study the harmonic oscillator, computing the spectrum of the Hamiltonian, and discussing various interpretations.
    In the next chapter we'll go back through the same steps but for fields obeying the Klein--Gordon and the Hamiltonian density.
    
    \section{Solution}
    There are two ways to solve the harmonic oscillator: the analytic way, involving Hermite polynomials, which is frankly, horrible, and the algebraic way, which is maybe more mysterious but is much more inline with the quantum field theory interpretation, so this is the way we'll proceed.
    
    Start by defining a new operators from the position and momentum operators:
    \begin{align}
        a \coloneqq \sqrt{\frac{m\omega}{2}} q + \frac{i}{\sqrt{2m\omega}} p.
    \end{align}
    The factors are chosen so that our final statements have a simple form, note that \(a\) is dimensionless.
    The Hermitian conjugate of this is
    \begin{equation}
        a^\hermit = \sqrt{\frac{m\omega}{2}} q - \frac{i}{\sqrt{2m\omega}} p.
    \end{equation}
    Note that \(a\) is not Hermitian, it's not an observable, for now we can just think of it as a mathematical trick.
    We'll see later that combinations of \(a\) and \(a^\hermit\) can be treated as observables with meaningful values.
    The motivation for this definition is that if \(a\) and \(a^\hermit\) commuted (they don't, as we'll see soon) we could factorise the Hamiltonian as
    \begin{equation}
        \omega aa^\hermit.
    \end{equation}
    
    A bit of algebra allows us to rewrite \(q\) and \(p\) in terms of \(a\) and \(a^\hermit\):
    \begin{equation}
        q = \frac{1}{2m\omega} (a + a^\dagger), \qqand p = -i\sqrt{\frac{m\omega}{2}} (a - a^\hermit).
    \end{equation}
    We can now compute the commutation relations for \(a\) and \(a^\hermit\) from the canonical commutation relation:
    \begin{align}
        i &= \commutator{q}{p}\\
        &= -\frac{i}{2}\commutator{a + a^\hermit}{a - a^\hermit}\\
        &= -\frac{i}{2}\left( \commutator{a}{a} - \commutator{a}{a^\hermit} + \commutator{a^\hermit}{a} - \commutator{a^\hermit}{a^\hermit} \right)\\
        &= i\commutator{a}{a^\hermit}.
    \end{align}
    Hence, we have that
    \begin{equation}
        \commutator{a}{a^\hermit} = 1.
    \end{equation}
    
    An alternative approach to quantum mechanics is to start with \(a\) and \(a^\hermit\), motivating them starting with the interpretation we'll see later, and then show that the canonical commutation relations are what they are as a consequence of this choice.
    
    We can write the Hamiltonian in terms of \(a\) and \(a^\hermit\):
    \begin{align}
        H &= -\frac{\omega}{4}(a - a^\hermit)^2 + \frac{\omega}{4}(a + a^\hermit)^2\\
        &= -\frac{\omega}{4}(a^2 - aa^\hermit - a^\hermit a + (a^\hermit)^2) + \frac{\omega}{4}(a^2 + aa^\hermit + a^\hermit a + (a^\hermit)^2)\\
        &= \frac{\omega}{2}(aa^\hermit + a^\hermit a).
    \end{align}
    Now use
    \begin{equation}
        1 = \commutator{a}{a^\hermit} = aa^\hermit - a^\hermit a \implies aa^\hermit = a^\hermit a + 1
    \end{equation}
    and we get
    \begin{equation}
        H = \frac{\omega}{2}(aa^\hermit + a^\hermit a) = \frac{\omega}{2}(a^\hermit a + 1 + a^\hermit a) = \omega\left( a^\hermit a + \frac{1}{2} \right).
    \end{equation}
    
    To compute the spectrum of the Hamiltonian we work with the operator \(N \coloneqq a^\hermit a\).
    This \emph{is} a Hermitian operator, \(N^\hermit = (a^\hermit a) = a^\hermit (a^\hermit)^\hermit = a^\hermit a = N\).
    We can then write the Hamiltonian as
    \begin{equation}
        H = \omega\left( N + \frac{1}{2} \right),
    \end{equation}
    if you remember what the energy eigenvalues of the harmonic oscillator are you may see where this is going.
    
    Let \(\ket{n}\) be an eigenstate of \(N\) with eigenvalue \(n\), so \(N\ket{n} = n\ket{n}\).
    For now all we know is that \(n \in \reals\), since \(N\) has real eigenvalues as a Hermitian operator.
    We choose to normalise these states so that
    \begin{equation}
        \braket{n}{m} = \delta_{nm}
    \end{equation}
    The use of \(n\) and \(m\) as labels, as well as the Dirac delta hints at something we will show later, that \(n\) is an integer, but for now we just treat \(n\) as a real number.
    
    The expectation value of \(N\) in the state \(\ket{n}\) is
    \begin{equation}
        \expected{N} = \bra{n}N\ket{n} = n\braket{n}{n} = n.
    \end{equation}
    Using \(N = a^\hermit a\) we can calculate this expectation value a different way:
    \begin{equation}
        n = \bra{n} N \ket{n} = \bra{n} a^\hermit a \ket{n} = \norm{a \ket{n}}^2,
    \end{equation}
    and since \(\norm{a \ket{n}}^2 \ge 0\) we have that \(n \ge 0\).
    
    Now consider commutators of \(N\) with \(a^\hermit\) and \(a\):
    \begin{align}
        \commutator{N}{a^\hermit} = \commutator{a^\hermit a}{a^\hermit} = a^\hermit \commutator{a}{a^\hermit} + \commutator{a^\hermit}{a^\hermit}a = a^\hermit.
    \end{align}
    Similarly,
    \begin{equation}
        \commutator{N}{a} = \commutator{a^\hermit a}{a} = a^\hermit\commutator{a}{a} + \commutator{a^\hermit}{a}a = -a.
    \end{equation}
    
    Suppose we have an eigenstate, \(\ket{n}\).
    What happens when we act on this with \(N\)?
    Well, it turns out that we get another eigenstate:
    \begin{align}
        N a^\hermit \ket{n} &= (\commutator{N}{a^\hermit} + a^\hermit N)\ket{n}\\
        &= (a^\hermit + a^\hermit N)\ket{n}\\
        &= a^\hermit \ket{n} + a^\hermit N \ket{n}\\
        &= a^\hermit \ket{n} + a^\hermit n \ket{n}\\
        &= (n + 1)a^\hermit \ket{n}.
    \end{align}
    So, \(a^\hermit \ket{n}\) is an eigenstate of \(N\) with eigenvalue \(n + 1\).
    That is, \(a^\hermit \ket{n} \propto \ket{n + 1}\).
    We can fairly easily figure out the constant of proportionality by considering \(\norm{a^\hermit\ket{n}}^2\):
    \begin{align}
        \norm{a^\hermit \ket{n}}^2 &= \bra{n} aa^\hermit \ket{n}\\
        &= \bra{n} (a^\hermit a + \commutator{a}{a^\hermit}) \ket{n}\\
        &= \bra{n} (a^\hermit a + 1) \ket{n}\\
        &= \bra{n} a^\hermit a \ket{n} + \braket{n}{n}\\
        &= \bra{n} N \ket{n} + 1\\
        &= n + 1.
    \end{align}
    Since the overall phase of a state has no physical significance we are free to choose the phase of the proportionality constant, so we make the sensible choice that its real and positive.
    That is, we choose the positive square root, \(\norm{a^\hermit \ket{n}} = \sqrt{n + 1}\), and we then have
    \begin{equation}
        a^\hermit \ket{n} = \sqrt{n + 1}\ket{n}.
    \end{equation}
    
    We can proceed similarly for \(a\ket{n}\), we have
    \begin{align}
        N a \ket{n} &= (\commutator{N}{a} + aN)\ket{n}\\
        &= (-a + aN)\ket{n}\\
        &= (n - 1)a \ket{n}.
    \end{align}
    So \(a\ket{n}\) is an eigenstate of \(N\) with eigenvalue \(n - 1\), so \(a\ket{n} \propto \ket{n - 1}\).
    We can, again, work out the constant of proportionality:
    \begin{equation}
        \norm{a\ket{n}}^2 = \bra{n} a^\hermit a \ket{n} = \bra{n} N \ket{n} = n.
    \end{equation}
    So, again choosing a real, positive, solution we have
    \begin{equation}
        a\ket{n} = 
        \begin{cases}
            \sqrt{n}\ket{n - 1} & n > 0,\\
            0 & n = 0,
        \end{cases}
    \end{equation}
    the reason for the careful distinction when \(n = 0\) is that the state \(\ket{0 - 1} = \ket{-1}\) is not defined, as the eigenvalues of \(N\) are nonnegative.
    The reason we don't have to exclude, say, \(n = 0.5\), which would have \(\ket{0.5 - 1} = \ket{-0.5}\), is that, as we will now show, \(n\) is a nonnegative integer, that is \(n \in \naturals = \{0, 1, 2, \dotsc\}\).
    
    Suppose that \(n \notin \naturals\).
    Consider some \(m \in \naturals \setminus \{0\}\).
    Then we can apply \(a\) \(m\) times to \(\ket{n}\):
    \begin{multline}
        a^m \ket{n} = \sqrt{n} a^{m - 1}\ket{n} = \sqrt{n}\sqrt{n - 1} a^{m - 2} \ket{n}\\
        = \dotsb = \sqrt{n} \sqrt{n - 1} \dotsm \sqrt{n - m + 1} \ket{n - m}.
    \end{multline}
    For sufficiently large \(m\) we will have \(n - m < 0\).
    However, \(n - m\) is an eigenvalue and all eigenvalues of \(N\) are nonnegative.
    This is a contradiction and so we must have \(n \in m\).
    In this case one of the square roots will vanish before we get to negative eigenvalues and we'll avoid the contradiction.
    
    Since \(H = \omega(N + 1/2)\) we can see that the eigenvalues of the Hamiltonian are
    \begin{equation}
        \left( n + \frac{1}{2} \right)\omega \qqwhere n \in \naturals.
    \end{equation}
    The eigenvectors of the Hamiltonian can all be written as
    \begin{equation}
        \ket{n} = \frac{(a^\hermit)^n}{\sqrt{n!}} \ket{0}
    \end{equation}
    where \(\ket{0}\) is such that when \(a\) acts on it we get zero:
    \begin{equation}
        a\ket{0} \coloneqq 0.
    \end{equation}
    Note that \(\ket{0}\) is an eigenstate with eigenvalue 0, and \(0\) is the zero vector, these are not the same.
    The factor of \(1/\sqrt{n!}\) is just normalisation, if \(\ket{0}\) is normalised then \(a^\hermit \ket{0} = \ket{1} = \sqrt{1}\ket{1}\), then \((a^\hermit)^2\ket{0} = \sqrt{1}a^\hermit \ket{1} = \sqrt{1} \sqrt{2} \ket{2}\), and so on, so we can get rid of all these factors with \(1/\sqrt{n!}\), and we're left with the normalised states \(\ket{n}\).
    
    The most important thing about the harmonic oscillator is that the energy eigenvalues are equally spaced.
    This makes harmonic oscillators very nice.
    Another nice property is that the energy eigenvalues are nonnegative.
    This means that the energy is bounded below, which is good as if it wasn't then systems would quickly decay to having negative infinite energies.
    The equally spaced eigenvalues is what makes all of quantum field theory possible.
    
    \section{Interpretations}
    \epigraph{It's fine, but in a sense, it's just wrong.}{Richard Ball}
    There are two ways to interpret the harmonic oscillator.
    Historically the first we discuss here was Schrödinger's interpretation, and actually came a year after the second interpretation we'll discuss, which is due to Heisenberg.
    However, for a long time Schrödinger's interpretation was preferred, mostly because it uses more familiar mathematics, like differential equations, and also because it aligns more similarly with the notion of a classical harmonic oscillator.
    
    \subsection{Wave Function Interpretation}
    \epigraph{This is the way you were taught the harmonic oscillator in kindergarten I guess.}{Richard Ball}
    The quantum harmonic oscillator is a quantised classical harmonic oscillator.
    We take the normal harmonic oscillator Hamiltonian, replace the position and momentum with operators, and solve the equations of motion.
    The picture that people then have in their head is a harmonic potential with equally spaced energy levels, as seen in \cref{fig:harmonic potential and energy levels}.
    
    \begin{figure}
        \tikzsetnextfilename{HO-wave-function-interpretation-energy-levels}
        \begin{tikzpicture}
            \draw[->, thick] (-2, 0) -- (2, 0) node [below] {\(q\)};
            \draw[->, thick] (0, 0) -- (0, 3) node [left] {\(E\)};
            \draw[highlight, very thick, domain={-sqrt(3)}:{sqrt(3)}] plot (\x, \x*\x);
            \foreach \y in {0, 0.5, ..., 2.5} {
                \draw[Blue, very thick] (-1, \y) -- (1, \y);
            }
        \end{tikzpicture}
        \caption{The harmonic potential and the evenly spaced energy levels it produces.}
        \label{fig:harmonic potential and energy levels}
    \end{figure}
    
    The physical variables are \(q\) and \(p\), with \(q = x\) the position operator, and \(p\) chosen to satisfy the canonical commutation relations, which it can be shown is the case if \(p = -i\diff{}/{x}\).
    We solve the equation \(a \ket{0} = 0\) to find the vacuum state \(\ket{0}\), and it's wave function \(\psi_0(x) = \braket{x}{0}\):
    \begin{equation}
        \psi_0(x) = \sqrt{\frac{m\omega}{2\pi}} \exp\left[ -\frac{1}{2}m\omega x^2 \right].
    \end{equation}
    We then find the wave equation for the \(n\)th excited state, \(\sqrt{n!}\psi_n(x) = \bra{x}(a^\hermit)^n \ket{0}\):
    \begin{equation}
        \psi_n(x) = \frac{1}{\sqrt{n!}} \left( 0\frac{i}{\sqrt{m\omega}} \diff{}{x} + i\sqrt{\frac{m\omega}{2}} x \right)^n \psi_0(x)
    \end{equation}
    where the expression in brackets is just \(a^\hermit = i/\sqrt{2m\omega} p + \sqrt{m\omega/2} q\) with the operators substituted in.
    After working through lots of derivatives of Gaussians we will find that the solutions are Gaussians times a Hermite polynomial, \(H_n\):
    \begin{equation}
        \psi_n(x) = \frac{1}{\sqrt{2^n n!}} \left( \frac{m\omega}{\pi} \right)^{1/4} \exp\left[ -\frac{1}{2}m\omega x^2 \right] H_n\left( \sqrt{m\omega}x \right).
    \end{equation}
    
    This interpretation is called \defineindex{first quantisation}, presumably because it is the first way that most people are taught to interpret the harmonic oscillator.
    
    \subsection{Quanta}
    The energy eigenvalues of the harmonic oscillator are evenly spaced.
    This allows us to interpret the state \(\ket{n}\) as consisting of \(n\) identical quanta, all of the same energy, \(\omega\).
    The state \(\ket{0}\) is then the vacuum state, where there are no quanta, and we have the vacuum energy \(\omega/2\).
    
    Rather than try to interpret what a harmonic oscillator is we focus on what it does.
    Think of it as a black box which can absorb a quanta of energy, so it has \(n + 1\) quanta, or it can emit a quanta, so it has \(n - 1\) quanta.
    Clearly these occurrences align with the action of \(\ket{a}^\hermit\) and \(\ket{a}\) respectively.
    This leads to us interpreting \(a^\hermit\) as a \defineindex{creation operator}, producing a quanta of energy in the black box, and \(a\) as a \defineindex{annihilation operator}, destroying a quanta of energy in the black box.
    Of course this energy must come from somewhere and go somewhere, but just within the black box it may as well be appearing and vanishing.
    
    This interpretation, while a bit odd at first, is actually closer to what we do in experiments.
    We scatter particles, essentially adding them to the black box, and see what comes out, that is, what leaves the black box.
    This interpretation also works well when considering atomic spectra, which occur in much the same way.
    
    This interpretation is intrinsically multiparticle, which is good for quantum field theory.
    This interpretation is called \defineindex{second quantisation}, presumably because it is the second way that most people are taught to interpret the harmonic oscillator.
    
    \section{Harmonic Oscillator with Time Dependence}
    If we include time dependence in the Harmonic oscillator the, working in the Heisenberg picture, this corresponds to replacing \(q\) and \(p\) with time dependent operators, which in term means introducing a time dependence to \(a\) and \(a^\hermit\).
    This time dependence doesn't change the first step of solving the Harmonic oscillator, which is finding \(q\) and \(p\) in terms of \(a\) and \(a^\hermit\), which gives us
    \begin{align}
        q(t) &= \frac{1}{\sqrt{2m\omega}} [a(t) + a^\hermit(t)],\\
        p(t) &= i\sqrt{\frac{m\omega}{2}} [a(t) - a^\hermit(t)].
    \end{align}
    The commutation relations for \(a(t)\), \(a^\hermit(t)\), and \(H\) are \(\commutator{a(t)}{a^\hermit(t)} = 1\), \(\commutator{H}{a(t)} = -\omega a(t)\), and \(\commutator{H}{a^\hermit(t)} = \omega a^\hermit(t)\), which are the same as if we replace \(H\) with \(N\), since \(H = \omega(N + 1/2)\).
    The time dependence doesn't change the commutators, which we can check quite easily:
    \begin{align}
        \commutator{a(t)}{a^\hermit(t)} &= \commutator{\e^{-iHt}a\e^{iHt}}{\e^{-iHt}a^\hermit\e^{iHt}}\\
        &= \e^{-iHt}a\e^{iHt}\e^{-iHt}a^\hermit\e^{iHt} - \e^{-iHt}a^\hermit\e^{iHt}\e^{-iHt}a\e^{iHt}\\
        &= \e^{-iHt}aa^\hermit\e^{iHt} - \e^{-iHt}a^\hermit a\e^{iHt}\\
        &= \e^{-iHt} \commutator{a}{a^\hermit}\e^{iHt}\\
        &= \e^{-iHt} 1 \e^{iHt}\\
        &= 1.
    \end{align}
    Likewise, the commutators with the Hamiltonian are unchanged:
    \begin{align}
        \commutator{H}{a^\hermit(t)} &= \commutator{H}{\e^{-iHt}a^\hermit \e^{iHt}}\\
        &= H\e^{-iHt}a^\hermit\e^{iHt} - \e^{-iHt}a^\hermit\e^{iHt} H\\
        &= \e^{-iHt} Ha^\hermit \e^{iHt} - \e^{-iHt} a^\hermit H \e^{iHt}\\
        &= \e^{-iHt} \commutator{H}{a^\hermit} \e^{iHt}\\
        &= \e^{-iHt} \omega a^\hermit \e^{iHt}\\
        &= \omega a^\hermit(t).
    \end{align}
    An similarly for \(\commutator{H}{a(t)}\).
    
    We can then solve the Heisenberg equation for the creation and annihilation operators:
    \begin{align}
        \dot{a}(t) = i \commutator{H}{a(t)} = -i \omega a(t).
    \end{align}
    This very simple differential equation, \(\dot{a}(t) = -i\omega a(t)\), has a plane wave solution,
    \begin{equation}
        a(t) = a \e^{-i\omega t},
    \end{equation}
    where \(a = a(0)\) is the annihilation operator in the Schrödinger picture.
    Taking the Hermitian conjugate of this we get
    \begin{equation}
        \dot{a}^\hermit(t) = a^\hermit \e^{i\omega t}.
    \end{equation}
    The position and momentum operators are then
    \begin{align}
        q(t) &= \frac{1}{\sqrt{2m\omega}} [a \e^{-i\omega t} + a^\hermit \e^{i\omega t}],\\
        p(t) &= i\sqrt{\frac{m\omega}{2}} [a \e^{-i\omega t} - a^\hermit \e^{i\omega t}].
    \end{align}
    
    \chapter{Mode Expansions}
    \section{Expanding the Field}
    The field \(\varphi(x)\) satisfies the Klein--Gordon equations.
    We've seen that this allows plane wave solutions, \(\e^{\pm i p \cdot x}\).
    This suggests that we can write a general solution as a superposition of wave solutions, which is just a Fourier transform.
    Since \(\varphi\) is a function of \(x\) this is really an inverse Fourier transform.
    For comparison consider the inverse transform of a single variable, real, one-dimensional function, \(f\):
    \begin{equation}
        f(x) = \int \frac{\dl{p}}{2\pi} (\tilde{f}(p) \e^{-ip x} + \tilde{f}^*(p) \e^{ip x}).
    \end{equation}
    Here \(\tilde{f}(p)\) is the Fourier transformed function, which acts as a Fourier coefficient here.
    We need both the \(\e^{ipx}\) and conjugate \(\e^{-ipx}\) terms to get a real function.
    The factor of \(2\pi\) is part of the normalisation, but exactly where it appears between the forward and inverse is just convention.
    
    We want our field to have on-shell momentum, that is \(p^2 = m^2\).
    To enforce this we include a factor of \(2\pi \delta(p^2 - m^2)\), where the \(2\pi\) is factor is from the integral representation of the Dirac delta, as the Fourier transform of one:
    \begin{equation}
        \int \dl{x} \, \e^{-ip \cdot x} = 2\pi \delta(p).
    \end{equation}
    We also want to restrict ourselves to positive energy solutions, so \(p_0 > 0\), we can enforce this with a factor of \(\heaviside(p_0)\), where \(\heaviside\)\index{\(\heaviside\), Heaviside step function} is the \defineindex{Heaviside step function} defined by
    \begin{equation}
        \heaviside(u) = 
        \begin{cases}
            1 & u > 0,\\
            1/2 & u = 0,\\
            0 & u < 0.
        \end{cases}
    \end{equation}
    
    The \defineindex{mode expansion} of \(\varphi\) is then
    \begin{equation}
        \varphi(x) = \int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p_0) [ a(\vv{p}) \e^{-ip\cdot x} + a^\hermit(\vv{p}) \e^{i p \cdot x} ].
    \end{equation}
    Here \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) are Fourier coefficients, we'll see later that they are the quantum field theory analogues of the creation and annihilation operators for the harmonic oscillator.
    They depend only on \(\vv{p}\), rather than the four-vector \(p\), since we can take \(\delta(p^2 - m^2)\) as fixing the value of \(p_0\).
    
    Define \(\omega(\vv{p}) \coloneqq +\sqrt{\vv{p}^2 + m^2}\), which is always nonnegative.
    This allows us to rewrite the argument of the Dirac delta:
    \begin{equation}
        (p_0 - \omega(\vv{p}))(p_0 + \omega(\vv{p})) = p_0p_0 - \omega(\vv{p})^2 = p_0p_0 - \vv{p}^2 -m^2 = p^2 - m^2.
    \end{equation}
    So, now we have \(\delta(\omega(\vv{p}))\) in our integral.
    Recall the distribution identity
    \begin{equation}
        \delta(f(x)) = \sum_i \frac{\delta(x - a_i)}{\abs{f'(a_i)}}
    \end{equation}
    where \(a_i\) are all the zeros of \(f\) lying in the range of integration.
    Since in the integral we are enforcing \(p_0 \ge 0\) the only time \(p^2 - m^2 = (p_0 - \omega(\vv{p}))(p_0 + \omega(\vv{p}))\) vanishes is when \(p_0 = \omega(\vv{p})\), and
    \begin{equation}
        \diffp{}{p} (p^2 - m^2) \bigg|_{p_0 = \omega(\vv{p})} = 2\omega(\vv{p})
    \end{equation}
    so, as distributions, we have
    \begin{equation}
        \delta(p^2 - m^2)\heaviside(p_0) = \frac{1}{2\omega(\vv{p})} \delta(p^0 - \omega(\vv{p})).
    \end{equation}
    
    This lets us write \(\varphi(x)\) as
    \begin{equation}
        \varphi(x) = \int \frac{\dl{^4 p}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} \delta(p^0 - \omega(\vv{p})) [ a(\vv{p}) \e^{-ip\cdot x} + a^\hermit(\vv{p}) \e^{i p \cdot x} ].
    \end{equation}
    Performing the \(p^0\) integral using the sifting property of the Dirac delta we have
    \begin{equation}
        \varphi(x) = \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} [ a(\vv{p}) \e^{-ip\cdot x} + a^\hermit(\vv{p}) \e^{i p \cdot x} ] \, \bigg|_{p_0 = \omega(\vv{p})}.
    \end{equation}
    
    At this point we make some remarks on this expansion:
    \begin{itemize}
        \item The expansion is manifestly Lorentz invariant, while the presence of \(p^0\) may be concerning orthochronous Lorentz transformations (the only type we consider in this course) don't change the sign of \(p^0\), and so \(\heaviside(p^0)\) is Lorentz invariant.
        \item Since \(p^0 = \omega(\vv{p})\) the energy is always positive, and so is bounded below.
        \item The integral over \(\delta(p^0 - \omega(\vv{p}))\) fixes \(p^0 = \omega(\vv{p})\), and from now on this will be implicit in the integrals, instead of writing \(|_{p_0 = \omega(\vv{p})}\).
        \item The operator \(\varphi(x)\) is in the Heisenberg picture, but the time dependence comes from the exponential factors, both \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) are time independent operators.
        \item We need both the \(a\) and \(a^\hermit\) terms for \(\varphi\) to be Hermitian.
        \item The measure
        \begin{equation}
            \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} = \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p^0)
        \end{equation}
        is Lorentz invariant, but takes a lot of writing, we will use the shorthand notation
        \begin{equation}
            \invariantmeasure{p} = \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} = \int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p^0).
        \end{equation}
    \end{itemize}
    
    Now that we've expanded the field \(\varphi\) it is easy to expand the conjugate field, \(\pi\), using \(\pi = \dot{\varphi}\):
    \begin{align}
        \pi(x) = \dot{\varphi}(x) &= -\frac{i}{2} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} [a(\vv{p}) \e^{-ip\cdot x} - a^\hermit(\vv{p}) \e^{ip \cdot x}]\\
        &= -\frac{i}{2} \int \invariantmeasure{p} [a(\vv{p}) \e^{-ip\cdot x} - a^\hermit(\vv{p}) \e^{ip \cdot x}].
    \end{align}
    Here we've used
    \begin{multline}
        \diffp{}{x^0} \e^{\pm ip \cdot x} = \diffp{}{x^0} \exp[\pm ip^0x_0 \mp p^ix_i]\\
        = \pm ip^0 \exp[\pm ip^0x_0 \mp p^ix_i] = \pm i p^0 \exp[\pm ip \cdot x]
    \end{multline}
    and then set \(p^0 = \omega(\vv{p})\).
    
    \section{The Fourier Coefficients}
    As with the Harmonic oscillator we now want to invert these relations to find expressions for \(a(\vv{p})\) and \(a^\hermit(\vv{p})\).
    Since these are inverse Fourier transformations the inversion happens through Fourier transformation.
    For a single variable, real, one-dimensional function, \(f\), the Fourier transform is
    \begin{equation}
        \tilde{f}(p) = \int \dl{x} \, f(x) \e^{ip \cdot x}.
    \end{equation}
    Using this, but in three dimensions, we have
    \begin{align*}
        \int \dl{^3\vv{x}} \, \e^{ip' \cdot x} \varphi(x) &= \int \! \invariantmeasure{p} \frac{1}{2\omega(\vv{p})} \int \! \dl{^3\vv{x}} \, \big[a(\vv{p}) \e^{i(p' - p) \cdot x} + a^\hermit \e^{i(p' + p)\cdot x}\big]\\
        &= \int \! \invariantmeasure{p} \, (2\pi)^3 \big[a(\vv{p}) \delta^3(\vv{p}' - \vv{p})\e^{i(p'^0 - p^0)t}\\
        &\qquad\qquad\qquad+ a^\hermit(\vv{p}) \delta^3(\vv{p}' + \vv{p})\e^{i(p'^0 + p^0)t} \big]\\
        &= \int \frac{\dl{^3\vv{p}}}{2\omega(\vv{p})} \big[a(\vv{p}) \delta^3(\vv{p}' - \vv{p})\e^{i(p'^0 - p^0)t}\\
        &\qquad\qquad\qquad+ a^\hermit(\vv{p}) \delta^3(\vv{p}' + \vv{p})\e^{i(p'^0 + p^0)t}\big].
    \end{align*}
    Here we've recognised the integral representation of the Dirac delta:
    \begin{equation}
        \int \dl{^3\vv{x}} \, \e^{-ip\cdot x} = (2\pi)^3 \delta^3(\vv{x}) \e^{ip^0t},
    \end{equation}
    and made the usual assumption that everything is sufficiently convergent for us to be able to swap the order of integrals.
    Performing the integrals now using the Dirac deltas, and the fact that \(\omega(-\vv{p}) = \omega(\vv{p})\), we have
    \begin{align}
        \int \dl{^3\vv{x}} \, \e^{ip' \cdot x} \varphi(x) &= \frac{1}{2\omega(\vv{p}')}a(\vv{p}') + \frac{1}{2\omega(-\vv{p}')} a^\hermit(-\vv{p}') \e^{2ip'^0t}\\
        &= \frac{1}{2\omega(\vv{p}')} \big[a(\vv{p}') + a^\hermit(-\vv{p}') \e^{2i\omega(\vv{p}')t}\big].
    \end{align}
    Here we've used the Dirac delta's to set \(\vv{p} = \pm\vv{p}'\) in each term as appropriate.
    Restricting ourselves to on-shell, positive energy, solutions this then fixes the value of \(p^0\) to \(\sqrt{\vv{p}'^2 + m^2} = \omega(\vv{p}')\).
    
    We can do exactly the same for \(\pi\), the only difference is signs and a factor of \(1/\omega(\vv{p})\), so we get
    \begin{equation}
        \int \dl{^3\vv{x}} \, \e^{ip'\cdot x}  \pi(x) = -\frac{i}{2}[a(\vv{p}') - a(-\vv{p}')^\hermit \e^{2i\omega(\vv{p}')t}].
    \end{equation}
    Adding \(i\) times this to \(\omega(\vv{p})\) times the result for \(\varphi\) we get
    \begin{equation}
        a(\vv{p}) = \int \dl{^3\vv{x}} \, \e^{ip \cdot x} [\omega(\vv{p}) \varphi(t, \vv{x}) + i\pi(t, \vv{x})].
    \end{equation}
    Taking the conjugate we get
    \begin{equation}
        a^\hermit(\vv{p}) = \int \dl{^3\vv{x}} \, \e^{-ip \cdot x} [\omega(\vv{p}) \varphi(t, \vv{x}) - i\pi(t, \vv{x})].
    \end{equation}
    
    Now we have expressions for \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) we can compute their commutator:
    \begin{align}
        \commutator{a(\vv{p})}{a^\hermit(\vv{p}')} &= \int \! \dl{^3\vv{x}} \! \int \dl{^3\vv{x}'} \, \e^{ip\cdot x - ip'\cdot x'}\\
        &\qquad\qquad\commutator{\omega(\vv{p}) \varphi(t, \vv{x}) + i\pi(t, \vv{x})}{\omega(\vv{p}')\varphi(t, \vv{x}') - i\pi(t, \vv{x}')}. \notag
    \end{align}
    Using the equal time commutation relations only the cross terms don't vanish, and so this becomes
    \begin{align}
        &\commutator{a(\vv{p})}{a^\hermit(\vv{p}')} = \int \! \dl{^3\vv{x}} \! \int \dl{^3\vv{x}'} \, \e^{ip\cdot x - ip'\cdot x'} \big(-i\omega(\vv{p})\commutator{\varphi(t, \vv{x})}{\pi(t, \vv{x}')}\\
        &\hspace{17.5em}+ i\omega(\vv{p}')\commutator{\pi(t, \vv{x})}{\varphi(t, \vv{x}')}\big). \notag\\
        &= \int \! \dl{^3\vv{x}} \! \int \dl{^3\vv{x}'} \, \e^{ip\cdot x - ip'\cdot x'} (\omega(\vv{p}) \delta^3(\vv{x} - \vv{x}') + \omega(\vv{p}') \delta^3(\vv{x} - \vv{x}'))\\
        &= \int \dl{^3\vv{x}} \, \e^{i(p - p')\cdot x} (\omega(\vv{p}) + \omega(\vv{p}'))\\
        &= (2\pi)^3 \delta(\vv{p} - \vv{p}') 2\omega(\vv{p})
    \end{align}
    where again we've recognised the integral representation of the Dirac delta, and made use of the resulting Dirac delta to set \(\vv{p} = \pm \vv{p}'\), and hence \(p^0 = p'^0\) choosing to stay on-shell and positive energy.
    We introduce the shorthand notation,
    \begin{equation}
        (2\pi)^3 \delta(\vv{p} - \vv{p}') 2\omega(\vv{p}) = \bardelta(\vv{p} - \vv{p}').
    \end{equation}
    This allows us to write
    \begin{equation}
        \commutator{a(\vv{p})}{a^\hermit(\vv{p}')} = \bardelta(\vv{p} - \vv{p}').
    \end{equation}
    Compare this to \(\commutator{a}{a^\hermit} = 1\) for the harmonic oscillator.
    We also have
    \begin{equation}
        \commutator{a(\vv{p})}{a(\vv{p}')} = \commutator{a^\hermit(\vv{p}')}{a^\hermit(\vv{p}')} = 0.
    \end{equation}
    
    Note that \(\bardelta(\vv{p} - \vv{p}')\) is just a number, not an operator.
    This notation is nice, since if we combine the invariant measure, \(\invariantmeasure{p}\), and \(\bardelta(\vv{p} - \vv{p}')\) then we can just pretend we have \(\dl{p}\) and \(\delta^3(\vv{p} - \vv{p})\):
    \begin{equation}
        \int \invariantmeasure{p} \, f(\vv{p}) \bardelta(\vv{p} - \vv{p}') = f(\vv{p}').
    \end{equation}
    
    \section{The Hamiltonian}\label{sec:the hamiltonian}
    To complete the analogy with the harmonic oscillator we need to find the Hamiltonian in terms of \(a(\vv{p})\) and \(a^\hermit(\vv{p})\).
    Starting with the hamiltonian
    \begin{equation}
        H = \frac{1}{2} \int \dl{^3\vv{x}} \left[ \pi(x)^2 + (\grad \varphi(x))^2 + m^2 \varphi(x)^2 \right]
    \end{equation}
    we can split this into three components,
    \begin{align}
        H_1 &= \frac{1}{2}\int \dl{^3\vv{x}} \, \pi(x)^2,\\
        H_2 &= \frac{1}{2}\int \dl{^3\vv{x}} \, (\grad\varphi(x))^2,\\
        H_3 &= \frac{1}{2}\int \dl{^3\vv{x}} \, m^2\varphi(x)^2,\\
    \end{align}
    We'll then treat each of these separately.
    
    Start with the first term.
    Substituting in the expansion for \(\pi(x)\), being careful to use different integration variables for each factor of \(\pi\), we get
    \begin{multline}
        H_1 = -\frac{1}{8} \int \!\! \dl{^3\vv{x}} \int \! \frac{\dl{^3\vv{p}}}{(2\pi)^3} \left[ a(\vv{p}) \e^{-ip\cdot x} - a^\hermit(\vv{p}) \e^{ip\cdot x} \right] \\
        \times\int \! \frac{\dl{^3\vv{p}'}}{(2\pi)^3} \left[ a(\vv{p}') \e^{-ip'\cdot x} - a^\hermit(\vv{p}') \e^{ip'\cdot x} \right].
    \end{multline}
    Expanding this all the integrand becomes
    \begin{multline}
        a(\vv{p}) a(\vv{p}') \e^{-i(p + p')\cdot x} - a(\vv{p}) a^\hermit(\vv{p}') \e^{-i(p - p')\cdot x}\\
        - a^\hermit(\vv{p})a(\vv{p}')\e^{i(p - p') \cdot x} + a^\hermit(\vv{p}) a^\hermit(\vv{p}') \e^{i(p + p')\cdot x}.
    \end{multline}
    We can perform the integration over position first, and, recalling that \(p^0 = \omega(\vv{p})\), we get two types of terms, time dependent and time independent, these come from recognising the integral representation of the Dirac delta:
    \begin{align}
        \int \dl{^3\vv{x}} \, \e^{\pm i(p + p') \cdot x} &= (2\pi)^3 \delta(\vv{p} + \vv{p}') \e^{\pm 2i\omega(\vv{p})t},\\
        \int \dl{^3\vv{x}} \, \e^{\pm i(p - p') \cdot x} &= (2\pi)^3 \delta(\vv{p} - \vv{p}').
    \end{align}
    We should expect that the time dependent terms in \(H\) will all cancel, since \(H\) is time independent.
    
    After performing the integral over positions we are left with the following integrand
    \begin{multline*}
        (2\pi)^3 a(\vv{p}) a(\vv{p}') \delta(\vv{p} - \vv{p}')\e^{-2i\omega(\vv{p})t} - (2\pi)^3 a(\vv{p}) a^\hermit(\vv{p}') \delta(\vv{p} - \vv{p}')\\
        - (2\pi)^3a^\hermit(\vv{p})a(\vv{p}')\delta(\vv{p} - \vv{p}') + (2\pi)^3a^\hermit(\vv{p}) a^\hermit(\vv{p}') \delta(\vv{p} + \vv{p}') \e^{2i\omega(\vv{p})t}.
    \end{multline*}
    Performing the integral over \(\vv{p}'\), and cancelling one factor of \((2\pi)^3\) we are left with
    \begin{align}
        H_1 &= -\frac{1}{8} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \big[ a(\vv{p}) a(\vv{p}) \e^{-2i\omega(\vv{p})t}  - a(\vv{p}) a^\hermit(\vv{p})\\
        &\qquad\qquad\qquad\qquad- a^\hermit(\vv{p}) a(\vv{p}) + a^\hermit(\vv{p})a^\hermit(\vv{p}) \e^{2i\omega(\vv{p})t} \big].\notag
    \end{align}
    Keeping only the time independent terms, since the time independent terms will cancel with terms in \(H_2\) and \(H_3\), we get the contribution from the first term:
    \begin{equation}
        H_1 \to \frac{1}{8} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \big[ a(\vv{p}) a^\hermit(\vv{p})+ a^\hermit(\vv{p}) a(\vv{p}) \big].
    \end{equation}
    Doing the same for the other two terms we get their time independent contributions:
    \begin{align}
        H_2 &\to \frac{1}{8} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{\vv{p}^2}{\omega(\vv{p})^2} \big[ a(\vv{p}) a^\hermit(\vv{p}) + a^\hermit(\vv{p}) a(\vv{p}) \big],\\
        H_3 &\to \frac{1}{8} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{m^2}{\omega(\vv{p})^2} \big[ a(\vv{p}) a^\hermit(\vv{p}) + a^\hermit(\vv{p}) a(\vv{p}) \big].
    \end{align}
    See \cref{app:hamiltonian mode expansion} for the full calculation.
    Noticing that \(\vv{p}^2 + m^2\) cancels with \(1/\omega(\vv{p})^2\) in the full expression we get
    \begin{equation}
        H = \frac{1}{4} \int \frac{\dl{^3p}}{(2\pi)^3} \big[ a(\vv{p})a^\hermit(\vv{p}) + a(\vv{p})a^\hermit(\vv{p}) \big].
    \end{equation}
    Compare this to the Harmonic oscillator result,
    \begin{equation}
        H = \frac{1}{2}\omega(aa^\hermit + a^\hermit a).
    \end{equation}
    
    We can now compute the commutators of \(H\) with \(a(\vv{p})\) and \(a^\hermit(\vv{p})\):
    \begin{align}
        \commutator{H}{a^\hermit(\vv{p})} &= \frac{1}{4} \int \frac{\dl{^3p'}}{(2\pi)^3} \commutator{a(\vv{p}')a^\hermit(\vv{p}') + a^\hermit(\vv{p}')a(\vv{p}')}{a^\hermit(\vv{p})}\\
        &= \frac{1}{4} \int \frac{\dl{^3p'}}{(2\pi)^3} \big( \commutator{a(\vv{p}')a^\hermit(\vv{p}')}{a^\hermit(\vv{p})} + \commutator{a^\hermit(\vv{p}')a(\vv{p}')}{a^\hermit(\vv{p})}  \big) \notag\\
        &= \frac{1}{4} \int \frac{\dl{^3p'}}{(2\pi)^3} \big( a(\vv{p}')\commutator{a^\hermit(\vv{p}')}{a^\hermit(\vv{p})} + \commutator{a(\vv{p}')}{a^\hermit(\vv{p})}a^\hermit(\vv{p}') \notag\\
        &\qquad\qquad\qquad+ a^\hermit(\vv{p}')\commutator{a(\vv{p}')}{a^\hermit(\vv{p})} + \commutator{a^\hermit(\vv{p}')}{a^\hermit(\vv{p})} \big)\\
        &= \frac{1}{4} \int \dl{^3p'} \, 2\omega(\vv{p}) [ \delta(\vv{p} - \vv{p}')a^\hermit(\vv{p}') + a^\hermit(\vv{p}') \delta(\vv{p}' - \vv{p}) ]\\
        &= \omega(\vv{p})a^\hermit(\vv{p}).
    \end{align}
    
    Similarly, with \(a(\vv{p})\) we get
    \begin{equation}
        \commutator{H}{a(\vv{p})} = -\omega(\vv{p})a(\vv{p}).
    \end{equation}
    Compare these to the harmonic oscillator results, \(\commutator{H}{a^\hermit} = \omega a^\hermit\) and \(\commutator{H}{a} = -\omega a\).
    
    We can view the quantum fields as being a quantum harmonic oscillator of energy \(\omega(\vv{p}) = \sqrt{\vv{p}^2 + m^2}\) for every Fourier mode, \(\vv{p}\).
    
    \section{Particle Interpretation}
    Consider an energy eigenstate, \(\ket{E}\), with eigenvalue \(E\), so \(H\ket{E} = E\ket{E}\).
    Then the state \(a^\hermit(\vv{p})\ket{E}\) is also an energy eigenstate, with energy \(E + \omega(\vv{p})\):
    \begin{align}
        Ha^\hermit(\vv{p})\ket{E} &= \left( \commutator{H}{a^\hermit(\vv{p})} + a^\hermit(\vv{p})H \right)\ket{E}\\
        &= (\omega(\vv{p})a^\hermit(\vv{p}) + a^\hermit(\vv{p})E)\\
        &= (\omega(\vv{p}) + E)a^\hermit(\vv{p})\ket{E}.
    \end{align}
    Similarly, \(a(\vv{p})\ket{E}\) is an energy eigenstate with energy \(E - \omega(\vv{p})\).
    
    This suggests that, following the interpretation of the harmonic oscillator, \(a^\hermit(\vv{p})\) is a \defineindex{creation operator}, creating a quantum with energy \(\omega(\vv{p}) = \sqrt{\vv{p}^2 + m^2}\), and \(a(\vv{p})\) is an \defineindex{annihilation operator}, destroying a quantum with energy \(\omega(\vv{p})\).
    Note that all quanta in this interpretation have positive energy.
    There is another interpretation, which we'll discuss later, where \(a^\hermit(\vv{p})\) creates a quantum with energy \(\omega(\vv{p})\) and \(a(\vv{p})\) creates a quantum with energy \(-\omega(\vv{p})\).
    
    Since the Hamiltonian is nonnegative, its defined as a sum of squares of real quantities, there must be some lowest energy state, \(\ket{0}\), which we call the \defineindex{vacuum state}.
    This state is such that any annihilation operator acting on it gives the zero vector:
    \begin{equation}
        a(\vv{p}) \ket{0} = 0 \quad \forall \vv{p}.
    \end{equation}
    
    Starting with \(\ket{0}\) we can act with creation operators to make new states.
    Each creation operator, \(a^\hermit(\vv{p})\), makes a new particle with energy \(\omega(\vv{p})\) and momentum \(\vv{p}\).
    For example, 
    \begin{equation}
        \ket{\psi} = a^\hermit(\vv{p}) a^\hermit(\vv{q}) a^\hermit(\vv{r}) \ket{0}
    \end{equation}
    is a state with three particles with total energy \(\omega(\vv{p}) + \omega(\vv{q}) + \omega(\vv{r})\), and total momentum \(\vv{p} + \vv{q} + \vv{r}\).
    
    Since all \(a^\hermit(\vv{p})\) commute the state
    \begin{equation}
        a^\hermit(\vv{p}) a^\hermit(\vv{q}) a^\hermit(\vv{r}) \ket{0}
    \end{equation}
    is the same as \(\ket{\psi}\).
    We interpret this as meaning that the particles are bosons, swapping any two particles leaves the state unchanged.
    This means the particles follow Bose--Einstein statistics.
    
    The space of all states which can be constructed in this way, called \define{Fock space}\index{Fock space!bosons}, is the space given by all complex-linear combinations of
    \begin{equation}
        a^\hermit(\vv{p_1}) \dotsm a^\hermit(\vv{p_n})\ket{0}
    \end{equation}
    for any values of \(n \in \naturals\).
    
    Formally, the Fock space (for bosons) is defined to be
    \begin{equation}
        F_+(\hilbertSpace) = \overline{\bigoplus_{n = 0}^{\infty} S \hilbertSpace^{\otimes n}} = \overline{\complex \oplus \hilbertSpace \oplus S(\hilbertSpace \otimes \hilbertSpace) \oplus S(\hilbertSpace \otimes \hilbertSpace \otimes \hilbertSpace) \oplus \dotsb}
    \end{equation}
    where \(\hilbertSpace\) is the single particle Hilbert space,
    \begin{equation}
        \hilbertSpace^{\otimes n} \coloneqq \bigotimes_{i = 1}^{n}\hilbertSpace = \underbrace{\hilbertSpace \otimes \dotsb \otimes \hilbertSpace}_{n \text{ times}},
    \end{equation}
    the operator \(S\) symmetrises all tensors, the direct sum, \(\oplus\), runs from \(n = 0\), for states with no particles, to infinity, with each term representing an \(n\)-particle state space, and the line represents that we complete the space to form a Hilbert space, this entails adding in the values of all absolutely convergent series.
    
    We make the normalisation choice that \(\braket{0}{0} = 1\), then, if \(\ket{\vv{p}} = a^\hermit(\vv{p})\ket{\vv{p}}\) is a single particle state we have
    \begin{align}
        \braket{\vv{p}}{\vv{q}} &= \bra{0} a(\vv{p}) a^\hermit(\vv{q})\ket{0}\\
        &= \bra{0} \commutator{a(\vv{p})}{a^\hermit(\vv{q})}\ket{0} + \bra{0} a^\hermit(\vv{q}) a(\vv{p})\ket{0}\\
        &= \bardelta(\vv{p} - \vv{q}).
    \end{align}
    Note that \(a(\vv{p})\ket{0}\) and \(\bra{0}a^\hermit(\vv{q}) = [a(\vv{q}) \ket{0}]^\hermit\) both vanish, since they are annihilators acting on the vacuum state.
    
    As with the harmonic oscillator we can define a \defineindex{number density operator}, \(N(\vv{p}) \coloneqq a^\hermit(\vv{p})a(\vv{p})\), the eigenvalues of which integrate to give the number of particles in the state with momentum \(\vv{p}\).
    
    \section{Ground State Energy}
    We can rewrite the Hamiltonian in terms of the number density operator:
    \begin{align}
        H &= \frac{1}{4} \int \invariantmeasure{p} \, \omega(\vv{p}) \big( a^\hermit(\vv{p}) a(\vv{p}) + a(\vv{p})a^\hermit(\vv{p}) \big)\\
        &= \frac{1}{2} \int \invariantmeasure{p} \, \omega(\vv{p}) \left( a^\hermit(\vv{p})a(\vv{p}) + \frac{1}{2} \commutator{a(\vv{p})}{a^\hermit(\vv{p})} \right)\\
        &= \frac{1}{2} \int \invariantmeasure{p} \, \omega(\vv{p}) \left( N(\vv{p}) + \omega(\vv{p})(2\pi)^3 \delta^3(\vv{p} - \vv{p}) \right)\\
        &= \frac{1}{2} \int \invariantmeasure{p} \, \omega(\vv{p}) \left( N(\vv{p}) + \omega(\vv{p})(2\pi)^3 \delta^3(\vv{0}) \right).
    \end{align}
    
    Clearly we have \(\bra{0}N(\vv{p})\ket{0} = \bra{0} a^\hermit(\vv{p})a(\vv{p})\ket{0} = 0\), and so
    \begin{equation}
        \bra{0}H\ket{0} = \frac{1}{2} \int \dl{^3\vv{p}} \, \omega(\vv{p}) \delta^3(\vv{0}).
    \end{equation}
    This is infinite.
    In fact, its \emph{very} infinite.
    Each \(\delta(0)\) is infinite, and we have three of them multiplied together, then we integrate over all momentum space, which is infinite, to make matters worse \(\omega(\vv{p})\) grows arbitrarily large with \(\abs{\vv{p}}\).
    
    The solution, as with so many similar problems in physics, is just not to think about it.
    All that we can measure is the energy of particles absorbed and emitted by the system.
    We cannot measure the ground state energy, so it doesn't matter that our theory gives a nonsensical answer when we try to compute the expected ground state energy.
    
    \subsection{Normal Ordering}
    One solution to the infinity arising in the ground state energy is to be careful about how we order \(a(\vv{p})\) and \(a^\hermit(\vv{p})\).
    If we keep all \(a^\hermit(\vv{p})\) on the left of all \(a(\vv{p})\) then, when we take expectation values, one of our \(a(\vv{p})\) will act on \(\ket{0}\) giving the zero vector, and one of our \(a^\hermit(\vv{p})\) will act on \(\bra{0}\), also giving the zero vector.
    
    Mathematically what happens is that when we change to order of operators to have all \(a^\hermit(\vv{p})\) on the left we'll get commutators of \(a(\vv{p})\) and \(a^\hermit(\vv{p}')\), which give rise to Dirac deltas which cancel with the existing ones, to leave a finite result.
    
    This idea that careful ordering of operators can avoid infinities goes beyond the expectation value of the ground state energy.
    Consider our field, \(\varphi(x)\).
    We can split this into parts,
    \begin{equation}
        \varphi(x) = \varphi^+(x) + \varphi^-(x)
    \end{equation}
    where
    \begin{equation}\label{eqn:phi +}
        \varphi^+(x) = \int \invariantmeasure{p} \, a(\vv{p}) \e^{-ip\cdot x},
    \end{equation}
    and
    \begin{equation}\label{eqn:phi -}
        \varphi^-(x) = \int \invariantmeasure{p} \, a^\hermit(\vv{p}) \e^{ip\cdot x} = (\varphi^+)^\hermit.
    \end{equation}
    If these were single particle states in quantum mechanics then they would correspond to positive and negative energy states respectively, but they aren't, although we'll see an interpretation along these lines later.
    
    Now suppose we want to consider the product \(\varphi(x)\varphi(y)\) for two arbitrary spacetime points, \(x\) and \(y\).
    We can expand this as follows:
    \begin{align}
        \varphi(x)\varphi(y) &= [\varphi^+(x) + \varphi^-(x)][\varphi^+(y) + \varphi^-(y)]\\
        &= \varphi^+(x)\varphi^+(y) + \varphi^+(x)\varphi^-(y) + \varphi^-(x)\varphi^+(y) + \varphi^-(x)\varphi^-(y).
    \end{align}
    The \defineindex{normal ordering} of \(\varphi(x)\varphi(y)\), denoted \(\normalordering{\varphi(x)\varphi(y)}\), is defined as the result of taking this product, but swapping terms so that all \(a^\hermit(\vv{p})\) appear on the left of all \(a(\vv{p}')\), that is all \(\varphi^+\) appear on the left of all \(\varphi^-\), so
    \begin{equation}
        \normalordering{\varphi(x)\varphi(y)} = \varphi^+(x)\varphi^+(y) + \varphi^+(x)\varphi^-(y) + \textcolor{highlight}{\varphi^+(y)\varphi^-(x)} + \varphi^-(x)\varphi^-(y).
    \end{equation}
    Then we have
    \begin{equation}
        \bra{0} \normalordering{\varphi(x)\varphi(y)} \ket{0} = 0.
    \end{equation}
    In general, if \(X\) is any nontrivial\footnote{That is, \(X\) is not just a scalar, in which case \(\bra{0}\normalordering{X}\ket{0} = X\braket{0}{0} = X\).} polynomial in \(a^\hermit(\vv{p})\) and \(a(\vv{p})\) we'll have
    \begin{equation}
        \bra{0} \normalordering{X} \ket{0} = 0.
    \end{equation}
    
    Subtracting the zero point energy of \(H\) from \(H\) is equivalent to normal ordering
    \begin{equation}
        \normalordering{H} = H - \bra{0} H \ket{0},
    \end{equation}
    since by normal ordering \(H\) we swap exactly those terms which give infinity, and all others give zero, so we can think of normal ordering as subtracting them off.
    
    From now on we'll assume that \(H\) and \(\vv{p}\) are normal ordered, often without writing it.
    
    \section{Ground State Momentum}
    The physical momentum operator is given by
    \begin{equation}
        \vv{P} = \int \dl{^3 \vv{x}} \, \pi(x) \grad\varphi(x).
    \end{equation}
    Recall that this comes from the \(T^{i0}\) component of the energy-momentum tensor.
    Expressing the fields in terms of \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) we get
    \begin{equation}
        \vv{P} = \frac{1}{2} \invariantmeasure{p} \, \vv{p} (a(\vv{p}) a^\hermit(\vv{p}) + a^\hermit(\vv{p})a(\vv{p}))
    \end{equation}
    where the factor of \(\vv{p}\) comes from computing \(\grad\varphi\).
    Rewriting this using the commutator of \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) we get
    \begin{equation}
        \vv{P} = \int \invariantmeasure{p} \, \vv{p}(N(\vv{p}) + \omega(\vv{p})(2\pi)^3\delta(\vv{0})).
    \end{equation}
    
    Unlike with the ground state energy the \(\delta(\vv{0})\) factor doesn't cause a problem.
    Integrating over all modes in the modes with momenta \(\vv{p}\) and \(-\vv{p}\) cancel, since \(\omega(\vv{p}) = \omega(-\vv{p})\).
    So, we have
    \begin{equation}
        \bra{0} \vv{P} \ket{0} = \bra{0} \normalordering{\vv{P}} \ket{0} = 0,
    \end{equation}
    that is, the vacuum state has no momentum, which really ought to be the case.
    
    \chapter{Covariant Commutators}
    \section{What are They}\label{sec:covariant commutators what are they}\index{covariant commutation relation!for scalars}
    The equal time commutation relations treat time specially.
    This is generally undesirable in relativity.
    So, in this section we derive a commutator, \(\commutator{\varphi(x)}{\varphi(y)}\) at two arbitrary spacetime points, \(x\) and \(y\).
    We start by writing
    \begin{equation}
        \varphi(x) = \varphi^+(x) + \varphi^-(x),
    \end{equation}
    where \(\varphi^+\) and \(\varphi^-\) are as in \cref{eqn:phi +,eqn:phi -}.
    Since \(\varphi^+\) only consists of annihilation operators and \(\varphi^-\) only consists of creation operators we have
    \begin{equation}
        \commutator{\varphi^+(x)}{\varphi^+(y)} = \commutator{\varphi^-(x)}{\varphi^-(y)} = 0.
    \end{equation}
    Hence, expanding out the commutator we have
    \begin{equation}
        \commutator{\varphi(x)}{\varphi(y)} = \commutator{\varphi^-(x)}{\varphi^+(y)} + \commutator{\varphi^-(x)}{\varphi^+(y)}.
    \end{equation}
    
    Consider the first of these,
    \begin{align}
        \commutator{\varphi^+(x)}{\varphi^-(y)} &= \int \invariantmeasure{p} \int \invariantmeasure{p'} \, \commutator{a(\vv{p})}{a^\hermit(\vv{p}')} \e^{-ip\cdot x + ip'\cdot y}\\
        &= \int \invariantmeasure{p} \int \invariantmeasure{p'} \, \bardelta(\vv{p} - \vv{p}') \e^{-ip\cdot x + ip' \cdot y}\\
        &= \int \invariantmeasure{p} \, \e^{-ip \cdot (x - y)}\\
        &\eqqcolon i\Delta^+(x - y),
    \end{align}
    where the last line defines \(\Delta^+\):
    \begin{equation}
        \Delta^+(x) \coloneqq i \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} \e^{ip\cdot x}
    \end{equation}
    The factor of \(i\) is a convention to make \(\Delta^+\) a real function.
    
    The second commutator can now easily be worked out using antisymmetry:
    \begin{equation}
        \commutator{\varphi^-(x)}{\varphi^+(y)} = -\commutator{\varphi^+(y)}{\varphi^-(x)} = -i\Delta^+(y - x) \eqqcolon i\Delta^-(x - y),
    \end{equation}
    where the last equality defines \(\Delta^-\).
    
    Combining these results we have
    \begin{align}
        \commutator{\varphi(x)}{\varphi(y)} = i\Delta^+(x - y) + i\Delta^-(x - y) \eqqcolon i\Delta(x - y)
    \end{align}
    where
    \begin{align}
        \Delta(x) &\coloneqq \Delta^+(x) + \Delta^-(x)\\
        &\hphantom{:}= i \int \invariantmeasure{p} (\e^{ip\cdot x} - \e^{-ip\cdot x})\\
        &\hphantom{:}= -2\int \invariantmeasure{p} \, \sin (p \cdot x).
    \end{align}
    Note that \(\Delta\) is a real (\(\Delta(x) = \Delta(x)^*\)), odd (\(\Delta(-x) = -\Delta(x)\)) function.
    This is what we would expect since the field is real and the commutator is antisymmetric.
    
    Consider the full form of \(\Delta^+\):
    \begin{equation}
        i\Delta^+(x) = \int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p_0) \e^{-ip\cdot x}.
    \end{equation}
    We can also write \(\Delta^-\) similarly:
    \begin{equation}
        i\Delta^-(x) = -\int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p_0) \e^{ip\cdot x}.
    \end{equation}
    If we make a change of variables, \(p \to -p\), then \(\dl{^4p} \to \dl{^4p}\), and
    \begin{equation}
        i\Delta^-(x) = -\int \frac{\dl{^4p}}{(2\pi)^4} 2\pi\delta(p^2 - m^2) \heaviside(-p_0) \e^{-ip\cdot x}.
    \end{equation}
    This allows us to write \(\Delta\) as
    \begin{equation}
        i\Delta(x) = \int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \varepsilon(p_0) \e^{-ip\cdot x}
    \end{equation}
    where
    \begin{equation}
        \varepsilon(p_0) \coloneqq \heaviside(p_0) - \heaviside(-p_0) = 
        \begin{cases}
            + 1 & p_0 > 0,\\
            0 & p_0 = 0,\\
            -1 & p_0 < 0,
        \end{cases}
    \end{equation}
    is the sign function.
    
    Note that for proper orthochronous Lorentz transformations \(\Delta\) is Lorentz invariant, since the sign of \(p_0\) doesn't change and all other terms are manifestly Lorentz invariant.
    
    \section{Microcausality}
    Suppose \(x\) is space-like, that is \(x^2 < 0\).
    Then there exists some proper orthochronous Lorentz transformation, \(\Lambda\), taking \(x\) to \(-x\).
    Space-like vectors lie outside the light cone, and we can view this transformation as rotating around the light cone.
    Since \(\Delta^{\pm}(x)\) are individually Lorentz invariant we can apply a Lorentz transformation to one of them and not the other when computing \(\Delta\) and get the same result, so
    \begin{align}
        \Delta(x) &= \Delta^+(x) + \Delta^-(x)\\
        &= \Delta^+(x) + \Delta^-(\Lambda x)\\
        &= \Delta^+(x) + \Delta^-(-x)\\
        &= \Delta^+(x) - \Delta^+(x)\\
        &= 0,
    \end{align}
    so \(\Delta\) vanishes for all space-like points.
    Hence, \(\commutator{\varphi(x)}{\varphi(y)}\) vanishes for all space-like separations, \(x - y\).
    
    On the other hand no such transformation exists for time-like or light-like points, since it would have to map from one half of the light cone to the other, which is a discontinuous transformation.
    
    We can interpret this as a requirement for causality, specifically for a nonzero commutator between fields at space-like separated points which happen to have the same time coordinate we'd need some form of communication at this time between these points, and that can't happen between space-like points at the same time without violating causality.
    
    This analysis tells us that \(\Delta\) is not analytic, it has a sudden jump from zero to nonzero at light-like points.
    
    \section{Equal Time Commutation Relations}
    The equal time commutation relations can now be viewed as a special case of the covariant commutation relations.
    Since we have \(\pi = \dot{\varphi}\) we want to compute \(\commutator{\varphi(t, \vv{x})}{\pi(t, \vv{x}')} = \commutator{\varphi(t, \vv{x})}{\dot{\varphi}(t, \vv{x}')}\).
    To do this take the time derivative of \(\Delta\):
    \begin{align}
        \diffp{}{x^0} \Delta(x - y) &= i \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} \diffp{}{x^0} \left[ \e^{ip\cdot(x - y)} - \e^{-ip\cdot(x - y)} \right]\\
        &=i \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} i\omega(\vv{p}) \left[ \e^{ip\cdot(x - y)} + \e^{-ip\cdot(x - y)} \right]\\
        &= -\frac{1}{2} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \left[ \e^{ip\cdot(x - y)} + \e^{-ip\cdot(x - y)} \right].
    \end{align}
    For the equal time commutation relations we have \(x^0 = y^0\) and so
    \begin{equation}
        p \cdot (x - y) = \vv{p} \cdot (\vv{x} - \vv{y}).
    \end{equation}
    Hence,
    \begin{align}
        \diffp{}{x^0} \Delta(x - y) &= -\frac{1}{2} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \left[ \e^{i\vv{p}\cdot(\vv{x} - \vv{y})} + \e^{-i\vv{p}\cdot(\vv{x} - \vv{y})} \right]\\
        &= -\delta^3(\vv{x} - \vv{y}),
    \end{align}
    where we've recognised the integral representation of the Dirac delta:
    \begin{equation}
        \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \e^{\pm i\vv{p} \cdot (\vv{x} - \vv{y})} = \delta^3(\vv{x} - \vv{y}).
    \end{equation}
    
    We then have
    \begin{align}
        \diffp{}{x^0} [i\Delta(x - y)] \bigg|_{x^0 = y^0 = t} &= \diffp{}{x^0} \commutator{\varphi(t, \vv{x})}{\varphi(t, \vv{y})}\\
        &= \commutator{\dot{\varphi}(t, \vv{x})}{\varphi(t, \vv{y})}\\
        &= \commutator{\pi(t, \vv{x})}{\varphi(t, \vv{y})}\\
        &= -i\delta^3(\vv{x} - \vv{y})
    \end{align}
    Hence,
    \begin{equation}
        \commutator{\varphi(t, \vv{x})}{\pi(t, \vv{y})} = i\delta^3(\vv{x} - \vv{y}).
    \end{equation}
    So, we can view the equal time commutation relations as a specific case of the covariant commutation relations.
    This is why the equal time commutation relations can still produce Lorentz invariant physics.
    
    \section{Contour Representation}\label{sec:contour representation}
    \begin{rmk}
        This section makes use of some complex analysis, in particular contour integrals and the residue theorem.
        See either \course{Methods of Theoretical Physics} or \course{Methods of Mathematical Physics} for details.
    \end{rmk}
    
    The contour representation of \(\Delta^{\pm}\) is
    \begin{equation}
        \Delta^{\pm}(x) = - \int_{C^{\pm}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2}.
    \end{equation}
    The contours \(C^{\pm}\) are anticlockwise circles containing the poles at \(\pm\omega(\vv{p})\).
    This is shown in \cref{fig:contours for Delta}.
    
    \begin{figure}
        \tikzsetnextfilename{contours-for-Delta}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [right] {\(\Re(p_0)\)};
            \draw[thick, ->] (0, -2) -- (0, 2) node [above] {\(\Im(p_0)\)};
            \fill[Red] (1, 0) circle [radius = 0.05cm] node [below, text=black] {\(\omega(\vv{p})\)};
            \fill[Red] (-1, 0) circle [radius = 0.05cm] node [below, text=black] {\(-\omega(\vv{p})\)};
            \draw[very thick, Blue] (1, 0) circle [radius = 0.8cm];
            \draw[very thick, Blue] (-1, 0) circle [radius = 0.8cm];
            \draw[very thick, Blue, ->] (1.01, 0.8) -- ++ (-0.02, 0);
            \draw[very thick, Blue, ->] (-1.01, 0.8) -- ++ (-0.02, 0);
            \node[below] at (1, -0.8) {\(C^+\)};
            \node[below] at (-1, -0.8) {\(C^-\)};
            \draw[very thick, Purple] (0, 0) circle [x radius = 2.2cm, y radius = 1.6 cm];
            \draw[very thick, Purple, ->] (0.01, 1.6) -- ++ (-0.02, 0);
            \node[below right] at (0, -1.6) {\(C^+ + C^-\)};
        \end{tikzpicture}
        \caption{The contours used in the contour representation of \(\Delta^{\pm}\) and \(\Delta\).}
        \label{fig:contours for Delta}
    \end{figure}
    
    To see why this works start with
    \begin{equation}
        p^2 - m^2 = (p_0 + \omega(\vv{p})) (p_0 - \omega(\vv{p})),
    \end{equation}
    which shows there are poles at \(p_0 = \pm \omega(\vv{p})\).
    We can then apply the residue theorem, which says that 
    \begin{equation}
        \oint_\gamma f(z) \dd{z} = 2\pi i \sum_i \Res(f, a_i)
    \end{equation}
    where \(\gamma\) is a closed contour, \(f\) is analytic on and inside \(\gamma\), \(a_i\) are the poles of \(f\) inside \(\gamma\), and \(\Res(f, a_i)\) is the residue of \(f\) at \(a_i\).
    
    We have a particularly easy case of two simple poles.
    In the case of a simple pole the residue at \(a_i\) is given by
    \begin{equation}
        \Res(f, a_i) = (z - a_i)f(z)|_{z = a_i},
    \end{equation}
    so
    \begin{equation}
        \Res\left( \frac{\e^{-ip\cdot x}}{(p_0 + \omega(\vv{p}))(p_0 - \omega(\vv{p}))}, \pm \omega(\vv{p}) \right) = \frac{\e^{-ip\cdot x}}{p_0 \pm \omega(\vv{p})} \bigg|_{p_0 = \mp \omega(\vv{p})} = \mp \frac{\e^{-ip\cdot x}}{2\omega(\vv{p})}.
    \end{equation}
    Hence,
    \begin{equation}
        -\int_{C^+} \frac{\dl{p_0}}{2\pi} \frac{\e^{-ip\cdot x}}{p^2 - m^2} = -i \frac{\e^{-ip\cdot x}}{2\omega(\vv{p})}\bigg|_{p_0 = \omega(\vv{p})},
    \end{equation}
    and
    \begin{equation}
        -\int_{C^+} \frac{\dl{p_0}}{2\pi} \frac{\e^{-ip\cdot x}}{p^2 - m^2} = i \frac{\e^{ip\cdot x}}{2\omega(\vv{p})}\bigg|_{p_0 = -\omega(\vv{p})}.
    \end{equation}
    That is,
    \begin{equation}
        -\int_{C^{\pm}} \frac{\dl{p_0}}{2\pi} \frac{\e^{-ip\cdot x}}{p^2 - m^2} = \mp i \frac{\e^{\mp ip\cdot x}}{2\omega(\vv{p})}\bigg|_{p_0 = \omega(\vv{p})}.
    \end{equation}
    Having performed the integral over \(p_0\) we are left with an integral over \(\vv{p}\) so
    \begin{equation}
        -\int_{C^{\pm}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2} = \mp i \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{\e^{mp ip \cdot x}}{2\omega(\vv{p})} = \Delta^{\pm}(x).
    \end{equation}
    
    The contour representation of \(\Delta\) is then simply given by
    \begin{equation}
        \Delta(x) = - \int_{C} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2}
    \end{equation}
    where \(C = C^+ + C^-\) is the combined contour, with the middle parts cancelling out, to give a contour containing both poles as shown in \cref{fig:contours for Delta}.
    
    \section{\texorpdfstring{\(\Delta\)}{Delta} as Green's Functions}
    To interpret \(\Delta\) and \(\Delta^{\pm}\) consider the Klein--Gordon equation,
    \begin{equation}
        0 = (\dalembertian + m^2)\varphi.
    \end{equation}
    Taking the Fourier transform and using the rule that \(\partial_x \to -ip\) we have
    \begin{equation}
        0 = ((-ip)^2 + m^2)\tilde{\varphi} \implies (p^2 - m^2)\tilde{\varphi} = 0.
    \end{equation}
    Transforming back then gives us the contour representation of \(\Delta\) or \(\Delta^{\pm}\), depending on the boundary conditions.
    
    This means we can interpret \(\Delta\) and \(\Delta^{\pm}\) as classical Green's functions for the Klein--Gordon equation, so
    \begin{equation}
        (\dalembertian + m^2)\Delta(x) = \delta^4(x).
    \end{equation}
    
    We call \(\Delta\) and \(\Delta^{\pm}\) propagators, since they tell us how a quantum propagates between states.
    
    \part{Interactions}
    \chapter{Scattering}
    \section{Interactions}
    \epigraph{Quantum things are generally more difficult than classical things.}{Richard Ball}
    So far, we have only considered Lagrangians which are quadratic in the field, we call these \define{free field Lagrangians}\index{free field Lagrangian}, since quadratic Lagrangians give linear equations of motion with plane wave solutions, which we interpret as many independent harmonic oscillators, each corresponding to a noninteracting particle.
    In order to have our particles do anything, such as be measured, we need interactions.
    This means introducing a nonquadratic term to our Lagrangian.
    
    \subsection{\texorpdfstring{\(\varphi^3\)}{Phi Cubed} Theory}
    The simplest nonquadratic term that we can add to our Lagrangian is a cubic term, giving the Lagrangian
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi) (\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2 - \frac{1}{6} g \varphi^3.
    \end{equation}
    The first two terms are just the usual Klein--Gordon Lagrangian, call it \(\lagrangianDensity\).
    We can treat this as the Lagrangian for a free particle.
    The extra term, \(-g\varphi^3/6\), is our interaction term, call it \(\lagrangianDensity_{\interaction}\).
    We call a theory with a cubic term like this a \define{\(\symbf{\varphi^3}\) theory}\index{\(\varphi^3\) theory}
    The factor of \(1/6\) in this factor is just conventional, its there to make equations simpler later.
    A \(\varphi^n\) theory would equivalently have a factor of \(1/n!\), each time we differentiate this factor becomes closer to 1.
    The factor of \(g\) is called the \defineindex{coupling constant}.
    It measures how strong the interaction is.
    
    We can easily apply the Euler--Lagrange equations to this modified Lagrangian, without much changing.
    We have
    \begin{equation}
        \partial_\mu \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} = \dalembertian\varphi, \qqand \diffp{\lagrangianDensity}{\varphi} = -m^2\varphi - \frac{1}{3}g \varphi^2.
    \end{equation}
    Hence, we have the equations of motion
    \begin{equation}
        (\dalembertian + m^2)\varphi = -\frac{1}{2}g \varphi^2.
    \end{equation}
    This is just the Klein--Gordon equation with some sort of source term.
    
    This is now a nonlinear differential equation.
    These are, in general, hard to solve.
    There is no general theory and we are usually restricted to specific examples with solutions.
    Because of this we assume that the interaction isn't very strong, that is that \(g \ll 1\), and then we solve by expanding in \(g\) and truncating at some appropriate point in our calculations.
    
    \section{The \texorpdfstring{\(S\)}{S} Matrix}
    Most processes in physics are, when it comes down to it, scattering.
    We see light after it scatters off an object, two objects collide in classical mechanics, two protons collide in the LHC, a whole bunch of particles collide all over the place and a statistical study of this gives statistical mechanics, and so on.
    Scattering is a very simple process, there are three steps:
    \begin{itemize}
        \item We start in some initial state with free particles of definite momentum, spin, etc.
        \item The particles undergo some scattering process, which we treat as a black box.
        \item We finish in some final state with free particles of definite momentum, spin, etc.
    \end{itemize}
    
    In order to allow us to consider free particles we assume that the initial state happens a long long time before the scattering, taking \(t \to -\infty\).
    Similarly we assume the final state is a long long time after the scattering, taking \(t \to +\infty\).
    Call the initial state \(\ket{\Psi, -\infty}\) and the final state \(\ket{\Psi, +\infty}\).
    Then an in between state at time \(t\) is \(\ket{\Psi, t}\).
    
    The initial and final state must somehow be related if we are to pass from one to the other.
    We take this relation to be of the form
    \begin{equation}
        \ket{\Psi, +\infty} = S\ket{\Psi, -\infty}
    \end{equation}
    where \(S\), known as the \define{\(\symbf{S}\) matrix}\index{S matrix@\(S\) matrix}, is the operator describing the scattering process.
    
    Now suppose that we start with some fixed, known initial state, \(\ket{i}\), so \(\ket{\Psi, -\infty} = \ket{i}\).
    We can choose some complete orthonormal basis of possible final states, \(\{\ket{f}\}\), which allows us to expand the final state as
    \begin{equation}
        \ket{\Psi, +\infty} = \sum_f \ket{f}\braket{f}{\Psi, +\infty}.
    \end{equation}
    We interpret
    \begin{equation}
        \abs{\braket{f}{\Psi, +\infty}}^2 = \probability(i \to f)
    \end{equation}
    as the probability that when we start in state \(\ket{i}\) our scattering process finishes in state \(\ket{f}\).
    
    Using the definition of \(S\) we can write the amplitude as
    \begin{equation}
        \braket{f}{\Psi, +\infty} = \bra{f} S \ket{\Psi, -\infty} = \bra{f} S \ket{i} \eqqcolon S_{fi},
    \end{equation}
    where \(S_{fi} \coloneqq \bra{f}S\ket{i}\) is a matrix element of the scattering operator.
    So the matrix elements of the scattering operator tell us the amplitude for a particular scattering process.
    
    We can expand the final state as
    \begin{equation}
        \ket{\Psi, +\infty} = \sum_f \ket{f} S_{fi},
    \end{equation}
    and take the conjugate to get
    \begin{equation}
        \bra{\Psi, +\infty} = \sum_f S_{if}^*\bra{f}.
    \end{equation}
    Combining this, and working with normalised states, we get
    \begin{equation}
        1 = \braket{\Psi, +\infty}{\Psi, +\infty} = \sum_{f, f'} S_{fi}S_{if'}^{*} \braket{f}{f'} = \sum_{f, f'} S_{fi}S_{if'}^*\delta_{ff'} = \sum_{f} S_{if}^*S_{fi}
    \end{equation}
    or in terms of operators,
    \begin{equation}
        \ident = S^\hermit S,
    \end{equation}
    so the scattering operator is unitary.
    
    We can interpret this as conservation of probability, since \(\{\ket{f}\}\) is a complete set of states we must have 
    \begin{equation}
        \sum_{f} \probability(i \to f) = 1,
    \end{equation}
    and hence if we have a definite start state that is also a complete set, so the evolution must be unitary.
    
    Note that the notion of the \(S\) matrix as discussed here is more general than in relativistic quantum mechanics since in QFT we can create and destroy particles, so our scattering can include processes like annihilation or pair production.
    
    \section{Interaction Picture}
    We now introduce the third, and final, picture of quantum mechanics, called the \defineindex{interaction picture}, or \define{Dirac picture}\index{Dirac picture|see{interaction picture}}.
    Suppose we have a Hamiltonian
    \begin{equation}
        H = H_0 + H_{\interaction}
    \end{equation}
    where \(H_0\) describes a free field and \(H_{\interaction}\) an interaction.
    
    In the Dirac picture both operators and states have time dependence.
    The states evolve as if they were in the Schrödinger picture, but under only the free Hamiltonian, so
    \begin{equation}
        \ket{\psi, t}_{\symrm{D}} \coloneqq \e^{iH_0t} \ket{\psi, t}_{\symrm{S}} = \e^{iH_0t} \e^{-iHt}\ket{\psi}_{\symrm{H}}
    \end{equation}
    where subscripts \(\symrm{D}\), \(\symrm{S}\), and \(\symrm{H}\) denote the Dirac, Schrödinger, and interaction pictures respectively.
    The first equality is the definition of a state in the Dirac picture, and the second comes from inverting the definition of a state in the Heisenberg picture:
    \begin{equation}
        \ket{\psi}_{\symrm{H}} \coloneqq \e^{iHt}\ket{\psi, t}_{\symrm{S}}.
    \end{equation}
    
    Operators in the Dirac picture transform as if they were in the Heisenberg picture, but only under the free Hamiltonian, so
    \begin{equation}
        A_{\symrm{D}}(t) = \e^{iH_0t} A_{\symrm{S}} \e^{-iH_0t}.
    \end{equation}
    
    Take the derivative of the equation defining a state in the Dirac picture, this gives
    \begin{equation}
        i\diffp{}{t} \ket{\psi, t}_{\symrm{D}} = -H_0\e^{iH_0t}\ket{\psi, t}_{\symrm{S}} + \e^{iH_0t} i\diffp{}{t}\ket{\psi, t}_{\symrm{S}}.
    \end{equation}
    Now recognise the final term as \(\e^{iH_0t}\) times one side of the Schrödinger equation:
    \begin{equation}
        i\diffp{}{t} \ket{\psi, t}_{\symrm{S}} = H\ket{\psi, t}_{\symrm{S}} = H_0 \ket{\psi, t}_{\symrm{S}} + H_{\interaction} \ket{\psi, t}_{\symrm{S}}.
    \end{equation}
    Substituting this into the previous equation the first term cancels leaving us with
    \begin{equation}
        i\diffp{}{t} \ket{\psi, t}_{\symrm{D}} = \e^{iH_0t} H_{\interaction} \ket{\psi, t}_{\symrm{S}} = \e^{iH_0t} H_{\interaction} \e^{-iH_0t}\ket{\psi, t}_{\symrm{D}} = H_{\interaction}^{\symrm{D}}\ket{\psi, t}_{\symrm{D}}.
    \end{equation}
    This shows that states evolve in time according to the free Hamiltonian, \(H_0\).
    The time evolution due to the interaction is exhibited by the operators.
    
    We derived the Heisenberg equation by differentiating an arbitrary operator in both pictures.
    Doing the same here, between the Dirac and Schrödinger picture gives rise to the equation of motion
    \begin{equation}
        \diffp{}{t} A_{\symrm{D}}(t) = i\commutator{H_0}{A_{\symrm{D}}(t)}.
    \end{equation}
    Note that \(H_0^{\symrm{S}} = H_0^{\symrm{D}} = H_0^{\symrm{H}}\), but in general \(H_{\interaction}^{\symrm{D}} \ne H_{\interaction}^{\symrm{S}}\), which comes from the general fact that \(\commutator{H_0}{H_{\interaction}} \ne 0\).
    
    From now on we will work entirely in the Dirac picture.
    Accordingly we will drop the labels \(\symrm{D}\), \(\symrm{S}\), and \(\symrm{H}\).
    
    \section{Dyson Series}
    The \defineindex{Dyson series} is a formal solution to the evolution equation
    \begin{equation}
        i\diffp{}{t}\ket{\Psi, t} = H_{\interaction}(t) \ket{\Psi, t}.
    \end{equation}
    Now we make the time dependence of the interaction Hamiltonian explicit.
    The solution to this is
    \begin{equation}\label{eqn:to iterate for dyson series}
        \ket{\Psi, t} = \ket{\Psi, -\infty} - i \int_{-\infty}^{t} \dl{t_1} \, H_{\interaction}(t_1) \ket{\Psi, t_1}.
    \end{equation}
    To see this just take the derivative:
    \begin{equation}
        i\diffp{}{t}\ket{\Psi, t} = i\diffp{}{t}\ket{\Psi, -\infty} + \diffp{}{t}\int_{-\infty}^{t} \dl{t_1} \, H_{\interaction}(t_1)\ket{\Psi, t_1} = H_{\interaction}(t) \ket{\Psi, t}.
    \end{equation}
    Here the first term vanishes as it is constant and the second term is easily computed by an application of the fundamental theorem of calculus.
    
    The problem is that this solution for \(\ket{\Psi, t}\) has \(\ket{\Psi, t_1}\) in it, in particular we integrate up to \(t_1 = t\).
    This means that we need to know the state at time \(t\) in order to evaluate the state at time \(t\).
    This seems like a problem.
    The solution is that we are assuming weak interactions.
    This means that the operator \(H_{\interaction}(t)\) is, in some sense, \enquote{small}, and the operator \(H_{\interaction}(t)H_{\interaction}(t_1)\) is \enquote{smaller}.
    Of course, operators don't have sizes in this way, but we can think about specific matrix elements and it all works out.
    This suggests that we can solve this problem recursively with each level of recursion being a smaller and smaller correction.
    The first level of recursion is to substitute \(\ket{\Psi, t_1}\) with the entire right hand side of \cref{eqn:to iterate for dyson series}, being careful to avoid reusing integration variables, giving
    \begin{equation}
        \ket{\Psi, t} = \ket{\Psi, -\infty} - i \int_{-\infty}^{t} \dl{t_1} \, H_{\interaction}(t_1) \left[ \ket{\Psi, -\infty} - i\int_{-\infty}^{t_1} \dl{t_2} \, H_{\interaction}(t_2) \ket{\Psi, t_2} \right].
    \end{equation}
    Note that the second integral is quadratic in the Hamiltonian, so is \enquote{smaller} than the rest of the terms.
    We can repeat this process, writing \(\ket{i} = \ket{\Psi, -\infty}\) for compactness, and we get
    \begin{equation*}
        \ket{\Psi, t} = \ket{i} - i \int_{-\infty}^{t} \dl{t_1} \, H_{\interaction}(t_1) \left[ \ket{i} - i \int_{-\infty}^{t_1} \dl{t_2} \, H_{\interaction}(t_2) \left\{ \ket{i} - i \int_{-\infty}^{t_2} \dl{t_3} \, H_{\interaction}(t) \ket{\Psi, t_3} \right\} \right].
    \end{equation*}
    
    Continuing on like this we have a series
    \begin{multline}
        \ket{\Psi, t} = \ket{i} + (-i) \int_{-\infty}^{t} \!\! \dl{t_1} \, H_{\interaction}(t_1) \ket{i} + (-i)^2 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} H_{\interaction}(t_1) H_{\interaction}(t_2) \ket{i}\\
        + (-i)^3 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} \int_{-\infty}^{t_2} \!\! \dl{t_3} \, H_{\interaction}(t_1) H_{\interaction}(t_2) H_{\interaction}(t_3) \ket{i} + \dotsb.
    \end{multline}
    Each term in this series is \enquote{smaller} than the previous term and so we can truncate at some point once we've achieved the required level of accuracy.
    
    Taking \(t\) to infinity we get
    \begin{multline}
        \ket{\Psi, +\infty} = \ket{i} + (-i) \int_{-\infty}^{\infty} \!\! \dl{t_1} \, H_{\interaction}(t_1) \ket{i} + (-i)^2 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} H_{\interaction}(t_1) H_{\interaction}(t_2) \ket{i}\\
        + (-i)^3 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} \int_{-\infty}^{t_2} \!\! \dl{t_3} \, H_{\interaction}(t_1) H_{\interaction}(t_2) H_{\interaction}(t_3) \ket{i} + \dotsb.
    \end{multline}
    Recognising that this has the form of an operator, albeit an infinite series of integrals, acting on the initial state to give the final state we can identify this operator as the \(S\) matrix:
    \begin{multline}
        S = \ident + (-i) \int_{-\infty}^{\infty} \!\! \dl{t_1} \, H_{\interaction}(t_1) + (-i)^2 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} H_{\interaction}(t_1) H_{\interaction}(t_2)\\
        + (-i)^3 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} \int_{-\infty}^{t_2} \!\! \dl{t_3} \, H_{\interaction}(t_1) H_{\interaction}(t_2) H_{\interaction}(t_3) + \dotsb.
    \end{multline}
    Or, more compactly,
    \begin{equation}
        S = \sum_{n = 0}^{\infty} (-i)^n \int_{-\infty}^{\infty} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} \dotsm \int_{-\infty}^{t_{n-1}} \!\! \dl{t_n} \, H_{\interaction}(t_1) H_{\interaction}(t_2) \dotsm H_{\interaction}(t_n).
    \end{equation}
    Notice that the times, \(t_1\), \(t_2\), and so on appear in increasing order, that is in the integral \(t_1 < t_2 < \dotsb < t_n\), we can neglect the endpoints of the integration ranges where we may have equality since individual points don't contribute to an integral.
    
    To proceed we make use of the following identity:
    \begin{multline}
        \int_{t_a}^{t_b} \!\! \dl{t_1} \int_{t_a}^{t_1} \!\! \dl{t_2} \int_{t_a}^{t_2} \dotsm \int_{t_a}^{t_{n-1}} \!\! \dl{t_n} \, A(t_1)A(t_2) \dotsb A(t_n)\\
        = \frac{1}{n!} \int_{t_a}^{t_b} \!\! \dl{t_1} \int_{t_a}^{t_b} \dl{t_2} \dotsm \int_{t_a}^{t_b} \!\! \dl{t_n} \timeOrdering[A(t_1)A(t_2) \dotsm A(t_n)].
    \end{multline}
    Here \(t_a\) and \(t_b\) are arbitrary initial and final times with \(t_a < t_b\), and \(A\) is an arbitrary operator.
    The operator \(\timeOrdering\) is called the \defineindex{time ordering}.
    We take the arguments, which are operators, and order according to evaluation time in order of increasing time from left to right, so that the first operator to act on some state is the operator evaluated at the earliest time.
    That is, for two operators
    \begin{align}
        \timeOrdering[A(t_1), B(t_2)] &\coloneqq
        \begin{cases}
            A(t_1)B(t_2) & t_1 > t_2\\
            B(t_2)A(t_1) & t_2 > t_1
        \end{cases}
        \\
        &\hphantom{:}= \heaviside(t_1 - t_2) A(t_1)B(t_2) + \heaviside(t_2 - t_1) B(t_2)A(t_1).
    \end{align}
    This can then be extended to \(n\) operators in the obvious way, although it becomes harder to write in terms of Heaviside step functions.
    
    We'll show how this works for the \(n = 2\) case, and then the general case follows by induction.
    Start with the right hand side of the identity.
    For \(n = 2\) we have
    \begin{equation}
        I = \int_{t_a}^{t_b} \dl{t_1} \int_{t_a}^{t_b} \dl{t_2} \, \timeOrdering[A(t_1)A(t_2)].
    \end{equation}
    We can split the integral over \(t_2\) into two parts, one running from \(t_a\) to \(t_1\), and the other from \(t_1\) to \(t_b\):
    \begin{equation}
        I = \int_{t_a}^{t_b} \dl{t_1} \left[ \int_{t_a}^{t_1} \dl{t_2} \, \timeOrdering[A(t_1)A(t_2)] + \int_{t_1}^{t_b} \dl{t_2} \, \timeOrdering[A(t_1) A(t_2)] \right].
    \end{equation}
    Consider the second term,
    \begin{equation}
        \int_{t_a}^{t_b} \dl{t_1} \int_{t_2}^{t_b} \dl{t_2} \, \timeOrdering[A(t_1) A(t_2)].
    \end{equation}
    This is a double integral.
    This corresponds to integrating over the triangle in the plane bounded by \(t_1 = t_2\), \(t_1 = t_a\), and \(t_2 = t_b\).
    We can integrate over the same region in a different way.
    Instead integrate along \(t_2\) from \(t_a\) to \(t_1\) and then along \(t_1\) from \(t_a\) to \(t_b\).
    This corresponds to the integral
    \begin{equation}
        \int_{t_a}^{t_b} \dl{t_2} \int_{t_a}^{t_2} \dl{t_1} \, \timeOrdering[A(t_1)A(t_2)].
    \end{equation}
    Now, \(t_1\) and \(t_2\) are just integration variables, and the integrand is symmetric in \(t_1\) and \(t_2\), as \(\timeOrdering[A(t_1)A(t_2)] = \timeOrdering[A(t_2)A(t_1)]\), the ordering of operators at different times within a time ordering is not important, since we're going to time order them.
    We are then free to rename \(t_1\) and \(t_2\), and in particular swap them without changing anything else,
    \begin{equation}
        \int_{t_a}^{t_b} \dl{t_1} \int_{t_a}^{t_1} \dl{t_2} \, \timeOrdering[A(t_1) A(t_2)].
    \end{equation}
    Combing this back with the first term we get
    \begin{equation}
        I = 2 \int_{t_a}^{t_b} \dl{t_1} \int_{t_a}^{t_1} \dl{t_2} \, \timeOrdering[A(t_1)A(t_2)].
    \end{equation}
    Noticing with these integration ranges the operators are already time ordered, so we can drop the explicit time ordering, and that \(2 = 2!\) and dividing through by 2 we have proven the identity for \(n = 2\).
    
    Now, suppose that the identity holds for \(k\) integrals, and we have \(k + 1\) integrals.
    We can use the induction hypothesis to swap \(k\) integrals, and then we pairwise swap integrals as above before the unswapped integral has also been swapped.
    
    \begin{figure}
        \tikzsetnextfilename{region-of-integration-for-identity}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 4) node[above] {\(t_2\)} -- (0, 0) -- (4, 0) node[right] {\(t_1\)};
            \draw[highlight, very thick] (0, 0) -- (4, 4);
            \coordinate (t1=t2) at (3.53, 3.8);
            \node[rotate around={45:(t1=t2)}] at (t1=t2) {\(t_1 = t_2\)};
            \node[below] at (1, 0) {\(t_a\)};
            \node[left] at (0, 1) {\(t_a\)};
            \node[below] at (3, 0) {\(t_b\)};
            \node[left] at (0, 3) {\(t_b\)};
            \draw[highlight] (1, 0) -- ++ (0, 1);
            \draw[highlight] (3, 0) -- ++ (0, 3);
            \draw[highlight] (0, 1) -- ++ (1, 0);
            \draw[highlight] (0, 3) -- ++ (3, 0);
            \draw[very thick, highlight, fill=highlight!50] (1, 1) -- (3, 3) -- (1, 3) -- cycle;
        \end{tikzpicture}
        \caption{The region of integration for the \(n = 2\) case of the identity.}
    \end{figure}
    
    We can use this identity to write
    \begin{equation}
        S = \sum_{n = 0}^{\infty} \frac{(-i)^n}{n!} \int_{-\infty}^{\infty} \!\! \dl{t_1} \int_{-\infty}^{\infty} \!\! \dl{t_2} \dotsm \int_{-\infty}^{\infty} \!\! \dl{t_n} \, \timeOrdering[H_{\interaction}(t_1) H_{\interaction}(t_2) \dotsm H_{\interaction}(t_n)].
    \end{equation}
    We can now replace \(H_{\interaction}\) with an integral over the Hamiltonian density giving
    \begin{equation}
        S = \sum_{n = 0}^{\infty} \frac{(-i)^n}{n!} \int \!\! \dl{x_1} \int \!\! \dl{x_2} \dotsm \int \!\! \dl{x_n} \, \timeOrdering[\hamiltonianDensity_{\interaction}(x_1) \hamiltonianDensity_{\interaction}(x_2) \dotsm \hamiltonianDensity_{\interaction}(x_n)].
    \end{equation}
    This is called the \defineindex{Dyson series}.
    
    Note that the interaction Hamiltonian density is given by \(\hamiltonianDensity_{\interaction} = -\lagrangianDensity_{\interaction}\).
    This follows directly from the definition
    \begin{equation}
        \hamiltonianDensity = \hamiltonianDensity_0 + \hamiltonianDensity_{\interaction} = \pi \dot{\varphi} - \lagrangianDensity = \pi \dot{\varphi} - \lagrangianDensity_0 - \lagrangianDensity_{\interaction}
    \end{equation}
    where the \(\pi\dot{\varphi} - \lagrangianDensity_0\) term is the unperturbed Hamiltonian density.
    Using this we can write
    \begin{align}
        S &= \sum_{n = 0}^{\infty} \frac{i^n}{n!} \int \!\! \dl{x_1} \int \!\! \dl{x_2} \dotsm \int \!\! \dl{x_n} \, \timeOrdering[\lagrangianDensity{\interaction}(x_1) \lagrangianDensity{\interaction}(x_2) \dotsm \lagrangianDensity{\interaction}(x_n)]\\
        &= \exp\left[ i\int \dl{^4x} \lagrangianDensity_{\interaction} \right].
    \end{align}
    So the \(S\) matrix can be viewed as the phase factor given by the interaction action.
    This last step is simply a formal rewriting recognising the exponential series.
    When expanding make sure to have each integral in each power be over a different variable, and to time order the integrand.
    From this form we can see that, assuming the action is Lorentz invariant, then so is the \(S\) matrix.
    This works because time ordering is Lorentz invariant, because it can be done using only terms like \(\heaviside(t - t')\), and each such term is invariant under proper orthochronous Lorentz transformations.
    
    \section{Interpretation}
    For a given initial state, \(\ket{i}\), and final state, \(\ket{f}\), we need to find terms in the interaction Lagrangian, \(\lagrangianDensity_{\interaction}\) which give a nonzero transition amplitude for \(\ket{i}\) to \(\ket{f}\).
    The entire effect of the interaction is in \(\lagrangianDensity_{\interaction}\), which is an operator given by a polynomial in the fields.
    This means that really it's just a collection of creation and annihilation operators.
    Therefore the entire effect of the interaction is to create and destroy particles.
    We then look for terms with
    \begin{itemize}
        \item annihilation operators destroying the initial state,
        \item creation operators creating the final state,
        \item paired creation/annihilation operators creating and then destroying intermediate states.
    \end{itemize}

    In terms of particles, we look for terms which destroy the incoming particles, potentially create and then destroy one or more new particles, then create some particles which form the final state.
    The initial and final particles must be on-shell for us to observe them.
    The intermediate particles however needn't be on-shell, since we cannot observe them.
    These are called \define{virtual particles}\index{virtual particle}.
    Don't put too much stock in the existence and off-shell nature of these virtual particles, they're really just a mathematical construct with different combinations of intermediate particles corresponding to different terms in the Dyson series.
    
    In the interaction picture the fields satisfy the free field equation, and therefore have the usual creation and annihilation operators.
    We can therefore use normal ordering, placing all creation operators on the left and all annihilation operators on the right, in order to ensure that incoming particles are destroyed and then new particles are created, rather than the other way round.
    
    \section{\texorpdfstring{\(\varphi^3\)}{Phi Cubed} Interactions}
    In \(\varphi^3\) theory the interaction Hamiltonian is
    \begin{equation}
        \hamiltonianDensity_{\interaction} = -\lagrangianDensity_{\interaction} = \frac{1}{6}g\normalordering{\varphi^3},
    \end{equation}
    where we now normal order the \(\varphi^3\) factor in order to destroy existing particles and then create new particles.
    
    We will consider 2--2 scattering, that is two particles come in, interact in some way, and two particles leave.
    This can be represented by a diagram like
    \vspace{2.4cm}
    \begin{equation}
        \smash{\rotatebox{45}{
            \tikzsetnextfilename{fd-2-2-phi-cubed-scattering-general-diagram}
            \feynmandiagram[inline=(v)]{
                {a, b, c, d} -- v [blob]
            };
        }}
    \end{equation}
    Here the blob represents some unknown black box type interaction.
    Time flows, in our convention, from left to right, so the left two legs represent the incoming particles, and the right two the outgoing particles.
    In order to destroy the incoming particles we start with a \(\varphi^+\varphi^+\) term.
    Then to create two new particles we need a \(\varphi^-\varphi^-\) term.
    Recall that we order operators from right to left, as we want the \(\varphi^+\varphi^+\) term to act on the initial before the \(\varphi^-\varphi^-\) term.
    We could therefore look for a
    \begin{equation}
        \varphi^-\varphi^-\varphi^+\varphi^+
    \end{equation}
    term in the series.
    However, we won't find such a term since in \(\varphi^3\) theory we can only have terms with a multiple of 3 factors of \(\varphi^{\pm}\).
    This means that there are no direct 2--2 scattering events in \(\varphi^3\) theory, such events must always go via at least one intermediate virtual particle state.
    So, next we look at the term in the series which is quadratic in the Hamiltonian, and hence has six factors of \(\varphi^{\pm}\).
    There are two possible orderings of terms with the required starting point of \(\varphi^+\varphi^+\) to annihilate the initial state and end point of \(\varphi^-\varphi^-\) to create the final state.
    They are
    \begin{equation}
        \varphi^-\varphi^- \varphi^- \varphi^+ \varphi^+ \varphi^+, \qqand \varphi^- \varphi^- \varphi^+ \varphi^- \varphi^+ \varphi^+.
    \end{equation}
    When acting on the initial state the the first vanishes, since after annihilating the initial state we are left with the vacuum state, which we then act on with the annihilator \(\varphi^+\).
    So, the only nonzero second term quadratic in the Hamiltonian is
    \begin{equation}
        \varphi^-\varphi^-\varphi^+\varphi^-\varphi^+\varphi^+.
    \end{equation}
    We interpret this as annihilating the initial state, creating a new particle, annihilating it, and then destroying it again.
    The amplitude for this particular term is proportional to \(g^2\), since each Hamiltonian introduces a factor of \(g\).
    Further, by assuming that energy is conserved and everything is local when we annihilate the two initial particles we must immediately create the virtual particle, and all of this must happen at the same position, so this is a single spacetime event, \(x_2\).
    Then when the virtual particle is annihilated we must immediately, and at the same location, create the new particles, so this is a second spacetime event, \(x_1\).
    These correspond to the two points at which the interaction Hamiltonians are evaluated
    so we'll have a nonzero term
    \begin{equation}
        -\frac{g^2}{36}\int_{-\infty}^{\infty} \dl^4{x_1} \int_{-\infty}^{\infty} \dl{^4x_2} \, \varphi^-(x_2)\varphi^-(x_2)\varphi^+(x_1)\varphi^-(x_2)\varphi^+(x_2)\varphi^+(x_2).
    \end{equation}
    Pictorially we can represent this as
    \begin{equation}
        \tikzsetnextfilename{fd-2-2-phi-cubed-single-virtual-particle}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[small, horizontal=v1 to v2] {
                    {i1, i2} -- v1 -- v2 -- {o1, o2}
                };
                \node[left] at (v1) {\(g\)};
                \node[right] at (v2) {\(g\)};
                \node[above, xshift=0.05cm] at (v1) {\(x_2\)};
                \node[above, xshift=-0.05cm] at (v2) {\(x_1\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Here the left and right legs represent the incoming and outgoing particles, and the single line in the middle is the virtual particle.
    
    Next, we would consider terms cubic in the Hamiltonian.
    However, things quickly get out of hand if we proceed as we did here.
    The cubic term will have nine factors of \(\varphi\).
    So, we want a way to do this all more systematically.
    This is the theory we will develop in the next chapter.
    
    \chapter{Wick's Theorem}
    To proceed we need to be able to deal with time orderings of operators.
    Our interpretation, of particles being created and destroyed, works best if we normal order things, placing all creation operators on the left and annihilation operators on the right.
    So, we look for a way to rewrite a time ordering in terms of normal orderings, and other quantities which can be calculated more easily.
    We proceed with the case of two operators, then we introduce a general theorem central to much of quantum field theory.
    
    \section{Two Operator Case}
    Suppose we have two operator valued fields, \(A\) and \(B\), which we can write as
    \begin{equation}
        A = A^+ + A^-, \qqand B = B^+ + B^-,
    \end{equation}
    where \(A^+\) and \(B^+\) are expressions in the annihilation operators and \(A^-\) and \(B^-\) are expressions in the creation operators.
    Consider the product \(\normalordering{A(x)B(x')}\).
    Notice that the normal ordering of a sum is the sum of the normal ordering of the individual terms, that is \(\normalordering{C + D} = \normalordering{C} + \normalordering{D}\).
    Expanding in terms of \(A^{\pm}\) and \(B^{\pm}\) we have
    \begin{align*}
        \normalordering{A(x)B(x')} &= \normalordering{[A^+(x) + A^-(x')][B^+(x') + B^-(x')]}\\
        &= \normalordering{A^+(x)B^+(x') + A^+(x)B^-(x') + A^-(x)B^+(x') + A^-(x)B^-(x')}\\
        &= A^+(x)B^+(x') + B^-(x')A^+(x) + A^-(x)B^+(x') + A^-(x)B^-(x').
    \end{align*}
    
    Now, consider what happens when we subtract this result from \(A(x)B(x')\) without the normal ordering, that is
    \begin{equation}
        A(x)B(x') = A^+(x)B^+(x') + A^+(x)B^-(x') + A^-(x)B^+(x') + A^-(x)B^-(x').
    \end{equation}
    The only term that doesn't cancel is the second term where we have to swap the ordering, so
    \begin{equation}
        A(x)B(x') - \normalordering{A(x)B(x')} = A^+(x)B^-(x') - B^-(x')A^+(x) = \commutator{A^+(x)}{B^-(x')}.
    \end{equation}
    
    The commutator of creation and annihilation operators is just a number, or more accurately a Dirac delta, and so it doesn't act on states, in particular
    \begin{equation}
        \bra{0} \commutator{A^+(x)}{B^-(x')}\ket{0} = \braket{0}{0} \commutator{A^+(x)}{B^-(x')} = \commutator{A^+(x)}{B^-(x')},
    \end{equation}
    since we choose to normalise the vacuum state.
    We also have
    \begin{equation}
        \bra{0} \commutator{A^+(x)}{B^-(x')}\ket{0} = \bra{0} A^+(x)B^-(x')\ket{0} - \bra{0} B^-(x')A^+(x)\ket{0} = \bra{0} A^+(x)B^-(x')\ket{0},
    \end{equation}
    where the second term vanishes since the annihilation operator \(A^+(x)\) acts on the vacuum.
    
    We then have
    \begin{equation}\label{eqn:almost wicks theorem for two operators}
        A(x)B(x') = \normalordering{A(x)B(x')} + \bra{0} A(x) B(x') \ket{0}.
    \end{equation}
    We're nearly there, recall that the definition of time ordering is
    \begin{equation}
        \timeOrdering[A(x)B(x')] = \heaviside(t - t')A(x)B(x') + \heaviside(t' - t)B(x')A(x),
    \end{equation}
    where \(t\) is the time component of \(x\) and \(t'\) is the time component of \(x'\).
    
    At this point we make the observation that it doesn't matter what order we write operators in if we are normal ordering them, since we're going to change the order anyway, so
    \begin{equation}
        \normalordering{A(x)B(x')} = \normalordering{B(x')A(x)}.
    \end{equation}
    Hence, if we normal order the time ordering we have
    \begin{align}
        \normalordering{\timeOrdering[A(x)B(x')]} &= \heaviside(t - t')\normalordering{A(x)B(x')} + \heaviside(t' - t)\normalordering{B(x')A(x)}\\
        &= \heaviside(t - t')\normalordering{A(x)B(x')} + \heaviside(t' - t)\normalordering{A(x)B(x')}\\
        &= \normalordering{A(x)B(x')}.
    \end{align}
    So, the normal ordering of a time ordering is just the normal ordering.
    
    Consider \cref{eqn:almost wicks theorem for two operators}.
    This holds for any product of two operators, in particular if we time order the operators it still works, since we can just separate into the \(t < t'\) and \(t > t'\) cases and then proceed without the time ordering.
    Hence,
    \begin{equation}
        \timeOrdering[A(x)B(x')] = \normalordering{A(x)B(x')} + \bra{0} \timeOrdering[A(x)B(x')] \ket{0}.
    \end{equation}
    Here we use \(\normalordering{\timeOrdering[A(x)B(x')]} = \normalordering{A(x)B(x')}\) to drop the time ordering in the first term on the right.
    
    This equation, and others like it, are very important in quantum field theory, so important that we introduce a notation to simplify it.
    We define the \define{Wick contraction}\index{Wick contraction!of scalars} of two operators, \(A\) and \(B\) to be
    \begin{equation}
        \wick{\c A(x) \c B(x')} \coloneqq \bra{0} \timeOrdering[A(x)B(x')] \ket{0}.
    \end{equation}
    Note that this is just a number, not an operator.
    This allows us to state Wick's theorem for two operators.
    
    \begin{lma}{}{lma:wick's theorem for two operators}
        For two operators, \(A\) and \(B\), such that \(A = A^+ + A^-\) and \(B = B^+ + B^-\) where \(A^+\) and \(B^+\) are expressions in the annihilation operator and \(A^-\) and \(B^-\) are expressions in the creation operators we have
        \begin{equation}
            \timeOrdering[A(x)B(x')] = \normalordering{A(x)B(x')} + \wick{\c A(x) \c B(x')}.
        \end{equation}
        \begin{proof}
            See above work.
        \end{proof}
    \end{lma}
    
    \section{Three Operator Case}
    The proof of Wick's theorem, the generalisation of the previous lemma to an arbitrary number of operators, proceeds inductively.
    It will be simpler to prove if we first prove the three operator case to see how the inductive logic works in detail.
    As before our goal is to express a time ordered product of operators in terms of their normal ordering and contractions.
    We now have an extra operator, \(C\), which we again assume can be written as \(C = C^+ + C^-\) where \(C^+\) is an expression in the annihilation operators and \(C^-\) an expression in the creation operators.
    
    We want to evaluate \(\timeOrdering[A(x_1)B(x_2)C(x_2)]\) where \(x_i\) has time component \(t_i\).
    For simplicity, and without loss of generality, we assume that \(t_1 > t_2 > t_3\).
    If this isn't the case we can reorder freely within the time ordering and then relabel our operators so that it is.
    We don't consider here the case where the operators are evaluated at the same time, we'll do this later.
    From now on we suppress the arguments to our operators, but be aware that they are not all evaluated at the same point.
    
    Since, by assumption, \(A\) is the first operator evaluated we can move it outside of the time ordering, since it will always appear on the left of any time ordered product of operators:
    \begin{equation}
        \timeOrdering[ABC] = A\timeOrdering[BC].
    \end{equation}
    Now we can apply \cref{lma:wick's theorem for two operators} to the time ordering of \(B\) and \(C\), giving
    \begin{equation}
        \timeOrdering[ABC] = A\normalordering{BC} + A\wick{\c B \c C}.
    \end{equation}
    Now express \(A\) as \(A^+ + A^-\), so
    \begin{align}
        \timeOrdering[ABC] &= (A^+ + A^-)\normalordering{BC} + \wick{\c B \c C}\\
        &= A^+ \normalordering{BC} + A^+\wick{\c B \c C} + A^- \normalordering{BC} + A^- \wick{\c B \c C}.
    \end{align}
    Notice that since \(A^-\) is a creation operator and is on the left of \(\normalordering{BC}\) we can move it into the normal ordering without changing anything, so
    \begin{equation}
        \timeOrdering[ABC] = A^+ \normalordering{BC} + A^+\wick{\c B \c C} + \normalordering{A^-BC} + A^- \wick{\c B \c C}.
    \end{equation}
    
    Things aren't quite so simple for \(A^+\normalordering{BC}\).
    In order to bring \(A^+\), which is an annihilation operator, into the normal ordering we need it to be on the right.
    We can achieve this at the cost of a commutator:
    \begin{equation}
        A^+ \normalordering{BC} = \normalordering{BC}A^+ + \commutator{A^+}{\normalordering{BC}} = \normalordering{BCA^+} + \commutator{A^+}{\normalordering{BC}}.
    \end{equation}
    Now that \(A^+\) has been brought into the normal ordering we can move it around within the ordering, so
    \begin{equation}
        A^+ \normalordering{BC} = \normalordering{A^+BC} + \commutator{A^+}{\normalordering{BC}}.
    \end{equation}
    The commutator can be evaluated with the identity
    \begin{equation}\label{eqn:commutator identity x and yz}
        \commutator{X}{YZ} = XYZ - YZX = XYZ - YXZ + YXZ - YZX = \commutator{X}{Y}Z + Y\commutator{X}{Z}.
    \end{equation}
    Since the commutator is just a number the normal ordering doesn't effect it, and so we have
    \begin{equation}
        A^+ \normalordering{BC} = \normalordering{A^+BC} + \normalordering{\commutator{A^+}{B^-}C} + \normalordering{B\commutator{A^+}{C^-}}.
    \end{equation}
    We've also used here that \(\commutator{X^+}{Y} = \commutator{X^+}{Y^-}\), since \(X^+\) commutes with \(Y^+\), as both are just formed from annihilation operators.
    
    Exactly as in the two operator case we can replace the commutator with its vacuum expectation value, and then drop the commutator, these are just numbers, so can be brought out of the normal ordering.
    \begin{align}
        A^+ \normalordering{BC} &= \normalordering{A^+BC} + \commutator{A^+}{B^-}\normalordering{C} + \commutator{A^+}{C^-}\normalordering{B}\\
        &= \normalordering{A^+BC} + \bra{0}\commutator{A^+}{B^-}\ket{0}\normalordering{C} + \bra{0}\commutator{A^+}{C^-}\ket{0}\normalordering{B}\\
        &= \normalordering{A^+BC} + \bra{0}A^+B^-\ket{0}\normalordering{C} + \bra{0}A^+C^-\ket{0}\normalordering{B}.
    \end{align}
    Notice that since we're normal ordering a single term which decomposes as the sum of creation and annihilation operators we don't really need the normal orderings any more, but we keep them so our result fits the general result.
    
    Going back to our starting point we now have
    \begin{align}
        \timeOrdering[ABC] &= A^+ \normalordering{BC} + A^+\wick{\c B \c C} + \normalordering{A^-BC} + A^- \wick{\c B \c C}\\
        &= \normalordering{ABC} + \bra{0}\timeOrdering[A^+B^-]\ket{0}\normalordering{C} + \bra{0}\timeOrdering[A^+C^-]\ket{0}\normalordering{B} + A\wick{\c B \c C} \notag\\
        &= \normalordering{ABC} + \wick{\c A \c B}\normalordering{C} + \wick{\c A \c C}\normalordering{B}.
    \end{align}
    Here we use
    \begin{equation}
        A^+\wick{\c B \c C} + A^- \wick{\c B \c C} = (A^+ + A^-) \wick{\c B \c C} = A\wick{\c B \c C},
    \end{equation}
    and
    \begin{equation}
        \normalordering{A^+ BC} + \normalordering{A^- BC} = \normalordering{(A^+ + A^-)BC} = \normalordering{ABC}
    \end{equation}
    
    The result here is that the time ordering is given by the normal ordering plus all pairs of contractions, normal ordering anything that isn't contracted.
    
    \section{General Theorem}
    We can now prove the general theorem relating time orderings of an arbitrary number of operators and their normal orderings and contractions.
    
    \begin{thm}{Wick's Theorem}{thm:wick}\index{Wick's theorem}
        Given operators, \(A, B, C, D, \dotsc\), which can all be written as a sum of an annihilator and creation operators.
        Suppose \(A\) is evaluated at \(x_1\), \(B\) at \(x_2\), \(C\) at \(x_3\), \(D\) at \(x_4\), and so on, and \(t_i\) is the time component of \(x_i\).
        Then if none of the \(t_i\) are equal we have
        \begin{align}
            \timeOrdering[ABCD\dotsm] &= \normalordering{ABCD\dotsm}\\
            &+ \wick{\c A \c B}\normalordering{CD\dotsm} + \wick{\c A \c C} \normalordering{BD\dotsm} + \wick{\c A \c D} \normalordering{BC} + \dotsb\\
            &+ \wick{\c B \c C} \normalordering{AD \dotsm } + \wick{\c B \c D} \normalordering{AC\dotsm} + \dotsb\\
            &\vdotswithin{+}\\
            &+ \wick{\c A \c B} \wick{\c C \c D} \normalordering{ \dotsm } + \wick{\c A \c C} \wick{\c B \c D} \normalordering{ \dotsm } + \wick{\c A \c D} \wick{\c B \c C} \normalordering{ \dotsm } + \dotsb\\
            &+ \text{all other possible contractions}.
        \end{align}
        That is, the time ordering is given by the normal ordering, plus the normal ordering contracting each pair of operators, plus the normal ordering contracting every possible pair of pairs of operators, and so on, doing every single contraction possible and normal ordering anything not contracted.
        
        \begin{proof}
            We proceed by strong induction on the number of operators.
            The one operator case is trivial and acts as the basis case, equivalently the two operator case can be the basis case and we then use \cref{lma:wick's theorem for two operators} as the proof of our basis case.
            
            Now, suppose that The hypothesis holds for \(k - 1\) operators for some positive integer \(k\).
            Take \(k\) operators \(A, B, C, D \dotsb\), evaluated at \(x_1, x_2, x_3, x_4, \dotsc\) respectively, with \(t_i\) the time component of \(x_i\).
            Without loss of generality we assume \(t_1 > t_2 > \dotsb > t_k\), if this isn't the case we can reorder within the time ordering and relabel such that it is the case.
            
            We can pull \(A\) outside of the time ordering, since it is evaluated last:
            \begin{equation}
                \timeOrdering[ABCD\dotsm] = A\timeOrdering[BCD\dotsm].
            \end{equation}
            Writing \(A = A^+ + A^-\) where \(A^+\) and \(A^-\) are expressions in the annihilation and creation operators respectively we have
            \begin{equation}
                \timeOrdering[ABCD\dotsm] = (A^+ + A^-)\timeOrdering[BCD\dotsm].
            \end{equation}
            The time ordering on the right has \(k - 1\) operators, so the induction hypothesis allows us to write this as
            \begin{align}
                \timeOrdering[ABCD\dotsm] &= (A^+ + A^-)(\normalordering{BCD\dotsm} + \wick{\c B \c C}\normalordering{B \dotsm}\\
                &\qquad+ \text{all contractions not involving \(A\)}).
            \end{align}
            We can trivially put \(A^-\) into all of the normal orderings, since it is a creation operator and is on the left.
            Therefore consider the \(A^+\) terms.
            We have
            \begin{align}
                A^+ \normalordering{BCD\dotsm} &= \normalordering{BCD\dotsm}A^+ + \commutator{A^+}{\normalordering{BCD\dotsm}}\\
                &= \normalordering{BCD\dotsm A^+} + \commutator{A^+}{\normalordering{BCD\dotsm}}\\
                &= \normalordering{A^+BCD\dotsm} + \commutator{A^+}{\normalordering{BCD\dotsm}},
            \end{align}
            Iteratively applying the identity in \cref{eqn:commutator identity x and yz} we have
            \begin{multline}
                A^+\normalordering{BCD\dotsm} = \normalordering{A^+BCD\dotsm} + \commutator{A^+}{B^-}\normalordering{CD\dotsm}\\
                + \commutator{A^+}{C^-} \normalordering{BD\dotsm} + \commutator{A^+}{D^-}\normalordering{BC\dotsm} + \dotsb.
            \end{multline}
            As with the two and three operator cases we can replace the commutators with contractions, giving
            \begin{multline}
                A^+\normalordering{BCD\dotsm} = \normalordering{A^+BCD} + \wick{\c A \c B}\normalordering{CD\dotsm}\\
                + \wick{\c A \c C}\normalordering{BD\dotsm} +  \wick{\c A \c D}\normalordering{BC\dotsm} + \dotsb.
            \end{multline}
            So we end up with all contractions containing \(A\) and some other operator, normal ordering all other operators.
            
            The exact same logic works for other terms, like
            \begin{equation}
                A^+ \wick{\c B \c C} \normalordering{D \dotsm} = \wick{\c A \c D} \wick{\c B \c C} \normalordering{\dotsm} + \dotsb,
            \end{equation}
            where we have all pairs of pairs of contractions with one pair being \(B\) and \(C\) and the other containing \(A\).
            Considering all such terms we see that we get all possible contractions involving \(A\).
            Going back to our result for \(\timeOrdering[ABCD\dotsm]\) as containing all contractions not involving \(A\), and recombining \(A^+\) and \(A^-\) to get \(A\) we prove the theorem.
        \end{proof}
    \end{thm}
    
    \begin{crl}{Wick's Theorem for Mixed Time Products}{}
        Suppose that some of the operators in the statement of \cref{thm:wick} are evaluated at the same time.
        Then the time ordering is instead given by all contractions not involving contractions of equal time operators and normal ordering everything left over.
        \begin{proof}
            This follows by applying Wick's theorem at unequal times, and then setting some times equal and seeing that contractions of equal time terms vanish, since
            \begin{equation}
                \timeOrdering(\normalordering{A(t, \vv{x})B(t, \vv{x}')}) = \normalordering{A(t, \vv{x})B(t, \vv{x}')}
            \end{equation}
            so
            \begin{equation}
                \wick{\c A \c B} = \timeOrdering[\normalordering{AB}] - \normalordering{AB} = 0.
            \end{equation}
        \end{proof}
    \end{crl}

    \section{Feynman Propagator}
    Consider the contraction
    \begin{align}
        \wick{\c \varphi(x) \c \varphi(x')} &= \bra{0} \timeOrdering[\varphi(x)\varphi(x')]\ket{0}\\
        &= \heaviside(t - t') \bra{0}  \varphi(x) \varphi(x')\ket{0} + \heaviside(t' - t) \bra{0} \varphi(x')\varphi(x)\ket{0}
    \end{align}
    where \(t\) is the time component of \(x\) and \(t'\) is the time component of \(x'\).
    
    We've seen that
    \begin{equation}
        i\Delta^+(x - x') = \commutator{\varphi^+(x)}{\varphi^-(x')}
    \end{equation}
    is just a number, and so
    \begin{equation}
        \bra{0} \commutator{\varphi^+(x)}{\varphi^-(x')} \ket{0} = \commutator{\varphi^+(x)}{\varphi^-(x')} \braket{0}{0} = \commutator{\varphi^+(x)}{\varphi^-(x')}.
    \end{equation}
    Hence, we have
    \begin{align}
        i\Delta^+(x - x') &= \bra{0} \commutator{\varphi^+(x)}{\varphi^-(x')} \ket{0}\\
        &= \bra{0} \varphi^+(x)\varphi^-(x') \ket{0}\\
        &= \bra{0} \varphi(x) \varphi(x') \ket{0},
    \end{align}
    where expanding the commutator the \(\varphi^-(x')\varphi^+(x)\) term annihilates the vacuum, and so vanishes, and in the last part expanding \(\varphi\) using \(\varphi^{\pm}\) shows that all terms apart from the \(\varphi^+(x)\varphi^-(x')\) term vanish as they involve annihilating the vacuum:
    \begin{align}
        \varphi(x)\varphi(x') &= (\varphi^+(x) + \varphi^-(x))(\varphi^+(x') + \varphi^-(x'))\\
        &= \varphi^+(x)\varphi^+(x') + \varphi^+(x)\varphi^-(x') + \varphi^-(x)\varphi^+(x') + \varphi^-(x)\varphi^-(x') \notag
    \end{align}
    
    Similarly, we have
    \begin{equation}
        i\Delta^-(x - x') = -\bra{0} \commutator{\varphi(x')}{\varphi(x)}\ket{0}.
    \end{equation}
    This just follows from \(\Delta^+(x) = -\Delta^-(-x)\).
    We can then define
    \begin{align}
        i\Delta_{\feynman} (x - x') &\coloneqq \wick{\c \varphi(x) \c \varphi(x')}\\
        &\hphantom{:}= \bra{0} \timeOrdering[\varphi(x)\varphi(x')]\ket{0}\label{eqn:feynman propagator as vacuum expectation of time ordered product}\\
        &= \heaviside(t - t') i\Delta^+(x - x') - \heaviside(t' - t)i\Delta^-(x - x').
    \end{align}
    This is called the \define{Feynman propagator}\index{Feynman propagator!for scalars}.
    Without the factor of \(i\) it's defined as
    \begin{equation}
        \Delta_{\feynman}(x) = \heaviside(t)\Delta^+(x) - \heaviside(-t)\Delta^-(x).
    \end{equation}
    The interpretation is as follow:
    \begin{itemize}
        \item For \(t > t'\) we have a positive energy, that is \(\Delta^+\), mode propagating from \(x'\) to \(x\).
        \item For \(t < t'\) we have a negative energy, that is \(\Delta^-\), mode propagating from \(x'\)  to \(x\).
    \end{itemize}
    
    Notice that we can rewrite the Feynman propagator as
    \begin{equation}
        \Delta_{\feynman}(x - x') = \heaviside(t - t')\Delta^+(x - x') + \heaviside(t' - t)\Delta^+(x' - x).
    \end{equation}
    This is has the same value but a different interpretation:
    \begin{itemize}
        \item For \(t > t'\) we have a positive energy, that is \(\Delta^+\), mode propagating from \(x'\) to \(x\).
        \item For \(t < t'\) we have a positive energy, that is \(\Delta^+\), mode propagating from \(x\) to \(x'\).
    \end{itemize}
    
    Comparing these interpretations we see that a negative energy mode (which we'll later associate with antiparticles) is equivalent to a positive energy mode travelling in the opposite direction.
    
    \subsection{Contour Representation}
    As well as this interpretation the Feynman propagator has the nice property of automatically encoding the time ordering.
    This isn't that useful in the current form, since we have explicit Heaviside step functions doing the encoding.
    However, the Feynman propagator has a contour representation which makes the time ordering less obvious.
    
    The contour representation of \(\Delta_{\feynman}\) is
    \begin{equation}
        \Delta_{\feynman}(x) = \int_{C_{\feynman}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2}
    \end{equation}
    where \(C_{\feynman}\) is the contour given by the real axis in the \(p_0\) plane with a slight dip below the pole at \(-\omega(\vv{p})\) and a slight dip above the pole at \(\omega(\vv{p})\).
    This is shown in \cref{fig:contour for feynman propagator}.
    
    \begin{figure}
        \tikzsetnextfilename{contour-for-feynman-propagator}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [right] {\(\Re(p_0)\)};
            \draw[thick, ->] (0, -2) -- (0, 2) node [above] {\(\Im(p_0)\)};
            \fill[Red] (1, 0) circle [radius = 0.05cm] node [below, text=black] {\(\omega(\vv{p})\)};
            \fill[Red] (-1, 0) circle [radius = 0.05cm] node [above, text=black] {\(-\omega(\vv{p})\)};
            \draw[very thick, Blue] (-3, 0) -- (-1.5, 0) arc (-180:0:0.5) -- (0.5, 0) arc(180:0:0.5) -- (3, 0);
            \draw[very thick, Blue, ->] (-3, 0) -- (-2.25, 0) node [below, text=black, yshift=-0.05cm] {\(C_{\symrm{F}}\)};
            \draw[very thick, Blue, ->] (1.5, 0) -- (2.25, 0);
        \end{tikzpicture}
        \caption{The contour used to compute the Feynman propagator.}
        \label{fig:contour for feynman propagator}
    \end{figure}
    
    To see why this works we consider two cases, first, suppose that \(x_0 > 0\).
    Then close the contour in the lower half plane, so that \(p_0\) has a large, negative, imaginary component and overall \(-ip\cdot x\) has a large, negative, real component.
    Closing with a semicircle Jordan's lemma tells us that the integral along this arc vanishes.
    Hence, the value of the integral along the \(C_{\feynman}\) contour is equal to the value of the integral around this closed loop.
    We can apply the residue theorem to the contour about the loop.
    There is a single pole enclosed in this loop, the one at \(\omega(\vv{p})\).
    However, we've already computed the integral of this function around a contour containing the pole \(\omega(\vv{p})\) in \cref{sec:contour representation}, the only difference is a minus sign not in the Feynman propagator contour representation, so we can conclude that the result is \(\Delta^+(x)\).
    
    Similarly, if \(x_0 < 0\) we close the contour in the upper half plane.
    Again the integral vanishes along this arc and we can then treat this as a contour containing only the pole at \(-\omega(\vv{p})\), the result of which tells us that this integral gives \(-\Delta^-(x)\).
    So, combining these results we see that we get the expected result of \(\pm \Delta^{\pm}(x)\) depending on the sign of \(x_0\).
    
    \subsubsection{The \texorpdfstring{\(i\varepsilon\)}{i epsilon} Prescription}
    This contour integral is not particular nice to evaluate, we have to awkwardly detour around the poles on the real axis.
    Instead we can slightly modify the integrand to give the representation
    \begin{equation}
        \Delta_{\feynman}(x) = \int \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2 + i\varepsilon}.
    \end{equation}
    This moves the two poles slightly off the real axis.
    Let \(\varepsilon > 0\) be a small positive real number, then
    \begin{equation}
        p^2 - m^2 + i\varepsilon = p_0^2 - \left( \omega(\vv{p}) - \frac{i\varepsilon}{2\omega(\vv{p})} \right)^2,
    \end{equation}
    just expand this out to check, and drop terms of order \(\varepsilon^2\).
    The poles are then shifted by \(i\varepsilon/[2\omega(\vv{p})]\), so that the pole which was at \(-\omega(\vv{p})\) is now at \(-\omega(\vv{p}) + i\varepsilon/[2\omega(\vv{p})]\), and the pole which was at \(\omega(\vv{p})\) is now at \(\omega(\vv{p}) - i\varepsilon/[2\omega(\vv{p})]\).
    This is shown in \cref{fig:contour for feynman propagator with i epsilon prescription}.
    We can then integrate along the real axis as normal, and then take \(\varepsilon \to 0\) after our computations have finished.
    
    \begin{figure}
        \tikzsetnextfilename{contour-for-feynman-propagator-with-i-epsilon-prescription}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [right] {\(\Re(p_0)\)};
            \draw[thick, ->] (0, -2) -- (0, 2) node [above] {\(\Im(p_0)\)};
            \fill[Red] (1, -0.3) circle [radius = 0.05cm] node [below, text=black, xshift=0.5cm] {\(\omega(\vv{p}) + i\varepsilon/[2\omega(\vv{p})]\)};
            \fill[Red] (-1, 0.3) circle [radius = 0.05cm] node [above, text=black, xshift=-0.55cm] {\(-\omega(\vv{p}) - i\varepsilon/[2\omega(\vv{p})]\)};
            \draw[very thick, Blue] (-3, 0) -- (3, 0);
            \draw[very thick, Blue, ->] (-3, 0) -- (-2.25, 0) node [below, text=black, yshift=-0.05cm] {\(C_{\symrm{F}}\)};
            \draw[very thick, Blue, ->] (1.5, 0) -- (2.25, 0);
        \end{tikzpicture}
        \caption{The contour for calculating the Feynman propagator using the \(i\varepsilon\) prescription.}
        \label{fig:contour for feynman propagator with i epsilon prescription}
    \end{figure}
    
    \subsection{Feynman Propagator as a Green's Function}
    Like the other propagators the Feynman propagator is a Green's function of the Klein--Gordon equation, that is
    \begin{equation}
        (\dalembertian + m^2) \Delta_{\feynman}(x) = \delta^4(x).
    \end{equation}
    The difference is just in the boundary conditions.
    
    The classical retarded and advanced Green's functions, as well as the \(\Delta^{\pm}\) propagators are Green's functions for an equation with Neumann boundary conditions, that is, the boundary conditions fix initial conditions on a function and its derivative, in our case we have an initial condition on \(\varphi\) and \(\pi = \dot{\varphi}\).
    This is a classical way of thinking, we have some initial state and we let it evolve according to some equation.
    The problem is that the assumption is we can know both \(\varphi\) and \(\pi\) at the same time, this is not possible due to the uncertainty principle.
    
    The Feynman propagator on the other hand corresponds to Dirichlet boundary conditions, where we set initial and final conditions on some function, in this case \(\varphi\).
    This is much closer to how we think in quantum mechanics, where we consider an initial and final state and transitions between them.
    
    \chapter{Feynman Diagrams}
    \label{chap:feynman diagrams}
    \epigraph{This is a great course because there are loads and loads of integrals to do but they're all really easy, so you don't feel any pain but you get to feel proud after.}{Richard Ball}
    In this section we develop the theory of Feynman diagrams.
    A diagrammatic representation of interactions which have the benefit of greatly simplifying the maths and also seeming quite physical.
    We stress however, that any one diagram is not a true representation of a physical process, rather it corresponds to a single term in an expansion describing a physical process.
    
    We start from the Dyson expansion,
    \begin{equation}
        S = \sum_{n = 0}^{\infty} S^{(n)},
    \end{equation}
    where
    \begin{equation}
        S^{(n)} = \frac{(-i)^n}{n!} \int \!\! \dl{^4x_1} \dotsm \int \!\! \dl{^4x_n} \, \timeOrdering[\hamiltonianDensity_{\interaction}(x_1) \dotsm \hamiltonianDensity_{\interaction}(x_n)].
    \end{equation}

    We will use \(\varphi^3\) theory as an example, but the process is basically the same, but usually with harder integrals, for other theories.
    This means that
    \begin{equation}
        \hamiltonianDensity_{\interaction}(x) = \frac{g}{3!} \normalordering{\varphi(x)^3} = \frac{g}{3!}\normalordering{(\varphi^+(x) + \varphi^-(x))^3}.
    \end{equation}
    We consider low order terms in this series, and develop Feynman diagrams as we go along.
    
    \section{Evaluating Low Order Terms}
    \subsection{\texorpdfstring{\(n = 0\)}{n = 0}}
    The \(n = 0\) term, \(S^{(0)}\), is just 1.
    This represents the case where nothing happens, our initial state just remains and is equal to the final state.
    
    \subsection{\texorpdfstring{\(n = 1\)}{n = 1}}
    The \(n = 1\) term is
    \begin{equation}
        S^{(1)} = \frac{-ig}{3!} \int \dl{^4x} \, ({\varphi^+}^3 + 3{\varphi^-}^2\varphi^+ + 3\varphi^-{\varphi^+}^2 + {\varphi^-}^3).
    \end{equation}
    We can consider each term in the integrand separately.
    
    The first term represents three annihilators, therefore this corresponds to three particles in the initial state entering, being destroyed, and then a vacuum final state.
    As a diagram we represent this as
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S1-3-0}
        \begin{tikzpicture}[baseline=(v)]
            \begin{feynman}
                \diagram[horizontal=i2 to v, small, layered layout]{
                    {i1, i2, i3} -- v
                };
                \node[right] at (v) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This interaction doesn't conserve momentum, so it must give zero contribution to the overall value of \(S^{(1)}\).
    
    The second term represents an annihilator and two creators, so this corresponds to a single particle entering, being destroyed, and then two new particles being created.
    Since all three factors of \(\varphi\) are evaluated at the same spacetime point, \(x\), this occurs all at once, which we draw as
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S1-1-2}
        \begin{tikzpicture}[baseline=(v)]
            \begin{feynman}
                \diagram[horizontal=i to v, small]{
                    i -- v,
                    v -- {o1, o2}
                };
                \node[below left] at (v) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    It's not immediately obvious, but this term cannot actually conserve momentum in \(\varphi^3\) theory, since all three particles have the same mass, and so it gives zero, which we'll show later.
    
    The third term is two annihilators and one creation operator, it corresponds to two particles being annihilated and then one new particle being created.
    As a diagram it is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S1-2-1}
        \begin{tikzpicture}[baseline=(v)]
            \begin{feynman}
                \diagram[horizontal=v to o, small]{
                    {i1, i2} -- v,
                    v -- o
                };
                \node[below right] at (v) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Like the previous term this vanishes in \(\varphi^3\) theory.
    
    The fourth term is three creation operators, so represents three particles being created from an initial vacuum state.
    Diagrammatically it is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S1-0-3}
        \begin{tikzpicture}[baseline=(v)]
            \begin{feynman}
                \diagram[horizontal=v to o2, small, layered layout]{
                    v -- {o1, o2, o3}
                };
                \node[left] at (v) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Like the first term this one doesn't conserve momentum and so must have no net contribution to \(S^{(1)}\).
    
    \subsection{\texorpdfstring{\(n = 2\)}{n = 2}}
    The \(n = 2\) term is
    \begin{equation}
        S^{(2)} = \frac{g^2}{2!(3!)^2} \int \dl{^4x} \int \dl{^4y} \timeOrdering(\normalordering{\varphi(x)^3} \normalordering{\varphi(y)^3}).
    \end{equation}
    We can use Wick's theorem to evaluate this normal ordered product.
    We can then split \(S^{(2)}\) further into four terms based on the number of contractions:
    \begin{equation}
        S^{(2)} = S^{(2)}_0 + S^{(2)}_1 + S^{(2)}_2 + S^{(2)}_{3},
    \end{equation}
    Where \(S^{(2)}_i\) is the term in \(S^{(2)}\) corresponding to terms with \(i\) contractions.
    Note that since we have six factors of \(\varphi\) we have a maximum of three contractions.
    
    \subsubsection{No Contractions}
    The terms with no contractions correspond to the first term in Wick's theorem,
    \begin{equation}
        \normalordering{\varphi(x)^3\varphi(y)^3}.
    \end{equation}
    Since there are no contractions between the field evaluated at the two spacetime points, \(x\) and \(y\), we get disconnected diagrams, such as
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-0-contractions}
        \begin{tikzpicture}[baseline=(vx)]
            \begin{feynman}
                \diagram[horizontal=vx to ox, small]{
                    {i1x, i2x} -- vx,
                    vx -- ox
                };
                \diagram[horizontal=iy to vy, small, xshift=2.5cm, yshift=-1.095cm]{
                    iy -- vy,
                    vy -- {o1y, o2y}
                };
                \node[below right] at (vx) {\(x\)};
                \node[below left] at (vy) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This is a single diagram corresponding to two separate processes occurring at \(x\) and \(y\).
    This diagram corresponds to the term
    \begin{equation}
        \varphi^-(x)\varphi^-(y)\varphi^-(y)\varphi^+(x)\varphi^+(x)\varphi^+(y),
    \end{equation}
    and other terms give similar diagrams.
    
    It can be shown that all disconnected diagrams vanish in \(\varphi^3\) theory.
    Further, any disconnected diagram is simply a combination of connected diagrams, so we don't every need to consider disconnected diagrams.
    In terms of physics disconnected diagrams correspond to multiple unrelated scattering events occurring, and we can simply treat them all separately.
    
    \subsubsection{One Contraction}
    Wick's theorem, generalised for equal times, doesn't have contractions between operators at the same time.
    Hence, we don't need to consider contractions of \(\varphi(x)\) with itself, or \(\varphi(y)\) with itself.
    This means that we have the term\\
    \begin{equation}
        3^2 \wick{\c \varphi(x) \c \varphi(y)} \normalordering{\varphi(x)^2 \varphi(y)^2}.
    \end{equation}
    The factor of \(3^2\) is combinatoric, it comes from there being three ways to select which \(\varphi(x)\) field to contract and three ways to select which \(\varphi(y)\) field to contract, and then since these are all really identical we can combine them into a single term with weighting \(3^2\).
    
    The contraction corresponds to a virtual particle propagating from \(x\) to \(y\).
    The four factors of \(\varphi\) in the normal ordering tell us that we'll have four external particles.
    There are three ways to do this, we can have 1-3, 2-2, or 3-1 interactions, where these are the number of initial particles to final particles.
    
    An example of a 1-3 term is given by the diagram
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-1-contraction-1-3}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[horizontal=i to v1, layered layout, small]{
                    i -- v1 -- v2 -- {o2, o3},
                    v1 -- o1,
                };
                \node[below left] at (v1) {\(x\)};
                \node[below] at (v2) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \varphi^+(x)\varphi^-(x)\varphi^-(y)\varphi^-(y).
    \end{equation}
    This term, in \(\varphi^3\) theory, doesn't conserve momentum since all particles have the same mass, so must give no overall contribution to \(S^{(2)}\).
    
    Similarly any 3-1 terms must vanish.
    
    More interesting are the 2-2 terms.
    There are three such terms.
    The first is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-1-contraction-2-2-s-channel}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[horizontal=v1 to v2, small]{
                    {i1, i2} -- v1 -- v2 -- {o1, o2}
                };
                \node[below] at (v1) {\(x\)};
                \node[below] at (v2) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \varphi^+(x)\varphi^+(x)\varphi^-(y)\varphi^-(y).
    \end{equation}
    We call this the \define{\(\symbf{s}\)-channel}\index{s-channel@\(s\)-channel} diagram.
    
    The second is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-1-contraction-2-2-t-channel}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[horizontal=i1 to o1, small]{
                    i1 -- v1 -- o1,
                    i2 -- v2 -- o2,
                    v1 -- v2,
                    i1 -- [draw=none] i2,
                    o1 -- [draw=none] o2
                };
                \node[below] at (v1) {\(y\)};
                \node[above] at (v2) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \varphi^+(x) \varphi^+(y) \varphi^-(x) \varphi^-(y).
    \end{equation}
    We call this the \define{\(\symbf{t}\)-channel}\index{t-channel@\(t\)-channel} diagram.
    
    The third is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-1-contraction-2-2-u-channel}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[horizontal=i1 to o1, small]{
                    i1 -- v1 -- [draw=none] o1,
                    i2 -- v2 -- [draw=none] o2,
                    v1 -- v2,
                    i1 -- [draw=none] i2,
                    o1 -- [draw=none] o2,
                };
                \draw (v2) -- (o1);
                \draw[line width=0.1cm, white] ($(v1)!0.3!(o2)$) -- ($(v1)!0.7!(o2)$);
                \draw (v1) -- (o2);
                \node[below] at (v1) {\(y\)};
                \node[above] at (v2) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)}\varphi^+(x)\varphi^+(y)\varphi^-(y)\varphi^-(x)
    \end{equation}
    
    If we take any of these three diagrams and swap \(x\) and \(y\) then nothing changes, so this gives us a factor of 2 to include, which nicely cancels the factor of \(1/2!\) in the full expression for \(S^{(2)}\).
    
    \subsubsection{Two Contractions}
    Terms with two contractions correspond to a term proportional to
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \wick{\c \varphi(x) \c \varphi(y)} \normalordering{\varphi(x)\varphi(y)}.
    \end{equation}
    The two contractions mean we have two internally propagating virtual particles, and the two normal ordered terms mean we have two external particles.
    
    There is a single such diagram, up to swapping \(x\) and \(y\):
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-2-contractions}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \vertex (i);
                \vertex[right=of i] (v1);
                \vertex[right=of v1] (v2);
                \vertex[right=of v2] (o);
                \draw (i) -- (v1);
                \draw (v2) -- (o);
                \draw (v1) to[bend left=90] (v2);
                \draw (v1) to[bend right=90] (v2);
                \node[right] at (v1) {\(x\)};
                \node[left, yshift=-0.05cm] at (v2) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    
    It can be shown that this term actually diverges, giving an infinite result.
    We call this a one \defineindex{loop diagram}, since it has a single loop.
    In contrast the diagrams we have considered so far without loops are \define{tree diagrams}\index{tree diagram}.
    
    \subsubsection{Three Contractions}
    Terms with three contractions correspond to a term proportional to
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \wick{\c \varphi(x) \c \varphi(y)} \wick{\c \varphi(x) \c \varphi(y)}.
    \end{equation}
    This has no external particles and three internal virtual particles propagating from \(x\) to \(y\).
    It corresponds to the diagram
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-3-contractions}
        \begin{tikzpicture}[baseline=(x)]
            \begin{feynman}
                \vertex (x);
                \vertex[right=of x] (y);
                \draw (x) -- (y);
                \draw (x) to[bend left=90] (y);
                \draw (x) to[bend right=90] (y);
                \node[left] at (x) {\(x\)};
                \node[right] at (y) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This is called a \define{vacuum bubble} since it is formed entirely of virtual particles, and both the initial and final state are vacuums.
    Since there are no external particles there are no physically observable contributions from terms like this.
    It can be shown that bubble diagrams factor out to all levels in all theories, so we don't need to consider them.
    The correspond to some virtual particles coming into existence and then all annihilating.
    
    \section{Feynman Diagrams in Momentum Space}
    \epigraph{As always in QFT you have to get everything right.}{Richard Ball}
    So far with these diagrams we've been working in position space, evaluating the fields at \(x\).
    External states are momentum eigenstates, this suggests that we should instead work in momentum space instead, which means working with \(a(\vv{p})\) and \(a^\hermit(\vv{p})\).
    
    We are interested in computing the matrix element
    \begin{equation}
        S_{fi} = \bra{f} S \ket{i}.
    \end{equation}
    We can express \(\ket{i}\) and \(\ket{f}\) as free particle states created by \(a^\hermit(\vv{p})\) from the vacuum state.
    These states have definite momentum and all of the particles are on-shell.
    
    Consider the one particle state with momentum \(\vv{p}\).
    This is created with a single creation operator acting on the vacuum:
    \begin{equation}
        \ket{\vv{p}} \coloneqq a^\hermit(\vv{p})\ket{0}.
    \end{equation}
    Now act on this with \(\varphi^+(x)\).
    We can expand \(\varphi^+(x)\) in momentum space using \cref{eqn:phi +}:
    \begin{align}
        \textcolor{Red}{\varphi^+(x)}\textcolor{Purple}{\ket{\vv{p}}} &= \textcolor{Red}{\int \invariantmeasure{p'} \, \e^{-ip' \cdot x} a(\vv{p}')} \textcolor{Purple}{a^\hermit(\vv{p})\ket{0}}\\
        &= \int \invariantmeasure{p'} \, \e^{-ip'\cdot x} \commutator{a(\vv{p}')}{a^\hermit(\vv{p}')} \ket{0}\\
        &= \int \invariantmeasure{p'} \, \e^{-ip'\cdot x} \bardelta(\vv{p} - \vv{p}') \ket{0}\\
        &= \e^{-ip\cdot x}\ket{0}.
    \end{align}
    Here we've used
    \begin{equation}
        \commutator{a(\vv{p}')}{a^\hermit(\vv{p})} \ket{0} = a(\vv{p}')a^\hermit(\vv{p})\ket{0} - a^\hermit(\vv{p})a(\vv{p}')\ket{0} = a(\vv{p}')a^\hermit(\vv{p})\ket{0}
    \end{equation}
    with the second term vanishing as we annihilate the vacuum.
    
    Taking the conjugate of this result, and using \((\varphi^+)^\hermit = \varphi^-\) we have
    \begin{equation}
        \bra{\vv{p}} \varphi^-(x) = \bra{0} \e^{ip\cdot x}.
    \end{equation}
    
    Since we're going to be working in momentum space it's useful to have an expression for the contraction in momentum space, we get this through the contour representation of the Feynman propagator:
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} = i \int \frac{\dl{^4k}}{(2\pi)^4} \frac{\e^{-ik\cdot (x - y)}}{k^2 - m^2 + i\varepsilon} = i\Delta_{\feynman}(x - y),
    \end{equation}
    where the \(k^0\) integral is performed along the real axis, and we set \(\varepsilon\) ot zero after our calculations.
    Note that we've written this here using \(k\), the wavenumber, which is the same as the momentum when \(\hbar = 1\).
    This is mostly to free up the letter \(p\) for the momentum of the particles.
    
    We are now in a position to start computing terms in the Dyson series, which we can view as an expansion in \(g\).
    We won't do all of the ones discussed above, just enough to get the idea.
    
    \subsection{First Order}
    Consider the first order term
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-first-order-full-calculation}
        \begin{tikzpicture}[baseline=(x)]
            \begin{feynman}
                \diagram[small, horizontal=i to x]{
                    i [particle=\(p\)] -- x -- {o1 [particle=\(q\)], o2 [particle=\(q'\)]}
                };
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Here we've labelled the particles with their momenta.
    The initial state contains just a single particle, so its \(\ket{i} = \ket{\vv{p}}\).
    The final states contain two particles, so the final state is a tensor product of single particle states:
    \begin{equation}
        \ket{f} = \ket{\vv{q},\ket{\vv{q}'}} = \ket{\vv{q}}\ket{\vv{q}'} = \ket{\vv{q}} \otimes \ket{\vv{q}'}.
    \end{equation}
    This works because we assume no interaction between the two final particles, at least not at first order in \(g\).
    
    The matrix element we wish to compute is
    \begin{equation}
        \bra{f} S^{(1)}_{\text{1-2}} \ket{i} = \bra{\vv{q}, \vv{q}'}\left[ \frac{-ig}{3!} \int \dl{^4x} 3\varphi^-(x)^2 \varphi^+(x) \right] \ket{\vv{p}}.
    \end{equation}
    Here \(S^{(1)}_{\text{1-2}}\) is the term in \(S^{(1)}\) corresponding to the term we wish to compute.
    The factor of 3 in the integral comes from the number of ways we can choose one of the factors of \(\varphi\) to be \(\varphi^+\).
    We can show that
    \begin{equation}
        \bra{\vv{q}, \vv{q}'} \varphi^-(x)^2 = 2\e^{iq\cdot x} \e^{iq'\cdot x}\bra{0},
    \end{equation}
    where the factor of two comes from the number of ways we can act on \(\bra{\vv{q}, \vv{q}'}\) with \(\varphi^-(x)^2\).
    We can either act on \(\bra{\vv{q}}\) and then \(\bra{\vv{q}'}\), or the other way round.
    We also have
    \begin{equation}
        \varphi^+(x)\ket{\vv{p}} = \e^{-ip\cdot x}\ket{0}.
    \end{equation}
    
    Putting this into our calculation, and noticing that the factors of \(2\) and \(3\) cancel with the \(3!\) we have
    \begin{align}
        \bra{f} S^{(1)}_{\text{1-2}} \ket{i} &= -ig \int \dl{^4x} \, \e^{iq\cdot x} \e^{iq'\cdot x} \e^{-ip\cdot x}\\
        &= -ig (2\pi)^4\delta^4(q + q' - p).
    \end{align}
    The Dirac delta enforces conservation of momentum, the momentum in, \(p\), must equal the momentum out, \(q + q'\), in order to get a nonzero result.
    
    All external particles are on-shell, and in \(\varphi^3\) theory all particles have the same mass.
    This means that \(p^2 = q^2 = q'^2 = m^2\).
    Now, consider \((p - q)^2\).
    Conservation of momentum tells us that \(p - q = q'\), and so \((p - q)^2 = q'^2 = m^2\).
    We can also expand \((p - q)^2\) to get
    \begin{align}
        m^2 &= (p - q)^2\\
        &= p^2 + q^2 - 2p\cdot q\\
        &= m^2 + m^2 + 2p\cdot q.
    \end{align}
    Hence, we have \(2p\cdot q = m^2\).
    In the rest frame of the incoming particle we have \(p^\mu = (m, \vv{0})\), and we can write \(q^\mu = (E, \vv{q})\) without loss of generality.
    We therefore have
    \begin{equation}
        2 p \cdot q = 2mE = m^2 \implies E = \frac{m}{2}.
    \end{equation}
    This is what we would expect, since the two particles are produced each must get exactly half of the energy available from the first particle, and that energy is just its mass.
    We also know that \(E^2 = m^2 + \vv{q}^2\), and so \(E \ge m\).
    This means that \(E = m/2\) can't satisfy \(p = q + q'\).
    Hence, the contribution from this term vanishes as the condition imposed by the Dirac delta can never be fulfilled.
    
    Note that in other theories with multiple particles with different masses it is possible for this term to be nonzero.
    It's also not true that all vertices like this vanish, the key difference is that if one of the particles is a virtual particle then it needn't satisfy the on-shell condition and so it can have arbitrary momentum, and we can conserve momentum with particles of the same mass.
    
    The key result is that, in \(\varphi^3\) theory,
    \begin{equation}
        \bra{f} S^{(1)}_{\text{1-2}} \ket{i} = 0.
    \end{equation}
    
    \subsection{Second Order}
    For now we restrict ourselves to tree diagrams, we'll come to loops later.
    There are three diagrams to consider then:
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-second-order-full-calculation-s}
        \feynmandiagram[horizontal=v1 to v2, inline=(v1)] {
            {i1 [particle=\(p\)], i2 [particle=\(p'\)]} -- v1 -- [edge label=\(k\)] v2 -- {o1 [particle=\(q\)], o2 [particle=\(q'\)]}
        };
        \quad
        \tikzsetnextfilename{fd-phi-cubed-second-order-full-calculation-t}
        \feynmandiagram[vertical'=v1 to v2, small, baseline=(current bounding box)] {
            i1 [particle=\(p\)] -- v1 -- o1 [particle=\(q\)],
            i2 [particle=\(p'\)] -- v2 -- o2 [particle=\(q'\)],
            v1 -- [edge label=\(k\)] v2,
            i1 -- [draw=none] i2,
            o1 -- [draw=none] o2
        };
        \quad
        \tikzsetnextfilename{fd-phi-cubed-second-order-full-calculation-u}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \begin{feynman}
                \diagram[horizontal=i1 to o1, small]{
                    i1 [particle=\(p'\)] -- v1 -- [draw=none] o1 [particle=\(q'\)],
                    i2 [particle=\(p\)] -- v2 -- [draw=none] o2 [particle=\(q\)],
                    v1 -- [edge label=\(k\)] v2,
                    i1 -- [draw=none] i2,
                    o1 -- [draw=none] o2,
                };
                \draw (v2) -- (o1);
                \draw[line width=0.1cm, white] ($(v1)!0.3!(o2)$) -- ($(v1)!0.7!(o2)$);
                \draw (v1) -- (o2);
                \node[below] at (v1) {\(y\)};
                \node[above] at (v2) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    
    Note that while we assume the same initial and final states the momenta of the virtual particles need not, and in fact aren't, the same, even though we've labelled them all \(k\).
    
    \subsubsection{\texorpdfstring{\(s\)}{s}-Channel}
    Let's start with the \(s\)-channel process.
    We want to compute
    \begin{equation*}
        \bra{f} S^{(2)}_{s} \ket{i} = \bra{\vv{q}, \vv{q}'}\left[ \frac{-g^2}{(3!)^2} \int \dl{^4x} \int \dl{^4y} \, \varphi^-(y)^2 3^2 \wick{\c \varphi(x) \c \varphi(y)} \varphi^+(x)^2 \right] \ket{\vv{p}, \vv{p}'}
    \end{equation*}
    We can compute the action of \((\varphi^{\pm})^2\) on our initial and final states fairly easily:
    \begin{align}
        \varphi^+(x)^2 \ket{\vv{p}, \vv{p}'} &= 2\e^{-ip\cdot x} \e^{-ip'\cdot x}\ket{0},\\
        \bra{\vv{q}, \vv{q}'} \varphi^-(y)^2 &= 2\e^{iq\cdot y} \e^{iq'\cdot y}\bra{0}.
    \end{align}
    Again, the factors of 2 come from there being two orders in which we can act on the paired states.
    
    Noticing that the factor of \(3^2\) in the integrand, as well as the factors of 2 above, cancel with the \(1/(3!)^2\) term, and using \(\braket{0}{0} = 1\), we have
    \begin{equation*}
        \bra{f} S^{(2)}_{s} \ket{i} = -g^2 \int \dl{^4x} \int \dl{^4y} \, \e^{iq\cdot y} \e^{iq'\cdot y} i \int \frac{\dl{^4k}}{(2\pi)^4} \frac{\e^{ik\cdot (x - y)}}{k^2 - m^2 + i\varepsilon} \e^{ip\cdot x} \e^{ip' \cdot x}.
    \end{equation*}
    Performing the \(x\) and \(y\) integrals we get more Dirac deltas:
    \begin{equation*}
        \bra{f} S^{(2)}_{s} \ket{i} = -ig^2 \int \frac{\dl{^4k}}{(2\pi)^4} (2\pi)^4 \delta^4(q + q' - k) (2\pi)^4 \delta^4(k - p - p') \frac{1}{k^2 - m^2 + i\varepsilon}.
    \end{equation*}
    The two Dirac deltas ensure that momentum is conserved at each vertex, and hence overall, since we must have \(q + q' = k\) and \(k = p + p'\) to have a nonzero contribution, and so \(p + p' = q + q'\).
    We can then perform the integral over \(k\) using one the second Dirac delta and we get
    \begin{equation}
        \bra{f} S^{(2)}_{s} \ket{i} = (2\pi)^4\delta^4(q + q' - p - p') \frac{-ig^2}{(p + p')^2 - m^2}.
    \end{equation}
    Note that we can drop the \(i\varepsilon\) term since \(k^2 = (p + p')^2 \ge 4m^2\), using \(p^2 = p'^2 = m^2\), and so the denominator never vanishes.
    It turns out that this is always the case for tree level diagrams, the \(i\varepsilon\) prescription is only needed in loop diagrams.
    If instead we had used the first Dirac delta to perform the integral we would have gotten a factor of \((q + q')^2\) in the denominator instead, but this is equal to \((p + p')^2\) by momentum conservation.
    
    The Dirac delta in this result is again conserving momentum.
    The interesting thing is the rest, which we call the \defineindex{Feynman amplitude} for this process:
    \begin{equation}
        \amplitude_s = \frac{-ig^2}{(p + p')^2 - m^2}.
    \end{equation}
    
    \subsubsection{\texorpdfstring{\(t\)}{t}-Channel}
    Now consider the \(t\)-channel diagram.
    Like the \(s\)-channel term the \(t\)-channel term comes from a term which is sixth order in \(\varphi\).
    In particular, it is \(\normalordering{\varphi(x)^3\varphi(y)^3}\), where we call the vertex shared by \(p\) and \(q\) \(x\), and the vertex shared by \(p'\) and \(q'\) \(y\).
    Since we have four external particles we must contract a pair of fields, and since contractions at the same point give zero we must contract \(\varphi(x)\) and \(\varphi(y)\).
    There are 3 ways to choose which \(\varphi(x)\) and 3 ways to choose which \(\varphi(y)\).
    We then have two factors of \(\varphi(x)\) left and two factors of \(\varphi(y)\).
    Looking back at the diagrams we see that a particle is created and destroyed at each vertex, as well as the internal line corresponding to the contraction.
    This means one of our \(\varphi(x)\) values contributes a \(\varphi^+(x)\) to this term, destroying an incoming particle, and the other contributes a \(\varphi^-(x)\), creating one of the final particles.
    There are two ways to select which \(\varphi(x)\) contributes \(\varphi^+(x)\) and which contributes \(\varphi^-(x)\).
    The exact same is true for \(\varphi(y)\), so on top of the two factors of 3 from choosing the contraction we have two factors of 2.
    This gives an overall combinatorial factor of \(3 \cdot 3 \cdot 2 \cdot 2 = (3!)^2\), cancelling the \((3!)^2\) which appears in the interaction term.
    Hence, we have
    \begin{multline*}
        \bra{f} S^{(2)}_{t} \ket{i}\\
        = \bra{\vv{q}, \vv{q}'} \left[ \frac{-g^2}{(3!)^2} \int \dl{^4x} \int \dl{^4y} \, (3!)^2\wick{\c \varphi(x) \c \varphi(y)} \varphi^-(x)\varphi^-(y)\varphi^+(x)\varphi^+(y) \right] \ket{\vv{p}, \vv{p}'}.
    \end{multline*}
    Now, as before we act on the initial state with the annihilators and the final state with the creation operators, giving
    \begin{align}
        \varphi^+(x)\varphi^+(y) \ket{\vv{p}, \vv{p}'} &= \e^{-ip \cdot x} \e^{-ip' \cdot y} \ket{0},\\
        \bra{\vv{q}, \vv{q}'} \varphi^-(x)\varphi^-(y) &= \e^{iq \cdot x} \e^{iq' \cdot y} \bra{0}.
    \end{align}
    Note that there is no factor of 2 this time, since the particle with momentum \(p\) must be destroyed at \(x\) in this interaction, and hence we cannot act on this particle with \(\varphi^+(y)\), doing so corresponds instead to a \(u\)-channel process.
    
    Putting this together with our results we have
    \begin{equation*}
        \bra{f} S^{(2)}_{t} \ket{i} = -g^2 \int \dl{^4x} \int \dl{^4y} \, \e^{iq\cdot x} \e^{iq' \cdot y} \int \frac{\dl{^4x}}{(2\pi)^4} \frac{\e^{-ik \cdot (y - x)}}{k^2 - m^2 + i\varepsilon} \e^{-ip \cdot x} \e^{-ip' \cdot y}.
    \end{equation*}
    Collecting the exponents into \(x\) and \(y\) exponentials we have
    \begin{equation*}
        \bra{f} S^{(2)}_{t} \ket{i} = -g^2 \int \dl{^4x} \int \dl{^4y} \int \frac{\dl{^4k}}{(2\pi)^4} \e^{i(q + k - p) \cdot x} \e^{i(q' - k - p')} \frac{1}{k^2 - m^2 + i\varepsilon}.
    \end{equation*}
    Performing the \(x\) and \(y\) integrals gives Dirac deltas:
    \begin{equation*}
        \bra{f} S^{(2)}_{t} \ket{i} = -g^2 \int \frac{\dl{^4k}}{(2\pi)^4} (2\pi)^4 \delta^4(q + k - p) (2\pi)^4 \delta^4(q' - k - p') \frac{1}{k^2 - m^2 + i\varepsilon}.
    \end{equation*}
    Performing the \(k\) integral with the first Dirac delta fixes \(k = p - q\), and so we get
    \begin{equation}
        \bra{f} S^{(2)}_{t} \ket{i} = (2\pi)^4\delta^4(q + q - p - p') \frac{-g^2}{(p - q)^2 - m^2}.
    \end{equation}
    As before we have the Dirac delta enforcing energy conservation, and then the amplitude is 
    \begin{equation}
        \amplitude_t = \frac{-ig^2}{(p - q)^2 - m^2}.
    \end{equation}
    
    Similarly we can compute the \(u\)-channel process, and we find
    \begin{equation}
        \bra{f} S^{(2)}_{u} \ket{i} = (2\pi)^4 \delta^4(q + q' - p - p') \frac{-ig^2}{(p - q')^2 - m^2}.
    \end{equation}
    That is, the amplitudes for the \(u\)-channel process is
    \begin{equation}
        \amplitude_u = \frac{-ig^2}{(p - q')^2 - m^2}.
    \end{equation}
    
    We now introduce the \defineindex{Mandelstam invariants}, \(s\), \(t\), and \(u\), defined as
    \begin{alignat}{2}
        s &= (p + p')^2 &&= (q + q')^2,\\
        t &= (p - q)^2 &&= (p' - q')^2,\\
        u &= (p - q')^2 &&= (p' - q)^2.
    \end{alignat}
    These are Lorentz invariants characterising each of the three channels.
    
    Consider the sum of these three invariants:
    \begin{align}
        s + t + u &= (p + p')^2 + (p - q)^2 + (p - q')^2\\
        &= p^2 + p'^2 + 2p\cdot p' + p^2 + q^2 - 2p\cdot q^ + p^2 + q'^2 - 2 p \cdot q'\notag \\
        &= 6m^2 + 2(p\cdot p' - p\cdot q - p\cdot q')
    \end{align}
    having used the on-shell condition, \(p^2 = p'^2 = q^2 = q'^2 = m^2\).
    Now using conservation of momentum we have \(q' = p + p' - q\), and so
    \begin{align}
        s + t + u &= 6m^2 + 2(p \cdot p' - p \cdot q - p \cdot [p + p' - q])\\
        &= 6m^2 + 2(p \cdot p' - p \cdot q - p \cdot p - p \cdot p' + p \cdot q)\\
        &= 6m^2 + 2m^2\\
        &= 4m^2.
    \end{align}
    This shows that, in a theory where all particles have the same mass, the three Mandelstam invariants are not independent.
    
    We are interested in the total matrix element at second order which, ignoring loops, is
    \begin{equation}
        \bra{f} S^{(2)} \ket{i} = \bra{\vv{q}, \vv{q}'} (S^{(2)}_s + S^{(2)}_t + S^{(2)}_u) \ket{\vv{p}, \vv{p}'}.
    \end{equation}
    Combing our results we have
    \begin{equation}
        \bra{f} S^{(2)} \ket{i} = (2\pi)^4 \delta^4(q + q' - p - p') (-ig^2) \left[ \frac{1}{s - m^2} + \frac{1}{t - m^2} + \frac{1}{u - m^2} \right]
    \end{equation}
    and the amplitude is
    \begin{align}
        \amplitude^{(2)} &= \amplitude_s + \amplitude_t + \amplitude_u
        &= -ig^2\left[ \frac{1}{s - m^2} + \frac{1}{t - m^2} + \frac{1}{u - m^2} \right].
    \end{align}
    Notice that the amplitude is invariant under swapping \(p\) and \(p'\) and/or swapping \(q\) and \(q'\), since this simply swaps \(t\) and \(u\) if we make one swap, or does nothing if we make two.
    This implies that our particles are bosons.
    
    Now consider scattering in the centre of mass frame.
    Here the incoming particles enter with equal and opposite momenta, and therefore also with equal energy since they have the same mass and are on shell.
    So, \(p^\mu = (E, \vv{p})\) and \(p'^\mu = (E, -\vv{p})\).
    The energy must split equally between the outgoing particles, so they also have energy \(E\), and the total momentum after the interaction must vanish so they have equal and opposite momenta, \(q^\mu = (E, \vv{q})\) and \(q'^\mu = (E, -\vv{q})\).
    
    Calculating \(s\) we have \(s = (p + p')^2 = 4E^2 \ge 4m^2\), where the inequality comes from \(E^2 = m^2 + \vv{p}^2 \ge m^2\).
    This is what justified us dropping the \(i\varepsilon\) earlier, since if \(s \ge 4m^2\) then \(s - m^2\) will never be zero.
    
    Since all particles have the same mass and energy and are on-shell we must have that \(\vv{q}^2 = \vv{p}^2\), hence
    \begin{equation}
        t = -(\vv{p} - \vv{q})^2 = -2\abs{\vv{p}}^2(1 - \cos \vartheta) \le 0
    \end{equation}
    where \(\cos\vartheta = \vv{p} \cdot \vv{q}/(\abs{\vv{p}\abs{\vv{q}}})\).
    Similarly,
    \begin{equation}
        u = -(\vv{p} + \vv{q})^2 = -2\vv{p}^2(1 + \cos\vartheta) \le 0.
    \end{equation}
    This means that \(t - m^2\) and \(u - m^2\) will always be negative, and in particular are never zero allowing us to drop the \(i\varepsilon\) term.
    This also implies that virtual particles are \emph{always} off-shell, as otherwise we would not be able to conserve momentum in 2-1 and 1-2 vertices.
    
    \section{Feynman Rules}
    The calculations above all follow roughly the same steps, this suggests that we can formulate a way to go from the diagrams to the final result without having to do these steps every time.
    This is what Feynman rules do.
    
    For any \(\varphi^3\) process we have
    \begin{equation}
        \bra{f} S \ket{i} = \delta_{fi} + (2\pi)^4 \delta^4\left( \sum p_f - \sum p_i \right) \sum_{n = 2}^{\infty} \amplitude^{(n)}.
    \end{equation}
    The Kronecker delta accounts for the case where nothing happens, the Dirac delta enforces energy conservation, where the sums are over all final and then all initial states.
    The interesting bit is the amplitude, \(\amplitude^{(n)}\).
    Wick's theorem tells us that the total amplitude, \(\amplitude\), has contributions from all topologically distinct diagrams\footnote{that is, all diagrams which are not related by rearranging where nodes and edges are}, and \(\amplitude^{(n)}\) has contributions from all topologically distinct \(n\) vertex diagrams.
    Further, we have to consider all diagrams with equal weight due to Wick's theorem, bt the factor of \(g^n\) means that we can drop higher order terms, since \(g\) is taken to be small.
    
    From what we've seen so far we can guess most of the Feynman rules, and the others aren't too bad to demonstrate.
    The \defineindex{Feynman rules}\index{Feynman rules!\(\varphi^3\) theory} for \(\varphi^3\) theory are
    \begin{itemize}
        \item Every vertex contributes a factor of \(-ig\) to the amplitude.
        \item Every internal line contributes a factor of \(i\tilde{\Delta}_{\feynman}(k) = i/(k^2 - m^2 + i\varepsilon)\) to the amplitude.
        \item Every external line contributes a factor of \(1\) to the amplitude.
        \item Impose conservation of four-momentum at every vertex.
        \item Every momentum, \(k\), which is not fixed by conservation of four-momentum gives a factor of
        \begin{equation}
            \int \frac{\dl{^4k}}{(2\pi)^4}.
        \end{equation}
        \item A symmetry factor (trivial for simple diagrams).
    \end{itemize}
    Note that \(\tilde{\Delta}_{\feynman}\) is the Fourier transform of \(\Delta_{\feynman}\), which is to say it is the integrand in the contour representation.
    
    \chapter{Getting Measurable Results}
    Experiments can't measure the \(S\) matrix directly.
    Instead we can measure transition rates between states, including decay rates and cross sections.
    In this chapter we'll discuss how to compute these from the \(S\) matrix for comparison with experimental results.
    
    Our starting point will be
    \begin{equation}
        S_{fi} = \delta_{fi} + (2\pi)^4\delta^4\left( \sum p_f - \sum p_i \right) \amplitude.
    \end{equation}
    The \(\delta_{fi}\) term corresponds to the case where nothing happens.
    The Dirac delta, where the sums are over all final and initial particles, enforces conservation of momentum, and \(\amplitude\) is the Feynman amplitude, which is what we compute in QFT.
    
    \section{Transition Rate}
    Consider the probability, \(P_{fi}\), of transitioning from the initial state \(\ket{i}\) to the final state \(\ket{f}\).
    This is simply the square of the \(S\) matrix, which is given by
    \begin{equation}
        P_{fi} = \abs{S_{fi}}^2 = (2\pi)^4\delta^4(0)(2\pi)^4 \sum_f \delta^4\left( \sum p_f - \sum p_i \right) \abs{\amplitude}^2.
    \end{equation}
    assuming \(i \ne f\), that is that something happens.
    We have used here \(\delta(x)^2 = \delta(0)\delta(x)\) as distributions, which follows since
    \begin{equation}
        \int \dl{x} \, f(x) \delta(x)^2 = \delta(0) f(0),
    \end{equation}
    having used the sifting property of one of the Dirac deltas.
    The first sum over \(f\), outside of the Dirac delta, corresponds to a sum, which will turn out to really be an integral, over allowed values of the final momentum.
    
    There's a problem here, in that \(\delta^4(0)\) is infinite.
    Consider
    \begin{equation}
        (2\pi)^4 \delta^4(p) = \int \dl{^4x}  \, \e^{-ip\cdot x}.
    \end{equation}
    This suggests that
    \begin{equation}
        (2\pi)^4\delta^4(0) = \int \dl{^4x} = VT
    \end{equation}
    where \(V\) is the infinite volume of space and \(T\) is the infinite length of time.
    So, \(VT\) is the volume of spacetime.
    To get around the problem we treat the entire system as if it is in a finite box in spacetime, and then proceed to compute quantities per unit spacetime volume.
    
    We therefore look to compute the transition rate per unit volume,
    \begin{equation}
        w_{fi} = \frac{P_{fi}}{VT} = \sum_f (2\pi)^4\delta(\sum p_f - \sum p_i) \abs{\amplitude}^2.
    \end{equation}
    We can proceed using the completeness relation,
    \begin{equation}
        1 = \int \invariantmeasure{p} \, \ket{\vv{p}}\bra{\vv{p}},
    \end{equation}
    which holds if
    \begin{equation}
        \braket{\vv{p}}{\vv{p}'} = \bardelta(\vv{p} - \vv{p}').
    \end{equation}
    Writing out the measure in full,
    \begin{equation}
        1 = \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} \ket{\vv{p}} \bra{\vv{p}}.
    \end{equation}
    
    The incident flux is
    \begin{equation}
        \braket{\vv{p}}{\vv{p}} = 2\omega(\vv{p}) (2\pi)^3 \delta^3(\vv{0}).
    \end{equation}
    Identifying \((2\pi)^3\delta^3(\vv{0}) = V\) we have
    \begin{equation}
        \frac{\braket{\vv{p}}{\vv{p}'}}{V} = 2\omega(\vv{p}).
    \end{equation}
    This implies we have \(2\omega(\vv{p})\) states per unit volume.
    
    Now consider the decay of a single particle, \(p\), into \(n\) particles, \(p_i\).
    The decay rate in the frame of the particle, that is the transition rate per decaying particle, is then
    \begin{equation}
        \dl{\Gamma} = (2\pi)^4 \delta\left( p - \sum p_f \right) \frac{1}{2\omega(\vv{p})} \prod_f \frac{\dl{^3\vv{p}}}{(2\pi)^32E_f} \abs{\amplitude}^2,
    \end{equation}
    where we write \(E_f = \omega(\vv{p_f})\).
    We can package the measure up into a single thing called the \(n\) particle \defineindex{phase space measure}:
    \begin{equation}
        \phaseSpaceMeasure \coloneqq (2\pi)^4 \delta^4\left( p - \sum p_f \right) \prod_f \frac{\dl{^3p_f}}{(2\pi)^32E_f},
    \end{equation}
    so
    \begin{equation}
        \dl{\Gamma} = \frac{1}{2E(\vv{p})} \abs{\amplitude}^2 \phaseSpaceMeasure,
    \end{equation}
    where \(E(\vv{p}) = \omega(\vv{p})\).
    
    \subsection{One to Two Decay}
    Consider the decay \(p \to p_1 + p_2\).
    When doing an experiment we have detectors covering some solid angle, and so we are interested in not just the decay rate, but the decay rate into some particular solid angle, which is
    \begin{equation}
        \diff{\Gamma}{\Omega}.
    \end{equation}
    
    We have two final particles, so we need to compute \(\phaseSpaceMeasure[2]\), which is given by
    \begin{equation}
        \phaseSpaceMeasure[2] = (2\pi)^4 \delta^4(p - p_1 - p_2) \frac{\dl{^3\vv{p_1}}}{(2\pi)^32\omega(\vv{p_1})} \frac{\dl{^3\vv{p_2}}}{(2\pi)^32\omega(\vv{p_2})}.
    \end{equation}
    We can perform the \(p_2\) integral using one of the Dirac deltas:
    \begin{equation}
        \int \dl{^3\vv{p_2}} \delta^4(p_1 + p_2 - p) = \delta(E_1 + E_2 - E),
    \end{equation}
    where we now fix \(\vv{p_2} = \vv{p} - \vv{p_1}\) and \(E_2 = \abs{\vv{p_2}}^2 + m_2^2\).
    We can then express the momentum space volume element for \(p_1\) in terms of polar variables:
    \begin{equation}
        \dl{^3p_1} = \abs{\vv{p_1}}^2 \dd{p_1} \dd{\Omega}.
    \end{equation}
    Now use
    \begin{equation}
        E_1^2 = \abs{\vv{p_1}}^2 + m_1^2 \implies E_1 \dd{E_1} = p_1\dd{p_1},
    \end{equation}
    since \(\dl{m_1} = 0\).
    Then, the partially integrated phase space is
    \begin{equation}
        \int \phaseSpaceMeasure[2] = \frac{1}{4\pi} \dd{\Omega} \frac{p_1 \dd{E_1}}E_2{} \delta(E_1 + E_2 - E).
    \end{equation}
    We can then write the phase space measure as
    \begin{equation}
        \int \phaseSpaceMeasure[2] = \frac{1}{(4\pi)^2} \frac{\abs{\vv{p}}}{m} \dd{\Omega}.
    \end{equation}
    Thus in the centre of mass frame the decay rate into some solid angle is
    \begin{equation}
        \left( \diff{\Gamma}{\Omega} \right)_{\text{CoM}} = \frac{1}{32\pi^2} \frac{\abs{\vv{p}}}{m^2} \abs{\amplitude}^2.
    \end{equation}
    
    \section{Cross Sections}
    When we consider collisions there is almost no point in considering collisions with three or more particles, since these are so unlikely to occur.
    So, consider a scattering of two particles, \(p_1\) and \(p_2\).
    We can work in the lab frame, where, say, the second particle is stationary, so
    \begin{equation}
        p_1 = (\omega(\vv{p_1}), \vv{p_1}), \qqand p_2 = (m_2, \vv{0}).
    \end{equation}
    We can also write \(E_1 = \omega(\vv{p_1})\).
    
    The incident flux, that is the number of particles crossing unit area per unit time, is given by the volume of particles which would cross through said area in unit time times the density of particles.
    The volume is simply \(\abs{\vv{v}}\), where \(\vv{v}\) is the velocity of the particle, since in one unit time the particles will have moved a distance \(\abs{\vv{v}}\) forward.
    The density of particles is \(2E_1\), so the incident flux is \(2E_1\abs{\vv{v}} = 2\abs{\vv{p_1}}\), where we've used \(\vv{p} = \gamma m\vv{v}\) and \(E = \gamma m\), so \(\abs{\vv{v}} = \abs{\vv{p}}/E\) as expected.
    
    The number of scattering centres in the target per unit volume is the density of particle states, which is \(2E_2 = 2m_2\).
    
    The differential cross section is defined as the transition rate per scattering centre per unit incident flux.
    The idea being that the number of scattering centres and the incident flux are both dependent on the experiment, and the differential cross section is independent of experimental details.
    The differential cross section is then
    \begin{equation}
        \dl{\sigma} = (2\pi)^4 \delta^4\left( \sum p_i - \sum p_f \right) \frac{1}{4\abs{\vv{p_1}}m_2} \left( \prod_f \frac{\dl{^3\vv{p_f}}}{(2\pi)^32E_f} \right) \abs{\amplitude}^2.
    \end{equation}
    
    Now we can use \((p_1 \cdot p_2)^2 = (E_1m_2)^2 = (\vv{p_1} + m_1)^2 m_2^2\) which gives \(\abs{\vv{p_1}}^2m_2^2 = (p_1\cdot p_2)^2 - m_1^2 m_2^2\), which is Lorentz invariant.
    This means that we can write the result in a frame independent way as
    \begin{equation}
        \dl{\sigma} = \frac{\abs{\amplitude}^2}{4\sqrt{(p_1 \cdot p_2)^2 - m_1^2 m_2^2}} \phaseSpaceMeasure.
    \end{equation}
    
    \subsection{Two to Two Scattering}
    Consider the case where we have two incoming and two outgoing particles, \(p_1 + p_2 \to p_3 + p_4\).
    In the centre of mass frame we have
    \begin{equation}
        \int \phaseSpaceMeasure[2] = \frac{1}{(4\pi)^2} \frac{p'}{W}
    \end{equation}
    where \(p' = \abs{\vv{p_3}} = \abs{\vv{p_4}}\), where equality between the magnitude of the momenta of the particles follows by conservation of momentum in the centre of mass frame, meaning \(\vv{p_1} + \vv{p_2} = \vv{0}\), so \(\vv{p_3} + \vv{p_4} = \vv{0}\) also.
    As well, \(W = E_1 + E_2 = E_3 + E_4\), where the second equality is just conservation of energy.
    
    In the centre of mass frame we have
    \begin{equation}
        p_1 = (E_1, \vv{p}), \qqand p_2 = (E_2, -\vv{p}).
    \end{equation}
    Define \([ = \abs{\vv{p}}]\), and so
    \begin{equation}
        \vv{p_1} \cdot \vv{p_2} = E_1E_2 + \vv{p}^2 \implies (p_1 \cdot p_2)^2 - m_1^2 m_2^2 = p^2W^2,
    \end{equation}
    where we've used the on-shell requirement to show this.
    
    Hence, the cross section for scattering per unit solid angle is
    \begin{equation}
        \left( \diff{\sigma}{\Omega} \right)_{\text{CoM}} = \frac{1}{64\pi^2} \frac{1}{W^2} \left( \frac{p'}{p} \right)^2 \abs{\amplitude}^2.
    \end{equation}
    Note that if all particles have the same mass then \(p' = p\) and so often we see this equation without the \(p'/p\) factor.
    This is also a valid approximation in the high energy case where the masses provide a negligible contribution to the energy-momentum relation.
    
    \subsubsection{In \texorpdfstring{\(\varphi^3\)}{Phi Cubed} Theory}
    In \(\varphi^3\) theory the two to two scattering gives us the amplitude
    \begin{equation}
        \amplitude = (-ig^2) \left[ \frac{1}{s - m^2} + \frac{1}{t - m^2} + \frac{1}{u - m^2} \right].
    \end{equation}
    Here \(s = W^2\).
    Hence
    \begin{equation}
        \left( \diff{\sigma}{\Omega} \right)_{\text{CoM}} = \frac{1}{64\pi^2} \frac{g^4}{s} \abs{\frac{1}{s - m^2} + \frac{1}{t - m^2} + \frac{1}{u - m^2}}^2.
    \end{equation}
    
    Now consider high energy fixed angle scattering.
    Then \(\abs{\vv{p}}^2 \gg m^2\), so \(\abs{\vv{p}}^2 \approx E^2\).
    We also have \(s = 4E^2 \approx 4\vv{p}^2\), \(t \approx -2E^2(1 - \cos\vartheta)\) and \(u \approx -2E^2(1 + \cos\vartheta)\).
    This means that the ratios \(s/u\) and \(s/t\) are fixed since \(\vartheta\) is fixed and the factors of \(E^2\) cancel.
    The angular distribution can be written as
    \begin{equation}
        \dl{\Omega} = 2\pi \dd{\cos \vartheta},
    \end{equation}
    and so
    \begin{equation}
        \left( \diff{\sigma}{(\cos \vartheta)} \right)_{\text{CoM}} = \frac{1}{32\pi} \frac{g^4}{s^3}\left( 1 + \frac{s}{t} + \frac{s}{u} \right)^2,
    \end{equation}
    where we get an extra factor of \(2\pi\) from the \(\int_0^{2\pi}\dl{\varphi}\) integral and the term in brackets follows from neglecting the masses and some trig identities.
    
    Note that dimensionally this all works out since the action, \(S\), has dimensions of \([S] = [\text{length}]^4[g\varphi^3]\), which follows from the action being the spacetime integral of \(-g\varphi^3/4!\), and so since action is dimensionless (when \(\hbar = 1\)) and length and energy have inverse units of each other (when \(c = 1\) also) we have \([g\varphi^3] = [\text{energy}]^4\).
    We know that \([(\partial \varphi)^2] = [\text{energy}]^4\), and so we have \([\varphi] = [\text{energy}]\), since \([\partial] = [\text{length}]^{-1} = [\text{energy}]\).
    Hence, we must have that \([g] = [\text{energy}]\) and so this entire quantity has units of \([g^4]/[s^3] = [\text{energy}]^4/([\text{momentum}]^2)^3 = [\text{momentum}]^{-2}\), since \([\text{energy}] = [\text{momentum}]\) (when \(c = 1\)).
    This tells us that the cross section drops off inversely proportionally to the momentum squared, so at high momentums interactions are much less likely, which is what we would expect, high momentum particles move past each other very quickly without much of a chance for interactions.
    
    \part{Complex Scalar Fields}
    \chapter{Charge}
    Classical fields, such as the electromagnetic field, are always real since they are observables.
    Wave functions on the other hand are always complex, since their time evolution, \(\ket{\psi, t} = \e^{iEt}\ket{\psi}\), demands it.
    In quantum field theory fields can be either real or complex, since they are not observable.
    However, the action, Lagrangian, and Hamiltonian must be real, this is needed to ensure that the \(S\) matrix is unitary, which is required for conservation of probability.
    Also, the Hamiltonian corresponds to the energy of the system which is observable, and hence real.
    
    \section{Multiple Real Fields}
    Consider the common definition of complex numbers which starts something like \enquote{a complex number is an ordered pair of real numbers with operations of \dots}.
    Much like this definition a complex field can be thought of as a pair of real fields.
    So, we need to be able to deal with multiple fields.
    Fortunately our work so far generalises very well to multiple real fields, we just have to sum over fields in certain expressions.
    
    Suppose we have \(N\) real scalar fields, \(\varphi_r\), with \(r = 1, \dotsc, N\).
    For simplicity, and because it's all we need for the complex case, we assume that all \(N\) fields correspond to particles of mass \(m\).
    The Lagrangian is then
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi_r(x))(\partial^\mu \varphi_r(x)) - m^2\varphi_r(x)^2,
    \end{equation}
    where we have an implied sum over \(r\).
    We can then define the conjugate fields, and we find
    \begin{equation}
        \pi_r(x) \coloneqq \diffp{\lagrangianDensity}{\dot{\varphi}_r(x)} = \dot{\varphi}_r(x).
    \end{equation}
    Any two distinct fields commute, and for two of the same field we have the usual commutation relations.
    That is, we have the equal time commutation relation
    \begin{equation}
        \commutator{\varphi_r(t, \vv{x})}{\pi_s(t, \vv{x}')} = i\delta_{rs} \delta(\vv{x} - \vv{x}'),
    \end{equation}
    and all other commutators vanish.
    
    The mode expansion is the same as for a single field, but we have independent creation and annihilation operators for each field:
    \begin{equation}
        \varphi_r(x) = \int \frac{\dl{^3\vv[p]}}{(2\pi)^3} \frac{1}{2\pi(\vv{p})} [a_r(\vv{p})\e^{-ip\cdot x} + a_r^\hermit(\vv{p})\e^{ip\cdot x}].
    \end{equation}
    These have the covariant commutation relations
    \begin{equation}
        \commutator{a_r(\vv{p})}{a_s^\hermit(\vv{p}')} = \delta_{rs} 2\omega(\vv{p}) (2\pi)^3\delta(\vv{p} - \vv{p}'),
    \end{equation}
    and all other commutators vanish.
    
    From here we proceed as before, we can define the energy-momentum tensor,
    \begin{equation}
        T^{\mu\nu}(x) \coloneqq \normalordering{(\partial^\mu \varphi_r(x))(\partial^\nu \varphi_r(x)) - \minkowskiMetric^{\mu\nu}\lagrangianDensity(x)},
    \end{equation}
    with an implied sum over \(r\).
    We then get the conserved momentum
    \begin{equation}
        P^\nu = \int \dl{^3x} \, T^{0\nu}(x),
    \end{equation}
    and the conserved current
    \begin{equation}
        j^\mu = \diffp{\lagrangianDensity}{(\partial_\mu \varphi_r)}\delta \varphi_r,
    \end{equation}
    with an implied sum over \(r\).
    
    The interpretation of \(a_r^\hermit(\vv{p})\) and \(a_r(\vv{p})\) as creation and annihilation operators is the same, each creating/annihilating particles of type \(r\).
    This follows since we have
    \begin{equation}
        \commutator{H}{a_r^\hermit(\vv{p})} = Ea_r^\hermit(\vv{p}), \qqand \commutator{H}{a_r(\vv{p})} = -Ea_r(\vv{p}).
    \end{equation}
    So, \(a_r^\hermit(\vv{p})\) creates a particle of type \(r\) and momentum \(\vv{p}\), which we might write as
    \begin{equation}
        a_r^\hermit(\vv{p}) \ket{0} = \ket{\vv{p}, r}.
    \end{equation}
    
    The only time when we \emph{don't} sum over the fields is when defining the number operator, which we want to count particles of type \(r\), not the total number of particles, so we define
    \begin{equation}
        N_r(\vv{p}) \coloneqq a_r^\hermit(\vv{p}) a_r(\vv{p}) \qquad \text{(no sum on \(r\)).}
    \end{equation}
    
    \section{Complex Scalar Field}
    Now, consider a complex scalar field, \(\varphi\).
    We can write this as a sum of two real fields, \(\varphi_1\) and \(\varphi_2\), one multiplied by \(i\):
    \begin{equation}
        \varphi(x) = \frac{1}{\sqrt{2}} [\varphi_1(x) + i\varphi_2(x)].
    \end{equation}
    The factor of \(1/\sqrt{2}\) here is so that the free Lagrangian comes out as the sum of two free Lagrangians for the real fields \(\varphi_1\) and \(\varphi_2\).
    
    Motivated by the need for the Lagrangian to be real we take the usual real scalar field Lagrangian and replace one of every pair of \(\varphi\)s with \(\varphi^\hermit\).
    We then treat \(\varphi\) and \(\varphi^\hermit\) as two independent fields, since from these we have
    \begin{equation}
        \varphi_1 = \frac{1}{\sqrt{2}}(\varphi + \varphi^\hermit), \qand i\varphi_2 = \frac{1}{\sqrt{2}}(\varphi - \varphi^\hermit),
    \end{equation}
    and we assume \(\varphi_1\) and \(\varphi_2\) are independent.
    Doing so we see that if we also drop the factor of \(2\) then we get
    \begin{align}
        \lagrangianDensity &= (\partial_\mu \varphi^\hermit)(\partial^\mu \varphi) - m^2\varphi^\hermit \varphi\\
        &= \frac{1}{2}(\partial_\mu \varphi_i)(\partial^\mu \varphi_i) - \frac{1}{2}m^2\varphi_i^2,
    \end{align}
    with an implied sum over \(i\).
    Note that we assume both fields have particles of the same mass, since otherwise we cannot guarantee that the Lagrangian will be real.
    
    We can define the conjugate field
    \begin{equation}
        \pi \coloneqq \diffp{\lagrangianDensity}{\dot{\varphi}} = \dot{\varphi}^\hermit, \qqand \pi^\hermit = \diffp{\lagrangianDensity}{\dot{\varphi}^\hermit} = \dot{\varphi}.
    \end{equation}
    We then have the equal time commutation relations
    \begin{equation}
        \commutator{\varphi(t, \vv{x})}{\pi(t, \vv{x}')} = \commutator{\varphi^\hermit(t, \vv{x})}{\pi^\hermit(t, \vv{x}')} = i\delta^3(\vv{x} - \vv{x}'),
    \end{equation}
    and all other commutators vanish.
    
    We can define the Hamiltonian as before:
    \begin{align}
        H &= \int \dl{^3\vv{x}} \, (\pi\dot{\varphi} + \pi^\hermit \dot{\varphi}^\hermit - \lagrangianDensity)\\
        &= \int \dl{^3\vv{x}} \, (\pi^\hermit \pi + (\grad \varphi^\hermit) \cdot (\grad \varphi) + m^2 \varphi^\hermit \varphi).
    \end{align}
    This second form makes it clear that this is real.
    
    Computing the Heisenberg equations of motion gives
    \begin{equation}
        (\dalembertian + m^2)\varphi = 0, \qqand (\dalembertian + m^2)\varphi^\hermit = 0.
    \end{equation}
    
    \section{Mode Expansion}\label{sec:mode expansion complex scalar field}
    The mode expansion is similar to the real case, except, in the real case we needed \(a\) and \(a^\hermit\) to give a real field.
    We don't need a real field here, but we do require a second term since this then causes things to cancel when the canonical commutation relations are evaluated outside of the light cone, which enforces causality.
    As a result we still include two terms in the mode expansion, but we no longer require one to be the Hermitian conjugate of the other, so we introduce some other operator, \(b^\hermit\), to replace \(a^\hermit\), with the conjugate for consistency with the real case:
    \begin{equation}
        \varphi(x) = \int \invariantmeasure{p} \, [a(\vv{p}) \e^{-ip\cdot x} + b^\hermit(\vv{p}) \e^{ip \cdot x}].
    \end{equation}
    We then have
    \begin{equation}
        \varphi^\hermit(x) = \int \invariantmeasure{p} \, [b(\vv{p}) \e^{-ip\cdot x} + a^\hermit(\vv{p})\e^{ip\cdot x}].
    \end{equation}
    
    We then have the equal time commutation relations
    \begin{equation}
        \commutator{a(\vv{p})}{a^\hermit(\vv{p})} = \commutator{b(\vv{p})}{b^\hermit(\vv{p})} = \bardelta(\vv{p} - \vv{p}'),
    \end{equation}
    and the two independent mode operators commute:
    \begin{equation}
        \commutator{a(\vv{p})}{b(\vv{p})} = \commutator{a^\hermit(\vv{p})}{b^\hermit(\vv{p})} = 0,
    \end{equation}
    as we as all other commutators vanishing.
    
    We can then show that
    \begin{alignat}{2}
        \commutator{H}{a^\hermit(\vv{p})} &= Ea^\hermit \qquad & \commutator{H}{a(\vv{p})} = -Ea(\vv{p}),\\
        \commutator{H}{b^\hermit(\vv{p})} &= Eb^\hermit \qquad & \commutator{H}{b(\vv{p})} = -Eb(\vv{p}).
    \end{alignat}
    Recall that a real scalar field can be interpreted as an infinite set of harmonic oscillators, one for each momentum, \(\vv{p}\).
    We can then interpret the complex  scalar field as two independent sets of harmonic oscillators, with two for each momentum, \(\vv{p}\).
    This leads to the interpretation of \(a^\hermit\) and \(b^\hermit\) as creating independent particles.
    These particles are bosons, both with respect to exchanging particles made by the same mode operator, so, for example, both created by \(a^\hermit\), and with respect to exchanging particles made by different mode operators, so for example, exchanging a particle created by \(a^\hermit\) with a particle created by \(b^\hermit\).
    
    The energy-momentum operator is
    \begin{equation}
        T^{\mu\nu} = \normalordering{(\partial^\mu\varphi^\hermit)(\partial^\nu \varphi) + (\partial^\nu \varphi^\hermit) (\partial^\mu \varphi) - \minkowskiMetric^{\mu\nu}\lagrangianDensity}.
    \end{equation}
    The conserved momentum is
    \begin{equation*}
        P^\nu = \int \dl{^3\vv{x}} \, T^{0\nu} = \int \invariantmeasure{p} \, p^\nu(a^\hermit(\vv{p})a(\vv{p}) + b^\hermit(\vv{p})b(\vv{p})) = \int \invariantmeasure{p} \, p^\nu (N_a(\vv{p}) + N_b(\vv{p})).
    \end{equation*}
    This means it is the momentum of the combined two sets of particles which is conserved, not the momentum of each set individually.
    
    \section{Charge Conservation}
    There must be something that sets apart particles created by \(a^\hermit\) and particles created by \(b^\hermit\).
    In this section we work out what this is.
    Consider the Lagrangian again,
    \begin{equation}
        \lagrangianDensity = (\partial_\mu\varphi^\hermit)(\partial^\mu\varphi) - m^2\varphi^\hermit \varphi.
    \end{equation}
    Notice that this is invariant under the transformation
    \begin{equation}
        \varphi \mapsto \e^{i\alpha}\varphi
    \end{equation}
    for some constant \(\alpha\).
    Notice that \(\alpha\) cannot depend on \(x\), since then the derivatives would cause problems.
    Under this transformation we have \(\varphi^\hermit \mapsto \e^{-i\alpha}\varphi^\hermit\) as well.
    This is a \(\unitary(1)\) symmetry\footnote{see the \course{Symmetries of Quantum Mechanics} or \course{Symmetries of Particles and Fields} course for details}.
    This is an internal symmetry, acting on the fields, rather than on spacetime.
    
    A symmetry implies a conserved quantity, so lets find out what it is\footnote{If the title of this section hasn't already given it away!}.
    Assume \(\alpha \ll 1\), so we have \(\exp[i\alpha] \approx 1 + i\alpha\), and hence
    \begin{equation}
        \varphi \mapsto \e^{i\alpha}\varphi \approx (1 + i\alpha)\varphi = \varphi + i\alpha \varphi,
    \end{equation}
    and hence
    \begin{equation}
        \delta \varphi = \e^{i\alpha}\varphi - \varphi \approx i\alpha\varphi.
    \end{equation}
    Similarly,
    \begin{equation}
        \delta \varphi^\hermit \approx -i\alpha\varphi.
    \end{equation}
    We then have
    \begin{equation}
        \diffp{\lagrangianDensity}{(\partial_\mu\varphi)} \delta \varphi = i\alpha(\partial^\mu\varphi^\hermit)\varphi, \qqand \diffp{\lagrangianDensity}{(\partial_\mu\varphi^\hermit)}\delta\varphi^\hermit = -i\alpha\varphi^\hermit(\partial^\mu\varphi).
    \end{equation}
    The conserved current is then, normal ordering to avoid later issues\footnote{actually, normal ordering isn't strictly necessary here, but it is in other similar cases so we'll do so here for consistency},
    \begin{align}
        j^\mu &= \normalordering{\diffp{\lagrangianDensity}{(\partial_\mu\varphi)}\delta\varphi + \diffp{\lagrangianDensity}{(\partial_\mu\varphi^\hermit)}\delta\varphi^\hermit}\\
        &= \normalordering{i\alpha(\partial^\mu\varphi^\hermit)\varphi - i\alpha\varphi^\hermit(\partial^\mu\varphi)}.
    \end{align}
    We can rescale this by a factor of \(-\alpha\) to get:
    \begin{equation}
        j^\mu = i\normalordering{\varphi^\hermit (\partial^\mu\varphi) - (\partial^\mu\varphi^\hermit)\varphi}
    \end{equation}
    Notice that we can easily show this is conserved, that is that \(\partial_\mu j^\mu = 0\):
    \begin{align}
        \partial_\mu j^\mu &= i\normalordering{[(\partial_\mu\varphi^\hermit)(\partial^\mu\varphi) + \varphi^\hermit(\dalembertian\varphi) - (\dalembertian\varphi^\hermit)\varphi - (\partial^\mu\varphi^\hermit)(\partial_\mu\varphi)]}\\
        &= i\normalordering{[\varphi^\hermit(\dalembertian\varphi) - (\dalembertian\varphi^\hermit)\varphi]}.
    \end{align}
    The equations of motion for the fields give \(\dalembertian\varphi = -m^2\varphi\) and \(\dalembertian\varphi^\hermit = -m^2\varphi^\hermit\), so
    \begin{equation}
        \partial_\mu j^\mu = i\normalordering{[\varphi^\hermit(-m^2\varphi) - (-m^2\varphi^\hermit)\varphi]} = 0,
    \end{equation}
    where we've used the normal ordering to allow us to swap the order of \(\varphi\) and \(\varphi^\hermit\).
    
    Along with this conserved current we have a conserved charge:
    \begin{equation}
        Q = \int \dl{^3\vv{x}} \, j^0 = i\int \dl{^3\vv{x}} \, \normalordering{(\varphi^\hermit \dot{\varphi} - \dot{\varphi}^\hermit \varphi)}.
    \end{equation}
    We can write \(\varphi\) and \(\varphi^\hermit\) in terms of mode operators, with the time derivatives bringing down a factor of \(\pm i\omega(\vv{p})\), giving
    \begin{align}
        Q &= i \int \dl{^3\vv{x}} \int \invariantmeasure{p} \int \invariantmeasure{p'}\\
        &\quad\quad \Big\{ \normalordering{[a^\hermit(\vv{p}) \e^{ip \cdot x} + b\e^{-ip \cdot x}]i\omega(\vv{p}')[-a(\vv{p}')\e^{-ip'\cdot x} + b^\hermit(\vv{p}')\e^{ip'\cdot x}]} \notag\\
        &\qquad\quad -i\omega(\vv{p})\normalordering{[a^\hermit(\vv{p})\e^{ip\cdot x} - b(\vv{p})\e^{-ip\cdot x}] [a(\vv{p}')\e^{-ip'\cdot x} + b^\hermit(\vv{p}')\e^{ip'\cdot x}]} \Big\}.\notag
    \end{align}
    After expanding this, integrating over \(x\) to get Dirac deltas, and then using these to perform the \(p'\) integral we're left with
    \begin{align}
        Q &= \int \invariantmeasure{p} \normalordering{(a^\hermit(\vv{p})a(\vv{p}) - b(\vv{p}) b^\hermit(\vv{p}))}\\
        &= \int \invariantmeasure{p} (a^\hermit(\vv{p}) a(\vv{p}) - b^\hermit(\vv{p})b(\vv{p})).
    \end{align}
    
    From this we can then show that the following commutation relations hold:
    \begin{alignat}{2}
        \commutator{Q}{a^\hermit(\vv{p})} &= \hphantom{-}a^\hermit \qquad & \commutator{Q}{a(\vv{p})} = -a(\vv{p}),\\
        \commutator{Q}{b^\hermit(\vv{p})} &= -b^\hermit \qquad & \commutator{Q}{b(\vv{p})} = \hphantom{-}b(\vv{p}).
    \end{alignat}
    So finally we have a difference between \(a\) and \(b\).
    This leads to the interpretation that
    \begin{itemize}
        \item \(a^\hermit(\vv{p})\) creates a particle with momentum \(\vv{p}\) and \enquote{charge} \(+1\),
        \item \(b^\hermit(\vv{p})\) creates a particle with momentum \(\vv{p}\) and \enquote{charge} \(-1\),
        \item \(a(\vv{p})\) annihilates a particle with momentum \(\vv{p}\) and \enquote{charge} \(+1\),
        \item \(b(\vv{p})\) annihilates a particle with momentum \(\vv{p}\) and \enquote{charge} \(-1\).
    \end{itemize}
    Here we write \enquote{charge}, since this may not be electrical charge, but it can be.
    
    So, a complex scalar field gives a theory with a pair of charged particles.
    Conservation of \(Q\) implies that particles must always be created and destroyed in pairs.
    This all follows simply from recognising that the Lagrangian must be real and so must depend only on products like \(\varphi^\hermit \varphi\).
    
    There is another interpretation of this, which is that \(a^\hermit\) creates particles, and \(b^\hermit\) creates antiparticles (or vice versa).
    An antiparticle being a particle with the same mass but opposite charge.
    Then
    \begin{itemize}
        \item \(a^\hermit(\vv{p})\) creates a particle with momentum \(\vv{p}\) and \enquote{charge} \(+1\),
        \item \(b^\hermit(\vv{p})\) creates an antiparticle with momentum \(\vv{p}\) and \enquote{charge} \(-1\),
        \item \(a(\vv{p})\) annihilates a particle with momentum \(\vv{p}\) and \enquote{charge} \(+1\),
        \item \(b(\vv{p})\) annihilates an antiparticle with momentum \(\vv{p}\) and \enquote{charge} \(-1\).
    \end{itemize}
    Notice that destroying a particle of charge \(+1\) is the same as creating a particle of charge \(-1\) and vice versa.
    
    For a real field we have \(a = b\) and we can interpret this as the statement that the particles in this theory are their own antiparticles.
    
    \section{Feynman Rules}
    \epigraph{It's useful for you to get used to confusing notation in this course, because it will get a whole lot worse.}{Richard Ball}
    Consider the mode expansions of the fields:
    \begin{align}
        \varphi &= \int \invariantmeasure{p} [a(\vv{p})\e^{-ip\cdot x} + b^\hermit(\vv{p})\e^{ip\cdot x}],\\
        \varphi^\hermit &= \int \invariantmeasure{p} [b(\vv{p})\e^{-ip\cdot x} + a^\hermit(\vv{p})\e^{ip\cdot x}].
    \end{align}
    Using these and the commutation relations for the mode operators we can derive the covariant commutator
    \begin{equation}
        \commutator{\varphi(x)}{\varphi^\hermit(x')} = i\Delta(x - x')
    \end{equation}
    where \(\Delta\) is the same function as for real scalar fields.
    The other commutators, \(\commutator{\varphi(x)}{\varphi(x')}\) and \(\commutator{\varphi^\hermit(x)}{\varphi^\hermit(x)}\), vanish.
    
    A result of this vanishing of of commutators between \(\varphi\)s or between \(\varphi^\hermit\)s is that contractions between two \(\varphi\)s or two \(\varphi^\hermit\)s also vanish, and so the only nonvanishing contractions are
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi^\hermit(x')} = \bra{0} \timeOrdering[\varphi(x) \varphi^\hermit(x')] \ket{0} = i\Delta_{\feynman}(x - x'),
    \end{equation}
    where \(\Delta_{\feynman}\) is the same as for a real scalar field.
    
    As before our state space is given by direct products of single particle states, which are now labelled with the charge as well as the momentum of the particle:
    \begin{equation}
        \ket{\vv{p}, +} = a^\hermit(\vv{p})\ket{0}, \qqand \ket{\vv{p}, -} = b^\hermit(\vv{p})\ket{0}.
    \end{equation}
    Writing \(\varphi = \varphi_+ + \varphi_-\) as before\footnote{We lower the \(+\) and \(-\) labels to make room for \(\hermit\).} we have
    \begin{alignat}{2}
        \varphi_+(x)\ket{\vv{p}, +} &= \e^{-ip\cdot x} \ket{0} \qquad & \varphi_+^\hermit(x)\ket{\vv{p}, -} &= \e^{-ip\cdot x}\ket{0},\\
        \bra{\vv{p}, +}\varphi_-^\hermit(x) &= \e^{ip\cdot x} \bra{0} \qquad & \bra{\vv{p}, -} \varphi_-(x) &= \e^{ip\cdot x}\bra{0}.
    \end{alignat}
    
    So, we conclude that
    \begin{itemize}
        \item \(\varphi_+\) destroys an incoming particle of charge \(+1\),
        \item \(\varphi_+^\hermit\) destroys an incoming particle of charge \(-1\),
        \item \(\varphi_-\) creates an outgoing particle of charge \(-1\),
        \item \(\varphi_-^\hermit\) creates an outgoing particle of charge \(+1\).
    \end{itemize}
    From this we can compute all the Feynman rules for any given interaction.
    
    \chapter{Interactions and Symmetries}
    \section{Types of Interaction}
    We choose to look for interactions which conserve charge.
    While we can easily construct theories where this isn't the case charge conservation is a common phenomena and we wish our theories to exhibit it.
    This means we can no longer have a \(\varphi^3\) interaction, since this would not conserve charge.
    Even if we consider a \(\varphi^3 + (\varphi^\hermit)^3\) interaction, so the Lagrangian is real, we still don't get charge conservation, since the first term transforms as \(\varphi^3 \mapsto \e^{3i\alpha}\varphi^3\), and the second as \((\varphi^{\hermit})^3 \mapsto \e^{-3i\alpha}(\varphi^\hermit)^3\).
    The only way to have a theory conserve charge is if every term in the Lagrangian has as many factors of \(\varphi\) as it does \(\varphi^\hermit\).
    There are a few ways to do this.
    
    \subsection{Four Point Interactions}
    Instead of the three point interaction, \(\varphi^3\), we can consider a four point interaction, \((\varphi^\hermit\varphi)^2\), where interactions involve four particles at once, this would lead to the interaction Lagrangian
    \begin{equation}
        \lagrangianDensity_{\interaction}^{(4)} = -\frac{\lambda}{2}(\varphi^\hermit \varphi)^2.
    \end{equation}
    To keep track of charge in our Feynman diagrams it is conventional to put an arrow on our Feynman diagrams pointing in the direction of charge flow.
    Note that charge flowing backwards in time is the same as negative charge flowing forward in time, that is antiparticles correspond to arrows pointing left and particles to arrows pointing right.
    
    There are three vertices in this theory which are all slightly different, but have the same shape.
    The first is
    \vspace{2.4cm}
    \begin{equation}
        \smash{\rotatebox{45}{
                \tikzsetnextfilename{fd-charge-4-point-interaction-neutral}
                \feynmandiagram[inline=(v)]{
                i1 -- [anti fermion] v -- [fermion] i2,
                o1 -- [fermion] v -- [anti fermion] o2
            };}}
    \end{equation}
    This corresponds to a particle and antiparticle coming in and a particle and antiparticle going out.
    The net charge in and out is then 0.
    So charge is conserved in this vertex.
    
    The next vertex is
    \vspace{2.4cm}
    \begin{equation}
        \smash{\rotatebox{45}{
                \tikzsetnextfilename{fd-charge-4-point-interaction-positive}
                \feynmandiagram[inline=(v)]{
                i1 -- [fermion] v -- [fermion] i2,
                o1 -- [fermion] v -- [fermion] o2
            };}}
    \end{equation}
    This corresponds to two particles coming in and two particles going out.
    The net charge in and out is then 2, so charge is conserved in this vertex.
    
    The final vertex is
    \vspace{2.4cm}
    \begin{equation}
        \smash{\rotatebox{45}{
                \tikzsetnextfilename{fd-charge-4-point-interaction-negative}
                \feynmandiagram[inline=(v)]{
                i1 -- [anti fermion] v -- [anti fermion] i2,
                o1 -- [anti fermion] v -- [anti fermion] o2
            };}}
    \end{equation}
    This corresponds to two antiparticles coming in and two antiparticles going out.
    The net charge in and out is then \(-2\), so charge is conserved in this vertex.
    
    As with the real case the constant factor in this interaction has been chosen such that every vertex gives a factor of \(-i\lambda\).
    
    \subsection{Three Point Interactions}
    If we introduce a new, neutral, real scalar field, \(\Phi\), then we can have a three point interaction of the form
    \begin{equation}
        \lagrangianDensity_{\interaction}^{(3)} = -y\Phi \varphi^\hermit \varphi.
    \end{equation}
    This is known as a \defineindex{Yukawa interaction}.
    
    As with \(\varphi^3\) theory we get three particles in each interaction.
    There are a variety of vertices we can create in this case.
    Denoting the neutral particle with a dashed line one possibility is
    \begin{equation}
        \tikzsetnextfilename{fd-charge-3-point-interaction-annihilation}
        \feynmandiagram[inline=(v), horizontal=v to o]{
            i1 -- [fermion] v,
            i2 -- [anti fermion] v,
            v -- [scalar] o
        };
    \end{equation}
    This corresponds to a particle and antiparticle annihilating into a neutral particle.
    
    Another vertex is
    \begin{equation}
        \tikzsetnextfilename{fd-charge-3-point-interaction-particle-emit}
        \feynmandiagram[inline=(v), horizontal=i to v]{
            i -- [fermion] v,
            v -- [scalar] o1,
            v -- [fermion] o2
        };
    \end{equation}
    This corresponds to a particle emitting a neutral particle.
    Similarly the vertex 
    \begin{equation}
        \tikzsetnextfilename{fd-charge-3-point-interaction-antiparticle-emit}
        \feynmandiagram[inline=(v), horizontal=i to v]{
            i -- [anti fermion] v,
            v -- [scalar] o1,
            v -- [anti fermion] o2
        };
    \end{equation}
    corresponds to an antiparticle emitting a neutral particle.
    Notice that in all of these diagrams the total charge is conserved at each vertex.
    
    As before the constant factors are chosen such that each vertex contributes a factor of \(-iy\) to the amplitude.
    
    A full Lagrangian in a theory with this interaction may take the form
    \begin{equation}
        \lagrangianDensity = (\partial_\mu\varphi^\hermit)(\partial_\mu\varphi) - m^2\varphi^\hermit \varphi + \frac{1}{2}(\partial_\mu \Phi)(\partial^\mu \Phi) - \frac{1}{2}M^2\Phi^2 - y\Phi \varphi^\hermit \varphi,
    \end{equation}
    corresponding to a neutral particle, call it \(s_0\), of mass \(M\) and charged (anti)particles, call them \(s_{\pm}\), of mass \(m\).
    
    In many ways this theory is not that different from the real \(\varphi^3\) theory.
    For example, we still have 2--2 scattering, such as \(s_+s_- \to s_+s_-\), \(s_+s_+ \to s_+s_+\), and \(s_-s_- \to s_-s_-\).
    The exchange particle in these cases is a neutral scalar, \(s_0\).
    Note that in the \(s_+s_- \to s_+s_-\) case, since the final two particles are distinct, there is no \(u\)-channel diagram.
    Similarly for \(s_{\pm}s_{\pm} \to s_{\pm}s_{\pm}\) there is no \(s\)-channel diagram, since the intermediate particle would have to carry a charge of \(\pm2\).
    
    There is one sort of interaction which we don't have in \(\varphi^3\) theory, and that is decays.
    If \(M > 2m\) then it is possible to have the interaction \(s_0 \to s_+s_-\).
    This would correspond to an \(n = 1\) contribution to the Dyson series, and these terms vanished for \(\varphi^3\) theory.
    This would look like
    \begin{equation}
        \tikzsetnextfilename{fd-neutral-decay-to-charged-particles}
        \feynmandiagram[inline=(v), horizontal=i to v]{
            i -- [scalar] v,
            v -- [fermion] o1,
            v -- [anti fermion] o2
        };
    \end{equation}
    The amplitude for this is \(\amplitude = -iy\).
    We can work out the decay rate starting with
    \begin{equation}
        \left( \diff{\Gamma}{\Omega} \right)_{\text{CoM}} = \frac{1}{32\pi^2} \frac{p}{M^2} \abs{\amplitude}^2,
    \end{equation}
    where \(p\) is the magnitude of the three-momentum of the two charged particles in the centre of mass frame.
    The amplitude squared will give \(\abs{\amplitude}^2 = \abs{-iy}^2 = y^2\).
    % TODO: Kinematics, where does p^2 = M^2 - 4m^2 come from?
%    In this frame the neutral particle has four-momentum \(p^{(0)} = (M, \vv{0})\), and the two charged particles have four-momenta \(p^{(+)} = (E, \vv{p})\) and \(p^{(-)} = (E, -\vv{p})\).
%    Note that they must have the same energy as, apart from charge, the decay is symmetric in the two particles.
%    Hence, we have
%    \begin{equation}
%        M^2 = (p^{(0)})^2 = (p^{(+)} + p^{(-)})^2 = 2m^2 - 2(E^2 + p^2).
%    \end{equation}
    We then have
    \begin{equation}
        \left( \diff{\Gamma}{\Omega} \right)_{\text{CoM}} = \frac{y^2}{64\pi^2} \frac{\sqrt{M^2 - 4m^2}}{M^2}.
    \end{equation}
    So we see that we must have \(M > 2m\) for this to be nonzero and real, assuming \(M, m \ge 0\).
    
    We just said \enquote{assuming \(M, m \ge 0\)}, but what if they aren't?
    In fact, what if they aren't even real?
    We can analytically extend this decay rate as a function of the complex variable \(M\).
    Recall that \(\sqrt{f(z)}\) has a branch cut from \(f(z) = 0\) to infinity.
    So the decay rate has a branch cut from \(M = 2m\) to infinity.
    This is part of a general rule, particles correspond to poles, such as the pole at \(m = \sqrt{s}\) in \(1/(s - m^2)\), and decays correspond to branch cuts, such as the one starting at \(M = 2m\) here.
    
    \section{Symmetries}
    In this section we'll study the effect of parity, time reversal, and charge conjugation on scalar fields.
    We'll start from their action on spacetime, and then derive their action on the fields and mode operators.
    These three symmetries are all \(\integers_2\) symmetries, meaning that, apart from doing nothing, we have a single operation and applying this operation twice is the same as doing nothing.
    The technical word for this is that these symmetries are \defineindex{idempotent}.
    
    \subsection{Parity}
    Parity acts on spacetime by sending \(\vv{x}\) to \(-\vv{x}\).
    That is
    \begin{equation}
        \parity \colon x^\mu = (t, \vv{x}) \mapsto \overbar{x}^\mu = (t, -\vv{x}).
    \end{equation}
    Since the direction of space is also changed the momentum is also changed by sending \(\vv{p}\) to \(-\vv{p}\):
    \begin{equation}
        \parity \colon p^\mu = (E, \vv{p}) \mapsto \overbar{p}^\mu = (E, -\vv{p}).
    \end{equation}
    We will use the notation \(\overbar{x}\) and \(\overbar{p}\) with this meaning throughout this section, including when discussing time reversal and charge conjugation.
    
    The definition of parity above is classical.
    In quantum field theory we have operators and states.
    Parity is promoted to be a unitary operator, \(\parity\), on the states, that is \(\parity^\hermit \parity = 1\).
    Starting with the simplest case, the vacuum state should be invariant under parity, since there's nothing there to change:
    \begin{equation}
        \parity \ket{0} = \ket{0}.
    \end{equation}
    The next simplest state is a single particle state, \(\ket{\vv{p}}\).
    We expect that the momentum of this particle should be reversed under parity:
    \begin{equation}
        \parity \ket{\vv{p}} = \ket{-\vv{p}}.
    \end{equation}
    We can write this momentum state as \(\ket{\vv{p}} = a^\hermit(\vv{p})\ket{0}\), and so we can identify the action of the parity operator on the state with an equivalent action on \(a^\hermit(\vv{p})\).
    Since \(a^\hermit(\vv{p})\) is an operator it transforms with two copies of \(\parity\) as
    \begin{equation}
        \parity a^\hermit(\vv{p}) \parity^{\hermit} = a^\hermit(-\vv{p}),
    \end{equation}
    so
    \begin{equation}
        (\parity a^\hermit(\vv{p}) \parity^\hermit)\ket{0} = a^\hermit(-\vv{p}) \ket{0} = \ket{-\vv{p}}.
    \end{equation}
    Taking the adjoint of this we have
    \begin{equation}
        [\parity a^\hermit(\vv{p}) \parity]^\hermit = (\parity^\hermit)^\hermit a(\vv{p}) \parity^\hermit = \parity a(\vv{p}) \parity^\hermit,
    \end{equation}
    and so
    \begin{equation}
        \parity a(\vv{p}) \parity^\hermit = [a^\hermit(-\vv{p})]^\hermit = a(-\vv{p}).
    \end{equation}
    
    To derive the effect on the field, which is an operator and so transforms as \(\varphi \mapsto \parity \varphi \parity^\hermit\), we can expand \(\varphi\) in the mode operators:
    \begin{align}
        \parity \varphi(x) \parity^\hermit &= \int \invariantmeasure{p} [\parity a(\vv{p}) \parity^\hermit \e^{-ip\cdot x} + \parity a^\hermit(\vv{p}) \parity^\hermit \e^{ip\cdot x}]\\
        &=- \int \invariantmeasure{p} [a(-\vv{p}) \e^{-ip\cdot x} + a^\hermit(-\vv{p})\e^{ip\cdot x}].
    \end{align}
    Note that the parity operator doesn't act on the phase factors, even though they depend on position and momentum, they're just scalars and so aren't affected by operators on the Hilbert space.
    Now take \(\vv{p} \to -\vv{p}\) in the integral, this doesn't change the measure or region of integration and takes \(p \to \overbar{p}\), giving
    \begin{equation}
        \parity \varphi(x) \parity^\hermit = \int \invariantmeasure{p} [a(\vv{p}) \e^{-i\overbar{p}\cdot x} + a^\hermit(\vv{p})\e^{i\overbar{p}\cdot x}].
    \end{equation}
    Now notice that \(\overbar{p} \cdot x = Et + \vv{p} \cdot \vv{x} = p \cdot \overbar{x}\), and so
    \begin{align}
        \parity \varphi(x) \parity^\hermit &= \int \invariantmeasure{p} [a(\vv{p})\e^{-ip\cdot \overbar{x}} + a^\hermit(\vv{p})\e^{ip\cdot \overbar{x}}]\\
        &= \varphi(\overbar{x}).
    \end{align}
    So under parity the field transforms as
    \begin{equation}
        \parity \colon \varphi(x) \mapsto \parity \varphi(x) \parity^\hermit = \varphi(\overbar{x}).
    \end{equation}
    This is what we would expect.
    Note that exactly the same argument can be applied to the conjugate field, and to complex scalar fields, they all transform the same.
    
    One thing that we didn't consider is that quantum states are only defined up to phase.
    This means that its possible to have an equivalent transformation which introduces a phase, so we could have that parity acts on the single particle state as
    \begin{equation}
        \parity \ket{\vv{p}} = \eta_{p} \ket{-\vv{p}}.
    \end{equation}
    Since \(\parity\) is idempotent we have \(\parity \parity \ket{\vv{p}} = \parity \eta_p \ket{-\vv{p}} = \eta_p^2 \ket{\vv{p}}\), and we must have \(\parity \parity \ket{\vv{p}} = \ket{\vv{p}}\), so \(\eta_p^2 = 1\), meaning \(\eta_p = \pm 1\).
    Above we assumed \(\eta_p = 1\), suppose instead that \(\eta_p = -1\), then the mode operators must transform as
    \begin{equation}
        \parity \colon a(\vv{p}) \mapsto \parity a(\vv{p}) \parity^\hermit = -a(-\vv{p}),
    \end{equation}
    and hence the field transforms as
    \begin{equation}
        \parity \colon \varphi(x) \mapsto \parity \varphi(x) \parity^\hermit = -\varphi(\overbar{x}).
    \end{equation}
    In this case we say that \(\varphi\) is a \defineindex{pseudoscalar} field.
    
    \subsection{Time Reversal}
    Time reversal acts on spacetime by sending \(t\) to \(-t\).
    That is
    \begin{equation}
        \timeReversal \colon x^\mu = (t, \vv{x}) \mapsto -\overbar{x}^\mu = (-t, \vv{x}).
    \end{equation}
    Since the direction of time reverses the direction of momentum, proportional to the velocity, \(\dot{x}\), and hence first order in time derivatives, changes direction.
    The energy is unaffected.
    The simplest explanation for why is that the kinetic energy is proportional to \(\dot{x}^2\) and so picks up two negatives upon time reversal, cancelling out for no overall change.
    So, momentum transforms under time reversal as
    \begin{equation}
        \timeReversal \colon p^\mu = (E, \vv{p}) \mapsto \overbar{p}^\mu = (E, -\vv{p}).
    \end{equation}
    
    Again when moving to quantum field theory time reversal is promoted to an operator, \(\timeReversal\) on the states.
    One effect of time reversal is that the initial and final state are swapped.
    This means that amplitudes change:
    \begin{equation}
        \timeReversal \colon \braket{f}{i} \mapsto \braket{i}{f} = \braket{f}{i}^*.
    \end{equation}
    So, time reversal means we have to take the complex conjugate of everything.
    Another way to view this is to consider the time evolution operator, \(\exp[-iEt]\).
    Under time reversal this changes to \(\exp[iEt] = \exp[-iEt]^*\).
    We say that \(\timeReversal\) is an antiunitary operator.
    
    \begin{dfn}{Antiunitary Operator}{}
        Let \(\hilbertSpace\) be a complex Hilbert space with inner product \(\innerproduct{-}{-}\), and let \(U \colon \hilbertSpace \to \hilbertSpace\) be an operator.
        Then \(U\) is an \defineindex{antiunitary operator} if
        \begin{equation}
            \innerproduct{Ux}{Uy} = \innerproduct{x}{y}^*
        \end{equation}
        for all \(x, y \in \hilbertSpace\).
    \end{dfn}
    
    This means that under time reversal the exponential phase factor transforms as
    \begin{equation}
        \timeReversal \colon \e^{ip\cdot x} \mapsto \e^{(-i)\overbar{p} \cdot (-\overbar{x})} = \e^{i\overbar{p}\cdot\overbar{x}} = \e^{ip \cdot x}.
    \end{equation}
    
    Consider the transformation of a single particle state:
    \begin{equation}
        \timeReversal\ket{\vv{p}} = \ket{-\vv{p}}.
    \end{equation}
    We must therefore have
    \begin{equation}
        \timeReversal a^\hermit(\vv{p}) \timeReversal^\hermit = a^\hermit(-\vv{p}), \qqand \timeReversal a(\vv{p}) \timeReversal^\hermit = a(-\vv{p}).
    \end{equation}
    The field then transforms as
    \begin{align}
        \timeReversal \varphi(x) \timeReversal^\hermit &= \int \invariantmeasure{p} [a(-\vv{p}) \e^{ip\cdot x} + a^\hermit(-\vv{p}) \e^{-ip\cdot x}]\\
        &= \int \invariantmeasure{p} [a(\vv{p}) \e^{i\overbar{p} \cdot x} + a(\vv{p})\e^{-i\overbar{p}\cdot x}]\\
        &= \int \invariantmeasure{p} [a(\vv{p}) \e^{-ip\cdot (-\overbar{x})} + a(\vv{p})\e^{ip \cdot (-\overbar{x})}]\\
        &= \varphi(-\overbar{x}).
    \end{align}
    Here we changed \(\vv{p}\) to \(-\vv{p}\) in the second line.
    This means the field transforms as
    \begin{equation}
        \timeReversal \colon \varphi(x) \mapsto \timeReversal \varphi(x) \timeReversal^\hermit = \varphi(-\overbar{x}).
    \end{equation}
    
    Again, we can include a phase factor, \(\eta_t\), which is restricted to be \(\pm 1\).
    Also all of this logic applies to both the conjugate field and to complex scalar fields.
    
    \subsection{Charge Conjugation}
    Classically charge conjugation changes the sign of electric charge.
    So
    \begin{equation}
        \chargeConjugation \colon Q \mapsto -Q.
    \end{equation}
    In quantum field theory this corresponds to exchanging particles and antiparticles.
    Hence, charge conjugation is only interesting for complex scalar fields.
    Charge conjugation acts on the single particle states according to
    \begin{equation}
        \chargeConjugation \ket{\vv{p}, +} = \ket{\vv{p}, -}, \qqand \chargeConjugation \ket{\vv{p}, -} = \ket{\vv{p}, +}.
    \end{equation}
    Exchanging particles and antiparticles is the same as exchanging mode operators \(a \leftrightarrow b\):
    \begin{alignat}{2}
        \chargeConjugation \colon a(\vv{p}) &\mapsto \chargeConjugation a(\vv{p}) \chargeConjugation^\hermit &&= b(\vv{p}),\\
        \chargeConjugation \colon a^\hermit(\vv{p}) &\mapsto \chargeConjugation a^\hermit(\vv{p}) \chargeConjugation^\hermit &&= b^\hermit(\vv{p}),\\
        \chargeConjugation \colon b(\vv{p}) &\mapsto \chargeConjugation b(\vv{p}) \chargeConjugation^\hermit &&= a(\vv{p}),\\
        \chargeConjugation \colon b^\hermit(\vv{p}) &\mapsto \chargeConjugation b^\hermit(\vv{p}) \chargeConjugation^\hermit &&= a^\hermit(\vv{p}).
    \end{alignat}
    The action on the field is then
    \begin{align}
        \chargeConjugation \varphi(x) \chargeConjugation^\hermit = \int \invariantmeasure{p} [b(\vv{p})\e^{-ip\cdot x} + a^\hermit(\vv{p})\e^{ip\cdot x}] = \varphi^\hermit(x),
    \end{align}
    so
    \begin{equation}
        \chargeConjugation\colon \varphi(x) \mapsto \chargeConjugation \varphi(x) \chargeConjugation^\hermit = \varphi^\hermit(x).
    \end{equation}
    Similarly,
    \begin{equation}
        \chargeConjugation\colon \varphi^\hermit(x) \mapsto \chargeConjugation \varphi^\hermit(x) \chargeConjugation^\hermit = \varphi(x).
    \end{equation}
    
    For a complex scalar field we also have the current, \(j^\mu\).
    Acting on this with charge conjugation exchanges \(\varphi^\hermit\) and \(\varphi\), giving
    \begin{align}
        \chargeConjugation j^\mu \chargeConjugation^\hermit &= i \chargeConjugation \normalordering{\varphi^\hermit \partial^\mu \varphi - \varphi \partial^\mu \varphi^\hermit} \chargeConjugation^\hermit\\
        &= i\normalordering{\varphi\partial^\mu \varphi^\hermit - \varphi^\hermit \partial^\mu \varphi}\\
        &= -j^\mu,
    \end{align}
    which is what we would expect.
    Since the total charge is defined as the spatial integral of \(j^0\) we also have
    \begin{equation}
        \chargeConjugation Q \chargeConjugation^\hermit = -Q,
    \end{equation}
    which is the same as the classical action of charge conjugation.
    
    \subsection{Acting on the Lagrangian}
    The free Lagrangian is
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi)(\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2
    \end{equation}
    for a real scalar field, or
    \begin{equation}
        \lagrangianDensity = (\partial_\mu \varphi^\hermit)(\partial^\mu \varphi) - m^2\varphi^\hermit \varphi
    \end{equation}
    for a complex scalar field.
    In either case the the result is invariant under \(\parity\), \(\timeReversal\), and \(\chargeConjugation\).
    
    Now suppose that the interaction Lagrangian is also invariant under \(\parity\), \(\timeReversal\) and \(\chargeConjugation\).
    Then, under \(\parity\) or \(\chargeConjugation\) the amplitude transforms as
    \begin{align}
        \parity \colon \amplitude &\mapsto \amplitude,\\
        \chargeConjugation \colon \amplitude &\mapsto \amplitude.
    \end{align}
    That is, the amplitude is unchanged.
    Under \(\timeReversal\) the amplitude transforms as
    \begin{equation}
        \timeReversal \colon \amplitude \mapsto \amplitude^*.
    \end{equation}
    However, in all three cases \(\abs{\amplitude}^2\) is unchanged and so the physics doesn't change.
    
    Invariance of the interaction term under these symmetries is not a given.
    For example, a \(\varphi^3\) interaction with a pseudoscalar field gains a minus sign under parity.
    However, for the purposes of this course we will only consider theories invariant under \(\parity\), \(\timeReversal\) and \(\chargeConjugation\) separately.
    In \course{Gauge Theories in Particle Physics} we will consider theories which aren't invariant under these symmetries individually, but instead are invariant under the combined \(\chargeConjugation \parity \timeReversal\)
    symmetry.
    
    \part{Dirac Equation}
    \chapter{Deriving the Dirac Equation}\index{Dirac equation}
    In this chapter we'll derive the Dirac equation in the way that Dirac did historically.
    He was motivated by looking for an equation without the negative energies of the Klein--Gordon equation.
    He failed to find such an equation, but it turns out that the negative energy solutions are actually important, and not just a problem to be eliminated.
    
    \section{The Dirac Equation}
    To avoid having to take square roots when computing the energy, which is the source of the negative energy terms in the Klein--Gordon equation, Dirac looked for a wave equation linear in \(\partial_t\).
    For this to be relativistic Dirac knew that the equation would also have to be linear in \(\partial_x\).
    Dirac also imposed one further constraint.
    The equation he was deriving should imply the Klein--Gordon equation since this is how we get wave equations with the relativistic energy momentum relation.
    
    Dirac started with the ansatz
    \begin{equation}
        \diffp{\psi}{t} + \vv{\alpha} \cdot \grad\psi + im\beta \psi = 0,
    \end{equation}
    which can be written in a Schrödinger-like form as
    \begin{equation}
        i\diffp{\psi}{t} = H\psi, \qqwhere H = -i\vv{\alpha} \cdot \grad + \beta m = \vv{\alpha} \cdot \vv{p} + \beta m
    \end{equation}
    is the \defineindex{Dirac Hamiltonian}.
    It should be noted that this is \emph{not} actually the Hamiltonian for a system satisfying the Dirac equation.
    
    Looking for real energies Dirac posited that \(H\) should be Hermitian, and hence \(\vv{\alpha}\) and \(\beta\) should be real.
    Next Dirac looked for other conditions on \(\vv{\alpha}\) and \(\beta\) which would enforce this equation implying the Klein--Gordon equation.
    Noticing that the Klein--Gordon equation has second derivatives we need to find a way to extract second derivatives from the Dirac equation.
    To do this we square the operator, but in such a way that the cross terms cancel, so we compute the product of the operator and its adjoint:
    \begin{equation}
        \left( \diffp{}{t} - \vv{\alpha} \cdot \grad - im\beta \right) \left( \diffp{}{t} + \vv{\alpha} \cdot \grad + im\beta \right)\psi = 0.
    \end{equation}
    Expanding this we have
    \begin{equation}
        \left( \diffp[2]{}{t} - \alpha_i\alpha_j \diffp{}{x_i, x_j} + m^2\beta^2 - im(\alpha_i \beta + \beta \alpha_i) \diffp{}{x_i} \right)\psi = 0.
    \end{equation}
    Looking at this we see that if \(\alpha_i\alpha_j = 1\) the cross term vanishes then we get the Klein--Gordon equation.
    The symmetry of \(\diffp{}/{x_i, x_j}\) implies that we should look for \(\alpha_i\alpha_j\) to be symmetric, so in terms of setting \(\alpha_i\alpha_j\) to 1 we impose
    \begin{equation}
        \anticommutator{\alpha_i}{\alpha_j} = 2\delta_{ij}
    \end{equation}
    where \(\anticommutator{A}{B} \coloneqq AB + BA\) is the \defineindex{anticommutator}.
    Similarly to get the Klein--Gordon equation we need \(\beta^2 = 1\).
    The cross term will vanish if
    \begin{equation}
        \anticommutator{\alpha_i}{\beta} = 0
    \end{equation}
    
    Clearly these requirements cannot be satisfied if \(\alpha_i\) and \(\beta\) are numbers.
    We can have relations like these if \(\alpha_i\) and \(\beta\) are \emph{matrices} though.
    Looking for \(H\) to be Hermitian means that \(\alpha_i\) and \(\beta\) must be \emph{Hermitian} matrices.
    Suppose \(\alpha_i\) and \(\beta\) are \(N \times  N\) matrices.
    Then \(\psi\) must be an object with \(N\) components.
    
    We can go on to derive further properties of \(\alpha_i\) and \(\beta\) which will help us to narrow down what matrices they are.
    First, \(\alpha_i^2 = \beta^2 = 1\) the eigenvalues must square to \(1\), so the eigenvalues of \(\alpha_i\) and \(\beta\) are \(\pm 1\).
    
    Next consider the trace of \(\alpha_i\).
    Using the anticommutation relations for \(\alpha_i\) and \(\beta\) we have
    \begin{alignat}{2}
        \tr \alpha_i &= \tr(\beta^2 \alpha_i) \qquad\qquad && \beta^2 = 1\\
        &= \tr(\beta \alpha_i \beta) \qquad\qquad && \tr \text{ is cyclic}\\
        &= \tr(-\alpha_i\beta^2) \qquad\qquad && \alpha_i \text{ and } \beta \text{ anticommute}\\
        &= -\tr(\alpha_i\beta^2) \qquad\qquad && \tr \text{ is linear}\\
        &= -\tr \alpha_i \qquad\qquad && \beta^2 = 1
    \end{alignat}
    So we can conclude that \(\tr \alpha_i = -\tr \alpha_i\) and so \(\tr\alpha_i = 0\).
    We can do exactly the same swapping \(\beta\) and \(\alpha_i\) and so \(\tr\beta = 0\).
    This means that \(\alpha_i\) and \(\beta\) are \emph{traceless} Hermitian matrices.
    
    In the basis where \(\alpha_i\) is diagonal the diagonal is just the eigenvalues, and so the trace is the sum of the eigenvalues.
    The trace is basis independent so the trace is \emph{always} the sum of eigenvalues.
    The fact that \(\alpha_i\) and \(\beta\) have eigenvalues \(\pm 1\) and the sum of the eigenvalues, that is the trace, must vanish means that \(\alpha_i\) and \(\beta\) must have an even number of eigenvalues, and hence \(\alpha_i\) and \(\beta\) are traceless Hermitian matrices of \emph{even dimension}.
    
    At this point we can just start trying to find a solution for \(\alpha_i\) and \(\beta\).
    We start with the simplest case, \(N = 2\).
    We know that the Pauli spin matrices,
    \begin{equation}
        \sigma_1 = 
        \begin{pmatrix}
            0 & 1\\
            1 & 0
        \end{pmatrix}
        , \qquad \sigma_2 = 
        \begin{pmatrix}
            0 & -i\\
            i & 0
        \end{pmatrix}
        , \qqand \sigma_3 = 
        \begin{pmatrix}
            1 & 0\\
            0 & -1
        \end{pmatrix}
        ,
    \end{equation}
    form a basis for the \(2 \times 2\) traceless Hermitian matrices, and so we posit that \(\alpha_i = \sigma_i\), or more accurately \(\sigma_i\) is a \emph{representation}\footnote{see \course{Symmetries of Quantum Mechanics} or \course{Symmetries of Particles and Fields} for details on what a representation is.} of \(\alpha_i\).
    The problem comes when we look for a \(2\times 2\) representation of \(\beta\).
    There is no other \(2\times 2\) Hermitian traceless matrix, and so we cannot find a value for \(\beta\) in the \(2\times 2\) matrices.
    This may not be a problem.
    If we're looking at a massless particle then \(m = 0\) gets rid of the \(\beta\) term, so we don't need a value for \(\beta\) and we can use \(2 \times 2\) matrices and 2 component spinors to describe such particles.
    
    Move on to the next simplest case, \(N = 4\), here there are solutions.
    One very common representation is the \defineindex{Dirac representation}, which is chosen such that \(\beta\) is diagonal:
    \begin{equation}
        \beta = 
        \begin{pmatrix}
            \ident_2 & 0\\
            0 & -\ident_2
        \end{pmatrix}
        , \qqand \alpha_i = 
        \begin{pmatrix}
            0 & \sigma_i\\
            \sigma_i & 0
        \end{pmatrix}
        .
    \end{equation}
    We call these the \defineindex{Dirac matrices} in this representation.
    
    These are complex, meaning that \(\psi\) is an object with four complex components.
    We call \(\psi\) a \defineindex{spinor}, for reasons we'll see later.
    As a four component thing it may be more accurate to write \(\psi\) with an index, say as \(\psi_a\), but spinor indices are traditionally suppressed unless there is ambiguity.
    When we do write spinor indices we'll choose values from the start of the alphabet, \(a, b, c, \dotsc\), as opposed to the Greek letters, \(\mu, \nu, \rho, \dotsc\) we use for Lorentz indices or \(i, j, k, \dotsc\) for spatial indices.
    
    \section{Covariant Form}
    The Dirac equation as we've written it so far is not \emph{manifestly} covariant.
    We can fix this by changing up our notation slightly.
    Start by multiplying through by \(i\beta\) to get
    \begin{equation}
        \left( i\beta\diffp{}{t} + i\beta\alpha_j\diffp{}{x_j} - m \right) \psi = 0,
    \end{equation}
    where we've used \(\beta^2 = 1\), although really we now know that \enquote{\(1\)} here is an identity matrix, which we leave implicit.
    Now define \(\gamma^0 = \beta\) and \(\gamma^i = \beta \alpha^i\).
    These form a four-vector, \(\gamma^\mu = (\gamma^0, \gamma^i) = (\beta, \beta\vv{\alpha})\)\index{\(\gamma^\mu\)|see{gamma matrices}}.
    We can then write
    \begin{equation}
        \left( i\gamma^0 \diffp{}{x_0} + i\gamma^j \diffp{}{x_j} - m \right) \psi = 0,
    \end{equation}
    or more compactly,
    \begin{equation}
        (i\gamma^\mu \partial_\mu - m)\psi = 0,
    \end{equation}
    or even more compactly,
    \begin{equation}
        (i\slashed{\partial} - m)\psi = 0
    \end{equation}
    where \(\slashed{a} \coloneqq \gamma^\mu a_\mu\) is a shorthand called \defineindex{slash notation}.

    Like \(\alpha_i\) and \(\beta\) these \(\gamma^\mu\) are really \(N \times N\) matrices, which we call the \defineindex{gamma matrices} for obvious reasons.
    If we reinstate the spinor indices then we might write \(\gamma^\mu_{ab}\), where \(\mu\) is an index telling us if we are considering \(\gamma^0\), \(\gamma^1\), \(\gamma^2\), or \(\gamma^3\) and the indices \(a\) and \(b\) tell us which component of the matrix we are considering.
    
    We can determine the anticommutation relations of \(\gamma^\mu\) using those of \(\alpha_i\) and \(\beta\), first, we have \(\anticommutator{\gamma^0}{\gamma^0} = \anticommutator{\beta}{\beta} = 2\beta^2 = 2\), next \(\anticommutator{\gamma^0}{\gamma^i} = \anticommutator{\beta}{\beta\alpha^i} = \beta^2\alpha^i + \beta\alpha^i\beta = \beta^2\alpha^i - \alpha^i\beta = 0\).
    Finally, \(\anticommutator{\gamma^i}{\gamma^j} = \anticommutator{\beta\alpha^i}{\beta\alpha^j} = \beta\alpha^i\beta\alpha^j + \beta\alpha^j\beta\alpha^i = -\beta\alpha^i\alpha^j\beta - \beta\alpha^j\alpha^i\beta = -\beta\anticommutator{\alpha^i}{\alpha^j}\beta = -\beta 2\delta^{ij} \beta = 2\delta^{ij} \).
    Hence,
    \begin{equation}
        \anticommutator{\gamma^\mu}{\gamma^\nu} = 2\minkowskiMetric^{\mu\nu}.
    \end{equation}
    This is called the \defineindex{Clifford algebra} of the gamma matrices.
    We take this as the defining property of the gamma matrices, and it is this algebra which is being represented by a particular choice of matrices.
    
    It is useful to define a \defineindex{fifth gamma matrix}\index{\(\gamma^5\)|see{fifth gamma matrix}}, \(\gamma^5 = \gamma_5\).
    This is \emph{not} part of the Clifford algebra.
    The 5 is just a label, it's not an index and no index will ever take the value of 5 (while we remain in 4 dimensions, even in more dimensions there are only ever four gamma matrices), we use the label 5 because some sources label the gamma matrices 1 through 4 instead of 0 through 3.
    This extra matrix is defined as
    \begin{equation}
        \gamma^5 = \gamma_5 \coloneqq i\gamma^0\gamma^1\gamma^2\gamma^3.
    \end{equation}
    \begin{wrn}
        The factor of \(i\) here is sometimes left out, in which case \(\gamma^5\) will be anti-Hermitian.
    \end{wrn}
    
    Using the anticommutator relations we have \(\gamma^\mu \gamma^\nu = -\gamma^\nu\gamma^\mu\) whenever \(\mu \ne \nu\), as well as \((\gamma^0)^2 = 1\) and \((\gamma^i)^2 = -1\), so
    \begin{alignat}{2}
        (\gamma^5)^2 &= -\gamma^0\gamma^1\gamma^2\gamma^3\gamma^0\gamma^1\gamma^2\gamma^3\\
        &= \gamma^1\gamma^2\gamma^3(\gamma^0)^2\gamma^1\gamma^2\gamma^3 \qquad && \gamma^0 \text{ past 3 }\gamma^\mu\\
        &= \gamma^1\gamma^2\gamma^3\gamma^1\gamma^2\gamma^3 && (\gamma^0)^2 = 1\\
        &= \gamma^2\gamma^3(\gamma^1)^2\gamma^2\gamma^3 && \gamma^1 \text{ past 2 } \gamma^\mu\\
        &= -\gamma^2\gamma^3\gamma^2\gamma^3 && (\gamma^1)^2 = -1\\
        &= \gamma^3(\gamma^2)^2\gamma^3 && \gamma^2 \text{ past 1 } \gamma^\mu\\
        &= -(\gamma^3)^2 && (\gamma^2)^2 = -1\\
        &= 1 && (\gamma^3)^2 = -1.
    \end{alignat}
    We can also derive anticommutator relations for \(\gamma^5\) with the other gamma matrices:
    \begin{align}
        \anticommutator{\gamma^5}{\gamma^0} &= i\gamma^0\gamma^1\gamma^2\gamma^3\gamma^0 + i(\gamma^0)^2\gamma^1\gamma^2\gamma^3\\
        &= -i(\gamma^0)^2\gamma^1\gamma^2\gamma^3 + i(\gamma^0)^2\gamma^1\gamma^2\gamma^3\\
        &= 0,\\
        \anticommutator{\gamma^5}{\gamma^1} &= i\gamma^0\gamma^1\gamma^2\gamma^3\gamma^1 + i\gamma^1\gamma^0\gamma^1\gamma^2\gamma^3\\
        &= i\gamma^0(\gamma^1)^2\gamma^2\gamma^3 - i\gamma^0(\gamma^1)^2\gamma^2\gamma^3\\
        &= 0,\\
        \anticommutator{\gamma^5}{\gamma^2} &= i\gamma^0\gamma^1\gamma^2\gamma^3\gamma^2 + i\gamma^2\gamma^0\gamma^1\gamma^2\gamma^3\\
        &= -i\gamma^0\gamma^1(\gamma^2)^2\gamma^3 + i\gamma^0\gamma^1(\gamma^2)^2\gamma^3\\
        &= 0,\\
        \anticommutator{\gamma^5}{\gamma^3} &= i\gamma^0\gamma^1\gamma^2(\gamma^3)^2 + i\gamma^3\gamma^0\gamma^1\gamma^2\gamma^3\\
        &= i\gamma^0\gamma^1\gamma^2(\gamma^3)^2 - i\gamma^0\gamma^1\gamma^2(\gamma^3)^2\\
        &= 0,
    \end{align}
    so in general
    \begin{equation}
        \anticommutator{\gamma^5}{\gamma^\mu} = 0.
    \end{equation}
    
    The Dirac representation for \(\alpha_i\) and \(\beta\) also gives a representation of the gamma matrices, namely
    \begin{equation}
        \gamma^0 = 
        \begin{pmatrix}
            \ident_2 & 0\\
            0 & -\ident_2
        \end{pmatrix}
        , \qquad \gamma^i = 
        \begin{pmatrix}
            0 & \sigma^i\\
            -\sigma^i & 0
        \end{pmatrix}
        , \qqand \gamma^5 = 
        \begin{pmatrix}
            0 & \ident_2\\
            \ident_2 & 0
        \end{pmatrix}
        .
    \end{equation}
    As with \(\alpha_i\) and \(\beta\) this is just one representation of the gamma matrices, the real definition is just anything satisfying the Clifford algebra.
    In this representation \(\gamma^0\) is Hermitian, \({\gamma^0}^\hermit = \gamma^0\), \(\gamma^i\) are anti-Hermitian, \({\gamma^i}^\hermit = -\gamma^i\), and \(\gamma^5\) is hermitian, \({\gamma^5}^\hermit = \gamma^5\).
    
    We can now show that the Dirac equation implies the Klein--Gordon equation.
    Start with the Dirac equation,
    \begin{equation}
        (i\slashed{\partial} - m)\psi = 0.
    \end{equation}
    Multiply by \(i\slashed{\partial} + m\) to get
    \begin{equation}
        (i\slashed{\partial} + m)(i\slashed{\partial} - m)\psi = 0.
    \end{equation}
    Expanding this the cross terms cancel and we get
    \begin{equation}
        (-\gamma^\mu \gamma^\nu \partial_\mu \partial_\nu + m^2)\psi = 0.
    \end{equation}
    Since \(\partial_\mu\partial_\nu\) is symmetric in \(\mu\) and \(\nu\) we can replace \(\gamma^\mu\gamma^\nu\) with the symmetrised product of \(\gamma^\mu\) and \(\gamma^\nu\), namely \(\anticommutator{\gamma^\mu}{\gamma^\nu}/2 = \eta^{\mu\nu}\), since any antisymmetric part of \(\gamma^\mu\gamma^\nu\) vanishes in the product with something totally symmetric.
    Hence we can replace \(\gamma^\mu\gamma^\nu\) with \(\eta^{\mu\nu}\):
    \begin{equation}
        (\eta^{\mu\nu}\partial_\mu\partial_\nu + m^2)\psi = 0 \implies (\dalembertian + m^2) \psi = 0,
    \end{equation}
    which is exactly the Klein--Gordon equation, so the Dirac equation does imply the Klein--Gordon equation as required.
    
    \section{Spinor Transformations}
    We've said that spinors are \emph{not} four-vectors, despite having four components in the Dirac representation.
    The difference is in their transformation rules.
    Recall that a four-vector, \(x\), is defined by transforming as
    \begin{equation}
        x \mapsto x' = \Lambda x \iff x^\mu = x'^\mu = \tensor{\Lambda}{^\mu_\nu}x^\nu
    \end{equation}
    for some Lorentz transformation \(\Lambda\).
    We now ask how a general spinor transforms under a Lorentz transformation.
    The Lorentz transformation acts on spacetime, not on the spinor, so we're looking for some spinor, \(\psi(x)\), to transform into another spinor, \(\psi'(x')\), when \(x\) transforms to \(x'\).
    
    Our starting point is the Dirac equation.
    Before transformation we have
    \begin{equation}
        (i\gamma^\mu \partial_\mu - m)\psi(x) = 0,
    \end{equation}
    and after transformation we expect the form of the Dirac equation to remain unchanged, that is
    \begin{equation}
        (i\gamma'^\mu \partial_\mu' - m)\psi'(x') = 0.
    \end{equation}
    Since \(m\) is a scalar it doesn't change under Lorentz transformations.
    The derivative is just a four-vector, so transforms as \(\partial_\mu \mapsto \partial'_\mu = \tensor{\Lambda}{^\mu_\nu}\partial_\nu\).
    We assume that \(\gamma'^\mu\) are still gamma matrices, that is they satisfy the Clifford algebra \(\{\gamma'^\mu, \gamma'^\nu\} = 2\eta^{\mu\nu}\).
    We can then, without loss of generality, take \(\gamma'^\mu = \gamma^\mu\), since we know that \(\gamma'^\mu\) must be a representation of this Clifford algebra and the physics shouldn't depend on our choice of representation, so we may as well choose the same representation before and after the transformation.
    
    We now assume that \(\psi\) transforms according to
    \begin{equation}
        \psi'(x') = S(\Lambda)\psi(x)
    \end{equation}
    where \(S(\Lambda)\) is some \(4 \times 4\) matrix depending on the Lorentz transformation \(\Lambda\), but \(S(\Lambda)\) is not necessarily a Lorentz transformation.
    Since the two frames are equivalent descriptions of the same physics it must be possible to go between them in both directions, so \(S(\Lambda)\) must be nonsingular.
    We can then write
    \begin{equation}
        \psi(x) = S^{-1}(\Lambda)\psi'(x').
    \end{equation}
    However, note that we can also go from the second frame to the first by the inverse Lorentz transformation, \(\Lambda^{-1}\), so we must have that
    \begin{equation}
        S^{-1}(\Lambda) = S(\Lambda^{-1}).
    \end{equation}
    We can now write out the original Dirac equation in terms of the transformed quantities by reversing our transformation, that is \(\partial_\mu = \tensor{\Lambda}{^\nu_\mu}\partial'_\nu\) and \(\psi(x) = S^{-1}(\Lambda)\psi'(x')\).
    Then the Dirac equation in the first frame can be written as
    \begin{equation}
        (i\gamma^\mu \tensor{\Lambda}{^\nu_\mu}\partial'_\nu - m)S^{-1}(\Lambda)\psi'(x') = 0.
    \end{equation}
    Now multiply this on the left by \(S(\Lambda)\) and, noting that everything apart from \(\gamma^\mu\) commutes with \(S(\Lambda)\), we get
    \begin{equation}
        (iS(\Lambda)\gamma^\mu S^{-1}(\Lambda) \tensor{\Lambda}{^\nu_\mu}\partial'_\nu - m)\psi'(x').
    \end{equation}
    We see that this takes the form of the Dirac equation if
    \begin{equation}
        S(\Lambda)\gamma^\mu S^{-1}(\Lambda)\tensor{\Lambda}{^\nu_\mu} = \gamma^\nu.
    \end{equation}
    Multiplying on the left by \(S^{-1}(\Lambda)\) and the right by \(S(\Lambda)\) we get
    \begin{equation}\label{eqn:spinor and Lorentz transformation of gamma matrices}
        \gamma^\mu \tensor{\Lambda}{^\nu_\mu} = S^{-1}(\Lambda)\gamma^\nu S(\Lambda).
    \end{equation}
    
    We can think of this as a sort of consistency equation between Lorentz transformations of four-vectors on the left and Lorentz transformations of spinors on the right.
    Note that \(\gamma^\mu\) is a four-vector as far as Lorentz indices are concerned, and so transforms under a single copy of \(\Lambda\).
    On the other hand \(\gamma^\mu_{ab}\) is a rank 2 tensor for the purposes of spinor indices, and so transforms under two copies of \(S(\Lambda)\) and \(S^{-1}(\Lambda)\).
    We can write this out in terms of spinor components as
    \begin{equation}
        \gamma^\mu_{ab} \tensor{\Lambda}{^\nu_\mu} = S^{-1}(\Lambda)^{ac} \gamma^\nu_{cd} S(\Lambda)^{db}.
    \end{equation}
    
    The question now is, what is \(S(\Lambda)\)?
    To answer this we consider the two basic types of Lorentz transformations.
    
    \subsection{Boosts}
    Consider a boost
    \begin{equation}
        \Lambda = 
        \begin{pmatrix}
            \cosh \omega & \sinh \omega & 0 & 0\\
            \sinh \omega & \cosh \omega & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    Then we claim that the corresponding spinor transformation matrix, \(S_B\), is given by
    \begin{equation}
        S_B = \exp\left[ \alpha^1 \frac{\omega}{2} \right] = \exp\left[ \gamma^0\gamma^1 \frac{\omega}{2} \right].
    \end{equation}
    
    Since \((\alpha^1)^2 = 1\) we can expand the exponential into odd and even parts, and since everything is real those parts will be hyperbolic, so
    \begin{equation}
        \exp\left[ \alpha^1 \frac{\omega}{2} \right] = \cosh\frac{\omega}{2} + \alpha^1 \sinh\frac{\omega}{2} = 
        \begin{pmatrix}
            \ident_2 \cosh\frac{\omega}{2} & \sigma^1 \sinh\frac{\omega}{2}\\
            \sigma^1 \cosh\frac{\omega}{2} & \ident_2\cosh\frac{\omega}{2}
        \end{pmatrix}
        .
    \end{equation}
    In the last step we choose to work in the Dirac representation.
    Now using \(S(\Lambda^{-1}) = S^{-1}(\Lambda)\) and considering the \(\mu = 0\) case we can compute the product \(S_B^{-1}\gamma^0S_B\), using
    \begin{equation}
        S_B^{-1} =
        \begin{pmatrix}
            \ident_2\cosh\frac{\omega}{2} & -\sigma^1\sinh\frac{\omega}{2}\\
            -\sigma^1\sinh\frac{\omega}{2} & \ident_2\cosh\frac{\omega}{2}
        \end{pmatrix}
    \end{equation}
    and we find
    \begin{equation}
        S_B^{-1}\gamma^0S_B = 
        \begin{pmatrix}
            \ident_2\cosh \omega & \sigma^1\sinh\omega\\
            -\sigma^1\sinh\omega & -\ident_2\cosh\omega
        \end{pmatrix}
        = \gamma^0 \cosh\omega + \gamma^1\sinh\omega.
    \end{equation}
    This shows that
    \begin{equation}
        S_B^{-1} \gamma^0 S_B = \tensor{\Lambda}{^0_\nu}\gamma^\nu.
    \end{equation}
    We can make similar arguments for the \(\mu = 1, 2, 3\) components by computing \(S_B^{-1}\gamma^\mu S_B\).
    We find that
    \begin{equation}
        \tensor{\Lambda}{^1_\nu}\gamma^\nu = \gamma^0\sinh \omega + \gamma^1 \cosh \omega, \quad \tensor{\Lambda}{^2_\nu}\gamma^\nu = \gamma^2, \qand \tensor{\Lambda}{^3_\nu}\gamma^\nu = \gamma^3.
    \end{equation}
    This is what we would expect so \(S_B\) is the correct transformation matrix.
    
    This generalises to a boost along some arbitrary direction given by the unit vector \(\vh{n}\), by
    \begin{equation}
        S_B = \exp\left[ \vv{\alpha} \cdot \vh{n} \frac{\omega}{2} \right].
    \end{equation}
    
    \subsection{Rotations}
    Now consider a rotation,
    \begin{equation}
        \Lambda = 
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & \cos\vartheta & \sin\vartheta & 0\\
            0 & -\sin\vartheta & \cos\vartheta & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    We claim that the corresponding spinor transformation matrix, \(S_R\), is given by
    \begin{equation}
        S_R = \exp\left[ i\Sigma^3 \frac{\vartheta}{2} \right]
    \end{equation}
    where
    \begin{equation}
        \Sigma^k = -\frac{i}{4}\varepsilon^{kij}\commutator{\gamma^i}{\gamma^j} \implies \Sigma^3 = -\frac{1}{2}
        \begin{pmatrix}
            \sigma^3 & 0\\
            0 & \sigma^3
        \end{pmatrix}
        .
    \end{equation}
    Choosing to work in the Dirac representation in the last step.
    More generally in the Dirac representation
    \begin{equation}
        \Sigma^k = 
        \begin{pmatrix}
            \sigma^k & 0\\
            0 & \sigma^k
        \end{pmatrix}
        .
    \end{equation}
    We can check that \((\Sigma^3)^2 = 1\), and so we can again expand the exponential in even and odd parts.
    This time the factor of \(i\) means we get normal trig:
    \begin{equation}
        S_R = \cos\frac{\vartheta}{2} + i\Sigma^3 \sin\frac{\vartheta}{2}.
    \end{equation}
    We can then proceed similarly to the boosts case computing \(S_R^{-1}\gamma^\mu S_R\) for each fixed value of \(\mu\).
    We find that
    \begin{alignat}{2}
        \tensor{\Lambda}{^0_\nu}\gamma^\nu &= \gamma^0, \qquad & \tensor{\Lambda}{^1_\nu}\gamma^\nu &= \gamma^1\cos\vartheta + \gamma^2\sin\vartheta,\\
        \tensor{\Lambda}{^2_\nu}\gamma^\nu &= -\gamma^1\sin\vartheta + \gamma^2\cos\vartheta, \qquad & \tensor{\Lambda}{^3_\nu}\gamma^\nu &= \gamma^3.
    \end{alignat}
    
    This generalises to a rotation about an axis along \(\vh{n}\) as
    \begin{equation}
        S_R = \exp\left[ i\vv{\Sigma} \cdot \vh{n} \frac{\vartheta}{2} \right].
    \end{equation}
    
    \subsection{General Lorentz Transformation}
    First note that \(S_R\) is unitary, but \(S_B\) is not.
    This corresponds to the fact that the rotations, \(\specialOrthogonal(3)\), form a compact subgroup of the Lorentz group, whereas the boosts form a noncompact subgroup\footnote{See \course{Symmetries of Particles and Fields} for details.}.
    However, for both rotations and boosts we have the relation
    \begin{equation}
        S^{-1} = \gamma^0 S^\hermit \gamma^0,
    \end{equation}
    and since any Lorentz transformation can be formed from rotations and boosts this holds in general.
    
    We can combine the two results into one for a general transformation:
    \begin{equation}
        S(\Lambda) = \exp\left[ -\frac{i}{4}\omega_{\mu\nu}\sigma^{\mu\nu} \right]
    \end{equation}
    where
    \begin{equation}
        \sigma^{\mu\nu} \coloneqq \frac{i}{2}\commutator{\gamma^\mu}{\gamma^\nu}
    \end{equation}
    and \(\omega^{ij} = \varepsilon^{ijk}\vartheta^k\) where \(\vv{\vartheta} = \vartheta \vh{n}\) parametrises a rotation by angle \(\vartheta\) about the unit vector \(\vh{n}\).
    
    Considering just the spatial indices we have
    \begin{equation}
        -\frac{i}{4}\omega_{ij}\sigma^{ij} = -\frac{i}{4}\varepsilon^{ijk}\vartheta^k \frac{i}{2}\commutator{\gamma^i}{\gamma^j} = \frac{i}{2} \vv{\vartheta} \cdot \vv{\Sigma},
    \end{equation}
    so we recover the rotation result.
    Similarly, considering \(\mu = 0\) and \(\nu = i\) we recover the boost result.
    
    Notice that for a rotation by \(2\pi\) we have
    \begin{equation}
        S_R(2\pi) = -S_R(0) = -\ident,
    \end{equation}
    so under a \(2\pi\) rotation a spinor transforms as \(\psi(x) \mapsto -\psi(x)\).
    This means that it takes a rotation of \(4\pi\) to get back to the start, that is \(S_R(4\pi) = \ident\).
    We can interpret this as the Dirac equation describing fermions.
    The factors of \(1/2\) in the exponentials defining \(S_R\) and \(S_B\) tell us that in particular these fermions are spin \(1/2\).
    One way to convince us that these are fermions is to consider two identical particles described by the Dirac equation.
    If we rotate about the midpoint between them by \(\pi\) then the particles swap places, as shown in \cref{fig:rotating fermions into each other}.
    However, two rotations by \(\pi\) are equivalent to one rotation by \(2\pi\), and so we get a minus sign.
    This is exactly what we would expect for fermions following Fermi--Dirac statistics, swapping identical fermions gives a negative sign in the wave function.
    
    \begin{figure}
        \tikzsetnextfilename{rotating-fermions-into-each-other}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \fill[highlight] (-1, 0) circle [radius = 0.1cm];
            \fill[highlight] (1, 0) circle [radius = 0.1cm];
            \draw[->, thick] (10:1) arc(10:170:1) node [midway, above, text=black] {\(\pi\)};
            \draw[->, thick] (190:1) arc(190:350:1) node [midway, below, text=black] {\(\pi\)};
        \end{tikzpicture}
        \caption{Rotating two particles into each other.}
        \label{fig:rotating fermions into each other}
    \end{figure}
    
    \chapter{Dirac Lagrangian}
    Our goal in this section will be to derive a Lagrangian which produces the Dirac equation as its equation of motion.
    To do so we first need to look at the sort of quantities that can appear in the Lagrangian.
    Presumably it includes a spinor, and since the Lagrangian must be real, and spinors are complex, we need to look for a way to take products with spinors in a way that results in a real value, sort of like how we consider products of \(\varphi^\hermit\) and \(\varphi\) in a complex scalar Lagrangian.
    However, since spinors transform in a somewhat more complicated manner than scalars we need to carefully consider which combinations of spinors give us Lorentz scalars.
    
    \section{Dirac Adjoint}
    We start by defining the \define{Dirac adjoint}\index{Dirac adjoint!of spinors} of a spinor, \(\psi\), to be
    \begin{equation}
        \diracadjoint{\psi} \coloneqq \psi^\hermit \gamma^0.
    \end{equation}

    Recall that under a Lorentz transformation, \(\Lambda\), a spinor, \(\psi\), transforms as
    \begin{equation}
        \psi \mapsto \psi' = S(\Lambda)\psi.
    \end{equation}
    The Hermitian conjugate of \(\psi\) then transforms as
    \begin{equation}
        \psi^\hermit \mapsto \psi'^\hermit = \psi^\hermit S(\Lambda)^\hermit.
    \end{equation}
    
    We want the Lagrangian to be real.
    For a complex scalar field this was achieved by building it from products of \(\varphi^\hermit\) and \(\varphi\).
    Consider then \((\gamma^\mu)^\hermit\).
    Every representation of the gamma matrices is equivalent to a unitary representation.
    This follows from Maschke's theorem\footnote{see \course{Symmetries of Quantum Mechanics} or \course{Symmetries of Particles and Fields}} since the gamma matrices generate a finite group, the elements of which are the identity, products of gamma matrices, and their negatives, that this group is finite follows since any product of gamma matrices can be reduced to a product containing each gamma matrix at most once using the anticommutation relations.
    Thus we have \((\gamma^\mu)^\hermit = (\gamma^\mu)^{-1}\).
    The commutation relations tell us that \((\gamma^0)^2 = 1\), so \((\gamma^0)^{-1} = \gamma^0\), and \((\gamma^i)^2 = -1\), so \((\gamma^i)^{-1} = -\gamma^i\).
    Now using \(-\gamma^i = -\gamma^i(\gamma^0)^2 = \gamma^0\gamma^i\gamma^0\), as well as \((\gamma^0)^3 = \gamma^0\), we have the identity
    \begin{equation}
        (\gamma^\mu)^\hermit = \gamma^0\gamma^\mu\gamma^0.
    \end{equation}
    
    Now consider \(S(\Lambda)^\hermit\).
    If we write \(S(\Lambda)\) as
    \begin{equation}
        S(\Lambda) = \exp\left[ -\frac{i}{4} \omega_{\mu\nu} \sigma^{\mu\nu} \right]
    \end{equation}
    then, using the fact that the parameters, \(\omega_{\mu\nu}\), are real we have
    \begin{equation}
        S(\Lambda)^\hermit = \exp\left[ \frac{i}{4}\omega_{\mu\nu} (\sigma^{\mu\nu})^{\hermit} \right].
    \end{equation}
    Now, we need to compute \((\sigma^{\mu\nu})^\hermit\).
    Start from the definition, \(\sigma^{\mu\nu} = i\commutator{\gamma^\mu}{\gamma^\nu}/2\).
    Taking the Hermitian conjugate turns \(i\) into \(-i\), and reverses the arguments of the commutator and sends them to their Hermitian conjugates, since taking the transpose reverses the order of a product:
    \begin{equation}
        (\sigma^{\mu\nu})^{\hermit} = -\frac{i}{2}\commutator{(\gamma^\nu)^\hermit}{(\gamma^\mu)^\hermit} = \frac{i}{2}\commutator{(\gamma^\mu)^{\hermit}}{(\gamma^\nu)^{\hermit}} = \frac{i}{2}\commutator{\gamma^0\gamma^\mu\gamma^0}{\gamma^0\gamma^\nu\gamma^0}
    \end{equation}
    where we've used the antisymmetry of the commutator to exchange the order of the arguments.
    Expanding the commutator whenever we have two neighbouring factors of \(\gamma^0\) they square to the identity and so we're left only with the outer factors of \(\gamma^0\), giving
    \begin{equation}
        (\sigma^{\mu\nu})^\hermit = \frac{i}{2}\gamma^0 \commutator{\gamma^\mu}{\gamma^\nu} \gamma^0 = \gamma^- \sigma^{\mu\nu} \gamma^0.
    \end{equation}
    
    Hence, we have
    \begin{equation}
        S(\Lambda)^{\hermit} = \exp\left[ \frac{i}{4}\omega_{\mu\nu}(\sigma^{\mu\nu})^\hermit \right] = \exp\left[ \frac{i}{4}\omega_{\mu\nu} \gamma^0 \sigma^{\mu\nu} \gamma^0 \right].
    \end{equation}
    Now considering \(\exp[\gamma^0 X \gamma^0]\) we have
    \begin{multline}
        \exp\left[ \gamma^0 X \gamma^0 \right] = \sum_{n = 0}^{\infty} \frac{1}{n!} (\gamma^0X\gamma^0)^n = \sum_{n = 0}^{\infty} \frac{1}{n!} \underbrace{\gamma^0X\gamma^0\gamma^0X\gamma^0 \dotsm \gamma^0X\gamma^0}_{n \text{ factors of } X}\\
        = \sum_{n = 0}^{\infty} \frac{1}{n!} \gamma^0X^n\gamma^0 = \gamma^0\exp[X]\gamma^0,
    \end{multline}
    since the paired factors of \(\gamma^0\) square to the identity.
    Hence,
    \begin{equation}
        S(\Lambda)^\hermit = \exp\left[ \frac{i}{4}\omega_{\mu\nu}\gamma^0\sigma^{\mu\nu}\gamma^0 \right] = \gamma^0\exp[ \frac{i}{4}\omega_{\mu\nu}\sigma^{\mu\nu} ] \gamma^0 = \gamma^0S^{-1}(\Lambda)\gamma^0.
    \end{equation}
    Hence the Hermitian conjugate transforms as
    \begin{equation}
        \psi^\hermit \mapsto \psi'^\hermit = \psi^\hermit S(\Lambda)^\hermit = \psi^\hermit \gamma^0 S^{-1}(\Lambda) \gamma^0
    \end{equation}
    and so the Dirac adjoint transforms as
    \begin{equation}
        \diracadjoint{\psi} = \psi^\hermit \gamma^0 \mapsto \diracadjoint{\psi}' = \textcolor{highlight}{\psi'^\hermit} \gamma^0 = \underbrace{\textcolor{highlight}{\psi^\hermit \gamma^0}}_{=\diracadjoint{\psi}} \textcolor{highlight}{S^{-1}(\Lambda)} \underbrace{\textcolor{highlight}{\gamma^0}\gamma^0}_{=1} = \diracadjoint{\psi} S^{-1}(\Lambda).
    \end{equation}
    
    \section{Dirac Bilinears}
    In order to form the Lagrangian we first need to consider what sort of quantities can appear in it.
    Clearly, the Lagrangian must be a real Lorentz scalar, so we look for ways to form Lorentz scalars from \define{Dirac bilinears}\index{Dirac bilinear}, which are objects of the form \(\diracadjoint{\psi} \Gamma \psi\) where \(\psi\) is some spinor satisfying the Dirac equation and \(\Gamma\) is a product of gamma matrices.
    It turns out that there are only a finite number of linearly independent possibilities for such objects.
    
    We start with the simplest case, where \(\Gamma\) is a product of zero gamma matrices, \(\diracadjoint{\psi}\psi\).
    We can easily see that this is a Lorentz scalar, since it transforms as
    \begin{equation}
        \diracadjoint{\psi}\psi \mapsto \diracadjoint{\psi}S^{-1}(\Lambda)S(\Lambda)\psi = \diracadjoint{\psi}\psi.
    \end{equation}
    
    The next simplest case is when \(\Gamma\) is a single gamma matrix, so let \(j^\mu = \diracadjoint{\psi}\gamma^\mu\psi\).
    Under a Lorentz transformation we have
    \begin{equation}
        j^\mu \mapsto j'^\mu = \diracadjoint{\psi}'\gamma^\mu\psi' = \diracadjoint{\psi}S^{-1}(\Lambda)\gamma^\mu S(\Lambda) \psi.
    \end{equation}
    Now recall that, by definition
    \begin{equation}
        S^{-1}(\Lambda) \gamma^\nu S(\Lambda) = \gamma^\nu \tensor{\Lambda}{^\mu_\nu},
    \end{equation}
    so
    \begin{equation}
        j^\mu \mapsto \diracadjoint{\psi}\tensor{\Lambda}{^\mu_\nu}\gamma^\nu \psi = \tensor{\Lambda}{^\mu_\nu} j^\nu.
    \end{equation}
    Hence \(j^\mu\) is a four-vector.
    
    We can form a scalar from this four vector by contracting with another four-vector.
    An important case is contraction with the derivative, we have a scalar
    \begin{equation}
        \diracadjoint{\psi}\gamma^\mu\partial_\mu\psi = \diracadjoint{\psi}\slashed{\partial}\psi.
    \end{equation}
    
    In order to form a Lagrangian it's not enough for the quantities to be scalars, they also need to be real.
    Starting with \(\diracadjoint{\psi}\psi\) we have
    \begin{equation}
        (\diracadjoint{\psi}\psi)^\hermit = (\psi^\hermit \gamma^0 \psi)^\hermit = \psi^\hermit \gamma^0 (\psi^\hermit)^\hermit = \psi^\hermit \gamma^0 \psi = \diracadjoint{\psi}\psi.
    \end{equation}
    So \(\diracadjoint{\psi}\psi\) is real.
    On the other hand we have
    \begin{equation}
        (\diracadjoint{\psi}\slashed{\partial}\psi)^\hermit = (\psi^\hermit \gamma^0 \gamma^\mu \partial_\mu \psi) = (\partial_\mu\psi^\hermit) (\gamma^\mu)^\hermit (\gamma^0)^\hermit \psi = (\partial_\mu \diracadjoint{\psi}) \gamma^\mu \psi.
    \end{equation}
    Here we've used \((\gamma^\mu)^\hermit (\gamma^0)^\hermit = \gamma^0\gamma^\mu\gamma^0 \gamma^0 = \gamma^0 \gamma^\mu\).
    It isn't immediately clear what this means for the realness of \(\diracadjoint{\psi}\slashed{\partial}\psi\).
    When forming the action we integrate over the Lagrangian, so we can consider this quantity under an integral:
    \begin{equation}
        \int \dl{^4x} \, (\diracadjoint{\psi} \slashed{\partial} \psi)^\hermit = \int \dl{^4x} (\partial_\mu \diracadjoint{\psi}) \gamma^\mu \psi = \int \dl{^4x} [\partial_\mu(\diracadjoint{\psi}\gamma^\mu \psi) - \diracadjoint{\psi}\gamma^\mu\partial_\mu\psi].
    \end{equation}
    Now, the first term is a total derivative, and we are free to add a total derivative to the Lagrangian without changing anything, so we can drop this term without changing the physics.
    The second term is just \(-\diracadjoint{\psi}\slashed{\partial}\psi\).
    So, for the purpose of forming our Lagrangian \((\diracadjoint{\psi}\slashed{\partial}\psi)\) is equivalent to \(-\diracadjoint{\psi}\slashed{\partial}\psi\).
    Hence, we can think of \(\diracadjoint{\psi}\slashed{\partial}\psi\) as a purely imaginary quantity, and so \(i\diracadjoint{\psi}\slashed{\partial}\psi\) is a real quantity.
    
    \section{Dirac Lagrangian}
    We now have enough to form the Dirac Lagrangian.
    We know that we can use \(\diracadjoint{\psi}\psi\) and \(i\diracadjoint{\psi}\slashed{\partial}\psi\) in our Lagrangian and after some thought we find that
    \begin{equation}
        \lagrangianDensity = \diracadjoint{\psi}(i\slashed{\partial} - m)\psi.
    \end{equation}
    The action is then
    \begin{equation}
        S = \int \dl{^4x} \, \diracadjoint{\psi}(i\slashed{\partial} - m)\psi.
    \end{equation}
    
    We can readily check that this is the correct Lagrangian by finding the equations of motion.
    Since \(\psi\) is complex we vary \(\psi\) and \(\diracadjoint{\psi}\) independently.
    First varying \(\diracadjoint{\psi}\) we have
    \begin{equation}
        \diffp{\lagrangianDensity}{\diracadjoint{\psi}} = (i\slashed{\psi} - m)\psi, \qand \diffp{\lagrangianDensity}{(\partial_\mu \diracadjoint{\psi})} = 0 \implies (i\slashed{\partial} - m)\psi = 0,
    \end{equation}
    which is exactly the Dirac equation.
    Now varying \(\psi\) we have
    \begin{equation}
        \diffp{\lagrangianDensity}{\psi} = -m\diracadjoint{\psi}, \qand \diffp{\lagrangianDensity}{(\partial_\mu \psi)} = i\diracadjoint{\psi}\gamma^\mu \implies i\partial_\mu\diracadjoint{\psi}\gamma^\mu + m\diracadjoint{\psi} = 0.
    \end{equation}
    This is simply the Hermitian conjugate of the Dirac equation:
    \begin{align}
        0 &= [i\gamma^\mu(\partial_\mu\psi) - m\psi]^\hermit\\
        &= -i(\partial_\mu\psi)^\hermit (\gamma^\mu)^\hermit - m\psi^\hermit\\
        &= -i\partial_\mu\psi^\hermit \gamma^0\gamma^\mu\gamma^0 - m\psi^\hermit\\
        &= -i\partial_\mu\diracadjoint{\psi}\gamma^\mu\gamma^0 - m\psi^\hermit.
    \end{align}
    Now multiply by \(-\gamma^0\) on the right and we have
    \begin{align}
        0 &= -[i\gamma^\mu(\partial_\mu\psi) - m\psi]^\hermit\gamma^0\\
        &= i\partial_\mu\diracadjoint{\psi} \gamma^\mu (\gamma^0)^2 + m\psi^\hermit\gamma^0\\
        &= i\partial_\mu\diracadjoint{\psi} \gamma^\mu + m\diracadjoint{\psi},
    \end{align}
    which is the equation derived by varying \(\psi\).
    
    As a complex object the spinor has a \(\unitary(1)\) symmetry given by
    \begin{equation}
        \psi \mapsto \e^{-i\alpha}\psi \implies \diracadjoint{\psi} \mapsto \e^{i\alpha}\diracadjoint{\psi},
    \end{equation}
    and so \(\diracadjoint{\psi}\psi\) and \(\diracadjoint{\psi}\slashed{\partial}\psi\) are invariant under this symmetry.
    Hence there is a conserved charge.
    To find it we consider the case of infinitesimal \(\alpha\), where we have
    \begin{equation}
        \delta \psi = \e^{-i\alpha}\psi - \psi \approx (1 - i\alpha)\psi - \psi = -i\alpha\psi.
    \end{equation}
    Hence the conserved current is, scaling out the factor of \(\alpha\),
    \begin{equation}
        j^\mu = \diffp{\lagrangianDensity}{(\partial_\mu \psi)} \delta\psi = \diracadjoint{\psi}\gamma^\mu\psi.
    \end{equation}
    The associated conserved charge is
    \begin{equation}
        Q = \int \dl{^3\vv{x}} \, j^0 = \int\dl{^3\vv{x}} \, \diracadjoint{\psi} \gamma^0 \psi = \int \dl{^3\vv{x}} \, \psi^\hermit \gamma^0 \gamma^0 \psi = \int \dl{^3\vv{x}} \, \psi^\hermit \psi.
    \end{equation}
    
    We can also derive a Hamiltonian for the Dirac equation.
    First, we define the conjugate variables
    \begin{equation}
        \pi = \diffp{\lagrangianDensity}{\dot{\psi}} = i\psi^\hermit, \qqand \diracadjoint{\pi} = \diffp{\lagrangianDensity}{\dot{\diracadjoint{\psi}}} = 0.
    \end{equation}
    Then we form the Hamiltonian density in the usual way:
    \begin{align}
        \hamiltonianDensity &= \pi\dot{\psi} + \diracadjoint{\pi}\diracadjoint{\psi} - \lagrangianDensity\\
        &= i\psi^\hermit \dot{\psi} - i\diracadjoint{\psi}\slashed{\partial}\psi + m\diracadjoint{\psi}\psi\\
        &= \diracadjoint{\psi}(-i\gamma^i\partial_i + m)\psi.
    \end{align}
    \begin{wrn}
        The Hamiltonian derived by integrating this over spacetime is \emph{not} the \enquote{Dirac Hamiltonian}, \(H = -i\vv{\alpha} \cdot \grad + \beta m\).
        This second quantity was mistakenly identified as the Hamiltonian by Dirac when he first derived his equation and the name has stuck.
    \end{wrn}
    
    It is also possible to derive the energy--momentum tensor for the Dirac Lagrangian, and we find that it is
    \begin{equation}
        T^{\mu\nu} = \diracadjoint{\psi} \gamma^\mu \partial^\nu \psi.
    \end{equation}
    
    \chapter{Solutions to the Dirac Equation}
    \section{Free Particle Solution}
    We look for a free particle solution to the Dirac equation,
    \begin{equation}
        (i\slashed{\partial} - m) \psi = 0.
    \end{equation}
    We'll work in the Dirac representation in this chapter.
    We start with the ansatz of plane wave solutions, multiplied by some spinor so that \(\psi\) is a spinor.
    Assuming that the particle is on-shell, \(p^2 = m^2\), we expect this spinor to depend on the three-momentum and mass of the particles, and be independent of position since we're looking for free solutions.
    So we look for solutions of the form
    \begin{equation}
        \psi(x) = u(\vv{p}) \e^{-ip\cdot x}
    \end{equation}
    for some spinor \(u\).
    Substituting this into the Dirac equation we have
    \begin{equation}
        (i\slashed{\partial} - m)u(\vv{p})\e^{-ip\cdot x} = iu(\vv{p})\slashed{\partial}\e^{-ip\cdot x} - mu(\vv{p})\e^{-ip\cdot x} = 0.
    \end{equation}
    Consider the derivative term:
    \begin{equation}
        \slashed{\partial}\e^{-ip\cdot x} = \gamma^\mu\partial_\mu \e^{-ip_\nu x^\nu} = -i\gamma^\mu p_\mu\e^{-ip\cdot x} = -i\slashed{p}\e^{-ip\cdot x}.
    \end{equation}
    Hence, substituting this back into the above equation and dividing through by the phase factor we have
    \begin{equation}
        (\slashed{p} - m) u(\vv{p}) = 0.
    \end{equation}
    
    In the Dirac representation \(u\) is a four-component spinor.
    We can further split it into two two-component spinors, \(\varphi\) and \(\chi\):
    \begin{equation}
        u = 
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        .
    \end{equation}
    We then use the block form of the gamma matrices:
    \begin{equation}
        \gamma^0 = 
        \begin{pmatrix}
            \ident_2 & 0\\
            0 & -\ident_2
        \end{pmatrix}
        , \qqand \gamma^i = 
        \begin{pmatrix}
            0 & \sigma^i\\
            -\sigma^i & 0
        \end{pmatrix}
        ,
    \end{equation}
    to write
    \begin{equation}
        \slashed{p} = \gamma^\mu p_\mu = \gamma^0 p_0 - \gamma^i p_i = 
        \begin{pmatrix}
            p^0 & 0\\
            0 & -p^0
        \end{pmatrix}
        -
        \begin{pmatrix}
            0 & \sigma^i p_i\\
            \sigma^i p_i & 0
        \end{pmatrix}
        =
        \begin{pmatrix}
            p^0 & -\vv{\sigma} \cdot \vv{p}\\
            \vv{\sigma} \cdot \vv{p} & -p^0
        \end{pmatrix}
        .
    \end{equation}
    Then we can write
    \begin{equation}
        (\slashed{p} - m) = 
        \begin{pmatrix}
            p^0 & -\vv{\sigma} \cdot \vv{p}\\
            \vv{\sigma} \cdot \vv{p} & -p^0
        \end{pmatrix}
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        = m
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        .
    \end{equation}
    
    \subsection{Positive Energy Solutions}
    Take \(E = p^0\), and then we have
    \begin{equation}
        \begin{pmatrix}
            E & -\vv{\sigma} \cdot \vv{p}\\
            \vv{\sigma} \cdot \vv{p} & -E
        \end{pmatrix}
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        = m
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        .
    \end{equation}
    Doing the matrix multiplication on the left we get
    \begin{equation}
        \begin{pmatrix}
            E\varphi - \vv{\sigma} \cdot \vv{p} \chi\\
            \vv{\sigma} \cdot \vv{p}\varphi - E\chi
        \end{pmatrix}
        = m
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        .
    \end{equation}
    So we must have
    \begin{align}
        E\varphi - \vv{\sigma} \cdot \vv{p}\chi &= m\varphi,\\
        \vv{\sigma} \cdot \vv{p}\varphi - E\chi &= m\chi,
    \end{align}
    rearranging gives
    \begin{align}
        (E - m)\varphi = \vv{\sigma} \cdot \vv{p}\chi,\\
        (E + m)\chi = \vv{\sigma} \cdot \vv{p}\varphi.
    \end{align}
    Solving the second for \(\chi\) we get
    \begin{equation}
        \chi = \frac{\vv{\sigma} \cdot \vv{p}}{E + m} \varphi.
    \end{equation}
    Substituting this into the first we get
    \begin{equation}
        (E - m)\varphi = \frac{(\vv{\sigma} \cdot \vv{p})^2}{E + m}\varphi.
    \end{equation}
    We can calculate \((\vv{\sigma} \cdot \vv{p})^2\):
    \begin{align}
        (\vv{\sigma} \cdot \vv{p})^2 &= \sigma^i p^i \sigma^j p^j\\
        &= p^ip^j \sigma^i\sigma^j\\
        &= p^ip^j \frac{1}{2}\anticommutator{\sigma^i}{\sigma^j}\\
        &= p^ip^j \delta^{ij}\\
        &= p^ip^j\\
        &= \vv{p}^2.
    \end{align}
    Here we've used the fact that \(p^ip^j\) is symmetric in \(i\) and \(j\) to replace \(\sigma^i\sigma^j\) with its symmetric part, \(\anticommutator{\sigma^i}{\sigma^j}/2\), since any antisymmetric part vanishes in the product with something symmetric.
    
    Hence, we have
    \begin{equation}
        (E - m)\varphi = \frac{\vv{p}^2}{E + m}\varphi \implies \varphi = \frac{\vv{p}}{E^2 - m^2}\varphi = \varphi.
    \end{equation}
    since \(E^2 - m^2 = \vv{p}^2\) for an on-shell particle.
    This means that \(\varphi\) is not fixed by the Dirac equation, only its relation to \(\chi\) is fixed.
    We can therefore choose \(\varphi\), and as a two-component spinor we make the choice that
    \begin{equation}
        \varphi^1 = 
        \begin{pmatrix}
            1\\ 0
        \end{pmatrix}
        , \qqand \varphi^2 = 
        \begin{pmatrix}
            0\\ 1
        \end{pmatrix}
        .
    \end{equation}
    This is a basis for the space of two-component spinors allowing us to express arbitrary \(\varphi\).
    We can then consider two positive energy solutions, one for each basis spinor:
    \begin{equation}
        u(\vv{p}, s) = N
        \begin{pmatrix}
            \varphi^s\\
            \frac{\vv{\sigma} \cdot \vv{p}}{E + m}\varphi^s
        \end{pmatrix}
        .
    \end{equation}
    Here \(N\) is some normalisation factor which we'll discuss later.
    
    Note that in the rest frame of the particles we get
    \begin{equation}
        u(\vv{0}, s) = N
        \begin{pmatrix}
            \varphi_s\\ 0
        \end{pmatrix}
    \end{equation}
    which is an eigenvector of the spin operator
    \begin{equation}
        \frac{1}{2}\Sigma^3 = \frac{1}{2}
        \begin{pmatrix}
            \sigma^3 & 0\\
            0 & \sigma^3
        \end{pmatrix}
    \end{equation}
    with eigenvalues \(1/2\) and \(-1/2\) for \(s = 1\) and \(s = 2\) respectively.
    We therefore interpret \(\varphi^1\) with spin up particles and \(\varphi^2\) with spin down particles.
    
    \subsection{Negative Energy Solutions}
    If instead we take \(p^\mu = (-E, -\vv{p})\) then we get slightly different solutions.
    First, we make the alternative ansatz
    \begin{equation}
        \psi(x) = v(\vv{p})\e^{+ip\cdot x}.
    \end{equation}
    Substituting this into the Dirac equation and cancelling the phase factor we get
    \begin{equation}
        (\slashed{p} + m)v(\vv{p}) = 0.
    \end{equation}
    Following the same steps of writing this as a block matrix and computing the two components and rearranging we get
    \begin{equation}
        \varphi = \frac{\vv{\sigma} \cdot \vv{p}}{E + m}\chi,
    \end{equation}
    so \(\varphi\) and \(\chi\) have swapped compared to the positive energy case.
    Again \(\chi\) is not fixed after substituting this into the equation from the other component, and we have the solution
    \begin{equation}
        v(\vv{p}, s) = N
        \begin{pmatrix}
            \frac{\vv{\sigma} \cdot \vv{p}}{E + m} \chi^s\\ \chi^s
        \end{pmatrix}
    \end{equation}
    where
    \begin{equation}
        \chi^1 = 
        \begin{pmatrix}
            0\\ 1
        \end{pmatrix}
        , \qqand \chi^2 = 
        \begin{pmatrix}
            -1\\ 0
        \end{pmatrix}
    \end{equation}
    is a basis for the space of two-component spinors.
    We choose this basis, rather than the more obvious choice of \(\chi^1 = (1, 0)^{\trans}\) and \(\chi^2 = (0, 1)^\trans\), since we want to interpret the negative energy particles as antiparticles, which have opposite spin to the corresponding particles, and then the negative sign comes from requiring charge conjugation symmetry.
    
    The important thing is that, despite Dirac's best attempts, there are still negative energy solutions.
    It turns out that these are unavoidable, since they correspond to antiparticles.
    
    \section{Spin, Angular Momentum and Helicity}
    We've interpreted
    \begin{equation}
        S_3 = \frac{1}{2}\Sigma^3 = \frac{1}{2}
        \begin{pmatrix}
            \sigma^3 & 0\\
            0 & \sigma^3
        \end{pmatrix}
    \end{equation}
    as measuring the spin along the \(z\)-axis.
    We can further define
    \begin{equation}
        \vv{S} \coloneqq \frac{1}{2}\vv{\Sigma} = \frac{1}{2}
        \begin{pmatrix}
            \vv{\sigma} & 0\\
            0 & \vv{\sigma}
        \end{pmatrix}
    \end{equation}
    as the spin operator with eigenvalues \(\pm 1/2\), and
    \begin{equation}
        \vv{S}^2 \coloneqq \frac{3}{4} (\vv{\Sigma}^3)^2 = \frac{3}{4}\ident_4
    \end{equation}
    is the spin operator with eigenvalues \(3/4 = 1/2(1/2 + 1)\).
    
    The operator \(\vv{\Sigma}\) only commutes with the Dirac Hamiltonian, \(\vv{\alpha} \cdot \vv{p} + \beta m\), in the rest frame of the particle.
    The angular momentum operator, \(\vv{L} = \vv{r} \times \vv{p}\), also only commutes with the Dirac Hamiltonian in the rest frame of the particle.
    However,
    \begin{equation}
        \vv{J} = \vv{L} + \vv{S}
    \end{equation}
    commutes with the Dirac Hamiltonian in any frame, suggesting we interpret it as the total angular momentum operator.
    
    Another choice of operator which commutes in any frame is the \defineindex{helicity} operator
    \begin{equation}
        h(\vv{p}) = \frac{\vv{\Sigma} \cdot \vv{p}}{\abs{\vv{p}}}.
    \end{equation}
    This measures the spin along the direction of propagation of the particle.
    
    \section{Normalisation}
    There are different conventions for how to normalise spinors.
    We'll make the choice to define
    \begin{equation}
        N = \sqrt{E + m}
    \end{equation}
    so
    \begin{equation}
        u(\vv{p}, s) = \sqrt{E + m}
        \begin{pmatrix}
            \varphi^s\\ \frac{\vv{\sigma} \cdot \vv{p}}{E + m}\varphi^s
        \end{pmatrix}
        , \qqand v(\vv{p}, s) = \sqrt{E + m}
        \begin{pmatrix}
            \frac{\vv{\sigma} \cdot \vv{p}}{E + m}\chi^s\\ \chi^s
        \end{pmatrix}
        .
    \end{equation}
    We then have
    \begin{equation}
        u^\hermit u = (E + m)\left[ \varphi^\hermit \varphi + \varphi^\hermit \left( \frac{\vv{\sigma} \cdot \vv{p}}{E + m} \right)^2 \varphi \right]
    \end{equation}
    and using \(\varphi^\hermit \varphi = 1\) for either spin and \((\vv{\sigma} \cdot \vv{p})^2 = \vv{p}^2 = E^2 - m^2\) we have
    \begin{align}
        u^\hermit u &= (E + m)\left[ 1 + \frac{E^2 - m^2}{(E + m)^2} \right]\\
        &= (E + m)\left[ 1 + \frac{(E + m)(E - m)}{(E + m)^2} \right]\\
        &= E + m + E - m\\
        &= 2E.
    \end{align}
    Similarly \(v^\hermit v = 2E\).
    
    Defining the Dirac adjoints, \(\diracadjoint{u} = u^\hermit \gamma^0\) and \(\diracadjoint{v} = v^\hermit \gamma^0\) we can compute \(\diracadjoint{u}u\) in the rest frame, since \(\diracadjoint{u}u\) is a Lorentz invariant scalar, and we find
    \begin{align}
        \diracadjoint{u}u &= (E + m)\left[
        \begin{pmatrix}
            \varphi^\hermit & \chi^\hermit
        \end{pmatrix}
        \gamma^0
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
        + 
        \begin{pmatrix}
            \varphi^\hermit & \chi^\hermit
        \end{pmatrix}
        \frac{(\vv{\sigma} \cdot \vv{p}) \gamma^0 (\vv{\sigma} \cdot \vv{p})}{(E + m)^2}
        \begin{pmatrix}
            \varphi\\ \chi
        \end{pmatrix}
         \right]\\
        &= E + m\\
        &= 2m
    \end{align}
    since the second term vanishes in the rest frame, with \(\vv{p} = \vv{0}\), and \(E = m\) in the rest frame.
    Similarly we can show that
    \begin{equation}
        \diracadjoint{v}v = -2m.
    \end{equation}
    Other quantities, such as \(\diracadjoint{u}v\), \(\diracadjoint{v}u\), and \(\diracadjoint{u}(\vv{p}, s)u(\vv{p}, s')\) with \(s \ne s'\) vanish.
    
    We've just computed the inner products of our spinor solutions, we can also compute outer products, \(u\diracadjoint{u}\).
    These are most interesting if we sum over spins, where we get the results
    \begin{align}
        \sum_s u(\vv{p}, s) \diracadjoint{u}(\vv{p}, s) &= \slashed{p} + m,\label{eqn:spin sum u}\\
        \sum_s v(\vv{p}, s) \diracadjoint{v}(\vv{p}, s) &= \slashed{p} - m.
    \end{align}
    Note that the sign is the opposite to the sign in \((\slashed{p} - m)u = (\slashed{p} + m)v = 0\).
    
    Finally, we can define the projection operators
    \begin{equation}
        \Lambda_{\pm} \coloneqq \frac{1}{2m}(\pm \slashed{p} + m).
    \end{equation}
    These have the effect of projecting on to the positive and negative energy eigenspaces:
    \begin{align}
        \Lambda_{+} u(\vv{p}, s) &= \frac{1}{2m}(\slashed{p} + m)u\\
        &= \frac{1}{2m} \sum_{s'} u(\vv{p}, s')\diracadjoint{u}(\vv{p}, s') u(\vv{p}, s)\\
        &= \frac{1}{2m} \delta_{ss'} u(\vv{p}, s') 2m\\
        &= u(\vv{p}, s),
    \end{align}
    and similarly
    \begin{equation}
        \Lambda_- v(\vv{p}, s) = v(\vv{p}, s).
    \end{equation}
    Since they are projection operators they are idempotent, \(\Lambda_{\pm}^2 = \Lambda_{\pm}\), and the are also orthogonal projectors, \(\Lambda_{+}\Lambda_{-} = \Lambda_{-}\Lambda_{+} = 0\).
    In particular we can show that \(\Lambda_+ u = u\), \(\Lambda_{+}v = 0\), \(\Lambda_- u = 0\), and \(\Lambda_{-}v = v\), so
    \begin{equation}
        \Lambda+ (\alpha u + \beta v) = \alpha u, \qqand \Lambda_- (\alpha u + \beta v) = \beta v
    \end{equation}
    for \(\alpha, \beta \in \complex\).
    
    \part{Quantising the Dirac Equation}
    \chapter{Quantisation}
    \section{Mode Expansion}
    The spinor \(\psi\) satisfying the Dirac equation is a complex object, so the mode expansion is similar to that of a complex scalar field, as seen in \cref{sec:mode expansion complex scalar field}.
    The difference is that as well as momenta labelling the states we also have spins, and instead of a scalar we want a spinor as a result.
    So, we sum over spins and include a basis spinor in each term.
    The mode expansion is then
    \begin{equation}
        \psi(x) = \sum_s \int \invariantmeasure{p} \, [a_s(\vv{p}) u(\vv{p}, s) \e^{-ip \cdot x} + b_s^\hermit(\vv{p}) v(\vv{p}, s) \e^{ip\cdot x}].
    \end{equation}
    The adjoint spinor then has the mode expansion
    \begin{equation}
        \diracadjoint{\psi}(x) = \sum_s \int \invariantmeasure{p} \, [b_s(\vv{p}) \diracadjoint{v}(\vv{p}, s) \e^{-ip\cdot x} + a_s^\hermit(\vv{p}) \diracadjoint{u}(\vv{p}, s) \e^{ip\cdot x}].
    \end{equation}
    
    In these expressions \(u\), \(v\), \(\diracadjoint{u}\), and \(\diracadjoint{v}\) are just classical spinor-valued functions satisfying the Dirac equation.
    The objects \(a_s\), \(b_s\), \(a_s^\hermit\), and \(b_s^\hermit\) are operators on the Hilbert space of states.
    This means we have five (or eight, depending on how you count) different spaces going on in this expression, and its important not to confuse them:
    \begin{itemize}
        \item Real space, which is the space \(x\) lives in.
        \item Momentum space, which is the space \(p\) lives in.
        \item The spinor space (and its dual) which \(u\), \(v\), \(\diracadjoint{u}\), and \(\diracadjoint{v}\) live in.
        \item The Hilbert space of states (and its dual) which \(a\), \(b\), \(a^\hermit\), and \(b^\hermit\) act on.
        \item The space of operators on this Hilbert space (and the space of operators on the dual Hilbert space) which is where \(a\), \(b\), \(a^\hermit\), and \(b^\hermit\) live.
    \end{itemize}
    
    We'll see that \(a_s^\hermit(\vv{p})\) and \(b_s^\hermit(\vv{p})\) can be interpreted as creating distinct types of particles of charge \(\vv{p}\) and spin \(s\), whereas \(a_s(\vv{p})\) and \(b_s(\vv{p})\) annihilate them.
    We then interpret particles of type \(a\) as particles, and particles of type \(b\) as antiparticles.
    
    \section{Equal Time Anticommutation Relations}
    Spinors represent spin \(1/2\) particles, these satisfy Fermi--Dirac statistics, meaning that exchanging two fermions gives a negative sign.
    The way we encode this is to have the mode operators obey \emph{anti}commutation relations, instead of commutation relations.
    
    We'll start by imposing equal time anticommutation relations, based on the equal time commutation relations for scalar fields, and then we'll explore what this means for the fields.
    In analogy with the scalar fields we look for the mode operators to anticommute, except possibly when they have the same momentum and spin, that is
    \begin{align}
        \anticommutator{a_r(\vv{p})}{a_s^\hermit(\vv{p}')} &= \delta_{rs} \bardelta(\vv{p} - \vv{p}'),\\
        \anticommutator{b_r(\vv{p})}{b_s^\hermit(\vv{p}')} &= \delta_{rs} \bardelta(\vv{p} - \vv{p}').
    \end{align}
    All other anticommutation relations are zero, such as \(\anticommutator{a_r(\vv{p})}{a_s(\vv{p}')} = 0\) and \(\anticommutator{a_s(\vv{p})}{b_r^\hermit(\vv{p}')}\).
    Note that this implies
    \begin{equation}
        a_r(\vv{p})a_r(\vv{p}) = a_r^\hermit(\vv{p})a_r^\hermit(\vv{p}) = b_r(\vv{p})b_r(\vv{p}) = b_r^\hermit(\vv{p})b_r^\hermit(\vv{p}) = 0.
    \end{equation}
    
    \section{Creation and Annihilation}
    In this section we'll work with and interpret \(a\) and \(a^\hermit\), however replacing all \(a\)s with \(b\)s everything will still hold.
    Recall that we can interpret the mode operators for a scalar field theory as creation and annihilation operators because they have the effect of sending an energy eigenstate to an energy eigenstate with a different energy.
    This in turn follows due to the commutation relation \(\commutator{a^\hermit(\vv{p})a(\vv{p})}{a^\hermit(\vv{p})} = \bardelta(\vv{p} - \vv{p}')\) and \(\commutator{a^\hermit(\vv{p}) a(\vv{p})}{a_s(\vv{p})} = -a_r(\vv{p})\bardelta(\vv{p} - \vv{p}')\), and the product of \(a\) and \(a^\hermit\) is part of the Hamiltonian..
    Well, we can show that the same commutation relation holds in this case too.
    
    Consider the following combination of anticommutators:
    \begin{equation}
        A\anticommutator{B}{C} - \anticommutator{A}{C}B = A(BC + CB) - (AC + CA)B = ABC - CAB = \commutator{AB}{C}.
    \end{equation}
    We therefore have
    \begin{align}
        \commutator{a_r^\hermit(\vv{p}) a_r(\vv{p})}{a_s^\hermit(\vv{p}')} &= a_r^\hermit(\vv{p}) \anticommutator{a_r(\vv{p})}{a_s^\hermit(\vv{p}')} - \anticommutator{a_r^\hermit(\vv{p})}{a_s^\hermit(\vv{p}')}a_r(\vv{p})\\
        &= a_r^\hermit(\vv{p}) \delta_{rs} \bardelta(\vv{p} - \vv{p}')
    \end{align}
    And similarly
    \begin{equation}
        \commutator{a_r^\hermit(\vv{p})a_r(\vv{p})}{a_s(\vv{p}')} = -a_r(\vv{p}) \delta_{rs}\bardelta(\vv{p} - \vv{p}').
    \end{equation}
    
    This means we can still interpret \(a_r^\hermit(\vv{p})a_r(\vv{p})\) as a number operator, the eigenvalue being the number of particles of type \(a\) with momentum \(\vv{p}\) and spin \(r\).
    Then \(a^\hermit_r(\vv{p})\) is a creation operator, creating a particle with momentum \(\vv{p}\) and spin \(r\), and \(a_r(\vv{p})\) is an annihilation operator, annihilating a particle with momentum \(\vv{p}\) and spin \(r\).
    Identical results hold replacing all \(a\)s with \(b\).
    
    One big difference with the scalar field case is which states can exist, that is what the Fock space is.
    All states are of the form
    \begin{equation}
        \ket{\vv{p_1}, s_1, \vv{p_2}, s_2, \dotsc, \vv{p_n}, s_n} = a_{s_1}^\hermit(\vv{p_1}) a_{s_2}^\hermit(\vv{p_2}) \dotsm a_{s_n}^\hermit(\vv{p_n}) \ket{0}
    \end{equation}
    where for \(i \ne j\) either \(\vv{p_i} \ne \vv{p_j}\) or \(s_i \ne s_j\).
    Suppose that we do try to create two particles with the same momentum and spin, that is, we try to produce the state
    \begin{equation}
        \ket{\vv{p}, s, \vv{p}, s} = a_s^\hermit(\vv{p})a_s^\hermit(\vv{p})\ket{0}.
    \end{equation}
    Then by the anticommutation relation we can swap the two creation operators at the cost of a negative, so we have
    \begin{equation}
        \ket{\vv{p}, s, \vv{p}, s} = -a_s^\hermit(\vv{p})a_s^\hermit(\vv{p})\ket{0}.
    \end{equation}
    This can only hold if \(\ket{\vv{p}, s, \vv{p}, s} = 0\).
    This is the genesis of the \defineindex{Pauli exclusion principle} in quantum field theory.
    If we try to create a two spin \(1/2\) particles in the same state we don't get a valid state vector.
    
    Another way of putting this is if we characterise a state by its occupation numbers, that is there are \(N_{s_1}(\vv{p_1})\) particles of type \(a\), momentum \(\vv{p_1}\), and spin \(s_1\), \(N_{s_2}(\vv{p_2})\) particles of type \(a\), momentum \(\vv{p_2}\), and spin \(s_2\), and so on, then \(N_s(\vv{p})\) is either 0 or 1.
    For bosons on the other hand the occupation number can be any nonnegative integer.
    
    Formally, the \define{Fock space}\index{Fock space!fermions} (for fermions) is defined to be
    \begin{equation}
        F_{-}(\hilbertSpace) = \overline{\bigoplus_{n=0}^{\infty} A \hilbertSpace^{\otimes n}} = \overline{\complex \oplus \hilbertSpace \oplus A(\hilbertSpace \otimes \hilbertSpace) \oplus A(\hilbertSpace \otimes \hilbertSpace \otimes \hilbertSpace)}
    \end{equation}
    where \(\hilbertSpace\) is the single particle Hilbert space,
    \begin{equation}
        \hilbertSpace^{\otimes n} \coloneqq \bigotimes_{i=1}^n \hilbertSpace = \underbrace{\hilbertSpace \otimes \dotsb \otimes \hilbertSpace}_{n \text{ times}},
    \end{equation}
    the operator \(A\) antisymmetries all tensors, and the line over the top of these statements denotes the Hilbert space completion, adding in the limits of all absolutely convergent series.
    Note that
    \begin{equation}
        F_-(\hilbertSpace) = \overline{\bigwedge \hilbertSpace}
    \end{equation}
    where \(\bigwedge V\) is the exterior algebra of \(V\), defined by taking the tensor algebra, \(T(V)\), and quotienting out by the ideal generated by tensors of the form \(x \otimes x\) for \(x \in V\).
   
   \section{Equal Time Anticommutation Relations of the Fields}
   The canonical momentum is
   \begin{equation}
       \pi(x) = i\psi^\hermit(x).
   \end{equation}
    We can then compute the equal time anticommutation relations for the field operators, \(\psi\) and \(\pi\), using this.
    To do this we need to consider spinor indices, which we denote by \(a\) and \(b\) here, note in the Dirac representation that these run from \(0\) to \(3\), as they index into four-component spinors, they aren't Lorentz indices where Latin letters typically run from \(1\) to \(3\).
    The equal time anticommutation relation between \(\psi\) and \(\pi\) is
    \begin{equation}
        \anticommutator{\psi_a(t, \vv{x})}{\pi_b(t, \vv{x}')} = \anticommutator{\psi_a(t, \vv{x})}{i\psi_b^\hermit(t, \vv{x}')} = i\delta_{ab} \delta(\vv{x} - \vv{x}').
    \end{equation}
    Often more useful is the equal time anticommutation relation between \(\psi\) and \(\diracadjoint{\psi}\),
    \begin{equation}
        \anticommutator{\psi_a(t, \vv{x})}{\diracadjoint{\psi}_b(t, \vv{x}')} = \gamma^0_{ab} \delta(\vv{x} - \vv{x}').
    \end{equation}
    Also, we have \(\anticommutator{\psi_a(t, \vv{x})}{\psi_b(t, \vv{x}')} = \anticommutator{\diracadjoint{\psi}_a(t, \vv{x})}{\diracadjoint{\psi}_b(t, \vv{x}')} = 0\).
    
    We will prove the equal time anticommutation relation for \(\psi\) and \(\diracadjoint{\psi}\).
    We have
    \begin{multline}
        \anticommutator{\psi_a(t, \vv{x})}{\diracadjoint{\psi}_b(t, \vv{x}')} =\\
        \qquad \sum_{s, s'} \int \invariantmeasure{p} \int \invariantmeasure{p'} \big[ \anticommutator{a_s(\vv{p})}{a_{s'}^\hermit(\vv{p}')} u_a(\vv{p}, s) \diracadjoint{u}_b(\vv{p}', s') \e^{-ip\cdot x + ip' \cdot x'}\\
        + \anticommutator{b_s^\hermit(\vv{p})}{b_{s'}(\vv{p}')} v_a(\vv{p}, s) \diracadjoint{v}_b(\vv{p}', s') \e^{ip\cdot x - ip' \cdot x'} \big]
    \end{multline}
    where the terms with anticommutators mixing the \(a\) and \(b\) mode operators vanish.
    Using the anticommutation relation this becomes
    \begin{multline}
        \anticommutator{\psi_a(t, \vv{x})}{\diracadjoint{\psi}_b(t, \vv{x}')} =\\
        \qquad \sum_{s, s'} \int \invariantmeasure{p} \int \invariantmeasure{p'} \big[ \delta_{ss'}\bardelta(\vv{p} - \vv{p}') u_a(\vv{p}, s) \diracadjoint{u}_b(\vv{p}', s') \e^{-ip\cdot x + ip' \cdot x'}\\
        + \delta_{s, s'} \bardelta(\vv{p} - \vv{p}') v_a(\vv{p}, s) \diracadjoint{v}_b(\vv{p}', s') \e^{ip\cdot x - ip' \cdot x'} \big].
    \end{multline}
    We can then use the Kronecker delta to remove one of the sums setting \(s = s'\) and the Dirac delta to remove one of the integrals, setting \(p' = p\):
    \begin{multline}
        \anticommutator{\psi_a(t, \vv{x})}{\diracadjoint{\psi}_b(t, \vv{x}')} =\\
        \sum_{s} \int \invariantmeasure{p} \big[ u_a(\vv{p}, s) \diracadjoint{u}_b(\vv{p}, s) \e^{-ip\cdot (x - x')} + v_a(\vv{p}, s) \diracadjoint{v}_b(\vv{p}, s) \e^{-ip \cdot (x' - x)} \big].
    \end{multline}
    Now we can use the spin sums
    \begin{equation}
        \sum_s u(\vv{p}, s)\diracadjoint{u}(\vv{p}, s) = \slashed{p} + m, \qqand \sum_s v(\vv{p}, s) \diracadjoint{v}(\vv{p}, s) = \slashed{p} - m,
    \end{equation}
    giving
    \begin{equation*}
        \anticommutator{\psi_a(t, \vv{x})}{\diracadjoint{\psi}_b(t, \vv{x}')} =
        \int \invariantmeasure{p} \big[ (\slashed{p} + m)_{ab} \e^{-ip\cdot (x - x')} + (\slashed{p} - m)_{ab} \e^{-ip \cdot (x' - x)} \big].
    \end{equation*}
    Now we use the fact that we're considering equal time commutation relations, meaning \(x^0 - x'^0 = 0\), to rewrite \(p \cdot (x - x') = -\vv{p} \cdot (\vv{x} - \vv{x}')\), with the negative coming from the metric signature.
    We also have \(\slashed{p} = \gamma^0p_0 - \gamma^ip_i\).
    We can use \(p_0 = \omega(\vv{p})\) to write this as \(\slashed{p} = \gamma^0\omega(\vv{p}) - \gamma^ip_i\), and the second term doesn't contribute to the integral because it is odd under \(p \to -p\), and the measure is even under this transformation.
    We can also make the transformation \(p \to -p\) in the second term, which makes both exponentials the same without changing the value of the integral, and then the mass terms cancel.
    Hence,
    \begin{align}
        \anticommutator{\psi_a(t, \vv{x})}{\diracadjoint{\psi}_b(t, \vv{x}')} &= \int \invariantmeasure{p} \, 2\omega(\vv{p}) \gamma^0_{ab} \e^{i\vv{p} \cdot (\vv{x} - \vv{x}')}\\
        &= \int \frac{\dl{^3p}}{(2\pi)^3}\frac{1}{2\omega(\vv{p})} 2\omega(\vv{p}) \gamma^0_{ab} \e^{i\vv{p} \cdot (\vv{x} - \vv{x}')}\\
        &= \int \frac{\dl{^3p}}{(2\pi)^3} \gamma^0_{ab} \e^{\vv{p} \cdot (\vv{x} - \vv{x}')}\\
        &= \gamma^0_{ab} \delta(\vv{x} - \vv{x}').
    \end{align}
    
    \section{Hamiltonian}
    \epigraph{Life is short. Certainly these lectures are.}{Richard Ball, skipping lots of algebra}
    
    The Hamiltonian is
    \begin{equation}
        H = \int \dl{^3\vv{x}} \left( i \psi^\hermit(x) \dot{\psi}(x) - \lagrangianDensity(x) \right).
    \end{equation}
    We can insert a factor of \((\gamma^0)^2 = 1\) to get
    \begin{align}
        H &= \int \dl{^3\vv{x}} \left( i \psi^\hermit(x) (\gamma^0)^2 \partial_0 \psi(x) - \lagrangianDensity(x) \right)\\
        &= \int \dl{^3\vv{x}} \left( i \diracadjoint{\psi}(x) \gamma^0 \partial_0 \psi(x) - \lagrangianDensity(x) \right).
    \end{align}
    Now using \(\lagrangianDensity = \diracadjoint{\psi}(i\slashed{\partial} - m)\psi = \diracadjoint{\psi}(i\gamma^0\partial_0 + i\gamma^i\partial_i - m)\psi\) to cancel the first term and give
    \begin{equation}
        H = \int \dl{^3\vv{x}} \left( -i\diracadjoint{\psi}(x) \vv{\gamma} \cdot \grad \psi(x) + m \diracadjoint{\psi}(x)\psi(x) \right)
    \end{equation}
    
    We can work out the gradient from the mode expansion:
    \begin{equation}
        \grad \psi = \sum_s \int \invariantmeasure{p} \, (-i\vv{p}) [a_s(\vv{p}) u(\vv{p}, s) \e^{-ip\cdot x} - b_s^\hermit(\vv{p}) v(\vv{p}, s)\e^{ip\cdot x}].
    \end{equation}
    Using this the Hamiltonian is given by
    \begin{align}
        H &= \sum_{s,s'}\int \dl{^3\vv{x}} \int \invariantmeasure{p} \int \invariantmeasure{p'} \, [(b_s(\vv{p})\diracadjoint{v}(\vv{p}, s)\e^{-ip\cdot x} + a_s^\hermit(\vv{p}) \diracadjoint{u}(\vv{p}, s) \e^{ip\cdot x})(-\vv{p} \cdot \vv{\gamma}) \notag\\
        &\qquad (a_{s'}(\vv{p}') u(\vv{p}', s') \e^{-ip'\cdot x} - b_{s'}^\hermit(\vv{p}') v(\vv{p}', s') \e^{ip'\cdot x})\\
        &\quad + m(b_s(\vv{p}) \diracadjoint{v}(\vv{p}, s) \e^{-ip\cdot x} + a_s^\hermit(\vv{p}) \diracadjoint{u}(\vv{p}, s) \e^{ip\cdot x})\\
        &\qquad (a_{s'}(\vv{p}') u(\vv{p}', s') \e^{-ip'\cdot x} + b_{s'}^\hermit(\vv{p}') v(\vv{p}', s') \e^{ip'\cdot x})]
    \end{align}
    Expanding everything, then simplifying using spin sums and the anticommutation relations we get the result
    \begin{equation}
        H = \frac{1}{2} \sum_s \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} [a_s^\hermit(\vv{p}) a_s(\vv{p}) + b_s^\hermit(\vv{p}) b_s(\vv{p}) - \delta_{ss} 2\omega(\vv{p})(2\pi)^3 \delta(0)].
    \end{equation}
    We see that this gives us a divergent ground state, which we can deal with by normal ordering.
    Normal ordering works for fermions almost identically to bosons, but when we exchange two terms to normal order them we include a minus sign.
    That is, if \(\psi = \psi^+ + \psi^-\) then the normal ordering of \(\psi_a\psi_b\) is
    \begin{align}
        \normalordering{\psi_a\psi_b} &= \normalordering{(\psi_a^+ + \psi_a^-)(\psi_b^+ + \psi_b^-)}\\
        &= \normalordering{\psi_a^+\psi_b^+ + \psi_a^+\psi_b^- + \psi_a^-\psi_b^+ + \psi_a^-\psi_b^-}\\
        &= \hphantom{\vcentcolon}\psi_a^+\psi_b^+ + \psi_a^+\psi_b^- \mathbin{\textcolor{highlight}{-}} \psi_b^+\psi_a^- + \psi_a^-\psi_b^-
    \end{align}
    This means that
    \begin{equation}
        \normalordering{\psi_a\psi_b} = -\normalordering{\psi_b\psi_a},
    \end{equation}
    so we can't just reorder terms within a normal ordering in the way we could for bosons.
    
    Suppose we had used commutators instead of anticommutators.
    As well as a lot of what we've done so far not working the biggest problem would be that the Hamiltonian would have a negative sign in front of the \(b_s^\hermit(\vv{p})b_s(\vv{p})\) term.
    This would mean that by adding particles of type \(b\) we would be able to make the energy arbitrarily negative, meaning we would have no ground state, and then no way to interpret the theory.
    
    The Hamiltonian is given in terms of the energy--momentum tensor by
    \begin{equation}
        H = \int \dl{^3\vv{x}} \, T^{00}, \qqwhere T^{\mu\nu} = i \normalordering{\diracadjoint{\psi} \gamma^\mu \partial^\nu \psi}.
    \end{equation}
    The conserved three-momentum is
    \begin{equation}
        \vv{P} = \sum_s \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{\vv{p}}{2\omega(\vv{p})} [a_s^\hermit(\vv{p})a_s(\vv{p}) + b_s^\hermit(\vv{p})b_s(\vv{p})].
    \end{equation}
    
    We also have the expected commutation relations
    \begin{alignat}{2}
        \commutator{H}{a_s^\hermit(\vv{p})} &= \omega(\vv{p})a_s^\hermit(\vv{p}), \qquad & \commutator{H}{a_s(\vv{p})} &= -\omega(\vv{p})a_s(\vv{p}),\\
        \commutator{H}{b_s^\hermit(\vv{p})} &= \omega(\vv{p})b_s^\hermit(\vv{p}), \qquad & \commutator{H}{b_s(\vv{p})} &= -\omega(\vv{p})b_s(\vv{p}).
    \end{alignat}
    This means we can interpret \(a_s^\hermit(\vv{p})\) and \(b_s^\hermit(\vv{p})\) as creating particles of energy \(\omega(\vv{p})\) and \(a_s(\vv{p})\) and \(b_s(\vv{p})\) annihilating particles of energy \(-\omega(\vv{p})\).
    
    \section{Charge Conservation}
    As the spinors are complex the Dirac Lagrangian is invariant under the \(\unitary(1)\) symmetry
    \begin{equation}
        \psi \mapsto \e^{i\alpha}\psi.
    \end{equation}
    The associated conserved current is
    \begin{equation}
        j^\mu = \normalordering{\diracadjoint{\psi} \gamma^\mu \psi},
    \end{equation}
    and the conserved charge is
    \begin{equation}
        Q = \int \dl{^3\vv{x}} j^0 = \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} [a_s^\hermit(\vv{p}) a_s(\vv{p}) - b_s^\hermit(\vv{p}) b_s(\vv{p})].
    \end{equation}
    This means we can interpret particles of type \(a\) as having charge \(+1\) and particles of charge \(b\) as having charge \(-1\), since the charge is given by the number operator for \(a\) minus the number operator for \(b\), so the total charge is the number of particles of type \(a\) minus the number of particles of type \(b\).
    
    Normally we rescale the charges by a factor \(q = -e\), where \(e\) is the \emph{magnitude} of charge of the electron.
    Then the interpretation is that
    \begin{itemize}
        \item particles of type \(a\) (particles) have charge \(-e\),
        \item particles of type \(b\) (antiparticles) have charge \(+e\).
    \end{itemize}
    
    \chapter{Covariant Anticommutation Relations}
    \section{Covariant Anticommutation Relations}
    The covariant anticommutator, \(\anticommutator{\psi(x)}{\diracadjoint{\psi}(y)}\), can be calculated by substituting in the mode expansions.
    To do this we first write \(\psi\) as \(\psi = \psi^+ + \psi^-\) where
    \begin{align}
        \psi^+(x) &= \sum_s \int \invariantmeasure{p} \, a_s(\vv{p}) u(\vv{p}, s) \e^{-ip\cdot x},\\
        \psi^-(x) &= \sum_s \int \invariantmeasure{p} \, b_s^\hermit(\vv{p}) u(\vv{p}, s) \e^{ip\cdot x},
    \end{align}
    and similarly for \(\diracadjoint{\psi}\).
    Then the only nonzero contributions to the anticommutator will come from \(\anticommutator{\psi^+}{\diracadjoint{\psi}^-}\) and \(\anticommutator{\psi^-}{\diracadjoint{\psi}^-}\).
    We'll compute the first here, including explicit spinor indices, \(a\) and \(b\):
    \begin{align}
        \anticommutator{\psi_a^+(x)}{\psi_b^-(y)} &= \sum_{s, s'} \int \invariantmeasure{p} \int \invariantmeasure{p'} \, \anticommutator{a_s(\vv{p})}{a_{s'}^{\vv{p}'}} u_a(\vv{p}, s) \diracadjoint{u}_b(\vv{p}', s') \e^{-ip\cdot x + ip' \cdot y}\\
        &= \sum_{s, s'} \int \invariantmeasure{p} \int \invariantmeasure{p'} \, \delta_{ss'} \bardelta(\vv{p} - \vv{p}') u_a(\vv{p}, s) \diracadjoint{u}_b(\vv{p}', s') \e^{-ip\cdot x + ip' \cdot y}\\
        &= \sum_{s} \int \invariantmeasure{p} \, u_a(\vv{p}, s) \diracadjoint{u}_b(\vv{p}, s) \e^{-ip\cdot x + ip \cdot y}\\
        &= \int \invariantmeasure{p} (\slashed{p} + m)_{ab} \e^{-ip\cdot (x - y)}.
    \end{align}
    Here we've made use of the spin sum in \cref{eqn:spin sum u}.
    Next, notice that
    \begin{equation}
        \slashed{\partial} \e^{-ip\cdot (x - y)} = \gamma^\mu \partial_\mu \e^{-ip\cdot (x - y)} = \gamma^\mu(-ip^\mu) \e^{-ip\cdot (x - y)} = -i\slashed{p}\e^{-ip\cdot(x - y)},
    \end{equation}
    so we can replace \(\slashed{p}\) with \(i\slashed{\partial}\), giving
    \begin{align}
        \anticommutator{\psi_a^+(x)}{\psi_b^-(y)} &= \int \invariantmeasure{p} (i\slashed{\partial} + m)_{ab} \e^{-ip\cdot (x - y)}\\
        &= (i\slashed{\partial} + m)_{ab} \int \invariantmeasure{p} \, \e^{-ip\cdot (x - y)}\\
        &= (i\slashed{\partial} + m)_{ab} i \Delta^+(x - y),
    \end{align}
    where \(\Delta^+\) is the same propagator as for the scalar case as defined in \cref{sec:covariant commutators what are they}.
    
    We've seen that squaring the operator in the Dirac equation gives the Klein--Gordon equation, so it shouldn't be too surprising that by acting on results for scalar fields with the Dirac operator we get results for a spinor field.
    We'll see that this pattern continues through most of the results in this section.
    
    Similarly, we can compute
    \begin{align}
        \anticommutator{\psi_a^-(x)}{\diracadjoint{\psi}_b^+(y)} &= -(i\slashed{\partial} + m)_{ab} \int \e^{ip\cdot (x - y)} \invariantmeasure{p}\\
        &= (i\slashed{\partial} + m)_{ab} i\Delta^{-}(x - y).
    \end{align}
    We can write these results as
    \begin{equation}
        \anticommutator{\psi^{\pm}(x)}{\diracadjoint{\psi}^{\mp}(y)} = iS^{\pm}(x - y)
    \end{equation}
    where
    \begin{equation}
        S^{\pm}(x) \coloneqq (i\slashed{\partial} + m) \Delta^{\pm}(x).
    \end{equation}
    We then have
    \begin{equation}
        \anticommutator{\psi(x)}{\diracadjoint{\psi}(y)} = iS(x - y),
    \end{equation}
    where
    \begin{equation}
        S(x) = S^+(x) + S^-(x) = (i\slashed{\partial} + m)\Delta(x).
    \end{equation}
    
    \subsection{Contour Representation}
    Recall the contour representation for \(\Delta^{\pm}\):
    \begin{equation}
        \Delta^{\pm}(x) = -\int_{C^{\pm}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2}.
    \end{equation}
    The contours, \(C^{\pm}\) are as given in \cref{fig:contours for Delta}.
    We can use this to give a contour representation for \(S^{\pm}\), going back to writing \(\slashed{p}\) in the place of \(i\slashed{\partial}\), since we can act on the exponential in the contour representation to give
    \begin{equation}
        S^{\pm}(x) = -\int_{C^{\pm}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\slashed{p} + m}{p^2 - m^2} \e^{-ip\cdot x}.
    \end{equation}
    Now we can write
    \begin{equation}
        (\slashed{p} + m)(\slashed{p} - m) = p^2 - m^2 \implies \slashed{p} + m = (p^2 - m^2)(\slashed{p} - m)^{-1}
    \end{equation}
    and then, in a slight abuse of  notation, write the inverse matrix \((\slashed{p} - m)^{-1}\) as \(1/(\slashed{p} - m)\) and rewrite this equation as
    \begin{equation}
        S^{\pm}(x) = -\int_{C^{\pm}} \frac{\dl{^4p}}{(2\pi)^4} \frac{(p^2 - m^2)(\slashed{p} - m)^{-1}}{p^2 - m^2} \e^{-ip\cdot x} = -\int_{C^{\pm}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{\slashed{p} - m}.
    \end{equation}
    
    \section{Fermion Propagator}
    For fermion operators \define{time ordering}\index{time ordering!of fermions} is defined similarly to bosons, but when we swap two fermion operators we should get a negative sign, so we define the time ordering of the fermion operators \(A(x)\) and \(B(x')\) to be
    \begin{equation}
        \timeOrdering[A(x)B(x')] = \heaviside(t - t') A(x)B(y) - \heaviside(t' - t)B(x')A(x).
    \end{equation}
    
    We can then define the \defineindex{fermion propagator} as
    \begin{equation}
        \bra{0} \timeOrdering[\psi(x) \diracadjoint{\psi}(x')] \ket{0}.
    \end{equation}
    First, consider the case where \(t > t'\), then \(\timeOrdering[\psi(x) \diracadjoint{\psi}(x')] = \psi(x) \diracadjoint{\psi}(x')\) and we have
    \begin{align}
        \bra{0} \psi(x) \diracadjoint{\psi}(x') \ket{0} &= \bra{0} [\psi^+(x) + \psi^-(x)][\diracadjoint{\psi}^+(x') + \diracadjoint{\psi}^-(x')]\ket{0}\\
        &= \bra{0} [\psi^+(x)\diracadjoint{\psi}^+(x') + \psi^+(x)\diracadjoint{\psi}^-(x')\\
        &\qquad + \psi^-(x)\diracadjoint{\psi}^+(x') + \psi^-(x)\diracadjoint{\psi}^-(x')]\ket{0}\\
        &= \bra{0} [\psi^+(x)\diracadjoint{\psi}^-(x') + \psi^-(x)\diracadjoint{\psi}^-(x')]\ket{0}\\
        &= \bra{0} \anticommutator{\psi^+(x)}{\diracadjoint{\psi}^-(x')} \ket{0}\\
        &= iS^+(x - x'),
    \end{align}
    where we've used
    \begin{equation}
        \psi^+(x)\ket{0} = \diracadjoint{\psi}^+(x') \ket{0} = 0.
    \end{equation}
    Similarly we find that for \(t < t'\)
    \begin{equation}
        \bra{0} \diracadjoint{\psi}(x') \psi(x) \ket{0} = iS^-(x - x').
    \end{equation}
    Hence, we have
    \begin{equation}
        \bra{0} \timeOrdering[\psi(x) \diracadjoint{\psi}(x')] \ket{0} = i S_{\feynman}(x - x'),
    \end{equation}
    where
    \begin{equation}
        S_F(x) = \heaviside(t) S^+(x) - \heaviside(-t) S^-(x) = (i\slashed{\partial} + m) \Delta_{\feynman}(x)
    \end{equation}
    where \(\Delta_{\feynman}\) is the Feynman propagator for a scalar field, and \(S_F(x)\) is the \define{Feynman propagator}\index{Feynman propagator!for fermions} for a fermion field.
    
    The interpretation is much the same as for a scalar field.
    If \(t' < t\) then we create a fermion at \(x'\) and propagate it to \(x\) where it is destroyed.
    If \(t < t'\) then we create an antifermion at \(x\) and propagate it to \(x'\) where it is destroyed.
    A fermion going in one direction is the same as an antifermion going in the opposite direction.
    
    Note that if we consider an electron then the charge is \(-e\), and this corresponds to a particle created by \(a^\hermit\) or \(\psi^+\), and the antiparticle, the positron, has charge \(+e\), and corresponds to an antiparticle created by \(b^\hermit\) or \(\psi^-\).
    
    \section{Interactions}
    In this section we'll assume that the fermions are electrons, \Pe, and the antifermions are positrons, \APe.
    The process of deriving the Dyson series doesn't change, since the interaction Lagrangian is a scalar in any theory.
    We suppose that the Lagrangian is made of \(\psi\) and \(\diracadjoint{\psi}\) pairs of the form \(\diracadjoint{\psi} \Gamma \psi\), where \(\Gamma\) is some product of gamma matrices, including \(\gamma^5\).
    The only real change to how we consider interactions from a scalar field theory is that in Wick's theorem when we swap the order of two fermions we need to introduce a relative minus sign.
    
    For a fermion we define the \define{Wick contraction}\index{Wick contraction!of fermions} as
    \begin{align}\label{eqn:wick contraction for fermions}
        \wick{\c{\psi}(x) \c{\diracadjoint{\psi}}(x')} &\coloneqq \bra{0} \timeOrdering[\psi(x) \diracadjoint{\psi}(x')] \ket{0}\\
        &\hphantom{:}= iS_{\feynman}(x - x')\\
        &\hphantom{:}= i \int \frac{\dl{^4p}}{(2\pi)^4} \frac{\slashed{p} + m}{p^2 - m^2 + i\varepsilon} \e^{-ip\cdot (x - y)}.
    \end{align}
    
    To evaluate the \(S\) matrix elements we need to know how \(\psi^{\pm}\) and \(\diracadjoint{\psi}^{\pm}\) act on a single particle state.
    First consider the single particle state consisting of an electron of momentum \(\vv{p}\) and spin \(s\):
    \begin{equation}
        \ket{\Pe, \vv{p}, s} = a_s^\hermit(\vv{p}) \ket{0}.
    \end{equation}
    We can act on this with \(\psi^+(x)\):
    \begin{equation}
        \psi^+(x) \ket{\Pe, \vv{p}, s} = \sum_{s'} \int \invariantmeasure{p'} \, u(\vv{p}', s') \e^{-ip'\cdot x} a_{s'}(\vv{p}') a_{s}^\hermit(\vv{p}) \ket{0}.
    \end{equation}
    Using the anticommutator relations we have
    \begin{equation}
        a_{s'}(\vv{p}) a_{s}^\hermit(\vv{p}) = \anticommutator{a_{s'}(\vv{p}')}{a_{s}^\hermit(\vv{p})} - a_{s}^\hermit(\vv{p}) a_{s'}(\vv{p}').
    \end{equation}
    We can then use \(a_{s'}(\vv{p}') \ket{0} = 0\) to rewrite the equation above as
    \begin{align}
        \psi^+(x) \ket{\Pe, \vv{p}, s} &= \sum_{s'} \int \invariantmeasure{p'} \, u(\vv{p}', s') \e^{-ip'\cdot x} \anticommutator{a_{s'}(\vv{p}')}{a_s^\hermit(\vv{p})} \ket{0}\\
        &= \sum_{s'} \int \invariantmeasure{p'} \, u(\vv{p}', s') \e^{-ip'\cdot x} \delta_{ss'} \bardelta(\vv{p} - \vv{p}') \ket{0}\\
        &= u(\vv{p}, s) \e^{-ip\cdot x} \ket{0}.
    \end{align}
    Similarly,
    \begin{align}
        \diracadjoint{\psi}^-(x) \ket{\APe, \vv{p}, s} &= \diracadjoint{v}(\vv{p}, s) \e^{-ip\cdot x} \ket{0},\\
        \bra{\Pe, \vv{p}, s}\psi^-(x) &= \bra{0} \diracadjoint{u}(\vv{p}, s) \e^{ip\cdot x},\\
        \bra{\APe, \vv{p}, s}\diracadjoint{\psi}^-(x) &= \bra{0} v(\vv{p}, s) \e^{ip\cdot x}.
    \end{align}
    
    If we were to proceed with computing various \(S\) matrix elements we would see that momentum conservation works the same as for the scalar case, we get a Dirac delta imposing momentum conservation/
    The initial and final particles work the same, but with an added spinor, and similarly the internal particles come with an extra spin sum.
    We can then give the \define{Feynman rules}\index{Feynman rules!fermions} as follows, note that a \tikzsetnextfilename{fd-dot-vertex-1}\feynmandiagram{a [dot]}; corresponds to a vertex and a lack of a \tikzsetnextfilename{fd-dot-vertex-2}\feynmandiagram{a [dot]}; corresponds to an external particle.
    \begin{itemize}
        \item For each external line, of momentum \(\vv{p}\) and spin \(s\), one of the following:
        \begin{tabular}{lll}
            -- For each initial electron a factor of & \(u(\vv{p}, s)\) & \hspace{2.33pt} \tikzsetnextfilename{fd-incoming-fermion}\feynmandiagram[horizontal=a to b]{a -- [fermion] b [dot]};\\
            -- For each final electron a factor of & \(\diracadjoint{u}(\vv{p}, s)\) & \tikzsetnextfilename{fd-outgoing-fermion}\feynmandiagram[horizontal=a to b]{a [dot] -- [fermion] b};\\
            -- For each initial positron a factor of & \(\diracadjoint{v}(\vv{p}, s)\) & \hspace{2.33pt} \tikzsetnextfilename{fd-incoming-antifermion}\feynmandiagram[horizontal=a to b]{a -- [anti fermion] b [dot]};\\
            -- For each final positron a factor of & \(v(\vv{p}, s)\) & \tikzsetnextfilename{fd-outgoing-antifermion}\feynmandiagram[horizontal=a to b]{a [dot] -- [anti fermion] b};\\
        \end{tabular}
        \item For each internal fermion line with four-momentum \(p\) a fermion propagator
        \begin{equation}
            i \tilde{S}_{\feynman}(p) = \frac{i(\slashed{p} + m)}{p^2 - m^2 + i\varepsilon} = \frac{i}{\slashed{p} - m + i\varepsilon} \qquad\quad \tikzsetnextfilename{fd-internal-fermion}\feynmandiagram[horizontal=a to b]{a [dot] -- b [dot]};
        \end{equation}
        \item For each four-momentum, \(q\), not fixed by conserving momentum at each vertex an integral
        \begin{equation}
            \int \frac{\dl{^4q}}{(2\pi)^4}.
        \end{equation}
        \item A phase factor \((-1)^I\), where \(I\) is the number of interchanges of neighbouring fermions needed to achieve normal ordering.
        \item For each vertex some interaction-dependent factor.
        \item For each closed loop a negative sign and a trace over spinor indices.
    \end{itemize}
    
    These last three points need to be expanded upon.
    The factor of \((-1)^I\) comes from exchanging fermions to normal order them, which gives a relative minus sign between the amplitudes for the interactions:
    \begin{equation}
        \tikzsetnextfilename{fd-2-2-fermions-no-swap}
        \tikz[baseline=(current bounding box)]{\draw (0, 0) -- (2, 0); \draw[-{Latex[width=1.5mm]}] (0, 0) -- (1, 0);\draw (0, -1) -- (2, -1); \draw[-{Latex[width=1.5mm]}] (0, -1) -- (1, -1);}
        = -
        \tikzsetnextfilename{fd-2-2-fermions-with-swap}
        \tikz[baseline=(current bounding box)]{\draw (0, -1) -- (2, 0); \draw[-{Latex[width=1.5mm]}] (0, -1) -- (1.5, -0.25);\draw[white, line width=1mm] (0, 0) -- (2, -1);\draw (0, 0) -- (2, -1); \draw[-{Latex[width=1.5mm]}] (0, 0) -- (1.5, -.75);}
    \end{equation}
    
    For the interaction term there are a few interactions we could consider.
    We could have a purely fermionic interaction Lagrangian, such as
    \begin{equation}
        \lagrangianDensity_{\interaction}^{(1)} = -G(\diracadjoint{\psi}\psi)^2,
    \end{equation}
    where \(G\) is some coupling constant.
    This is the simplest such interaction, since we must have a pair of \(\diracadjoint{\psi}\) and \(\psi\) for the Lagrangian to be a scalar, and we need higher than second order for an interaction term.
    In this case we get a factor of \(-iG\) for each vertex.
    Another possibility is something like
    \begin{equation}
        \lagrangianDensity_{\interaction}^{(2)} = -G(\diracadjoint{\psi}\gamma^\mu\psi)^2.
    \end{equation}
    We can also have mixed scalar/fermion interactions, by introducing some scalar field, \(\varphi\), a possibility is
    \begin{equation}
        \lagrangianDensity_{\interaction}^{(3)} = -y\varphi \diracadjoint{\psi}\psi,
    \end{equation}
    where \(y\) is a coupling constant.
    This is called a \defineindex{Yukawa interaction}.
    We get a factor of \(-iy\) for each vertex.
    We can also consider a pseudoscalar interaction,
    \begin{equation}
        \lagrangianDensity_{\interaction}^{(4)} = -y\varphi \diracadjoint{\psi} \gamma^5 \psi.
    \end{equation}
    
    Suppose we go with the scalar interaction \(\lagrangianDensity_{\interaction} = -y\varphi\diracadjoint{\psi}\psi\).
    One possible diagram is
    \begin{equation}
        \tikzsetnextfilename{fd-scalar-fermion-interaction-example}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dashed] (-2, 0) -- (-0.75, 0);
            \draw[dashed] (0.75, 0) -- (2, 0);
            \draw (0, 0) circle [radius = 0.75];
            \draw[-{Latex[width=1.5mm]}] (0.1, 0.75) -- ++ (0.001, 0);
            \draw[-{Latex[width=1.5mm]}] (-0.1, -0.75) -- ++ (-0.001, 0);
            \node[below left, xshift=0.075cm] at (-0.75, 0) {\scriptsize\(x\)};
            \node[below right, xshift=-0.075cm] at (0.75, 0) {\scriptsize\(y\)};
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \varphi(x) \wick{ \c2{\diracadjoint{\psi}}_a(x) \c1{\psi}_a(x) \varphi(y) \c1{\diracadjoint{\psi}}_b(y) \c2{\psi}_b(y)}.
    \end{equation}
    We can compute the contraction with \(\psi\) first using \cref{eqn:wick contraction for fermions}, which gives
    \begin{equation}
        \wick{\c{\psi}_a(x) \c{\diracadjoint{\psi}}_b(y)} = iS_{ab}^{\feynman}(x - y).
    \end{equation}
    Note that we simply move the \(\feynman\) label up to make room for the spinor indices, its just the normal propagator, \(S^{\feynman} = S_{\feynman}\).
    In order to compute the other contraction, with \(\diracadjoint{\psi}\) first, we have to swap the two fields, picking up a negative sign, and then we can use \cref{eqn:wick contraction for fermions} again, giving
    \begin{align}
        \wick{\c{\diracadjoint{\psi}}_a(x) \c{\psi}_b(y)} &= -\wick{\c{\psi}_b(y) \c{\diracadjoint{\psi}}_a(x)}\\
        &= -iS^{\feynman}_{ba}(y - x).
    \end{align}
    If we then consider the product of these contractions we see that we have
    \begin{equation}
        \wick{\c{\psi}_a(x) \c{\diracadjoint{\psi}}_b(y)} \wick{\c{\diracadjoint{\psi}}_a(x) \c{\psi}_b(y)} = [iS_{ab}^{\feynman}(x - y)][-iS^{\feynman}_{ba}(y - x)].
    \end{equation}
    Considering some matrices, \(A\) and \(B\), we have \(A_{ab}B_{bc} = (AB)_{ac}\), so \(A_{ab}B_{ba} = (AB)_{aa} = \tr(AB)\).
    Hence,
    \begin{equation}
        \wick{\c{\psi}_a(x) \c{\diracadjoint{\psi}}_b(y)} \wick{\c{\diracadjoint{\psi}}_a(x) \c{\psi}_b(y)} = [iS_{ab}^{\feynman}(x - y)][-iS^{\feynman}_{ba}(y - x)] = -\tr(iS_{\feynman}(x - y)  iS_{\feynman}(y - x)).
    \end{equation}
    Hence,
    \begin{equation}
        \varphi(x) \wick{ \c2{\diracadjoint{\psi}}_a(x) \c1{\psi}_a(x) \varphi(y) \c1{\diracadjoint{\psi}}_b(y) \c2{\psi}_b(y)} = -\varphi(x)\varphi(y) \tr(iS_{\feynman}(x - y)  iS_{\feynman}(y - x)).
    \end{equation}
    
    \chapter{Computing Matrix Elements}
    \section{Spin Sums}
    To evaluate decay rates and cross sections we need to evaluate \(\abs{\amplitude}^2 = \amplitude^\hermit \amplitude\).
    Note that \(\amplitude\) is just a number, so we could just use \(\abs{\amplitude}^2 = \amplitude^*\amplitude\), but \(\amplitude\) is typically formed as a product of matrices, and its often easier to compute \(\amplitude^\hermit \amplitude\), which is completely equivalent.
    
    A typical experiment may consist of many initial and final particles, all which potentially have spin.
    Most of the time the initial particles may have any allowed spin, and so we average over all spins for the input particles.
    In the case of spin \(1/2\) fermions we treat the initial particles as being in the state
    \begin{equation}
        \ket{\vv{p}} = \frac{1}{2} \sum_s \ket{\vv{p}, s}.
    \end{equation}
    Similarly, the particles at the end of the process may have any spin, although there will be restrictions based on conservation of total angular momentum, however these restrictions are taken care of in the process of computing \(\amplitude\) and so we don't need to consider them again.
    If we are not measuring the final spin then our resulting particles are in a superposition of spin states, and so we treat the final particle state as
    \begin{equation}
        \ket{\vv{q}} = \sum_s \ket{\vv{q}, s}.
    \end{equation}
    
    For example, in a process with two fermions in both the initial and final state where the initial fermions may have any spin and we don't measure the spin of the final fermions we are actually interested in computing
    \begin{equation}
        \mean{\abs{\amplitude}^2} = \frac{1}{4} \sum_{s,s'} \sum_{r, r'} \amplitude^\hermit \amplitude
    \end{equation}
    where \(s\) and \(s'\) are the spins of the initial particles and \(r\) and \(r'\) the spins of the final particles.
    
    The typical amplitude will look like
    \begin{equation}
        \amplitude = \diracadjoint{u}(\vv{p}', s') \Gamma u(\vv{p}, s),
    \end{equation}
    possibly with \(v\)s in the place of one or more \(u\)s if we have antiparticles.
    The \(u(\vv{p}, s)\) corresponds to an incoming fermion, for simplicity we'll assume an electron.
    The \(\diracadjoint{u}(\vv{p}', s')\)  corresponds to an outgoing fermion.
    The \(\Gamma\) represents the propagator and vertex terms.
    We then have
    \begin{align}
        \amplitude^\hermit &= [u^\hermit(\vv{p}', s') \gamma^0 \Gamma u(\vv{p}, s)]^\hermit\\
        &= u^\hermit(\vv{p}, s) \Gamma^\hermit (\gamma^0)^\hermit u(\vv{p}', s')\\
        &= \diracadjoint{u}(\vv{p}, s) \diracadjoint{\Gamma} u(\vv{p}', s').
    \end{align}
    Here we've used \((\gamma^0)^\hermit = \gamma^0\) and defined \(\diracadjoint{\Gamma} \coloneqq \gamma^0\gamma^\hermit \gamma^0\)\index{Dirac adjoint!of gamma matrices}.
    If \(\Gamma\) is a product gamma matrices, as is often the case with spin \(1/2\) particles interacting, then
    \begin{align}
        \diracadjoint{\Gamma} &= \gamma^0\Gamma^\hermit \gamma^0\\
        &= \gamma^0 (\gamma^\mu \gamma^\nu \dotsc \gamma^\rho)^\hermit \gamma^0\\
        &= \gamma^0 [(\gamma^\rho)^\hermit \dotsm (\gamma^\nu)^\hermit (\gamma^\mu)^\hermit] \gamma^0\\
        &= \gamma^0 [\gamma^0\gamma^\rho\gamma^0 \dotsm \gamma^0\gamma^\nu\gamma^0\gamma^0\gamma^\mu\gamma^0]\gamma^0\\
        &= \gamma^\rho \dotsm \gamma^\nu \gamma^\mu.
    \end{align}
    Here we've used \((\gamma^\mu)^\hermit = \gamma^0 \gamma^\mu \gamma^0\) and \((\gamma^0)^2 = 1\).
    So, the result is \(\diracadjoint{\Gamma}\) is given by \(\Gamma\) with the order of the gamma matrices reversed.
    If \(\Gamma\) also contains an odd number of \(\gamma^5\) matrices then we pick up a negative sign, since \((\gamma^5)^\hermit = \gamma^5 = \gamma^5(\gamma^0)^2 = -\gamma^0\gamma^5\gamma^0\), since \(\gamma^5\) anticommutes with all gamma matrices.
    
    So, we want to be able to compute
    \begin{equation}
        \amplitude^\hermit \amplitude = \diracadjoint{u}(\vv{p}, s) \diracadjoint{\Gamma} u(\vv{p}', s') \diracadjoint{u}(\vv{p}', s') \Gamma u(\vv{p}, s).
    \end{equation}
    Now consider this with explicit spinor indices:
    \begin{align}
        \amplitude^\hermit \amplitude = \diracadjoint{u}_a(\vv{p}, s) \diracadjoint{\Gamma}_{ab} u_b(\vv{p}', s') \diracadjoint{u}_c(\vv{p}', s') \Gamma_{cd} u_d(\vv{p}, s).
    \end{align}
    After including the spinor indices the terms are just numbers and so commute, so rewrite this as
    \begin{equation}
        \amplitude^\hermit \amplitude = u_d(\vv{p}, s) u_d(\vv{p}, s) \diracadjoint{u}_a(\vv{p}, s) \diracadjoint{\Gamma}_{ab} u_b(\vv{p}', s') \diracadjoint{u}_c(\vv{p}', s') \Gamma_{cd}.
    \end{equation}
    Now consider the averaged matrix element:
    \begin{align}
        \mean{\abs{\amplitude}^2} &= \frac{1}{4} \sum_{s, s'} \amplitude^\hermit \amplitude\\
        &= \frac{1}{4} \sum_{s, s'} u_d(\vv{p}, s) \diracadjoint{u}_a(\vv{p}, s) \diracadjoint{\Gamma}_{ab} u_b(\vv{p}', s') \diracadjoint{u}_c(\vv{p}', s') \Gamma_{cd}\\
        &= \frac{1}{4} \tr\bigg( \sum_{s, s'} u_d(\vv{p}, s) \diracadjoint{u}(\vv{p}, s) \diracadjoint{\Gamma} u_b(\vv{p}', s') \diracadjoint{u}_c(\vv{p}', s') \Gamma \bigg).
    \end{align}
    Here we've recognised that
    \begin{equation}
        u_{\textcolor{highlight}{d}}(\vv{p}, s) \diracadjoint{u}_a(\vv{p}, s) \diracadjoint{\Gamma}_{ab} u_b(\vv{p}', s') \diracadjoint{u}_c(\vv{p}', s') \Gamma_{c\textcolor{highlight}{e}}
    \end{equation}
    is the element on row \(d\) and column \(e\) of a matrix.
    Setting \(e = d\) and using the summation convention as we have above then gives the trace of this matrix.
    
    So, we've reduced the amplitude to a trace of spinors and gamma matrices.
    We can reduce it further by using the spin sums in \cref{eqn:spin sum u},
    \begin{equation}
        \sum_s u(\vv{p}, s) \diracadjoint{\vv{p}, s} = \slashed{p} + m.
    \end{equation}
    We then have
    \begin{equation}
        \mean{\abs{\amplitude}^2} = \frac{1}{4} \tr[(\slashed{p} + m) \diracadjoint{\Gamma} (\slashed{p} + m) \Gamma].
    \end{equation}
    We can then compute this using various identities for gamma matrices, including, but not limited to,
    \begin{alignat}{2}
        \gamma^\mu \gamma_\mu &= 4 \qquad & \gamma^\mu \slashed{a}\slashed{b}\slashed{c} \gamma_\mu &= -2\slashed{c}\slashed{b}\slashed{a}\\
        \gamma^\mu \slashed{a} \gamma_\mu &= -2\slashed{a} \qquad& \gamma^\mu \slashed{a}\slashed{b}\slashed{c}\slashed{d}\gamma_\mu &= 2(\slashed{d}\slashed{a}\slashed{b}\slashed{c} + \slashed{c}\slashed{b}\slashed{a}\slashed{d})\\
        \gamma^\mu \slashed{a}\slashed{b} \gamma_\mu &= 4 a\cdot b& \tr(\slashed{a} \slashed{b}) &= 4 a \cdot b\\
        \tr(\slashed{a}\slashed{b}\slashed{c}\slashed{d}) &= \mathrlap{4[(a \cdot b)(c \cdot d) - (a \cdot c)(b \cdot d) + (a \cdot d)(b \cdot c)]}\\
        \tr(\gamma^5\slashed{a}\slashed{b}\slashed{c}\slashed{d}) &= 4i\varepsilon_{\mu\nu\rho\sigma}a^\mu b^\nu c^\rho d^\sigma \quad &
        \tr(\slashed{a}_1 \slashed{a}_2 \dotsm \slashed{a}_n) &= \mathrlap{0 \text{ for } n \text{ odd}}
    \end{alignat}
    
    \section{Examples}
    In this section we'll compute some examples arising from a Yukawa interaction,
    \begin{equation}
        \lagrangianDensity_{\interaction} = -y \varphi \diracadjoint{\psi} \psi.
    \end{equation}
    These interactions will be three-point interactions involving a scalar particle, \(S\), of mass \(M\), and a fermion/antifermion pair of mass \(m\), which we assume to be electrons and positrons unless stated otherwise.
    For this interaction the vertex term is simply \(-iy\).
    
    \subsection{Scalar Decay}
    So long as \(M \ge 2m\) we can have the scalar particle decay into an electron/positron pair,
    \begin{equation}
        S \to  \Pe \APe.
    \end{equation}
    Suppose the scalar particle has momentum \(p\) and the electron and positron have momenta \(q\) and \(q'\), and spins \(s\) and \(s'\), respectively.
    The Feynman diagram for this is
    \begin{equation}
        \tikzsetnextfilename{fd-scalar-decay-amplitude-calculation-example}
        \feynmandiagram[horizontal=i to v, inline=(i)]{
            i [particle = \(p\)] -- [scalar] v,
            v -- [fermion] o1 [particle = \(q\)],
            v -- [anti fermion] o2 [particle = \(q'\)],
        };
    \end{equation}
    Here we have a single vertex contributing \(-iy\) to the amplitude.
    We then have an outgoing electron, contributing \(\diracadjoint{u}(\vv{q}, s)\), and an outgoing positron, contributing \(v(\vv{q}', s')\).
    Hence, the amplitude is
    \begin{equation}
        \amplitude = (-iy) \diracadjoint{u}(\vv{q}, s) v(\vv{q}', s').
    \end{equation}
    The averaged amplitude is then given by summing over the final spins, no factor of \(1/4\) is needed this time as we don't have any initial spins to average over:
    \begin{align}
        \mean{\abs{\amplitude}} &= \sum_{s, s'} \amplitude^\hermit \amplitude\\
        &= \sum_{s, s'} y^2 \diracadjoint{v}(\vv{q}, s') u(\vv{q}, s) \diracadjoint{u}(\vv{q}, s) v(\vv{q}', s')\\
        &= y^2\sum_{s, s'} \diracadjoint{v}_a(\vv{q}', s') u_a(\vv{q}, s) \diracadjoint{u}_b(\vv{q}, s) v_b(\vv{q}', s')\\
        &= y^2 \sum_{s, s'} u_a(\vv{q}, s) \diracadjoint{u}_b(\vv{q}, s) v_b(\vv{q}', s')\diracadjoint{v}_a(\vv{q}', s')\\
        &= y^2 \sum_{s,s'}\tr[u(\vv{q}, s) \diracadjoint{u}(\vv{q}, s) v(\vv{q};, s') \diracadjoint{v}(\vv{q}', s')]\\
        &= y^2 \tr[(\slashed{q} + m)(\slashed{q}' - m)]
    \end{align}
    where we've used the spin sum for the \(v\) spinors, \(\sum_s v(\vv{p}, s) \diracadjoint{v}(\vv{p}, s) = \slashed{p} - m\).
    We then have
    \begin{equation}
        \mean{\abs{\amplitude}^2} = y^2 \tr[\slashed{q}\slashed{q}' - m\slashed{q} + m\slashed{q}' - m^2].
    \end{equation}
    The two cross terms don't contribute, since they each have a single gamma matrix so their trace vanishes.
    The mass term has an implicit \(\ident_4\), and so the trace is \(4m^2\).
    The first term has trace \(4 q \cdot q'\) by one of our identities.
    Hence,
    \begin{equation}
        \mean{\abs{\amplitude}^2} = 4y^2(q \cdot q' - m^2).
    \end{equation}
    Conservation of momentum tells us that \(p = q + q'\).
    Squaring this we have \(M^2 = 2m^2 + 2q \cdot q'\), and so \(q \cdot q' = (M^2 - 2m^2)/2\), which we can use to rewrite the result as
    \begin{equation}
        \mean{\abs{\amplitude}^2} = 2y^2(M^2 - 4m^2).
    \end{equation}
    
    The decay rate in the centre of mass frame is then
    \begin{equation}
        \diff{\Gamma}{\Omega} = \frac{1}{(4\pi)^2} \frac{\abs{\vv{q}}}{M^2} \mean{\abs{\amplitude}^2} = \frac{y^2}{16\pi^2} M\left( 1 - \frac{4m^2}{M^2} \right)^{3/2}.
    \end{equation}
    Here we've used the centre of mass frame so \(p^\mu = (M, 0)\), \(q^\mu = (E, \vv{q})\), and \(q'^\mu = (E, -\vv{q})\), and so \(p^2 = M^2\) and \(p^2 = (q + q')^2 = (2E)^2\) and so \(\abs{\vv{q}}^2 = E^2 - m^2 = M^2/4 - m^2\).
    
    We can integrate this over the sphere to get the total decay rate,
    \begin{equation}
        \Gamma = 4\pi^2 \diff{\Gamma}{\Omega} = \frac{y^2}{4\pi}M\left( 1 - \frac{4m^2}{M^2} \right)^{3/2},
    \end{equation}
    where we get the factor of \(4\pi\) from the integral over the sphere, as there is no angular dependence.
    There is no angular dependence because the scalar particle has no preferred direction.
    Preferred directions usually come from spins or polarisations, which naturally select a particular direction.
    
    Notice that if we analytically continue the decay rate as a function of \(M\) there is a branch point when \(M^2 = 4m^2\).
    We typically get a branch point at the threshold when a process \emph{just} becomes possible.
    
    \subsection{Elastic Scattering}
    Consider the elastic scattering process
    \begin{equation}
        \Pe\Pe \to \Pe\Pe.
    \end{equation}
    There are two diagrams contributing to this at first order, they are the \(t\)-channel diagram,
    \begin{equation}
        \tikzsetnextfilename{fd-t-channel-fermions-scalar-propagator}
        \begin{tikzpicture}[>={Latex}]
            \draw (0, 1) node [left] {\(p, s\)} -- (3, 1) node [right] {\(q, r\)};
            \draw (0, -1) node [left] {\(p', s'\)} -- (3, -1) node [right] {\(q', r'\)};
            \draw[dashed] (1.5, 1) -- (1.5, -1);
            \draw[->] (0, 1) -- (0.75, 1);
            \draw[->] (0, -1) -- (0.75, -1);
            \draw[->] (0, 1) -- (2.25, 1);
            \draw[->] (0, -1) -- (2.25, -1);
        \end{tikzpicture}
    \end{equation}
    and the \(u\)-channel diagram,
    \begin{equation}
        \tikzsetnextfilename{fd-u-channel-fermions-scalar-propagator}
        \begin{tikzpicture}[>={Latex}]
            \draw (0, 1) node [left] {\(p, s\)} -- (1.5, 1) -- (3, -1) node [right] {\(q', r'\)};
            \draw[->] (1.5, 1) -- ($(1.5,1)!0.75!(3,-1)$);
            \draw[white, line width=0.15cm] (1.5, -1) -- (3, 1);
            \draw (0, -1) node [left] {\(p', s'\)} -- (1.5, -1) -- (3, 1) node [right] {\(q, r\)};
            \draw[dashed] (1.5, 1) -- (1.5, -1);
            \draw[->] (0, 1) -- (0.75, 1);
            \draw[->] (0, -1) -- (0.75, -1);
            \draw[->] (1.5, -1) -- ($(1.5,-1)!0.75!(3,1)$);
        \end{tikzpicture}
    \end{equation}
    There is no \(s\)-channel diagram as it would require a virtual particle with charge \(-2\), which doesn't exist in this theory.
    
    Suppose the amplitude for the \(t\)-channel process is \(\amplitude_t\) and the amplitude for the \(u\)-channel process is \(\amplitude_u\).
    The total amplitude (to first order) is then
    \begin{equation}
        \amplitude = \amplitude_t - \amplitude_u.
    \end{equation}
    The negative sign arises due to the crossing of fermion lines swapping the fermions in the \(u\)-channel diagram.
    The amplitude squared is then
    \begin{equation}
        \amplitude^\hermit \amplitude = \amplitude_t^\hermit \amplitude_t + \amplitude_u^\hermit \amplitude_u - (\amplitude_t^\hermit \amplitude_u + \amplitude_u^\hermit \amplitude_t).
    \end{equation}
    The last term is called the \defineindex{interference term}.
    
    Call the spins of the momentum \(p\) electron \(s\) and the spin of the momentum \(p'\) particle \(s'\).
    Note that the spin of the particles cannot change, since the scalar propagator is spinless, and so we don't need to sum over final spins, only average over initial spins.
    
    The amplitude for the \(t\)-channel process is
    \begin{equation}
        \amplitude_t = -y^2\diracadjoint{u}(\vv{q}, r) u(\vv{p}, s) \frac{i}{(p - q)^2 - M^2 + i\varepsilon} \diracadjoint{u}(\vv{q}', r') u(\vv{p}', s').
    \end{equation}
    The amplitude for the \(u\)-channel process is
    \begin{equation}
        \amplitude_u = -y^2 \diracadjoint{u}(\vv{q}', r') u(\vv{p}, s) \frac{i}{(p - q')^2 - M^2 + i\varepsilon} \diracadjoint{u}(\vv{q}, r) u(\vv{p}', s').
    \end{equation}
    
    Hence, defining \(P_t = i/(t - M^2 + i\varepsilon)\) as the propagator term we have
    \begin{align}
        \frac{1}{4} \sum_{\mathclap{s, s'\!, r, r'}} \amplitude_t^\hermit \amplitude_t &= \frac{y^4}{4} \abs{P_t}^2 \sum_{\mathclap{s, s'\!, r, r'}} \diracadjoint{u}(\vv{p}', s') u(\vv{q}', r') \diracadjoint{u}(\vv{p}, s) u(\vv{q}, r) \notag\\
        &\qquad \times\diracadjoint{u}(\vv{q}, r) u(\vv{p}, s) \diracadjoint{u}(\vv{q}', r') u(\vv{p}', r')\\
        &= \frac{y^4}{4} \abs{P_t}^2 \sum_{\mathclap{s, s'\!, r, r'}} \tr[u(\vv{p}, s)\diracadjoint{u}(\vv{p}, s) u(\vv{q}, r) \diracadjoint{u}(\vv{q}, r)] \notag\\
        &\qquad \times\tr[u(\vv{p}', s')\diracadjoint{u}(\vv{p}', s') u(\vv{q}', r') \diracadjoint{u}(\vv{q}', r')]\\
        &= \frac{y^4}{4} \abs{P_t}^2 \tr[(\slashed{p}' + m)(\slashed{q}' + m)] \tr[(\slashed{p} + m)(\slashed{q} + m)]
    \end{align}
    We can calculate both traces the same way:
    \begin{align}
        \tr[(\slashed{p} + m)(\slashed{q} + m)] &= \tr[\slashed{p}\slashed{q} + m\slashed{p} + m\slashed{q} + m^2]\\
        &= 4(p \cdot q + m^2).
    \end{align}
    So, we have
    \begin{equation}
        \frac{1}{4}\sum_{\mathclap{s, s'\!, r, r'}} \amplitude_t^\hermit \amplitude_t = 4y^4 \abs{P_t}^2 (p' \cdot q' + 4m^2)(p \cdot q + 4m^2).
    \end{equation}
    
    Similarly, for the \(u\)-channel process, defining \(P_u = i/(u - M^2 + i\varepsilon)\), we have
    \begin{align}
        \frac{1}{4} \sum_{\mathclap{s, s'\!, r, r'}} \amplitude_u^\hermit \amplitude_u &= \frac{y^4}{4} \abs{P_u}^2 \sum_{\mathclap{s, s'\!, r, r'}} \diracadjoint{u}(\vv{p}', s') u(\vv{q}, r) \diracadjoint{u}(\vv{p}, s) u(\vv{q}', r') \notag\\
        &\qquad\times\diracadjoint{u}(\vv{q}', r') u(\vv{p}, s) \diracadjoint{u}(\vv{q}, r) u(\vv{p}', s')\\
        &= \frac{y^4}{4} \abs{P_u}^2 \sum_{\mathclap{s, s'\!, r, r'}} \tr[u(\vv{p}, s)\diracadjoint{u}(\vv{p}, s)u(\vv{q}', r')\diracadjoint{u}(\vv{q}', r')] \notag\\
        &\qquad \times \tr[u(\vv{p}', s')\diracadjoint{u}(\vv{p}', s') u(\vv{q}, r) \diracadjoint{u}(\vv{q}, r)]\\
        &= \frac{y^4}{4} \abs{P_u}^2 \tr[(\slashed{p} + m)(\slashed{q}' + m)] \tr[(\slashed{p}' + m)(\slashed{q} + m)]\\
        &= 4y^4\abs{P_u}^2(p \cdot q' + 4m^2)(p' \cdot q + 4m^2).
    \end{align}
    
    The first cross term is
    \begingroup
    \allowdisplaybreaks
    \begin{align}
        \sum_{\mathclap{s, s'\!, r, r'}} \amplitude_t^\hermit \amplitude_u &= \frac{y^4}{4} P_t^*P_u \sum_{\mathclap{s, s'\!, r, r'}} \diracadjoint{u}(\vv{p}'
        , s') u(\vv{q}', r') \diracadjoint{u}(\vv{p}, s) u(\vv{q}, r)\notag\\
        &\qquad \times\diracadjoint{u}(\vv{q}', r') u(\vv{p}, s) \diracadjoint{u}(\vv{q}, r) u(\vv{p}', s')\\
        &= \frac{y^4}{4} P_t^*P_u \sum_{\mathclap{s, s'\!, r, r'}} \diracadjoint{u}_a(\vv{p}', s') u_a(\vv{q}', r') \diracadjoint{u}_b(\vv{p}, s) u_b(\vv{q}, r)\notag\\
        &\qquad \times\diracadjoint{u}_c(\vv{q}', r') u_c(\vv{p}, s) \diracadjoint{u}_d(\vv{q}, r) u_d(\vv{p}', s')\\
        &= \frac{y^4}{4} P_t^*P_u \sum_{\mathclap{s, s'\!, r, r'}} u_c(\vv{p}, s) \diracadjoint{u}_b(\vv{p}, s) u_b(\vv{q}, r) \diracadjoint{u}_d(\vv{q}, r) \notag\\
        &\qquad \times u_d(\vv{p}', s') \diracadjoint{u}_a(\vv{p}', s') u_a(\vv{q}', r') \diracadjoint{u}_c(\vv{q}', r')\\
        &= \frac{y^4}{4} P_t^*P_u \sum_{\mathclap{s, s'\!, r, r'}} \tr[u(\vv{p}, s)\diracadjoint{u}(\vv{p}, s) u(\vv{q}, r) \diracadjoint{u}(\vv{q}, r) \notag\\
        &\qquad \times u(\vv{p}', s') \diracadjoint{u}(\vv{p}', s') u(\vv{q}', r') \diracadjoint{u}(\vv{q}', r')]\\
        &= \frac{y^4}{4} P_t^*P_u \tr[(\slashed{p} + m) (\slashed{q} + m) (\slashed{p}' + m) (\slashed{q}' + m)].
    \end{align}
    \endgroup
    
    Expanding this and keeping only terms with an even number of gamma matrices, since the trace of an odd number of gamma matrices vanishes, we get
    \begin{equation*}
        \frac{1}{4}\sum_{\mathclap{s, s'\!, r, r'}} \amplitude_t^\hermit \amplitude_u = \frac{y^4}{4} P_t^*P_u \tr[\slashed{p}\slashed{q}\slashed{p}'\slashed{q}' + m^2(\slashed{p}\slashed{q} + \slashed{p}\slashed{p}' + \slashed{p}\slashed{q}' + \slashed{q}\slashed{p}' + \slashed{q}\slashed{q}' + \slashed{p}'\slashed{q}') + m^4].
    \end{equation*}
    We can then use the identities
    \begin{align}
        \tr[\ident_4] &= 4, \qquad \tr[\slashed{a}\slashed{b}] = 4a \cdot b,\\
        \tr[\slashed{a}\slashed{b}\slashed{c}\slashed{d}] &= 4[(a\cdot b)(c \cdot d) - (a \cdot c)(b \cdot d) + (a \cdot d)(b \cdot c)],
    \end{align}
    and we have
    \begin{multline}
        \frac{1}{4}\sum_{\mathclap{s, s'\!, r, r'}} \amplitude_t^\hermit \amplitude_u = y^4 P_t^*P_u \{(p \cdot q)(p' \cdot q') - (p \cdot p')(q \cdot q') + (p \cdot q')(q \cdot p')\\
        + m^2[p \cdot q + p \cdot p' + p \cdot q' + q \cdot p' + q \cdot q' + p' \cdot q'] + m^4\}.
    \end{multline}

    Now consider the second cross term:
    \begin{align}
        \frac{1}{4}\sum_{\mathclap{s, s'\!, r, r'}} \amplitude_u^\hermit \amplitude_t &= \frac{y^4}{4} P_u^*P_t \sum_{\mathclap{s, s'\!, r, r'}} \diracadjoint{u}(\vv{p}', s') u(\vv{q}, r) \diracadjoint{u}(\vv{p}, s) u(\vv{q'}, r') \notag\\
        &\qquad \times \diracadjoint{u}(\vv{q}, r) u(\vv{p}, s) \diracadjoint{u}(\vv{q}', r') u(\vv{p}', s')\\
        &= \frac{y^4}{4} P_u^*P_t \sum_{\mathclap{s, s'\!, r, r'}} \diracadjoint{u}_a(\vv{p}', s') u_a(\vv{q}, r) \diracadjoint{u}_b(\vv{p}, s) u_b(\vv{q'}, r') \notag\\
        &\qquad \times \diracadjoint{u}_c(\vv{q}, r) u_c(\vv{p}, s) \diracadjoint{u}_d(\vv{q}', r') u_d(\vv{p}', s')\\
        &= \frac{y^4}{4} P_u^*P_t \sum_{\mathclap{s, s'\!, r, r'}} u_a(\vv{q}, r) \diracadjoint{u}_c(\vv{q}, r) u_c(\vv{p}, s) \diracadjoint{u}_b(\vv{p}, s) \notag\\
        &\qquad \times u_b(\vv{q}', r') \diracadjoint{u}_d(\vv{q}', r') u_d(\vv{p}', s') \diracadjoint{u}_a(\vv{p}', s')\\
        &= \frac{y^4}{4} P_u^*P_t \sum_{\mathclap{s, s'\!, r, r'}} \tr[u(\vv{q}, r) \diracadjoint{u}(\vv{q}, r) u(\vv{p}, s) \diracadjoint{u}(\vv{p}, s) \notag\\
        &\qquad \times u(\vv{q}', r') \diracadjoint{u}(\vv{q}', r') u(\vv{p}', s') \diracadjoint{u}(\vv{p}', s')]\\
        &= \frac{y^4}{4} P_u^*P_t \sum_{\mathclap{s, s'\!, r, r'}}[(\slashed{q} + m)(\slashed{p} + m)(\slashed{q}' + m)(\slashed{p}' + m)]\\
        &= \frac{y^4}{4} P_u^*P_t \sum_{\mathclap{s, s'\!, r, r'}}[(\slashed{p} + m)(\slashed{q}' + m)(\slashed{p}' + m)(\slashed{q} + m)]\\
        &= y^4 P_u^*P_t \{(p \cdot q')(p' \cdot q) - (p \cdot p')(q' \cdot q) + (p \cdot q)(q' \cdot p')\\
        &\quad + m^2[p \cdot q' + p \cdot p' + p \cdot q + q' \cdot p' + q' \cdot q + p' \cdot q] + m^4\}.\notag
    \end{align}
    So, since \(P_t^*P_u = P_u^*P_t\) after dropping the \(i\varepsilon\) terms, we find that \(\amplitude_u^\hermit \amplitude_t = \amplitude_t^\hermit \amplitude_u\) in this case and so
    \begin{multline}
        \frac{1}{4}\sum_{\mathclap{s, s'\!, r, r'}} (\amplitude_t^\hermit \amplitude_u + \amplitude_u^\hermit \amplitude^t) = 2y^4 P_t^*P_u \{(p \cdot q)(p' \cdot q') - (p \cdot p')(q \cdot q') + (p \cdot q')(q \cdot p')\\
        + m^2[p \cdot q + p \cdot p' + p \cdot q' + q \cdot p' + q \cdot q' + p' \cdot q'] + m^4\}.
    \end{multline}
    
    Putting this all together the amplitude squared averaged over incoming spins and summed over outgoing spins is
    \begin{align}
        \frac{1}{4} \sum_{\mathclap{s, s'\!, r, r'}} \abs{\amplitude}^2 &= 2y^4\bigg[ \frac{2}{(t - M^2)^2} (p' \cdot q' + 4m^2)(p \cdot q + 4m^2)\\
        &\qquad + \frac{2}{(u - M^2)^2} (p \cdot q' + 4m^2)(p' \cdot q + 4m^2) \notag\\
        &\qquad + \frac{1}{(t - M^2)(u - M^2)}\{(p \cdot q)(p' \cdot q') - (p \cdot p')(q \cdot q') \notag\\
        &\qquad + (p \cdot q')(q \cdot p') + m^2[p \cdot q + p \cdot p' + p \cdot q' + q \cdot p'\notag\\
        &\qquad + q \cdot q' + p' \cdot q'] + m^4\} \bigg]\notag\\
    \end{align}
    
    \subsubsection{Annihilation}
    Consider the annihilation process
    \begin{equation}
        \Pe \APe \to \Pe \APe.
    \end{equation}
    There are two diagrams contributing to this at first order, they are the \(t\)-channel diagram,
    \begin{equation}
        \tikzsetnextfilename{fd-t-channel-bhabha-scattering-s-channel}
        \begin{tikzpicture}[>={Latex}]
            \draw (0, 1) node [left] {\(p, s\)} -- (1, 0);
            \draw (2.5, 0) -- (3.5, 1) node [right] {\(q, r\)};
            \draw (0, -1) node [left] {\(p', s'\)} -- (1, 0);
            \draw (2.5, 0) -- (3.5, -1) node [right] {\(q', r'\)};
            \draw[dashed] (1, 0) -- (2.5, 0);
            \draw[->] (0, 1) -- ($(0, 1)!.5!(1, 0)$);
            \draw[-<] (0, -1) -- ($(0, -1)!.5!(1, 0)$);
            \draw[->] (2.5, 0) -- ($(2.5, 0)!.5!(3.5, 1)$);
            \draw[-<] (2.5, 0) -- ($(2.5, 0)!.5!(3.5, -1)$);
        \end{tikzpicture}
    \end{equation}
    and the \(t\)-channel diagram, where the electron and positron just scatter off each other without annihilating,
    \begin{equation}
        \tikzsetnextfilename{fd-t-channel-bhabha-scattering-t-channel}
        \begin{tikzpicture}[>={Latex}]
            \draw (0, 1) node [left] {\(p, s\)} -- (3, 1) node [right] {\(q, r\)};
            \draw (0, -1) node [left] {\(p', s'\)} -- (3, -1) node [right] {\(q', r'\)};
            \draw[dashed] (1.5, 1) -- (1.5, -1);
            \draw[->] (0, 1) -- (0.75, 1);
            \draw[<-] (0, -1) -- (0.75, -1);
            \draw[->] (0, 1) -- (2.25, 1);
            \draw[<-] (0, -1) -- (2.25, -1);
        \end{tikzpicture}
    \end{equation}
    There is no \(u\)-channel diagram as the particles are distinguishable.
    The total amplitude for this process is
    \begin{equation}
        \amplitude = \amplitude_s + \amplitude_u,
    \end{equation}
    and so
    \begin{equation}
        \amplitude^\hermit \amplitude = \amplitude_s^\hermit \amplitude_s + \amplitude_t^\hermit \amplitude_t + (\amplitude_s^\hermit\amplitude_t + \amplitude_t^\hermit\amplitude_s).
    \end{equation}
    
    The calculation is almost identical to the calculation for electron scattering, except that some particles are replaced with antiparticles, so we need replace some \(u\)s with \(\diracadjoint{v}\)s and some \(\diracadjoint{u}\)s with \(v\)s.
    We also have an \(s\)-channel instead of a \(u\)-channel.
    
    Instead of doing this calculation we consider the similar process
    \begin{equation}
        \Pe \APe \to \Pmu \APmu.
    \end{equation}
    This simplifies matters slightly since there is no \(t\)-channel diagram, as the final state particles differ from the initial state particles.
    For the purposes of quantum field theory we can treat (anti)muons as identical to (anti)electrons but with a different mass, so we just have two fermion fields involved in this interaction.
    
    The \(s\)-channel amplitude, which is just the total amplitude to first order as their is only one tree level diagram for this process, is
    \begin{equation}
        \amplitude = \amplitude_s = (-iy)^2 \diracadjoint{u}(\vv{q}, r) v(q', r') \frac{i}{(p + p')^2 - M^2 + i\varepsilon} \diracadjoint{v}(\vv{p}', s')u(\vv{p}, s).
    \end{equation}
    Averaging over incoming spins and summing over outgoing spins we get
    \begin{align}
        \mean{\abs{\amplitude}^2} &= \frac{1}{4} \sum_{\mathclap{s, s'\!, r, r'}} \amplitude^\hermit \amplitude\\
        &= \frac{y^4}{4} \frac{1}{(s - M^2)^2}\\
        &\qquad \times \sum_{r, r'} \diracadjoint{v}(q', r') u(q, r) \diracadjoint{u}(q, r) v(q', r') \sum_{s, s'} \diracadjoint{u}(p, s) v(p', s') \diracadjoint{v}(p', s') u(p, s)\\
        &= \frac{y^4}{4} \frac{1}{(s - M^2)^2} \tr[(\slashed{q} + m_{\text{μ}})(\slashed{q}' - m_{\text{μ}})] \tr[(\slashed{p} + m_{\symrm{e}})(\slashed{p}' - m_{\symrm{e}})]\\
        &= \frac{4y^4}{(s - M^2)^2} (q \cdot q' - m_{\text{μ}}^2) (p \cdot p' - m_{\symrm{e}}^2)\\
        &= \frac{y^4}{(s - M^2)^2} (s - 4m_{\text{μ}}^2)(s - 4m_{\symrm{e}}^2).
    \end{align}
    Here we assume that \(s\) doesn't achieve the value of \(M^2\), so we can drop the \(i\varepsilon\) term.
    In the last step we've used \(s = (p + p')^2 = m_{\symrm{e}}^2 + m_{\text{μ}}^2 + 2p \cdot p'\), and a \(s = (q + q')^2 = m_{\symrm{e}}^2 + m_{\text{μ}}^2 + 2q \cdot q'\) to rewrite the result in terms of Mandelstam variables.
    
    The differential cross section in the centre of mass frame for this process is then
    \begin{equation}
        \diff{\sigma}{\Omega} = \frac{1}{64\pi^2} \frac{y^4}{(s - M^2)^2} \frac{1}{s} \sqrt{\frac{s - 4m_{\text{μ}}^2}{s - 4m_{\symrm{e}}^2}} (s - 4m_{\symrm{e}}^2) (s - 4m_{\text{μ}}^2).
    \end{equation}
    We can easily integrate this since there is no angular dependence to get the total cross section,
    \begin{equation}
        \frac{1}{16\pi} \frac{y^4}{(s - M^2)^2} \frac{1}{s} \sqrt{\frac{s - 4m_{\text{μ}}^2}{s - 4m_{\symrm{e}}^2}} (s - 4m_{\symrm{e}}^2) (s - 4m_{\text{μ}}^2).
    \end{equation}
    Notice that at high energies, when \(s \gg m_{\symrm{e}}, m_{\text{μ}}, M\), the cross section goes as \(1/s\).
    Essentially meaning that the faster the incoming particles are going the less likely they are to interact and annihilate before passing each other.
    
    Notice that this diverges as \(s \to M^2\).
    We call this a \defineindex{resonance} for the scalar particle.
    These can be useful to find new particles, we get a large spike in the cross section when a particle goes on-shell, which tells us the mass of that particle.
    While  the result is divergent here if we included all orders it would turn out that we don't get divergence, just a large peak.
    
    Close to the peak the cross section nearly factors into
    \begin{equation}
        \amplitude(\Pe\APe \to S) \amplitude(S \to \Pmu\APmu).
    \end{equation}
    The interpretation is that as the scalar particle becomes \emph{almost} on-shell the process is \emph{almost} a pair of electrons annihilating and then a scalar particle decaying as two separate processes.
    This is a general feature, that as particles go on-shell amplitudes factor.
    
    Despite using binary on/off-shell or real/virtual particle language there's really much more of a continuum between real and virtual particles.
    In fact, a lot of what we think of as real particles are just relatively stable virtual particles.
    
    \chapter{CPT For Fermions}
    In this chapter we'll explore how we can construct parity, time reversal, and charge conjugation operators for Dirac fermions in such a way that the Dirac equation is invariant.
    
    \section{Parity}
    Under parity \(x^\mu = (t, \vv{x}) \mapsto \overbar{x}^\mu = (t, -\vv{x}) = x_\mu\), and \(\partial_\mu = (\partial_t, \grad) \mapsto \overbar{\partial}_\mu = (\partial_t, -\grad) = \partial^\mu\).
    Suppose that under a parity transformation \(\psi(x) \mapsto \psi_{P}(x) = P\psi(\overbar{x})\) for some spin matrix, \(P\).
    We then require the gamma matrices to transform as
    \begin{equation}
        \gamma^\mu \mapsto P\gamma^\mu P^{-1} = (\gamma^0, -\gamma^i) = \gamma_\mu,
    \end{equation}
    so that starting with the Dirac equation, \((i\slashed{\partial} - m)\psi(x)\), under parity we get
    \begin{equation}
        P(i\gamma^\mu \partial_\mu - m) \psi_(x) = 0 \implies (i\gamma_\mu \partial^\mu - m)\psi_P(x) = 0.
    \end{equation}
    A solution is that \(P = P^{-1} = \gamma^0\).
    As we have \(\gamma^0\gamma^i\gamma^0 = -\gamma^i\) and \((\gamma^0)^3 = \gamma^0\), which follows from \((\gamma^0)^2 = 1\) and \(\anticommutator{\gamma^0}{\gamma^i} = 0\).
    We could more generally take \(P = \e^{i\varphi}\gamma^0\), but this doesn't give us anything more interesting.
    
    The adjoint spinor transforms as
    \begin{equation}
        \diracadjoint{\psi} = \psi^\hermit \gamma^0 \mapsto (\gamma^0\psi)^\hermit \gamma^0 = \psi^\hermit \gamma^0 \gamma^0 = \diracadjoint{\psi}\gamma^0 = \diracadjoint{\psi}P^{-1}.
    \end{equation}
    Hence, we have
    \begin{equation}
        \diracadjoint{\psi} \psi \mapsto \diracadjoint{\psi} P^{-1}P \psi = \diracadjoint{\psi}\psi,\\
        \diracadjoint{\psi} \gamma^\mu \psi \mapsto \diracadjoint{\psi} P^{-1} \gamma^\mu P \psi = \diracadjoint{\psi} \gamma^\mu \psi,\\
    \end{equation}
    which is what we'd expect for a scalar \(\diracadjoint{\psi}\psi\) and a vector current, \(\diracadjoint{\psi}\gamma^\mu \psi\).
    Importantly, this means that \(\lagrangianDensity = \diracadjoint{\psi}(i\slashed{\partial} - m)\psi\) is invariant under parity inversion.
    
    The spinors transform as \(u(\vv{p}, s) \mapsto \gamma^0u(\vv{p}, s) = u(-\vv{p}, s)\), which can be shown easily in the Dirac representation.
    Spin doesn't change under parity, since \(\vv{r} \times \vv{p}\) doesn't change, this corresponds to the transformation of the creation operators transforming as
    \begin{equation}
        \parity a_s^\hermit(\vv{p}) \parity^\hermit = a_s^\hermit(-\vv{p}), \qqand \parity b_s^\hermit(\vv{p}) \parity^\hermit = b_s^\hermit(-\vv{p}).
    \end{equation}
    This follows by the same argument as for scalar fields.
    
    \section{Time Reversal}
    Under time reversal \(x^\mu \mapsto -\overbar{x}^\mu = -x_\mu\), and \(\partial_\mu \mapsto -\overbar{\partial}_\mu = -\partial^\mu\).
    The time reversal operator is antiunitary, we take a complex conjugate when we reverse time.
    Suppose that \(\psi(x) \mapsto \psi_T(x) = T\psi(-\overbar{x})\) for some spin matrix \(T\).
    Then we require the gamma matrices to transform as
    \begin{equation}
        \gamma^\mu \mapsto T(\gamma^\mu)^* T^{-1} = \gamma_\mu.
    \end{equation}
    As with parity this guarantees the invariance of the Dirac equation, we just replace \(\psi\) with \(\psi_T\).
    
    A solution to this is given by noting that in the Dirac representation \(\gamma^2\) is imaginary and the other gamma matrices are real.
    The solution then is to take \(T = -T^{-1} = \gamma^1\gamma^3\), since \((\gamma^1\gamma^3)^2 = -1\) and we have
    \begin{align}
        -\gamma^1\gamma^3\gamma^0\gamma^1\gamma^3 &= \gamma^0,\\
        -\gamma^1\gamma^3\gamma^1\gamma^1\gamma^3 &= -\gamma^1,\\
        -\gamma^1\gamma^3\gamma^2\gamma^1\gamma^3 &= -\gamma^2,\\
        -\gamma^1\gamma^3\gamma^3\gamma^1\gamma^3 &= -\gamma^3.
    \end{align}
    
    The adjoint spinor transforms as
    \begin{multline}
        \diracadjoint{\psi} = \psi^\hermit \gamma^0 \mapsto (\gamma^1\gamma^3\psi)^\hermit \gamma^0 = \psi^\hermit (\gamma^3)^\hermit (\gamma^1)^\hermit \gamma^0\\
        = \psi^\hermit \gamma^0 (\gamma^3)^\hermit (\gamma^1)^\hermit = \diracadjoint{\psi} (\gamma^3)^\hermit (\gamma^1)^\hermit = \diracadjoint{\psi} T^{-1}.
    \end{multline}
    
    Then, we have
    \begin{equation}
        \diracadjoint{\psi}\psi \mapsto \diracadjoint{\psi}T^{-1}T\psi = \diracadjoint{\psi}\psi,\\
        \diracadjoint{\psi}\gamma^\mu\psi \mapsto \diracadjoint{\psi}T^{-1} (\gamma^\mu)^*T\psi = \diracadjoint{\psi}\gamma_\mu \psi.
    \end{equation}
    Again, this is what we would expect for a scalar and a vector current.
    The Lagrangian is unchanged under time reversal.
    
    The spinors transform as \(u(\vv{p}, s) \mapsto Tu(\vv{p}, s)^* = u(-\vv{p}, -s)\), which can be shown using the Dirac representation.
    Alternatively, under time reversal \(\vv{r}\) doesn't change but \(\vv{p} \mapsto -\vv{p}\), so \(\vv{r} \times \vv{p} \mapsto -\vv{r} \times \vv{p}\).
    
    This corresponds to the creation operators transforming as
    \begin{equation}
        \timeReversal a_s^\hermit(\vv{p}) \timeReversal^\hermit = a_{-s}^\hermit(-\vv{p}), \qqand \timeReversal b_s^\hermit(\vv{p}) \timeReversal^\hermit = b_{-s}^\hermit(-\vv{p}).
    \end{equation}
    
    \section{Charge Conjugation}
    Charge conjugation exchanges particles and antiparticles, just as for scalars.
    Both \(\vv{p}\) and \(s\) are left unchanged, corresponding to the mapping of creation operators
    \begin{equation}
        \chargeConjugation a_s^\hermit(\vv{p}) \chargeConjugation^\hermit = a_{s}^\hermit(\vv{p}), \qqand \chargeConjugation b_s^\hermit(\vv{p}) \chargeConjugation^\hermit = b_{s}^\hermit(\vv{p}).
    \end{equation}
    
    The charge conjugated field is then
    \begin{equation}
        \psi_c = \chargeConjugation \psi \chargeConjugation^\hermit = \sum_s \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} [a_s^\hermit(\vv{p})v(\vv{p}, s)\e^{ip\cdot x} + b_s(\vv{p})u(\vv{p}, s)\e^{-ip\cdot x}].
    \end{equation}
    We also have
    \begin{equation}
        \psi^* = (\psi^\hermit)^\trans = \sum_s \int \frac{\dl{^3\vv[p]}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} [a_s^\hermit(\vv{p})u(\vv{p}, s)^* \e^{ip\cdot x} + b_s(\vv{p}) v(\vv{p}, s)^* \e^{-ip\cdot x}].
    \end{equation}
    Here \(\hermit\) is the Hermitian adjoint in both the Hilbert space of states and the spinor space.
    The transpose, \(\trans\), is only in spinor space.
    Under charge conjugation we have
    \begin{equation}
        \psi \mapsto \psi_c = c\psi^*
    \end{equation}
    where \(c\) is some spinor matrix such that
    \begin{equation}
        cu_s^* = v_s, \qquad cv_s^* = u_s, \qqand cc^* = 1.
    \end{equation}
    
    For the Dirac equation to remain unchanged we need the gamma matrices to transform as
    \begin{equation}
        c (\gamma^\mu)^* c^{-1} = -\gamma^\mu.
    \end{equation}
    A solution is given by \(c = c^{-1} = c^* = i\gamma^2\).
    
    In the Dirac representation
    \begin{equation}
        c = i\gamma^2 =
        \begin{pmatrix}
            0 & i\sigma^2\\
            -i\sigma^2 & 0
        \end{pmatrix}
    \end{equation}
    and then we have
    \begin{equation}
        cu(\vv{p}, s)^* = N
        \begin{pmatrix}
            \frac{-i\sigma^2\vv{\sigma}^* \cdot \vv{p}}{E + m} \varphi^s\\
            -i\sigma^2\varphi^s
        \end{pmatrix}
        = N
        \begin{pmatrix}
            \frac{-\vv{\sigma} \cdot \vv{p}}{E + m}(-i\sigma^2\varphi^s)\\
            -i\sigma^2\varphi^s
        \end{pmatrix}
        = v(\vv{p}, s),
    \end{equation}
    so long as \(\chi_s = -i\sigma^2\varphi_s\), which is the case with our definition of \(\chi_s\), and explains the nonobvious choice of \(\chi_1 = (0, 1)^\trans\) and \(\chi_2 = (-1, 0)^\trans\).
    So, charge conjugation flips the basis spinors.
    
    We can write these equations in terms of the Dirac adjoint:
    \begin{equation}
        \psi_c = i\gamma^2\psi^* = i\gamma^2\gamma^0\diracadjoint{\psi}^\trans = C\diracadjoint{\psi}^\trans
    \end{equation}
    where this equation defines the spinor matrix \(C = i\gamma^2 \gamma^0\).
    Under charge conjugation we then have
    \begin{equation}
        \diracadjoint{\psi} \mapsto (i\gamma^2 \psi^*)^\hermit \gamma^0 = \psi^\trans(-i(\gamma^2)^*\gamma^0) = \psi^\trans C = -\psi^\trans C^{-1}
    \end{equation}
    and so
    \begin{equation}
        \diracadjoint{\psi}\psi \mapsto -\psi^\trans C^{-1}C\diracadjoint{\psi}^\trans = -\psi^\trans \diracadjoint{\psi}^\trans
    \end{equation}
    and this gives the same physics as \(\diracadjoint{\psi}\psi\), which is what we would expect for a scalar.
    Normal ordering, since both fields are evaluated at the same point, \(\normalordering{\diracadjoint{\psi}\psi}\) will be invariant under charge conjugation.
    Similarly,
    \begin{equation}
        \normalordering{\diracadjoint{\psi}\gamma^\mu\psi} \mapsto -\normalordering{\psi^\trans C^{-1} \gamma^\mu C \diracadjoint{\psi}^\trans} = \diracadjoint{\psi^\trans (\gamma^\mu)^\trans \diracadjoint{\psi}^\trans} = -\normalordering{\diracadjoint{\psi}\gamma^\mu\psi}
    \end{equation}
    where the minus sign comes from commuting fermions.
    So, as expected, the current is reversed under charge conjugation.
    The Lagrangian is invariant, up to an overall unobservable total derivative:
    \begin{equation}
        \lagrangianDensity = \normalordering{\diracadjoint{\psi}(i\slashed{\partial} - m)\psi} \mapsto \normalordering{\psi^\trans (i(\gamma^\mu)^\trans \partial_\mu - m)\diracadjoint{\psi}^\trans} = \normalordering{{-i}\partial_\mu\diracadjoint{\psi}\gamma^\mu\psi - m\diracadjoint{\psi}\psi}.
    \end{equation}
    
    To conclude, under parity, time reversal and charge conjugation
    \begin{alignat}{3}
        P: \qquad \psi &\mapsto P\psi, \qquad & \diracadjoint{\psi} &\mapsto \diracadjoint{\psi}P^{-1}, \qquad & P& = \gamma^0,\\
        T: \qquad \psi &\mapsto T\psi, \qquad & \diracadjoint{\psi} &\mapsto \diracadjoint{\psi}T^{-1}, \qquad & T& = \gamma^1\gamma^3,\\
        C: \qquad \psi &\mapsto C\diracadjoint{\psi}^\trans, \qquad & \diracadjoint{\psi} &\mapsto -\psi C^{-1}, \qquad & P& = i\gamma^2\gamma^0.
    \end{alignat}
    
    \part{Quantising the Electromagnetic Field}
    \chapter{Quantising the Electromagnetic Field}
    \epigraph{The hardest of all, except the gravitational field.}{Richard Ball}
    \section{Classical Electrodynamics}
    \begin{rmk}
        See \course{Classical Electrodynamics} for details.
    \end{rmk}
    \subsection{Maxwell's Equations}
    The electromagnetic field strength tensor is
    \begin{equation}
        F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu
    \end{equation}
    where \(A^\mu = (\varphi, \vv{A})\) is the electromagnetic four-potential.
    The Lagrangian
    \begin{equation}
        \lagrangianDensity = -\frac{1}{4} F_{\mu\nu}F^{\mu\nu} - A_\mu J^\mu
    \end{equation}
    gives the classical equations of motion
    \begin{equation}
        \partial_\mu F^{\mu\nu} = J^\nu,
    \end{equation}
    which is just \defineindex{Maxwell's equations} in a very compact form.
    
    The four divergence of Maxwell's equation vanishes:
    \begin{equation}
        0 = \partial_\nu \partial_\mu F^{\mu\nu} = \partial_\nu J^\nu,
    \end{equation}
    since \(\partial_\nu \partial_\mu\) is symmetric in \(\mu\) and \(\nu\), whereas \(F^{\mu\nu}\) is antisymmetric.
    This equation, \(\partial_\nu J^\nu = 0\), is a continuity equation, and tells us that \(J^\nu\) is a conserved current.
    
    This places a constraint on the field, \(A^\mu\).
    This makes quantising the electromagnetic field significantly more challenging than quantising scalar or fermionic fields.
    Fundamentally this is due to gauge invariance, a property not present for the scalar or fermion fields we have considered so far.
    
    \subsection{Gauge Invariance}
    The Lagrangian is \defineindex{gauge invariant}, by this we mean that the transformation
    \begin{equation}
        A^\mu \mapsto A^\mu - \partial^\mu \chi
    \end{equation}
    for some scalar field \(\chi\), with everywhere continuous third derivatives, leaves the Lagrangian, and hence the action and the physics, unchanged.
    Under this transformation we have
    \begin{align}
        F^{\mu\nu} &\mapsto \partial^\mu(A^\nu - \partial^\nu \chi) - \partial^\nu(A^\mu - \partial^\mu \chi)\\
        &= \partial^\mu A^\nu - \partial^\mu\partial^\nu - \partial^\nu A^\mu + \partial^\nu\partial^\mu \chi\\
        &= \partial^\mu A^\nu - \partial^\nu A^\mu\\
        &= F^{\mu\nu}.
    \end{align}
    We also have
    \begin{equation}
        A_\mu J^\mu \mapsto A_\mu J^\mu - (\partial_\mu\chi)J^\mu.
    \end{equation}
    So, the action, assuming that we integrate over some volume, \(V\), such that the scalar field \(\chi\) vanishes on the boundary of \(V\), transforms as
    \begin{align}
        S[A_\mu] &\mapsto S[A_\mu] - \int_V \dl{^4x} \, (\partial_\mu\chi) \mkern2mu J^\mu\\
        &= S[A_\mu] - \int_V \dl{^4x} \, [\partial_\mu(\chi J^\mu) - \chi\partial_\mu J^\mu]\\
        &= S[A_\mu] - \int_{\partial V} \dl{S_\mu} \, \chi J^\mu + \int_{V} \dl{^4x} \chi \partial_\mu J^\mu\\
        &= S[A_\mu]
    \end{align}
    where the boundary integral over \(\partial V\) vanishes by assumption and the volume integral over \(V\) vanishes as \(\partial_\mu J^\mu = 0\).
    
    We can think of \(\partial_\mu \chi\) as a zero mode of the equations of motion, which are
    \begin{align}
        J^\mu &= \partial_\nu F^{\nu\mu}\\
        &= \partial_\nu (\partial^\nu A^\mu - \partial^\mu A^\nu)\\
        &= \partial_\nu \partial^\nu A^\mu - \partial_\nu\partial^\mu A^\nu\\
        &= \dalembertian A^\mu - \partial^\nu \partial^\mu A_\nu\\
        &= \minkowskiMetric^{\mu\nu} \dalembertian A_\nu - \partial^\nu \partial^\mu A_\nu\\
        &= (\minkowskiMetric^{\mu\nu} \dalembertian - \partial^\nu \partial^\mu) A_\nu.
    \end{align}
    However, we have
    \begin{equation}
        (\minkowskiMetric^{\mu\nu} \dalembertian - \partial^\mu \partial^\nu)\partial_\nu \chi = \dalembertian \partial^\mu \chi - \partial^\mu \dalembertian \chi = 0,
    \end{equation}
    and so the operator \(\minkowskiMetric^{\mu\nu} \dalembertian - \partial^\mu \partial^\nu\) has a zero eigenvalue, and so has no inverse.
    This means that the Green's function for the equations of motion, and hence the propagator, doesn't exist.
    
    Gauge invariance, which requires current conservation, implies a constraint, which reduces the degrees of freedom.
    This means that at most three of the components of \(A^\mu\), which are the degrees of freedom, are independent.
    
    \subsection{Canonical Constraints}
    Consider the specific case when \(\mu = 0\), then we have
    \begin{equation}
        (\minkowskiMetric^0\nu \dalembertian - \partial^\nu \partial^0)A_\nu = J^0.
    \end{equation}
    Separating spatial and temporal components we have
    \begin{equation}
        \ddot{A}^0 - \laplacian A^0 - \ddot{A}^0 + \div \dot{\vv{A}} = J^0 \implies -\laplacian A^0 + \div\dot{\vv{A}} = J^0.
    \end{equation}
    This means that \(A^0\) is not a genuine dynamical variable as there is no \(\dot{A}^0\) term in the equation of motion, and this in turn implies that there is no quadratic \((\dot{A}^0)^2\) term in the Lagrangian.
    
    Now consider the Hamiltonian formalism.
    We have the canonical momenta
    \begin{equation}
        \pi^\mu(x) = \diffp{\lagrangianDensity}{\dot{A}_\mu} = -F^{0\mu}(x).
    \end{equation}
    This means we have \(\pi^0 = -F^{00} = 0\), as \(F^{\mu\nu}\) is antisymmetric.
    There is then no momentum conjugate to \(A^0\), further solidifying our argument that it is not a valid dynamical variable.
    For the spatial components we have
    \begin{equation}
        \pi^i = -F^{0i} = -\dot{A}^i + \partial^i A^0 = E^i \implies \vv{\pi} = -\dot{\vv{A}} + \grad \varphi = \vv{E}.
    \end{equation}
    Taking the divergence then gives
    \begin{equation}
        \div\vv{\pi} = \laplacian A^0 - \div\dot{\vv{A}} = J^0 \iff \div \vv{E} = \rho,
    \end{equation}
    where \(J^\mu = (\rho, \vv{J})\).
    This imposes an extra constraint on the \(\pi^i\), they must satisfy \(\partial_i \pi^i = J^0\), and so only two of the canonical momenta are independent.
    So, we loose another degree of freedom, meaning we only have two left.
    
    \subsection{Quantum Theory}
    In the quantum theory we expect \(A_\mu\) to correspond to a massless spin 1 particle, the photon.
    We know that massless particles only have two degrees of freedom, their helicity, which is \(\pm 1\).
    So, of the four fields, \(A^\mu\), we only expect two of them to be physical, agreeing with our arguments above.
    
    Note that this is a fundamental fact for massless particles, we can't fix it by choosing a different Lagrangian, or by changing what dynamical variables we consider.
    Essentially the arguments above don't work if we include a mass term, \(-m^2A^2/2\), in the Lagrangian.
    A full proof of this is beyond this course, but a group theoretical argument in its favour is given in \course{Symmetries of Particles and Fields}.
    Essentially the representations of the Poincar\'e group corresponding to \(p^2 = m^2 = 0\) but with nonzero momentum have only two degrees of freedom.
    
    \subsection{Approaches to Quantisation}
    There are multiple approaches that are commonly taken to quantise the electromagnetic field and get around these problems, the four most common are:
    \begin{itemize}
        \item develop the Hamiltonian dynamics of the constrained system and then use this to aid in our quantisation,
        \item ignore the problem and quantise away, introducing spurious degrees of freedom which we fix later by imposing constraints, called the \defineindex{Gupta--Bleuler method},
        \item modify the Lagrangian by adding in a term which breaks gauge invariance, this is particularly suited to the path integral formalism, so will be addressed in the other half of this course, %TODO: add link to relevant section when it comes
        \item adopt a gauge in which there are only two physical degrees of freedom, quantise, and then generalise to restore manifest Lorentz covariance and gauge invariance, this keeps the physics clear, but is somewhat cumbersome.
    \end{itemize}
    It is the last of these approaches which we shall adopt here.
    The motivation is similar to solving a problem in relativity where we compute a value in a specific frame, write it in a manifestly Lorentz invariant way, and then it must hold in all frames.
    
    \section{Quantisation in the Coulomb Gauge}
    Consider the gauge transformation \(A^\mu \mapsto A^\mu - \partial^\mu \chi\).
    We've already seen that this doesn't change the physics.
    The spatial part of this transformation is \(\div\vv{A} \mapsto \div\vv{A} - \laplacian\chi\).
    We can then choose \(\chi\) such that \(\div\vv{A} = 0\).
    
    For a free field the equation of motion implies that \(\laplacian\vv{A} = \vv{0}\), and so we can, given suitable boundary conditions, take \(A^0 = 0\).
    Adding in this second condition gives the Coulomb gauge, which is explicitly given by setting
    \begin{equation}
        A^0 = 0, \qqand \div\vv{A} = 0.
    \end{equation}
    Note that these conditions are not Lorentz invariant, but they are consistent with the invariant Lorenz gauge condition, \(\partial_\mu A^\mu = 0\).
    However, since all gauge choices are equivalent we can still use these conditions to get Lorentz invariant physics.
    Having two conditions, such as those above, fixes two degrees of freedom.
    
    The free Maxwell equations in the Coulomb gauge are simply \(\dalembertian\vv{A} = \vv{0}\).
    The solutions to these are plane waves, with some vector amplitude,
    \begin{equation}
        \vv{A} = \vv{\varepsilon}^r(\vv{k}) \e^{-ik\cdot x}.
    \end{equation}
    We include the label \(r = 1, 2\) here labelling the two different solutions we expect to find.
    We call \(\vv{\varepsilon}^r\) the \defineindex{polarisation}.
    
    Substituting this ansatz into the equation of motion we have
    \begin{equation}
        0 = \dalembertian[\vv{\varepsilon}^r(\vv{k}) \e^{-ik\cdot x}] = -k^2 \vv{\varepsilon}^r(\vv{k}) \e^{-ik\cdot x},
    \end{equation}
    so, we must have \(\vv{k}^2 = 0\).
    The condition that \(\div\vv{A} = 0\) gives
    \begin{equation}
        0 = \div(\vv{\varepsilon}^r(\vv{k})\e^{-ik \cdot x}) = i\vv{k} \cdot \vv{\varepsilon}^r(\vv{k}) \e^{-ik\cdot x},
    \end{equation}
    so we must have \(\vv{k} \cdot \vv{\varepsilon}^r(\vv{k}) = 0\).
    This means that \(\vv{\varepsilon}^r\) are transverse.
    
    We're already not Lorentz invariant, so we may as well pick a frame.
    Choose one in which \(\vv{k} = \abs{\vv{k}}(0, 0, 1)\).
    There are two common choices for \(\vv{\varepsilon}^r\), since the only requirement is that they are linearly independent and orthogonal to \(\vv{\varepsilon}^r\).
    The first is \(\vv{\varepsilon}^1 = (1, 0, 0)\) and \(\vv{\varepsilon}^2 = (0, 1, 0)\).
    The second is circular polarisation, where \(\vv{\varepsilon}^{\pm} = (\vv{\varepsilon}^1 \pm i\vv{\varepsilon}^2)/\sqrt{2}\).
    This second choice is nice because \(\vv{\varepsilon}^{\pm}\) correspond to helicity eigenstates, however the increased complexity of the polarisation vectors is a drawback.
    It doesn't matter which we choose, its simply a choice of basis, so we'll work generally and allow for either.
    
    While we're making semi-arbitrary choices we may as well make another and define \(\vv{\varepsilon}^3 = \vv{k}/\abs{\vv{k}}\), a unit vector in the direction of \(\vv{k}\).
    So, \(\vv{\varepsilon}^3 = (0, 0, 1)\) in the chosen frame.
    
    
    Choosing one of these bases one can easily check that
    \begin{equation}
        \vv{\varepsilon}^r \cdot \vv{\varepsilon}^{s*} = \delta^{rs}, \qqand \sum_{r=1,2,3} \varepsilon_i^r\varepsilon_j^{r*} = \delta_{ij},
    \end{equation}
    where complex conjugation allows for complex polarisations, like the circularly polarised states.
    Rearranging the second of these we get
    \begin{equation}
        \sum_{r=1,2} \varepsilon_i^r \varepsilon_j^{r*} = \delta_{ij} - \varepsilon_i^3\varepsilon_j^{3*} = \delta_{ij} - \frac{k_ik_j}{\abs{\vv{k}}^2} \eqqcolon P_{ij}(\vv{k})
    \end{equation}
    where this defines the \defineindex{transverse projector}, \(P_{ij}(\vv{k})\).
    This has the property
    \begin{align}
        P_{ij}P_{jk} &= \left( \delta_{ij} - \frac{k_ik_j}{\abs{\vv{k}}^2} \right)\left( \delta_{jk} - \frac{k_jk_k}{\abs{\vv{k}}^2} \right)\\
        &= \delta_{ij}\delta_{jk} - \delta_{ij}\frac{k_jk_k}{\abs{\vv{k}}^2} - \frac{k_ik_j}{\abs{\vv{k}}^2}\delta_{jk} + \frac{k_ik_jk_jk_k}{\abs{\vv{k}}^4}\\
        &= \delta_{ik} - \frac{k_ik_k}{\abs{\vv{k}}^2} - \frac{k_ik_k}{\abs{\vv{k}}^2} + \frac{k_ik_k}{\abs{\vv{k}}^2}\\
        &= \delta_{ik} - \frac{k_ik_k}{\abs{\vv{k}}^2}\\
        &= P_{ik}.
    \end{align}
    
    Now expand the free electromagnetic field in terms of the complete set of Coulomb gauge plane wave solutions.
    Since the field is real we have a term and its Hermitian conjugate:
    \begin{equation}
        \vv{A}(x) = \sum_{r = 1, 2} \int \invariantmeasure{k} [\vv{\varepsilon}_r(\vv{k}) a_r(\vv{k} \e^{-ik\cdot x}) + \vv{\varepsilon}_r^*(\vv{k}) a_r^\hermit(\vv{k})\e^{ik\cdot x}]
    \end{equation}
    where we have \(k^2 = k_0^2 - \abs{\vv{k}}^2 = 0\) and \(k_0 = \omega(\vv{k})\).
    Note that whether the labels \(r\) are up or down is not important, and so here we move them to make room for the complex conjugate.
    
    To quantise we assume the usual commutation relations,
    \begin{equation}
        \commutator{a_r(\vv{k})}{a_s^\hermit(\vv{k}')} = \delta_{rs} \bardelta(\vv{k} - \vv{k}'),
    \end{equation}
    with all other commutators vanishing.
    We therefore expect that \(a_r^\hermit(\vv{k})\) creates a photon with momentum \(\vv{k}\) and energy \(\omega(\vv{k}) = \sqrt{\vv{k}^2} = \abs{\vv{k}}\), since \(m = 0\).
    
    In the Coulomb gauge the Lagrangian is
    \begin{equation}
        \lagrangianDensity = \frac{1}{2} \dot{A}_i^2 - \frac{1}{2}(\partial_iA_j)^2.
    \end{equation}
    The Hamiltonian density is then
    \begin{equation}
        \hamiltonianDensity = \frac{1}{2}\dot{A}_i^2 + \frac{1}{2}(\partial_i A_j)^2.
    \end{equation}
    The Hamiltonian is then given by
    \begin{align}
        H &= \frac{1}{2} \int \dl{^3\vv{x}} \normalordering{\dot{\vv{A}}^2}\\
        &= \sum_{r=1,2} \int \invariantmeasure{k} \, \omega(\vv{k}) a_r^{\hermit}(\vv{k}) a_r(\vv{k}).
    \end{align}
    
    As usual we can write the field as a positive and negative energy component, \(\vv{A} = \vv{A}_+ + \vv{A}_-\), and compute the \define{covariant commutation relations}\index{covariant commutation relation!for photons}:
    \begin{align}
        iD_+^{ij}(x - y) &\coloneqq \commutator{A_+^i(x)}{A_-^j(y)}\\
        &\hphantom{:}= \sum_{r, r' = 1, 2} \int \invariantmeasure{k} \int \invariantmeasure{k'} \, \varepsilon_r^i(\vv{k}) \varepsilon_{r'}^j(\vv{k}')^* \commutator{a_r(\vv{k})}{a_{r'}^\hermit(\vv{k}')} \e^{-ik \cdot x + i k' \cdot y}\\
        &\hphantom{:}= \sum_{r, r' = 1, 2} \int \invariantmeasure{k} \int \invariantmeasure{k'} \, \varepsilon_r^i(\vv{k}) \varepsilon_{r'}^j(\vv{k}')^* \delta_{rr'} \bardelta(\vv{k} - \vv{k}') \e^{-ik \cdot x + i k' \cdot y}\\
        &\hphantom{:}= \sum_{r = 1, 2} \int \invariantmeasure{k} \, \varepsilon_r^i(\vv{k}) \varepsilon_r^j(\vv{k})^* \e^{-ik(x - y)}\\
        &= \int \invariantmeasure{k} P^{ij}(\vv{k}) \e^{-ik\cdot (x - y)}.
    \end{align}
    Now define
    \begin{align}
        iD_{-}^{ij}(x - y) = \commutator{A_-^i(x)}{A_+^j(y)} = -D_+^{ij}*(y - x).
    \end{align}
    We then define the \define{Feynman propagator}\index{Feynman propagator!for photons},
    \begin{equation}
        iD_{\feynman}^{ij}(x - y) \coloneqq \bra{0} \timeOrdering[A^i(x) A^j(y)] \ket{0} = i\int \frac{\dl{^4k}}{(2\pi)^4} \frac{P^{ij}(\vv{k})}{k^2 + i\varepsilon} \e^{-ik\cdot (x - y)}.
    \end{equation}
    This isn't covariant, but it does contain all the relevant physics.
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % Redefine some things to fit Luigi's notation
    \renewcommand{\interaction}{\text{int}}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
    \part{Gaussian Integrals}
    \chapter{Gaussian Integrals}
    This part of the course is full of \define{Gaussian integrals}\index{Gaussian!integral}, so we'll spend some time to get comfortable with them, starting from the simplest one-dimensional case, and then adding in constants, before moving on to the arbitrary dimension case.
    \section{One Dimension}
    \subsection{No Constant Coefficient}
    It is well known that
    \begin{equation}
        Z_{1} \coloneqq \int_{-\infty}^{\infty} \dl{x} \, \exp\left\{ -\frac{1}{2}x^2 \right\} = \sqrt{2\pi}.
    \end{equation}
    This follows by considering
    \begin{align}
        Z_{1}^2 &= \int_{-\infty}^{\infty} \dl{x} \int_{-\infty}^{\infty} \dl{y} \, \exp\left\{ -\frac{1}{2}(x^2 + y^2) \right\}\\
        &= \int_{0}^{2\pi} \dl{\vartheta} \int_{0}^{\infty} \dl{r} \, r \exp\left\{ -\frac{1}{2}r^2 \right\}\\
        &= 2\pi \left[ -\exp\left\{ -\frac{1}{2}r^2 \right\} \right]_{0}^{\infty}\\
        &= 2\pi,
    \end{align}
    and so \(Z_1 = \sqrt{Z_1^2} = 2\pi\).
    
    \subsection{With Coefficient}
    If we include a coefficient, \(a\), with \(\Re(a) > 0\) then we have
    \begin{equation}
        Z_a \coloneqq \int_{-\infty}^{\infty} \dl{x} \, \exp\left\{ -\frac{1}{2}ax^2 \right\} = \sqrt{\frac{2\pi}{a}},
    \end{equation}
    For \(a \in \reals\) this follows by a change of variables, \(x \mapsto u/\sqrt{a}\), so \(\dl{x} = \dl{u}/\sqrt{a}\), and since \(a > 0\) and so \(\sqrt{a} > 0\) the integration limits are unchanged so we have
    \begin{equation}
        Z_a = \frac{1}{\sqrt{a}} \int_{-\infty}^{\infty} \dl{u} \, \exp\left\{ -\frac{1}{2}u^2 \right\} = \frac{1}{\sqrt{a}} Z_1 = \sqrt{\frac{2\pi}{a}}.
    \end{equation}
    The same thing holds for \(a \in \complex\) with \(\Re(a) > 0\), but we have to consider some contour integral and its not as straight forward to show.
    
    \subsection{With Linear Term}
    Now consider
    \begin{equation}\label{eqn:gaussian 1d with linear term}
        Z_a(b) \coloneqq \int_{-\infty}^{\infty} \dl{x} \, \exp\left\{ -\frac{1}{2}ax^2 + bx \right\} = \sqrt{\frac{2\pi}{a}} \exp\left\{ -\frac{b^2}{2a} \right\}.
    \end{equation}
    This is shown by completing the square:
    \begin{equation}
        -\frac{1}{2}ax^2 + bx = -\frac{a}{2}\left( x + \frac{b}{a} \right)^2 + \frac{b^2}{2a}.
    \end{equation}
    Hence,
    \begin{equation}
        Z_a(b) = \int_{-\infty}^{\infty} \dl{x} \, \exp\left\{ -\frac{1}{2}a\left( x - \frac{b}{a} \right) \right\} \exp\left\{ -\frac{b^2}{2a} \right\}.
    \end{equation}
    Making a change of variables \(x \mapsto u + b/a\), so \(\dl{x} = \dl{u}\) and the integration region is unchanged we get
    \begin{equation*}
        Z_a(b) = \exp\left\{ -\frac{b^2}{2a} \right\} \int_{-\infty}^{\infty} \!\! \dl{u} \, \exp\left\{ -\frac{1}{2}au^2 \right\} = \exp\left\{ -\frac{b^2}{2a} \right\} Z_a = \sqrt{\frac{2\pi}{a}} \exp\left\{ -\frac{b^2}{2a} \right\}.
    \end{equation*}
    
    \section{Arbitrary Dimension}
    \subsection{Diagonal Case}
    Let \(x \in \reals^n\), then
    \begin{equation}
        Z_I \coloneqq \int \dl{^nx} \, \exp\left\{ -\frac{1}{2}x^\trans x \right\} = (2\pi)^{n/2}
    \end{equation}
    where an integral without bounds is understood to be over all of \(\reals^n\).
    This follows since \(x^\trans x = x_ix_i\) and we then can factorise this into a product of \(n\) one-dimensional Gaussian integrals, each giving a factor of \(\sqrt{2\pi}\):
    \begin{equation}
        Z_I = (Z_1)^n = (2\pi)^{n/2}.
    \end{equation}
    
    Now let \(A \in \matrices{n}{\complex}\) be diagonal with \(\Re(A_{ij}) > 0\) for all \(i, j = 1, \dotsc, n\).
    Then we have
    \begin{equation}
        Z_A \coloneqq \int \dl{^nx} \, \exp\left\{ -\frac{1}{2}x^\trans A x \right\} = (2\pi)^{n/2} \left( \prod_{i=1}^{n} \frac{1}{\sqrt{A_{ij}}} \right).
    \end{equation}
    This follows since \(x^\trans Ax = x_iA_{ij}x_j\), and for \(A\) diagonal we have \(x^\trans Ax = x_i^2A_{ii}\), so \(Z_A\) factorises into a product of one-dimensional Gaussian integrals:
    \begin{equation}
        Z_A = \prod_{i = 1}^{n} Z_{A_{ii}} = (2\pi)^{n/2} \frac{1}{\sqrt{A_{ij}}}.
    \end{equation}
    Now notice that for a diagonal matrix
    \begin{equation}
        \det A = \prod_{i = 1}^{n} A_{ii},
    \end{equation}
    so
    \begin{equation}
        Z_A = (2\pi)^{n/2} (\det A)^{-1/2}.
    \end{equation}
    
    \subsection{Diagonalisable Case}
    Suppose that instead of being diagonal \(A\) is symmetric, and hence diagonalisable.
    That is, there exists some \(D \in \specialOrthogonal(n)\) such that \(A' = D^{-1}AD\) is diagonal.
    Consider the change of variables \(x \mapsto Dx\), and \(x^\trans \mapsto x^\trans D^\trans = x^{\trans}D^{-1}\).
    Since \(\det D = 1\) we have \(\dl{^nx} \mapsto \det(D) \dl{^nx} = \dl{^nx}\).
    Under this transformation we then have
    \begin{align}
        Z_A &= \int \dl{^nx} \, \exp\left\{ -\frac{1}{2}x^\trans Ax \right\}\\
        &= \int \dl{^nx} \, \exp\left\{ -\frac{1}{2}x^\trans D^{-1}ADx \right\}\\
        &= (2\pi)^{n/2}(\det(D^{-1}AD))^{-1/2}\\
        &= (2\pi)^{n/2} (\det A)^{-1/2},
    \end{align}
    having used
    \begin{equation*}
        \det(D^{-1}AD) = \det(D^{-1})\det(A)\det(D) = \det(D)^{-1}\det(A)\det(D) = \det A.
    \end{equation*}
    
    This shows that if \(A \in \matrices{n}{\complex}\) is a symmetric matrix with \(\Re(A_{ij}) \ge 0\) for all \(i, j = 1, \dotsc, n\) and the eigenvalues of \(A\), call them \(a_i\), are nonzero, which guarantees that \(\det A \ne 0\), then
    \begin{equation}
        Z_A \coloneq \int \dl{^nx} \, \exp\left\{ -\frac{1}{2}x^\trans Ax \right\} = (2\pi)^{n/2} (\det A)^{-1/2}.
    \end{equation}
    
    \subsection{With Linear Term}
    Now let \(b \in \reals^n\) and consider
    \begin{equation}
        Z_A(b) \coloneqq \int \dl{^nx} \, \exp\left[ -\frac{1}{2} x^\trans Ax + b^\trans x \right].
    \end{equation}
    Consider the change of variables
    \begin{equation}
        x = u + \Delta b
    \end{equation}
    where \(\Delta = A^{-1}\).
    We then have \(\dl{^nx} = \dl{^nu}\), and there is no change to the region of integration.
    Note that since \(A\) is symmetric so is \(\Delta\).
    We then have
    \begin{align}
        -\frac{1}{2}x^\trans A x &= -\frac{1}{2}(u^\trans + b^\trans \Delta^\trans) A (u + \Delta b)\\
        &= -\frac{1}{2}(u^\trans A u + u^\trans A\Delta b + b^\trans \Delta^\trans A u + b^\trans \Delta^\trans A \Delta b).
    \end{align}
    Now we can use \(\Delta^\trans = \Delta\) and \(u^\trans b = u_ib_i = b^\trans u\) to get
    \begin{align}
        -\frac{1}{2} x^\trans A x &= -\frac{1}{2}(u^\trans A u + u^\trans b + b^\trans u + b^\trans \Delta b)\\
        &= -\frac{1}{2}u^\trans A u - b^\trans u - b \Delta b.
    \end{align}
    Similarly,
    \begin{equation}
        b^\trans x = b^\trans u + b^\trans \Delta b,
    \end{equation}
    and so
    \begin{align}
        -\frac{1}{2}x^\trans A x + b^\trans x &= -\frac{1}{2}u^\trans A u - b^\trans u - \frac{1}{2} b^\trans \Delta b + b^\trans u + b^\trans \Delta b\\
        &=  -\frac{1}{2} u^\trans A u + \frac{1}{2} b^\trans \Delta b.
    \end{align}
    Hence, we have
    \begin{equation}
        \exp\left\{ -\frac{1}{2}x^\trans A x + b^\trans x \right\} = \exp\left\{-\frac{1}{2} u^\trans A u\right\} \exp\left\{ \frac{1}{2} b^\trans \Delta b \right\}.
    \end{equation}
    Note that these are just exponentials of numbers, so everything commutes and the usual exponential laws apply.
    Finally, we have
    \begin{align}
        Z_A(b) &= \int \dl{^nx} \, \exp\left\{ -\frac{1}{2}x^\trans A x + b^\trans x \right\}\\
        &= \exp\left\{ \frac{1}{2}b^\trans \Delta b \right\} \int \dl{^nu} \, \exp\left\{ -\frac{1}{2}u^\trans A u \right\}\\
        &= Z_A\exp\left\{ \frac{1}{2}b^\trans \Delta b \right\}\\
        &= (2\pi)^{n/2} (\det A)^{-1/2} \exp\left\{ -\frac{1}{2}b^\trans \Delta b \right\}.
    \end{align}
    
    \section{Generating Function}
    Let \(\mu\) be a measure on \(\reals^n\).
    For our purposes a measure, \(\mu\), is something such that \(\dl{\mu(x)} = \dl{^nx}\, \Omega(x)\) where \(\Omega\) is some integrable function.
    We define the \define{expectation value}\index{expectation value!with respect to a measure} of \(F \colon \reals^n \to \reals\) with respect to \(\mu\) to be
    \begin{equation}
        \expected{F}_\mu \coloneqq \int \dl{\mu(x)} \, F(x) = \int \dl{^nx} \, \Omega(x) F(x).
    \end{equation}
    
    We assume that the measure is normalised such that
    \begin{equation}
        \int \dl{\mu(x)} = 1.
    \end{equation}
    This allows for a probability interpretation of our results.
    
    We then define the \defineindex{generating function}
    \begin{equation}
        Z_\mu(b) \coloneqq \expected{\e^{b^\trans x}}_\mu = \int \dl{\mu(x)} \, \exp\{b^\trans x\}.
    \end{equation}
    We can expand the exponential in a series to get
    \begin{equation}
        \exp\{b^\trans x\} = \sum_{\ell = 0}^{\infty} \frac{1}{\ell!} (b^\trans x)^n = \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} b_{i_1}\dotsm b_{i_\ell} x_{i_1} \dotsm x_{i_\ell}.
    \end{equation}
    Applying this to the generating function, and realising that we can use the linearity of the integral to pull out all the sums and factors of \(b_i\), we have
    \begin{equation}
        Z_\mu(b) = \sum_{\ell = 0}^{\infty} \frac{1}{\ell!} \sum_{i_1, \dotsc, i_\ell = 1}^{n} b_{i_1} \dotsm b_{i_\ell} \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu
    \end{equation}
    We call
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu = \int \dl{\mu(x)} \, x_{i_1} \dotsm x_{i_\ell}
    \end{equation}
    the \defineindex{correlation function} of \(x\) with respect to the measure \(\mu\).
    In particular it's called the \(\ell\)-point correlation function, since it has \(\ell\) points, \(x_{i_1}\) through \(x_{i_\ell}\).
    
    From this work we can see that the generating function is simply a polynomial in \(b_i\), after computing the integrals in the correlation functions they are just numbers.
    We can then take derivatives to get
    \begin{equation}
        \diffp{}{_k} Z_\mu(b) = \int \dl{\mu(x)} \, \diffp{}{b_k} \exp\{b_ix_i\} = \int \dl{\mu(x)} \, x_k \exp\{b_ix_i\}.
    \end{equation}
    This allows us to write the correlation function as
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu = \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}} Z_\mu(b) \bigg|_{b = 0},
    \end{equation}
    since \(\exp[b^\trans x]|_{b = 0} = 1\).
    
    We can use this same method to find the expectation value of any function with a Taylor series.
    If
    \begin{equation}\label{eqn:taylor series}
        F(x) = \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} F_{i_1, \dotsc, i_{\ell}} x_{i_1} \dotsm x_{i_\ell}
    \end{equation}
    for some constants \(F_{i_1, \dotsc, i_{\ell}}\), then
    \begin{equation}
        \expected{F}_\mu = \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} F_{i_1, \dotsc, i_{\ell}} \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu.
    \end{equation}
    
    \section{Gaussian Generating Functions}
    Consider the \define{Gaussian measure}\index{Gaussian!measure}
    \begin{equation}
        \dl{\mu_0(x)} \coloneqq \dl{^nx} \, \Omega_0(x) = \dl{^n x} \, \symcal{N}_0 \exp\left\{ -\frac{1}{2} x^\trans A x \right\}
    \end{equation}
    where \(A \in \matrices{n}{\complex}\) is a symmetric matrix with \(\Re(A_{ij}) \ge 0\) for all \(i, j = 1, \dotsc, n\) and with nonzero eigenvalues, and \(\symcal{N}_0\) is the normalisation factor
    \begin{equation}
        \symcal{N}_0 = (2\pi)^{-n/2} (\det A)^{1/2} = Z_A^{-1}
    \end{equation}
    which ensures that
    \begin{equation}
        \int \dl{\mu_0(x)} = 1.
    \end{equation}
    
    We can then define the \define{Gaussian generating function}\index{Gaussian!generating function},
    \begin{equation}
        Z_0(b) = \frac{Z_A(b)}{Z_A} = \exp\left\{ \frac{1}{2} b^\trans \Delta b \right\}.
    \end{equation}
    Then the \define{Gaussian correlation function}\index{Gaussian!correlation function} is
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_0 \coloneqq \expected{x_{i_1} \dotsm x_{i_\ell}}_{\mu_0} = \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}} \exp\left\{ \frac{1}{2}b^\trans \Delta b \right\}\bigg|_{b = 0}.
    \end{equation}
    
    If \(F\) is an arbitrary function with a Taylor series as in \cref{eqn:taylor series} then
    \begin{align}
        \expected{F}_0 &= \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_\ell = 1}^{n} F_{i_1, \dotsc, i_{\ell}} \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_{\ell}}} \exp\left\{ \frac{1}{2} b^\trans \Delta b \right\} \bigg|_{b = 0}\\
        &= F(\diffp{}/{b}) \exp\left\{ \frac{1}{2} b^\trans \Delta b \right\}\bigg|_{b = 0}
    \end{align}
    where \(F(\diffp{}/{b})\) is a formal operator expression given by \enquote{evaluating} the Taylor series for \(F\) at the partial derivatives:
    \begin{equation}
        F(\diffp{}/{b}) = \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} F_{i_1, \dotsc, i_{\ell}} \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}}.
    \end{equation}
    
    \subsection{One Point Gaussian Correlator}
    Consider the one point Gaussian correlation function,
    \begin{equation}
        \expected{x_k}_0 = \diffp{}{b_k} \expected{\exp\{b^\trans x\}}_0 \bigg|_{b = 0} = \diffp{}{b_k} \exp\left\{ \frac{1}{2}b^\trans \Delta b \right\}.
    \end{equation}
    We have
    \begin{equation}
        \diffp{}{b_k} \exp\left\{ \frac{1}{2}b_i\Delta_{ij}b_j \right\} = \frac{1}{2}[\Delta_{kj}b_j + b_i\Delta_{ik}]\exp\{b^\trans \Delta b\} = \Delta_{kj}b_j \exp\{b^\trans \Delta b\},
    \end{equation}
    where we've used the symmetry of \(\Delta\) and \(\diffp{b_i}/{b_k} = \delta_{ik}\).
    We then have
    \begin{equation}
        \expected{x_k}_0 = (\Delta_{kj}b_j) \exp\left\{ \frac{1}{2}b^\trans \Delta b \right\}\bigg|_{b = 0} = 0.
    \end{equation}
    We could have realised that this must vanish since our integrand is an odd function of \(x\) over an even range.
    We can interpret the one point correlation as the mean of a normally distributed value.
    
    \subsection{Two Point Gaussian Correlator}
    Consider the two point Gaussian correlation function,
    \begin{equation}
        \expected{x_kx_l}_{0} = \diffp{}{b_k}\diffp{}{b_l} \expected{\exp\{b^\trans x\}}_0 \bigg|_{b = 0}.
    \end{equation}
    We've already done the first derivative, so we just have to compute
    \begin{align}
        \expected{x_kx_l}_0 &= \diffp{}{b_l} \left( (\Delta_{kj b_j})\exp\left\{ \frac{1}{2}b^\trans \Delta b \right\} \right)\bigg|_{b = 0}\\
        &= [\delta_{kkj}\Delta_{kj} + (\Delta_{kj}b_j)(\Delta_{li}b_i)] \exp\left\{ \frac{1}{2}b^\trans b \right\}\bigg|_{b = 0}\\
        &= \Delta_{kl}.
    \end{align}
    We can interpret the two point correlation as the covariance of two normally distributed values.
    
    \subsection{General Gaussian Correlator}
    Now consider the \(\ell\)-point Gaussian correlation function
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_0 = \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}} \expected{\exp\{b^\trans x\}}_0 \bigg|_{b = 0}.
    \end{equation}
    In general the derivatives will either act on the exponential term, giving a factor of \(\Delta b\), or they can act on an existing \(\Delta b\) term giving a \(\Delta\) term.
    The only nonzero terms after setting \(b = 0\) are those where each \(\Delta b\) has been acted on by a derivative to remove the \(b\) dependence.
    This requires an even number of derivatives to get a nonzer result.
    In the case when \(\ell\) is even the process for determining the result is as follows:
    \begin{itemize}
        \item Write all pairings \((i_p, i_q)\), which can be formed from the indices \(i_1, \dotsc, i_\ell\).
        Treat \((i_p, i_q)\) and \((i_q, i_p)\) as the same and no index can be in a pair with itself.
        \item Build the set, \(P\), of all sets of pairings, so the elements of \(P\) are sets of \(\ell/2\) pairs such that every index appears in exactly one pair.
        \item The correlator is given by
        \begin{equation}
            \expected{x_{i_1}\dotsm x_{i_\ell}}_0 = \sum_{P} \Delta_{i_p i_q} \dotsm \Delta_{i_ri_s}.
        \end{equation}
    \end{itemize}
    This is \defineindex{Wick's theorem}.
    
    To make this clearer lets consider some examples.
    First, the two point correlator, \(\expected{x_{i_1}x_{i_2}}_0\).
    We have two indices, \(i_1\) and \(i_\ell\), so there is a single pairing, \((i_1, i_2)\), and we have
    \begin{equation}
        \expected{x_{i_1}x_{i_2}}_0 = \Delta_{i_1i_2}.
    \end{equation}
    We can also write this as
    \begin{equation}
        \expected{x_{i_1}x_{i_2}}_0 = \wick{\c x_{i_1} \c x_{i_2}}
    \end{equation}
    where
    \begin{equation}
        \wick{\c x_{i_1} \c x_{i_2}} \coloneqq \Delta_{i_1i_2}.
    \end{equation}
    
    Now consider the four point correlator, \(\expected{x_{i_1}x_{i_2}x_{i_3}x_{i_4}}_0\).
    The set of possible pairings is
    \begin{equation}
        P = \{\{(i_1, i_2), (i_3, i_4)\}, \{(i_1, i_3), (i_2, i_4)\}, \{(i_1, i_4), (i_2, i_3)\}\}.
    \end{equation}
    The correlator is then
    \begin{align}\label{eqn:4 point correlator}
        \expected{x_{i_1}x_{i_2}x_{i_3}x_{i_4}}_0 &= \Delta_{i_1i_2}\Delta_{i_3i_4} + \Delta_{i_1i_3}\Delta_{i_2i_3} + \Delta_{i_1i_4}\Delta_{i_2i_3}\\
        &= \wick{\c x_{i_1} \c x_{i_2}} \wick{\c x_{i_3} \c x_{i_4}} + \wick{\c1 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c2 x_{i_4}} + \wick{\c2 x_{i_1} \c1 x_{i_2} \c1 x_{i_3} \c2 x_{i_4}}.
    \end{align}
    
    Finally, lets do the six point correlation function.
    We'll jump straight to the contractions:
    \begin{align}
        \expected{x_{i_1}x_{i_2}x_{i_3}x_{i_4}x_{i_5}x_{i_6}} &=
        \wick{\c x_{i_1} \c x_{i_2}} \wick{\c x_{i_3} \c x_{i_4}} \wick{\c x_{i_5} \c x_{i_6}}
        + \wick{\c x_{i_1} \c x_{i_2}} \wick{\c1 x_{i_3} \c2 x_{i_4} \c1 x_{i_5} \c2 x_{i_6}}\\
        &+ \wick{\c x_{i_1} \c x_{i_2}} \wick{\c2 x_{i_3} \c1 x_{i_4} \c1 x_{i_5} \c2 x_{i_6}}
        + \wick{\c1 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c2 x_{i_4}} \wick{\c x_{i_5} \c x_{i_6}}\\
        &+ \wick{\c1 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c1 x_{i_6}}
        + \wick{\c1 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c1 x_{i_5} \c2 x_{i_6}}\\
        &+ \wick{\c2 x_{i_1} \c1 x_{i_2} \c1 x_{i_3} \c2 x_{i_4}} \wick{\c x_{i_5} \c x_{i_6}}
        + \wick{\c1 x_{i_1} \c2 x_{i_2} \c3 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c3 x_{i_6}}\\
        &+ \wick{\c1 x_{i_1} \c3 x_{i_2} \c2 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c3 x_{i_6}}
        + \wick{\c2 x_{i_1} \c1 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c1 x_{i_6}}\\
        &+ \wick{\c2 x_{i_1} \c1 x_{i_2} \c3 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c3 x_{i_6}}
        + \wick{\c1 x_{i_1} \c3 x_{i_2} \c2 x_{i_3} \c2 x_{i_4} \c1 x_{i_5} \c3 x_{i_6}}\\
        &+ \wick{\c2 x_{i_1} \c1 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c1 x_{i_5} \c2 x_{i_6}}
        + \wick{\c3 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c2 x_{i_4} \c1 x_{i_5} \c3 x_{i_6}}\\
        &+ \wick{\c3 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c3 x_{i_6}}
    \end{align}
    
    \begin{cde}{}{}
        Here's some Mathematica code to generate all pairs.
        \textcolor{Red}{WARNING:} Horribly inefficient code!
        It generates all permutations of the indices, groups them into pairs, then eliminates repeats, so its \(\order(\ell!)\) in memory, I can't even run it for \(\ell = 12\).
        \begin{lstlisting}[gobble=12, language=mathematica, mathescape]
            Union[Sort /@ 
              Union[Sort /@ Partition[#[[1]], 2] & /@ 
               Table[
                 {#[[i]], #[[i + 1]]}, {i, 1, Length[#], 2}] &@
               Permutations[Range[$\ell$]]
            (* replace $\textcolor{codeCommentColor}{\ell}$ with number of indices (even) *)
        \end{lstlisting}
    \end{cde}
    
    As you can see the number of indices grows quickly.
    Suppose we have \(2m\) indices for some integer \(m\).
    Then there are \(2m - 1\) options for what to pair the first variable with, since it can't be paired with itself.
    Then take the next unpaired variable, there are \(2m - 3\) variables to pair it with, since it can't be paired with itself or either of the already paired variables.
    The next unpaired variable has \(2m - 5\) potential pairings, and so on, until we have four variables left, one of these has three options for pairing, and then we're left with two variables which we have to pair.
    That is, the number of pairings is given by
    \begin{equation}
        \text{number of pairings} = (2m - 1)(2m - 3) \dotsm 3 \dotsm 1 = (2m - 1)!!
    \end{equation}
    where \(!!\) is the double factorial, defined recursively as \(n!! = n(n - 2)!!\) with \(1!! = 0!! = 1\).
    Here are the number of pairings for the first few nonzero correlators:
    \begin{equation*}
        \begin{array}{rrrrrrrrrr}
            2 & 4 & 6  & 8   & 10  & 12    & 14     & 16      & 18       & 20\\
            1 & 3 & 15 & 105 & 945 & \num{10395} & \num{135135} & \num{2027025} & \num{34459425} & \num{654729075}\\
        \end{array}
    \end{equation*}
    So its fair to say that evaluating general correlators with more than a couple of points quickly becomes intractable.

    \chapter{Perturbed Correlators}
    \section{General Theory}
    As with the Gaussian case we can write the expected value of any analytic function, \(F\), with respect to some measure, \(\mu\), as a sum over correlators with respect to \(\mu\):
    \begin{align}
        \expected{F}_\mu &= \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_\ell = 1}^{n} F_{i_1\dotso i_{\ell}} \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu\\
        &= \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} F_{i_1\dotso i_\ell} \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}} Z_\mu(b) \bigg|_{b = 0}\\
        &= F(\diffp{}/{b}) Z_\mu(b)|_{b = 0}.
    \end{align}
    
    We now consider a slightly more general measure, \(\dl{\mu(x)} = \dl{^nx} \,\Omega(x)\) where
    \begin{equation}
        \Omega(x) = \frac{1}{Z_\lambda} \e^{-S_\lambda(x)}
    \end{equation}
    where
    \begin{equation}
        S_\lambda(x) = S_0(x) + \lambda V(x)
    \end{equation}
    with \(S_0(x) = x^\trans Ax/2\), giving the usual Gaussian measure, and \(V\) being some \enquote{potential} acting as a perturbation.
    We will assume \(\lambda\) is small and consider expansions in \(\lambda\).
    The factor \(Z_\lambda\) is given by
    \begin{align}
        Z_\lambda &= \int \dl{^nx}  \, \exp[-S_\lambda(x)]\\
        &= \int \dl{^nx} \, \exp[-S_0(x)]\exp[-\lambda V(x)]\\
        &= Z_0 \expected{\exp[-\lambda V(x)]}_0.
    \end{align}
    We will be interested in computing the value of
    \begin{equation}
        \frac{Z_\lambda}{Z_0} = \expected{\exp[-\lambda V(x)]}_0 = \sum_{k = 0}^{\infty} \frac{(-\lambda)^k}{k!} \expected{V(x)^k}_0.
    \end{equation}
    We can work out the expectation value once we know \(V\) by expanding \(V\).
    
    \section{\texorpdfstring{\(x^4\)}{x to the four} Potential}
    \epigraph{The first instant of a long series of asking \enquote{how do we count?}}{Luigi Del Debbio}
    Consider the example
    \begin{equation}
        V(x) = \frac{1}{4!} \sum_{i = 1}^n x_i^4.
    \end{equation}
    We can quickly write down a series for \(Z_\lambda/Z_0\) in terms of correlators:
    \begin{equation}
        \frac{Z_\lambda}{Z_0} = 1 - \lambda \frac{1}{4!} \sum_i \expected{x_i^4}_0 + \frac{\lambda^2}{2}\frac{1}{(4!)^2} \sum_i \sum_j \expected{x_i^4 x_j^4}_0 + \order(\lambda^3).
    \end{equation}
    Now we just need to compute the correlators.
    
    Starting with \(\expected{x_i^4}_0\).
    This is just a special case of the four point correlator in \cref{eqn:4 point correlator} where we set all indices equal.
    This means that we have
    \begin{align}
        \expected{x_ix_ix_ix_i}_0 &= \Delta_{ii}\Delta_{ii} + \Delta_{ii}\Delta_{ii} + \Delta_{ii}\Delta_{ii}\\
        &= \wick{\c x_i \c x_i} \wick{\c x_i \c x_i} + \wick{\c1 x_i \c2 x_i \c1 x_i \c2 x_i} + \wick{\c2 x_i \c1 x_i \c1 x_i \c2 x_i}\\
        &= 3 \Delta_{ii}^2.
    \end{align}
    Notice that we could have worked this out without needing to do the full four point correlator for arbitrary indices, we just need to find a way to count the number of pairings.
    In this case there are 3 choices for what to pair the first \(x_i\) with, then the two remaining \(x_i\)s must be paired, so there are 3 terms, and all three are the same.
    
    This is good because as we saw for the six point correlator things quickly get out of hand, and the next term involves an 8 point correlator, which would be even worse at \((8 - 1)!! = 105\) terms.
    So, what sort of pairings are there going to be in this correlator?
    First there are going to be pairings akin to two copies of the four point correlator, with all \(x_i\)s paired with \(x_i\)s, and all \(x_j\)s paired with \(x_j\)s.
    There are nine ways to do this, getting a factor of three from pairing up \(x_i\)s and a factor of three from pairing up \(x_j\)s.
    This means we have the term \(9\Delta_{ii}^2\Delta_{jj}^2\).
    
    The next logical term to consider is one which pairs one \(x_i\) with one \(x_j\), however, if we do so we're left with an odd number of \(x_i\)s, so another of these must pair with an \(x_j\).
    So, our next term pairs two \(x_i\)s with two \(x_j\)s, and then the remaining \(x_i\)s are paired with each other and likewise for the remaining \(x_j\)s.
    How many ways are there to do this?
    There are actually (at least) two ways to count this.
    First, we have \(\binom{4}{2}\) ways to choose the two \(x_i\) to pair with \(x_j\)s.
    Then we have four choices of which \(x_j\) to pair with the first and 3 choices of which \(x_j\) to pair with the second, for a total of \(\binom{4}{2} \cdot 4 \cdot 3 = 72\) terms.
    The second way to count starts with choosing two \(x_i\)s, again giving a factor of \(\binom{4}{2}\), and then choosing two \(x_j\)s, for another factor of \(\binom{4}{2}\).
    We can then pair up between these pairs in two ways, pairing the first of each pair, or the first of the \(x_i\)s and the second of the \(x_j\)s, for a factor of 2, giving \(\binom{4}{2}^2 \cdot 2 = 72\).
    Hence, we have the term \(72\Delta_{ij}^2\Delta_{ii}\Delta_{jj}\).
    
    There's no way to pair up exactly three \(x_i\)s with \(x_j\)s, so the only other term is pairing all four \(x_i\)s with an \(x_j\).
    This can be done by holding the \(x_i\) in some fixed order, then considering all permutations of the four \(x_j\)s, and pairing them up in order for each permutation.
    So, we have a factor of \(4! = 24\).
    The final term is then \(24\Delta_{ij}^4\).
    
    At this point there are some checks we can do.
    First, we should, including repeated terms, have 8 indices, and 4 \(\Delta\)s in each term.
    Second, adding up the combinatorial factors should give \(105\), the total number of ways to pair 8 things, and indeed this does work out.
    So, we conclude that
    \begin{equation}
        \expected{x_i^4 x_j^4}_0 = 9\Delta_{ii}^2\Delta_{jj}^2 + 72\Delta_{ij}^2\Delta_{ii}\Delta_{jj} + 24\Delta_{ij}^4.
    \end{equation}
    
    Hence, to second order, we have
    \begin{align}
        \frac{Z_\lambda}{Z_0} &= 1 - \frac{\lambda}{4!} \sum_{i} 3\Delta_{ii}^2 + \frac{\lambda^2}{2 (4!)^2}\sum_{i,j}[9\Delta_{ii}^2\Delta_{jj}^2 + 72\Delta_{ij}^2\Delta_{ii}\Delta_{jj} + 24\Delta_{ij}^4]\\
        &= 1 - \frac{\lambda}{8} \sum_i \Delta_{ii}^2 + \lambda^2\sum_{i,j} \left[ \frac{1}{128}\Delta_{ii}^2\Delta_{jj}^2 + \frac{1}{16}\Delta_{ij}^2\Delta_{ii}\Delta_{jj} + \frac{1}{48}\Delta_{ij}^4 \right]
    \end{align}
    
    \subsection{Diagrammatic Representation}
    There is a diagrammatic representation of terms like this.
    For each propagator, \(\Delta_{ij}\), we draw a line connecting the nodes \(i\) and \(j\):
    \begin{equation}
        \Delta_{ij} = 
        \tikzsetnextfilename{fd-propagator}
        \begin{tikzpicture}[baseline=(i.base)]
            \draw (0, 0) node[left] (i) {\(i\)} -- (1, 0) node[right] {\(j\)};
        \end{tikzpicture}
    \end{equation}
    Note that the position of the nodes doesn't matter, just how they're connected.
    For terms like \(\Delta_{ii}\) with a repeated index we have a sum over \(i\) when these appear.
    As a diagram a repeated index means a closed loop, and so any closed loop implies a sum:
    \begin{equation}
        \sum_i \Delta_{ii} = 
        \tikzsetnextfilename{fd-propgator-repeated-index}
        \begin{tikzpicture}[baseline=(i.base)]
            \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [left] (i) {\(i\)};
        \end{tikzpicture}
    \end{equation}
    Another example might be the term \(\sum_{j}\Delta_{ij}\Delta_{jj}\), which corresponds to the diagram
    \begin{equation}
        \sum_j \Delta_{ij}\Delta_{jj} = 
        \tikzsetnextfilename{fd-propagator-repeated-index-2}
        \begin{tikzpicture}[baseline=(i.base)]
            \draw[looseness=1.5] (0, 0) node[left] (i) {\(i\)} -- (1, 0) to[out=40, in=90] (2, 0) to[out=270, in=-40] (1, 0) node [below] {\(j\)};
        \end{tikzpicture}
    \end{equation}
    Fun Fact™: Diagrams like this one are called tadpole diagrams!
    
    Using this we can write the first order (in \(\lambda\)) \(\Delta_{ii}^2\) term as
    \begin{equation}
        \sum_i \Delta_{ii}^2 = 
        \tikzsetnextfilename{fd-first-order-x4-term}
        \begin{tikzpicture}[baseline={(0, -0.1)}]
            \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(i\)};
            \draw[looseness=1.5, xscale=-1] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0);
        \end{tikzpicture}
    \end{equation}
    Similarly we can write the following
    \begin{equation}
        \expected{x_i^4 x_j^4}_0 = 9\Delta_{ii}^2\Delta_{jj}^2 + 72\Delta_{ij}^2\Delta_{ii}\Delta_{jj} + 24\Delta_{ij}^4.
    \end{equation}
    as
    \begin{equation}
        \sum_{i,j} \expected{x_i^4x_j^4} = 9
        \tikzsetnextfilename{fd-second-order-x4-term-1}
        \begin{tikzpicture}[baseline={(0, -0.6)}]
            \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(i\)};
            \draw[looseness=1.5, xscale=-1] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0);
            \begin{scope}[yshift=-1cm]
                \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(j\)};
                \draw[looseness=1.5, xscale=-1] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0);
            \end{scope}
        \end{tikzpicture}
        + 72 \,
        \tikzsetnextfilename{fd-second-order-x4-term-2}
        \begin{tikzpicture}[baseline={(0, -0.1)}]
            \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(i\)};
            \draw[looseness=1.5, xshift=-1cm, xscale=-1] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(j\)};
            \draw (0, 0) to[bend right=40] (-1, 0);
            \draw (0, 0) to[bend left=40] (-1, 0);
        \end{tikzpicture}
        + 24 \,
        \tikzsetnextfilename{fd-second-order-x4-term-3}
        \begin{tikzpicture}[baseline=(i.base)]
            \draw (0, 0) node [left] (i) {\(i\)} to[bend right=20] (1, 0) node [right] {\(j\)};
            \draw (0, 0) node [left] (i) {\(i\)} to[bend right=60] (1, 0);
            \draw (0, 0) node [left] (i) {\(i\)} to[bend left=20] (1, 0);
            \draw (0, 0) node [left] (i) {\(i\)} to[bend left=60] (1, 0);
        \end{tikzpicture}
    \end{equation}
    
    \subsection{Computing \texorpdfstring{\(\log(Z_\lambda/Z_0)\)}{log(Zlambda/Z0)}}\label{sec:computing log Zlambda/Z0}
    Suppose we want to compute \(\log(Z_\lambda/Z_0)\).
    To do this we notice that \(Z_\lambda/Z_0 = 1 + \order(\lambda)\), and so we can use the Taylor expansion
    \begin{equation}
        \log(1 + \varepsilon) = \sum_{n = 1}^{\infty} \frac{(-1)^{n + 1}}{n}\varepsilon^n \approx \varepsilon - \frac{1}{2}\varepsilon^2 + \frac{1}{3}\varepsilon^3 - \dotsb.
    \end{equation}
    Since we already have \(\varepsilon\) here as a power series in \(\lambda\) we should work this out in powers of \(\lambda\).
    The first order term has only a single contribution, from the first order term in \(\lambda\).
    That is, to first order
    \begin{equation}
        \log\left( \frac{Z_\lambda}{Z_0} \right) = -\frac{\lambda}{8}\sum_i \Delta_{ii}^2 + \order(\lambda^2).
    \end{equation}
    For the second order term there are two contributions, we have the first order term squared, which also picks up a factor of \(-1/2\) from the expansion of the log:
    \begin{equation}
        -\frac{1}{2} \frac{\lambda^2}{8^2} \sum_{i} \Delta_{ii}^2 \sum_j \Delta_{jj}^2 = -\frac{1}{2} \frac{\lambda^2}{128} \sum_{i, j} \Delta_{ii}^2\Delta_{jj}^2.
    \end{equation}
    We also have the first order in \(\varepsilon\) term from the second order in \(\lambda\) term:
    \begin{equation}
        \lambda^2 \sum_{i,j} \left[ \frac{1}{128}\Delta_{ii}^2\Delta_{jj}^2 + \frac{1}{16} \Delta_{ij}^2\Delta_{ii}\Delta_{jj} + \frac{1}{48}\Delta_{ij}^4 \right].
    \end{equation}
    Notice that the first term here cancels with the other \(\lambda^2\) term, to give the second order result
    \begin{equation}
        \log\left( \frac{Z_\lambda}{Z_0} \right) = -\frac{\lambda}{8} \sum_{i} \Delta_{ii}^2 + \lambda^2 \left[ \frac{1}{128}\Delta_{ii}^2\Delta_{jj}^2 + \frac{1}{16}\Delta_{ij}^2 \Delta_{ii} \Delta_{jj} + \frac{1}{48}\Delta_{ij}^4 \right] + \order(\lambda^3).
    \end{equation}
    
    In terms of diagrams, this corresponds to
    \begin{multline}
        \log\left( \frac{Z_\lambda}{Z_0} \right) = -\frac{\lambda}{8} 
        \vcenter{\hbox{\includegraphics{tikz-external/fd-first-order-x4-term}}}\\
        + \frac{\lambda^2}{16} \vcenter{\hbox{\includegraphics{tikz-external/fd-second-order-x4-term-2}}} + \frac{\lambda^2}{48} \vcenter{\hbox{\includegraphics{tikz-external/fd-second-order-x4-term-3}}} + \order(\lambda^3).
    \end{multline}
    Notice that there are no disconnected diagrams, they cancelled out.
    In fact disconnected diagrams will cancel out here to all orders, and this isn't just the case for this potential.
    Taking the log of the generating function yields the generating function of connected contributions.
    We'll prove this later in \cref{sec:disconnected diagrams}.
    
    \section{Correlators}
    \epigraph{That's it. That's QFT done.}{Luigi Del Debbio}
    Much of the rest of the course will be focused on computing various correlators with respect to some perturbed Gaussian, and of course linking these mathematical expressions to physics.
    We'll often be interested in calculating
    \begin{align}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_\lambda &= \frac{1}{Z_\lambda} \int \dl{^nx} \exp[-S_\lambda(x)] x_{i_1} \dotsm x_{i_\ell}\\
        &= \frac{Z_0}{Z_\lambda} \frac{1}{Z_0} \int \dl{^nx} \exp[-S_\lambda(x)] x_{i_1} \dotsm x_{i_\ell}\\
        &= \frac{Z_0}{Z_\lambda} \frac{1}{Z_0} \int \dl{^nx} \exp[-S_0(x)] \sum_{k = 0}^{\infty} \frac{(-\lambda)^k}{k!} V(x)^k x_{i_1} \dotsm x_{i_\ell}\\
        &= \frac{Z_0}{Z_\lambda} \sum_{k = 0}^{\infty} \frac{(-\lambda)^k}{k!} \expected{V(x)^kx_{i_1} \dotsm x_{i_\ell}}_0.
    \end{align}
    
    The good news is that we've just seen how to compute \(Z_\lambda/Z_0\), so all we need to do is invert this result to get \(Z_0/Z_\lambda\).
    This can be done in a similar way to the logarithm by again noticing that \(Z_\lambda/Z_0 = 1 - \order(\lambda)\) and then using the result
    \begin{equation}
        \frac{1}{1 - \varepsilon} = \sum_{n = 0}^{\infty} \varepsilon^n \approx 1 + \varepsilon + \varepsilon^2 + \dotsb.
    \end{equation}
    
    The results we get will be of the form
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_0 = \order(\lambda^0) + \order(\lambda) + \order(\lambda^2) + \dotsb.
    \end{equation}
    The order of \(\lambda^0\) (i.e., constant) term will be the result we get with an unperturbed Gaussian.
    We then have first order corrections, and second order corrections, and so on.
    
    \part{Path Integrals in Quantum Mechanics}
    \chapter{Path Integrals in QM}
    \section{Preliminaries}
    In this chapter we will develop the theory of path integrals in nonrelativistic quantum mechanics.
    This should then rapidly generalise to relativistic quantum field theory, but for now we stick to the nonrelativistic case to avoid unnecessary complications.
    
    Working in quantum mechanics we have a position operator, \(\operator{Q}\), with eigenstates \(\ket{q}\), with eigenvalues \(q\):
    \begin{equation}
        \operator{Q}\ket{q} = q\ket{q}.
    \end{equation}
    We consider a system evolving under the Hamiltonian \(\operator{H}\).
    We will be interested in computing the transition amplitude between two position eigenstates at two different times.
    That is, we want to compute
    \begin{equation}
        \braket{q', t'}{q, t}
    \end{equation}
    where \(t' > t\).
    This amplitude can be computed in terms of the states \(\ket{q}\) and \(\ket{q'}\) and the time evolution operator:
    \begin{equation}
        \braket{q', t'}{q, t} = \bra{q'} \e^{-i\operator{H}(t' - t)}\ket{q}.
    \end{equation}
    This is where path integrals come in.
    
    \section{Constructing the Path Integral}
    \epigraph{We assume this limit exists, and I have no intrest in proving it does.}{Luigi Del Debbio}
    Let \(T = t' - t\), so we're computing
    \begin{equation}
        \braket{q', t'}{q, t} = \bra{q'} \e^{-i\operator{H}T}\ket{q}.
    \end{equation}
    We can split up the time interval between \(t\) and \(t'\) into \(n - 1\) steps of length \(\varepsilon = T/n\).
    Define \(t_k = t + k\varepsilon\), so \(t_0 = t\) and \(t_n = t + n\varepsilon = t + T = t'\).
    We can then take our single factor of \(\exp[-i\operator{H}T]\) and split it into \(n\) factors of \(\exp[-i\operator{H}\varepsilon]\):
    \begin{equation}
        \braket{q', t'}{q, t} = \bra{q'} \underbrace{\e^{-i\operator{H}\varepsilon} \dotsm \e^{-i\operator{H}\varepsilon}}_{n \text{ factors}} \ket{q}.
    \end{equation}
    
    Now take the completeness relation,
    \begin{equation}
        \int \dl{q} \, \ket{q} \bra{q} = \ident.
    \end{equation}
    We can insert this \(n - 1\) times between each of the factors of \(\exp[-i\operator{H}t]\) giving
    \begin{equation}
        \int \left( \prod_{k = 1}^{n - 1} \dl{q_k} \right) \bra{q} \e^{-i\operator{H}\varepsilon} \ket{q_{n-1}} \bra{q_{n-1}} \e^{-i\operator{H}\varepsilon} \ket{q_{n-2}}\bra{q_{n-2}} \dotsm \ket{q_1} \bra{q_1} \e^{-i\operator{H}\varepsilon} \ket{q}.
    \end{equation}
    Now suppose that \(\varepsilon\) is small, or equivalently \(n\) is large.
    We can expand the exponential:
    \begin{equation}
        \e^{-i\operator{H}\varepsilon} = 1 - i\varepsilon\operator{H} + \order(\varepsilon^2).
    \end{equation}
    
    Suppose we have the Hamiltonian
    \begin{equation}
        \operator{H} = \frac{\operator{P}^2}{2} + V(\operator{Q})
    \end{equation}
    where we choose units such that the mass is unity.
    First consider the terms with the potential, here we simply replace the argument of \(V\) with the appropriate position, using \(V(\operator{Q}) \ket{q} = \sum_{\ell = 1}^{\infty} V^{(\ell)}(0) \operator{Q}^\ell \ket{q} = \sum_{\ell = 1}^{\infty} V^{(\ell)}(0)q^\ell = V(q)\ket{q}\), and so
    \begin{align}
        \bra{q_k} V(\operator{Q}) \ket{q_{k - 1}} &= V(q_{k-1}) \braket{q_k}{q_{k-1}}\\
        &= V(q_{k - 1})\delta(q_k - q_{k-1})\\
        &= V\left( \frac{q_k + q_{k-1}}{2} \right)\delta(q_k - q_{k-1})
    \end{align}
    where in the last step we note that for nonzero solutions we have \(q_k = q_{k - 1}\), and in this case \((q_k + q_{k-1})/2 = q_k\).
    This choice is required to make things converge later.
    We can then insert the integral representation of the Dirac delta,
    \begin{equation}
        \delta(q - q') = \int \frac{\dl{p}}{2\pi} \e^{ip(q - q')},
    \end{equation}
    giving
    \begin{equation}
        \bra{q_k} V(\operator{Q}) \ket{q_{k-1}} = \int \frac{\dl{p_k}}{2\pi} V\left( \frac{q_k + q_{k-1}}{2} \right) \e^{ip_k(q_k - q_{k-1})}.
    \end{equation}
    
    For the momentum term we can insert a complete set of states,
    \begin{equation}
        \int \dl{p} \ket{p}\bra{p} = \ident,
    \end{equation}
    giving
    \begin{align}
        \frac{1}{2}\bra{q_k} \operator{P}^2 \ket{q_{k-1}} &= \int \dl{p_k} \, \frac{1}{2} \bra{q_k}\operator{P}^2 \ket{p_k}\braket{p_k}{q_{k-1}}\\
        &= \int \dl{p_k} \, \frac{p_k^2}{2} \braket{q_k}{p_k} \braket{p_k}{q_{k-1}}\\
        &= \int \frac{\dl{p_k}}{2\pi} \, \frac{p_k^2}{2} \e^{ip_k(q_k - q_{k-1})}
    \end{align}
    where in the last step we've recognised the amplitudes as the wave function of the state momentum eigenstate \(\ket{p_k}\) in the position basis,
    \begin{equation}
        \braket{q}{p} = \frac{1}{\sqrt{2\pi}} \e^{ipq}.
    \end{equation}
    
    We can take this and go back to writing \(1 - i\varepsilon\operator{H}\) as \(\exp[-i\operator{H}\varepsilon]\), and the results will agree to first order in \(\varepsilon\), that is
    \begin{multline}
        \bra{q_k} \e^{-i\operator{H}\varepsilon} \ket{q_{k-1}} =\\
        \int \frac{\dl{p_k}}{2\pi} \exp\left\{ -i\varepsilon \left[ \frac{p_k^2}{2} + V\left( \frac{q_k + q_{k-1}}{2} \right) \right] \right\} \exp[ip_k(q_k - q_{k-1})] + \order(\varepsilon^2).
    \end{multline}
    We can recognise the term in the first exponential as the Hamiltonian evaluated at \(\tilde{q}_k = (q_k + q_{k-1})/2\) and \(p_k\), allowing us to rewrite this as
    \begin{equation}
        \bra{q_k} \e^{-i\operator{H}\varepsilon}\ket{q_{k-1}} = \int \frac{\dl{p_k}}{2\pi} \exp[-i\varepsilon H(\tilde{q}_k, p_k)]\exp[ip_k(q_k - q_{k-1})] + \order(\varepsilon^2).
    \end{equation}
    Now combine the two exponentials and factor out \(i\varepsilon\):
    \begin{equation}
        \bra{q_k} \e^{-i\operator{H}\varepsilon}\ket{q_{k-1}} = \int \frac{\dl{p_k}}{2\pi} \exp\left\{ i\varepsilon\left[ p_k\frac{q_k - q_{k-1}}{\varepsilon} - H(\tilde{q}_k, p_k) \right] \right\} + \order(\varepsilon^2).
    \end{equation}
    
    We can now put all of these factors together to get the desired result, although we have to take the limit of \(n \to \infty\), so \(\varepsilon \to 0\), in order to make it exact.
    We end up with a product of the above terms, with the product of exponentials becoming a sum in the exponential.
    We should take the limit in such a way that \(T = \varepsilon n\) is constant.
    The result is
    \begin{gather}
        \braket{q', t'}{q, t} =\\
        \lim_{n\to \infty} \int \left( \prod_{k=1}^{n-1} \dl{q_k} \right) \left( \prod_{j=1}^{n} \frac{\dl{p_j}}{2\pi} \right) \exp\left\{ i\varepsilon \sum_{m=1}^{n} \left[ p_m\frac{q_m - q_{m-1}}{\varepsilon} - iH(\tilde{q}_m, p_m) \right] \right\}.\notag
    \end{gather}
    We assume that this limit exists.
    In the limit the sum becomes an integral and \((q_m - q_{m-1})/\varepsilon\) becomes the velocity, the distinction between \(q_m\) and \(\tilde{q}_m\) is not important in the limit.
    We introduce the following shorthand for this expression:
    \begin{equation}
        \braket{q', t'}{q, t} = \int \DL{q} \DD{p} \exp\left\{ i\int_{t}^{t'} \!\! \dl{\tau} \, \left[ p(\tau)\dot{q}(\tau) - H(q(\tau), p(\tau)) \right] \right\}.
    \end{equation}
    This is the \defineindex{path integral} form of the amplitude.
    Note that we absorb the \(1/(2\pi)\) factor for each momentum integral into the measure.
    
    \subsection{Quadratic Momentum}
    For Hamiltonians, like the one we had above, which are quadratic in the momentum we can partially evaluate this integral.
    Consider the integral
    \begin{equation*}
        \int \frac{\dl{p_k}}{2\pi} \exp\left[ ip_k(q_k - q_{k-1}) - i\varepsilon\frac{p_k^2}{2} \right] = (2\pi i\varepsilon)^{-1/2} \exp\left[ i\varepsilon\left( \frac{q_k - q_{k-1}}{\varepsilon} \right)^2 \right],
    \end{equation*}
    where we've used \cref{eqn:gaussian 1d with linear term} with \(a = i\varepsilon\) and \(b = q_k - q_{k-1}\).
    Using this we can rewrite the amplitude as
    \begin{gather}
        \braket{q',t'}{q,t} =\\
        \qquad \lim_{n\to \infty} \int \left( \prod_{k=1}^{n-1} \dl{q_k} \right) (2\pi i\varepsilon)^{-n/2} \exp\left\{ i\varepsilon \sum_{m=1}^{n} \left[ \frac{1}{2}\left( \frac{q_m - q_{m-1}}{\varepsilon} \right)^2 - V(\tilde{q}_m) \right] \right\} \notag
    \end{gather}
    In the limit \((q_m - q_{m-1})/\varepsilon\) becomes \(\dot{q}\), and the sum becomes an integral.
    We then have
    \begin{equation}
        \frac{1}{2}\dot{q}^2 - V(q) = \lagrangian(q, \dot{q}),
    \end{equation}
    so we can identify the exponent as the integral of the Lagrangian, which is just the action, \(S[q]\).
    We can also absorb the \((2\pi i\varepsilon)^{-n/2}\) into the measure and, again assuming the limit exists, we get the shorthand
    \begin{equation}
        \braket{q', t'}{q, t} = \int_{q, q'} \!\! \DL{q} \, \exp\left[ i\int_t^{t'} \dl{\tau} \, \lagrangian(q, \dot{q}) \right] = \int_{q,q'} \!\! \DL{q} \, \e^{-iS[q]}.
    \end{equation}
    Here we write \(q,q'\) as the limit of the integrals to remind us that \(q\) and \(q'\) are the fixed endpoints.
    
    The interpretation of this integral is that we split time into very narrow slices.
    We then integrate over all positions at each time.
    An alternative way of viewing this, rather than considering one time slice at a time, is that we consider all possible paths between \(q\) and \(q'\), integrating over them with the weighting \(\exp\{iS[q]\}\).
    This gives the quantum amplitude.
    
    Consider this equation in units where \(\hbar\ne 1\).
    Then since the action has units of \(\hbar\) and the argument to the exponential must be dimensionless we must have that
    \begin{equation}
        \braket{q't'}{q,t} = \int_{q,q'} \!\! \DL{q} \, \e^{-iS[q]/\hbar}.
    \end{equation}
    Now consider the classical limit, where \(\hbar\) is much smaller than any other quantity with the same units appearing in our expressions.
    We can treat this as taking the limit \(\hbar \to 0\).
    In this limit \(1/\hbar\) is very large, and so small changes in the rest of the exponent cause large oscillations in the value.
    Most of these oscillations cancel out, and only when \(S[q]\) is stationary do we get a meaningful contribution.
    This is the root of the principal of least (or stationary) action.
    This can be made rigorous with the method of stationary phase, see the course \course{Methods of Mathematical Physics} for more details.
    Now set \(\hbar = 1\) again.
    
    \section{Correlators}
    \subsection{One Point Correlator}
    Suppose we wish to calculate the matrix element
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t}) \ket{q,t}.
    \end{equation}
    We assume that \(t < \overbar{t} = t + \overbar{k}\varepsilon = t_{\overbar{k}} < t'\), this is a reasonable assumption since we'll be taking time to be a continuous parameter in the limit.
    We can write the operator \(\operator{Q}(\overbar{t})\) in the Schrödinger picture as
    \begin{equation}
        \operator{Q}(t) = \e^{i\operator{H}\overbar{t}} \operator{Q} \e^{-i\operator{H}\overbar{t}}.
    \end{equation}
    We can then write the correlator in terms of time independent states:
    \begin{equation}
        \bra{q',t'}\operator{Q}(\overbar{t})\ket{q,t} = \bra{q'} \e^{-i\operator{H}(t' - \overbar{t})} \operator{Q} \e^{i\operator{H}(\overbar{t} - t)}\ket{q}.
    \end{equation}
    We follow the same procedure as before, splitting time into narrow slices.
    We then split the exponential into many identical factors of \(\exp[-i\operator{H}\varepsilon]\):
    \begin{equation}
        \bra{q',t'}\operator{Q}(\overbar{t})\ket{q,t} = \bra{q'} \underbrace{\e^{-i\operator{H}\varepsilon} \dotsm \e^{-i\operator{H}\varepsilon}}_{\overbar{k} \text{ factors}} \operator{Q} \underbrace{\e^{-i\operator{H}\varepsilon} \dotsm \e^{-i\operator{H}\varepsilon}}_{n - \overbar{k} \text{ factors}} \ket{q}.
    \end{equation}
    
    We can then proceed to insert the identity a bunch in terms of the completeness relation.
    The result will be mostly the same, but we'll have one factor different.
    Splitting into kinetic and potential terms again we'll have
    \begin{equation}
        \bra{q_{\overbar{k}}} \operator{Q}(\overbar{t}) V(\operator{Q}) \ket{q_{\overbar{k}-1}} = q_{\overbar{k}-1} V(\tilde{q}_{\overbar{k}}) \delta(q_{\overbar{k}} - q_{\overbar{k} - 1}),
    \end{equation}
    which differs from the other terms by a factor of \(q_{\overbar{k}}\).
    The result is that we get an extra factor of \(q_{\overbar{k}}\) in our integral:
    \begin{gather}
        \bra{q',t'} \operator{Q}(\overbar{t})\ket{q,t} =\\
        \lim_{n\to\infty} \left( \prod_{k=1}^{n-1}\dl{q_k} \right) \left( \prod_{j=1}^{n} \frac{\dl{p_j}}{2\pi} \right) q_{\overbar{k}-1} \exp\left\{ i\varepsilon \sum_{m=1}^{n} \left[ p_m \frac{q_m - q_{m-1}}{\varepsilon} - H(\tilde{q}_m, p_m) \right] \right\}.\notag
    \end{gather}
    Which, assuming the limit exists, we write as
    \begin{equation}
        \bra{q',t'}\operator{Q}(\overbar{t})\ket{q,t} = \int \DL{q} \DD{p} \, q(\overbar{t}) \exp\left\{ i \int_t^{t'} \!\! \dl{\tau} \, [p(\tau) \dot{qq}(\tau) - H(q, p)] \right\}.
    \end{equation}
    If the Hamiltonian has quadratic dependence on momentum we can again partially complete the integral to get
    \begin{gather}
        \bra{q',t'} \operator{Q}(\overbar{t})\ket{q,t} =\\
        \lim_{n\to\infty} \int\left( \prod_{k=1}^{n-1} \dl{q_k} \right) (2\pi i\varepsilon)^{-n/2} q_{\overbar{k}} \exp\left\{ i\delta\sum_{m=1}^{n} \left[ \frac{1}{2}\left( \frac{q_m - q_{m-1}}{\varepsilon} \right)^2 - V(\tilde{q}_m) \right] \right\}. \notag
    \end{gather}
    If this limit exists then we write this as
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t})\ket{q,t} = \int_{q,q'} \!\! \DL{q} \, q(\overbar{t}) \exp\left[ i \int_t^{t'} \!\! \dl{\tau} \, \lagrangian(q, \dot{q}) \right].
    \end{equation}
    
    \subsection{Two Point Correlator}
    Suppose we wish to calculate
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t}_1)\operator{Q}(\overbar{t}_2)\ket{q,t}.
    \end{equation}
    We assume that \(t < \overbar{t}_2 < \overbar{t}_1 < t'\).
    Exactly the same logic as before applies, except we now have two of these altered factors, so get two factors of \(q\) in our final path integral.
    The result is
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t}_1)\operator{Q}(\overbar{t}_2)\ket{q,t} = \int_{q,q'} \!\! \DL{q} \, q(\overbar{t}_1)q(\overbar{t}_2) \exp\left[ i\int_t^{t'} \!\! \dl{\tau} \, \lagrangian(q, \dot{q}) \right].
    \end{equation}

    Here we see something interesting.
    On the right hand side it doesn't matter if we write \(q(\overbar{t}_1)q(\overbar{t_2})\) or \(q(\overbar{t}_2)q(\overbar{t_1})\), since these are just numbers.
    However, the order of \(\operator{Q}(\overbar{t}_1)\operator{Q}(\overbar{t}_2)\) is important.
    This ambiguity is due to the assumption that \(\overbar{t}_2 < \overbar{t}_1\).
    If this isn't the case then the path integral here instead represents
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t}_2)\operator{Q}(\overbar{t}_1)\ket{q,t}.
    \end{equation}
    
    We can compactly summarise this by writing
    \begin{equation}
        \bra{q',t'} \timeOrdering[\operator{Q}(\overbar{t}_1)\operator{Q}(\overbar{t}_2)]\ket{q,t} = \int_{q,q'} \!\! \DL{q} \, q(\overbar{t}_1)q(\overbar{t}_2) \exp\left[ i\int_t^{t'} \!\! \dl{\tau} \, \lagrangian(q, \dot{q}) \right]
    \end{equation}
    where
    \begin{equation}
        \timeOrdering[A(t)B(t')] = \heaviside(t - t')A(t)B(t) + \heaviside(t' - t)B(t)A(t)
    \end{equation}
    is the time ordered product of the operators \(A(t)\) and \(B(t')\).
    
    Taking this process to its logical conclusion we have
    \begin{multline}\label{eqn:time ordered positions}
        \bra{q',t'} \timeOrdering[\operator{Q}(\overbar{t}_1) \dotsm \operator{Q}(\overbar{t}_\ell)]\ket{q,t} =\\
        \int_{q,q'} \!\!\ \DL{q} \, q(\overbar{t}_1) \dotsm q(\overbar{q}_\ell) \exp\left[ i \int_t^{t'} \!\! \dl{\tau} \, \lagrangian(q, \dot{q}) \right].
    \end{multline}
    
    \chapter{Generating Functions}
    \section{Functional Derivatives}
    Consider a \defineindex{functional}, \(F\), which associates a number, \(F[u]\), to a function, \(u \colon \reals^D \to \reals\).
    The functional derivative, \(\diffd{F}/{u(x)}\), is defined such that
    if we vary the function \(u\),
    \begin{equation}
        u(x) \mapsto u(x) + \delta u(x),
    \end{equation}
    then the variation in \(F\) is
    \begin{equation}
        \delta F = F[u + \delta u] - F[u] = \int \dl{^Dx} \, \diffd{F}{u(x)} \delta u(x).
    \end{equation}
    Compare this to to the case of a function \(f \colon \reals^D \to \reals\) when we vary each argument according to \(x_k \mapsto x_k + \delta x_k\) and so \(f\) varies as
    \begin{equation}
        \delta f = f(x + \delta x) - f(x) = \sum_k \diffp{f}{x_k}\delta x_k.
    \end{equation}
    
    In this function case we have the identity
    \begin{equation}
        \diffp*{x_i}{x_j} = \delta_{ij},
    \end{equation}
    and in the functional case we have the analogous identity
    \begin{equation}
        \diffd*{f(y)}{f(x)} = \delta(x - y).
    \end{equation}
    This, and the fact that the product rule and chain rule also apply to functional derivatives, is pretty much all we'll need.
    
    \section{Sources}\label{sec:sources}
    Consider two time dependent functions, \(f\) and \(h\).
    We can add terms proportional to these into our path integral.
    We call such terms source terms.
    In particular, we'll be interested in the path integral with sources given below:
    \begin{gather}
        \braket{q', t'}{q, t}_{f,h}\\
        \coloneqq \int \!\! \DL{p} \DD{q} \, \exp\left\{ i\int_t^{t'} \!\! \dl{\tau} \, [p(\tau)\dot{q}(\tau) - H(q(\tau), p(\tau))] + f(\tau)q(\tau) + h(\tau)p(\tau) \right\}. \notag
    \end{gather}
    
    Consider the following functional derivative:
    \begin{align}
        A(\overbar{\tau}) &= \diffd{}{f(\overbar{\tau})} \exp\left[ i\int_t^{t'} \dl{\tau} \, f(\tau)q(\tau) \right]\\
        &= \left( \diffd{}{f(\overbar{\tau})} i\int_t^{t'} \dl{\tau} \, f(\tau) q(\tau) \right) \exp\left[ i\int_t^{t'} \dl{\tau} \, f(\tau) q(\tau) \right],
    \end{align}
    where the last step is an application of the chain rule.
    We now assume we can move the derivative through the integral to get
    \begin{align}
        A(\overbar{\tau}) &= \left( i\int_t^{t'} \dl{\tau} \, \diffd{}{f(\overbar{\tau})} f(\tau)q(\tau) \right) \exp\left[ i\int_t^{t'} \dl{\tau} \, f(\tau) q(\tau) \right]\\
        &= \left( i\int_t^{t'} \dl{\tau} \, \delta(\tau - \overbar{\tau}) q(\tau) \right) \exp\left[ i\int_t^{t'} \dl{\tau} \, f(\tau) q(\tau) \right]\\
        &= iq(\overbar{\tau}) \exp\left[ i\int_t^{t'} \dl{\tau} \, f(\tau) q(\tau) \right].
    \end{align}
    Hence, we can generate factors of \(q(\overbar{\tau})\) through
    \begin{equation}
        q(\overbar{\tau}) \exp\left[ i\int_t^{t'} \dl{\tau} \, f(\tau) q(\tau) \right] = \frac{1}{i}\diffd{}{f(\overbar{\tau})} \exp\left[ i\int_t^{t'} \dl{\tau} \, f(\tau) q(\tau) \right].
    \end{equation}
    
    We can also apply this to \(\braket{q',t'}{q,t}_{f,h}\) to get factors of \(q(\overbar{\tau})\), since all of the terms without \(f(\tau)\) in them don't interact with the functional derivative.
    That is, we have
    \begin{gather}
        \frac{1}{i} \diffd{}{f(\overbar{\tau})}
        \braket{q', t'}{q, t}_{f,h}
        \coloneqq \int \!\! \DL{p} \DD{q} \, \textcolor{highlight}{q(\overbar{\tau})}\\
        \qquad\qquad\exp\left\{ i\int_t^{t'} \!\! \dl{\tau} \, [p(\tau)\dot{q}(\tau) - H(q(\tau), p(\tau))] + f(\tau)q(\tau) + h(\tau)p(\tau) \right\}. \notag
    \end{gather}
    We can apply the functional derivative again to get another factor,
    \begin{gather}
        \left( \frac{1}{i} \diffd{}{f(\overbar{\tau}_1)} \right) \left( \frac{1}{i} \diffd{}{f(\overbar{\tau}_2)} \right)
        \braket{q', t'}{q, t}_{f,h} \coloneqq \int \!\! \DL{p} \DD{q} \, q(\overbar{\tau}_1) \textcolor{highlight}{q(\overbar{\tau}_2)}\\
        \qquad\qquad\exp\left\{ i\int_t^{t'} \!\! \dl{\tau} \, [p(\tau)\dot{q}(\tau) - H(q(\tau), p(\tau))] + f(\tau)q(\tau) + h(\tau)p(\tau) \right\}. \notag
    \end{gather}
    Repeating this we have
    \begin{gather}
        \left( \frac{1}{i} \diffd{}{f(\overbar{\tau}_1)} \right) \dotsm \left( \frac{1}{i} \diffd{}{f(\overbar{\tau}_n)} \right)
        \braket{q', t'}{q, t}_{f,h} \coloneqq \int \!\! \DL{p} \DD{q} \, q(\overbar{\tau}_1) \dotsm q(\overbar{\tau}_n)\\
        \qquad\qquad\exp\left\{ i\int_t^{t'} \!\! \dl{\tau} \, [p(\tau)\dot{q}(\tau) - H(q(\tau), p(\tau))] + f(\tau)q(\tau) + h(\tau)p(\tau) \right\}. \notag
    \end{gather}
    
    Comparing this to \cref{eqn:time ordered positions} we see that
    \begin{equation}
        \bra{q', t'} \timeOrdering[\operator{Q}(t_1) \dotsm \operator{Q}(t_n)] \ket{q, t} = \left( \frac{1}{i} \diffd{}{f(t_1)} \right) \dotsm \left( \frac{1}{i} \diffd{}{f(t_2)} \right) \braket{q', t'}{q, t} \bigg|_{f = h = 0}.
    \end{equation}
    
    \section{Projection onto the Ground State}
    Often it is useful to compute the amplitude for a system to evolve from the vacuum state at time \(t\) to the vacuum state at time \(t'\) under some external sources, \(f\) and \(h\).
    That is, we want to compute \(\braket{0, t'}{0, t}\), where \(\ket{0, t}\) is the vacuum state at time \(t\).
    Note that this is an eigenstate of the Hamiltonian, \emph{not} the position.
    We can insert a complete set of states twice to get
    \begin{align}
        \braket{0, t'}{0, t}_{f,h} &= \int \dl{q} \dd{q'} \, \braket{0, t'}{q', t'}_{f,h} \braket{q', t'}{q, t}_{f,h} \braket{q, t}{0, t}_{f,h}\\
        &= \int \dl{q} \dd{q'} \, \varphi_0^*(q') \braket{q', t'}{q, t}_{f,h} \varphi_0(q).
    \end{align}
    Here we use the ground state wave function in the position basis:
    \begin{equation}
        \varphi_0(q) = \braket{q, t}{0, t}, \implies \varphi_0^*(q) = \braket{0, t}{q, t}.
    \end{equation}
    More generally if \(\operator{H}\) is the Hamiltonian for our system then we have the energy eigenstate \(\ket{n}\), which is such that
    \begin{equation}
        \operator{H}\ket{n} = E_n,
    \end{equation}
    and the energy eigenfunction
    \begin{equation}
        \varphi_n(q) = \braket{q}{n}.
    \end{equation}
    We assume that the energy levels are defined such that the ground state energy vanishes, that is \(E_0 = 0\).
    
    Now suppose that \(f\) and \(h\) have support in \([t, t']\).
    That is, outside the interval \([t, t']\) \(f\) and \(h\) vanish.
    Suppose also that we have times \(T\) and \(T'\) such that \(T < t < t' < T'\).
    Then between \(T\) and \(t\), and \(t'\) and \(T'\), the sources vanish and we can easily compute \(\braket{q,t}{Q,T}\) by inserting a complete set of energy eigenstates:
    \begin{align}
        \braket{q, t}{Q, T}_{f,h} &= \bra{q} \e^{-i\operator{H}(t - T)} \bra{Q}\\
        &= \sum_n \bra{q} \e^{-i\operator{H}(t - T)} \ket{n} \braket{n}{Q}\\
        &= \sum_n \e^{-iE_n(t - T)} \braket{q}{n} \braket{n}{Q}\\
        &= \sum_n \e^{-iE_n(t - T)} \varphi_n(q) \varphi_n^*(Q).
    \end{align}
    
    We can analytically extend this result to time with a small imaginary part, \(T_I = (1 - i\varepsilon)T\) where \(\varepsilon > 0\).
    We then have
    \begin{equation}
        \e^{-iE_n(t - T_I)} = \e^{-iE_n(t - T + i\varepsilon T)} = \e^{-iE_n(t - T)} \e^{\varepsilon E_nT}.
    \end{equation}
    If we consider the limit \(T \to -\infty\) then all states apart from the ground state, where \(E_0 = 0\), will be exponentially suppressed.
    We then have
    \begin{equation}
        \lim_{T \to -\infty} \braket{q, t}{Q, T_I}_{f,h} = \braket{q}{0}\braket{0}{Q} = \varphi_0(q)\varphi_0^*(Q).
    \end{equation}
    
    We can do the same with \(\braket{Q',T'}{q',t'}\), we get
    \begin{align}
        \braket{Q',T'}{q',t'}_{f,h} &= \bra{Q} \e^{-i\operator{H}(T' - t')} \ket{t}\\
        &= \sum_n \bra{Q'} \e^{-i\operator{H}(T' - t')} \ket{n} \braket{n}{q'}\\
        &= \sum_n \e^{-iE_n(T' - t')} \braket{Q'}{n}\braket{n}{q'}\\
        &= \sum_n \e^{-iE_n(T' - t')} \varphi_n(Q')\varphi_n^*(q').
    \end{align}
    Analytically extending this result to \(T_I' = (1 - i\varepsilon)T'\), and so
    \begin{equation}
        \e^{-iE_n(T_I' - t')} = \e^{-iE_n(T' - i\varepsilon T' - t')} \e^{-iE_n(T' - t')}\e^{-\varepsilon E_nT'},
    \end{equation}
    so this time all states apart from the ground state are exponentially suppressed in the limit \(T' \to \infty\).
    We then have
    \begin{equation}
        \lim_{T' \to \infty} \braket{Q', T'_I}{q', t'}_{f,h}  = \braket{Q'}{0} \braket{0}{q'} = \varphi_0(Q') \varphi_0^*(q').
    \end{equation}
    
    Putting these two results together we find that
    \begin{equation}
        \lim_{\substack{\mathllap{T} \to \mathrlap{-\infty}\\ \mathllap{T'} \to \mathrlap{+\infty}}} \frac{\braket{Q', (1 - i\varepsilon)T'}{Q, (1 - i\varepsilon)T}_{f,h}}{\varphi_0^*(Q) \varphi_0(Q')} = \braket{0, t'}{0, t}_{f,h}.
    \end{equation}
    Notice that we can get the same result by the transformation \(\operator{H} \mapsto (1 - i\varepsilon)\operator{H}\).
    Note that the only dependence on the boundary is in the derivative on the left hand side, which is simply a normalisation factor which doesn't effect the values of derivatives with respect to the sources.
    
    \section{Weyl Ordering}
    For more complicated Hamiltonians we may have to consider terms with products of \(\operator{P}\) and \(\operator{Q}\).
    We then need a prescription for ordering terms in the Hamiltonian so that the amplitude matches the path integral.
    We've used the mid-point prescription, evaluating the potential at \(\tilde{q}_k = (q_{k} + q_{k-1})/2\), to the same effect in this simple case.
    In the more complex case we use the \defineindex{Weyl ordering}.
    Here the order of a product of powers of some operators, \(\operator{A}\) and \(\operator{B}\), is defined by considering the operator \((\alpha\operator{A} + \beta\operator{B})^n\) and expanding in powers of \(\alpha\) and \(\beta\):
    \begin{equation}
        (\alpha\operator{A} + \beta\operator{B})^n = \sum_k \frac{n!}{k!\ell!} \alpha^k \beta^\ell [\operator{A}^k\operator{B}^\ell].
    \end{equation}
    Here \([\operator{A}^k\operator{B}^\ell]\) is the Weyl ordered product.
    Notice that \(\ell = n - k\) and
    \begin{equation}
        \frac{n!}{k!\ell!} = \binom{n}{k} = \binom{n}{\ell}
    \end{equation}
    
    \begin{exm}{}{}
        Suppose we want to compute \([\operator{A}\operator{B}]\).
        This is quadratic in the operators so we consider \((\alpha\operator{A} + \beta\operator{B})^2\):
        \begin{equation}
            (\alpha\operator{A} + \beta\operator{B})^2 = \alpha^2\operator{A} + \alpha\beta(\operator{A}\operator{B} + \operator{B}\operator{A}) + \beta\operator{B}^2.
        \end{equation}
        We then take the terms proportional to \(\alpha\beta\) and, after dividing by \(n!/(k!\ell!) = 2!/(1!1!) = 2\) we get the Weyl ordering,
        \begin{equation}
            [\operator{A}\operator{B}] = \frac{1}{2}(\operator{A}\operator{B} + \operator{B}\operator{A}).
        \end{equation}
        
        Now suppose we want to compute all third order Weyl orderings.
        We consider
        \begin{multline}
            (\alpha \operator{A} + \beta \operator{B})^3
            \alpha^3\operator{A}^3 + \alpha^2\beta(\operator{A}^2\operator{B} + \operator{A}\operator{B}\operator{A} + \operator{B}\operator{A}^2)\\
            + \alpha\beta^2(\operator{A}\operator{B}^2 + \operator{B}\operator{A}\operator{B} + \operator{B}^2\operator{A}) + \beta^3\operator{B}^3.
        \end{multline}
        We then have
        \begin{align}
            [\operator{A}^3] &= \frac{3!1!}{3!} \operator{A}^3 = \operator{A}^3,\\
            [\operator{A}^2\operator{B}] &= \frac{2!1!}{3!}(\operator{A}^2\operator{B} + \operator{A}\operator{B}\operator{A} + \operator{B}\operator{A}^2) = \frac{1}{3}(\operator{A}^2\operator{B} + \operator{A}\operator{B}\operator{A} + \operator{B}\operator{A}^2),\\
            [\operator{A}\operator{B}^2] &= \frac{1!2!}{3!}(\operator{A}\operator{B}^2 + \operator{B}\operator{A}\operator{B} + \operator{B}^2\operator{A}) = \frac{1}{3}(\operator{A}\operator{B}^2 + \operator{B}\operator{A}\operator{B} + \operator{B}^2\operator{A}),\\
            [\operator{B}^3] &= \frac{1!3!}{3!}\operator{B}^3 = \operator{B}^3.
        \end{align}
        
        Finally, suppose we want to compute \([\operator{A}^2\operator{B}^2]\).
        Expanding \((\alpha \operator{A} + \beta \operator{B})^5\) we find the relevant term is
        \begin{multline}
            \alpha^3\beta^2(\operator{A}^3\operator{B}^2 + \operator{A}^2\operator{B}\operator{A}\operator{B} + \operator{A}^2\operator{B}^2\operator{A} + \operator{A}\operator{B}\operator{A}^2\operator{B} + \operator{A}\operator{B}\operator{A}\operator{B}\operator{A}\\
            + \operator{A}\operator{B}^2\operator{A}^2+ \operator{B}\operator{A}^3\operator{B} + \operator{B}\operator{A}^2\operator{B}\operator{A} + \operator{B}\operator{A}\operator{B}\operator{A}^2 + \operator{B}^2\operator{A}^3)
        \end{multline}
        and so the Weyl ordered product is this result divided by \(5!/(3!2!) = 10\)
        \begin{multline}
            [\operator{A}^3\operator{B}^2] = \frac{1}{10} (\operator{A}^3\operator{B}^2 + \operator{A}^2\operator{B}\operator{A}\operator{B} + \operator{A}^2\operator{B}^2\operator{A} + \operator{A}\operator{B}\operator{A}^2\operator{B}\\
            + \operator{A}\operator{B}\operator{A}\operator{B}\operator{A} +\operator{A}\operator{B}^2\operator{A}^2 + \operator{B}\operator{A}^3\operator{B} + \operator{B}\operator{A}^2\operator{B}\operator{A} + \operator{B}\operator{A}\operator{B}\operator{A}^2 + \operator{B}^2\operator{A}^3)
        \end{multline}
    \end{exm}
    
    \part{Path Integrals in Scalar Field Theory}
    \chapter{Noninteracting Scalar Field Theory}
    \section{Scalar Field Theory Recap}
    The Lagrangian for a free real scalar field, \(\varphi\), is
    \begin{equation}
        \lagrangianDensity_0(\varphi, \partial_\mu\varphi) = \frac{1}{2}(\partial_\mu\varphi)(\partial^\mu\varphi) - \frac{1}{2}m^2\varphi^2.
    \end{equation}
    Applying the Euler--Lagrange equations,
    \begin{equation}
        \partial_\mu\left( \diffp{\lagrangianDensity_0}{(\partial_\mu\varphi)} \right) - \diffp{\lagrangianDensity_0}{\varphi} = 0
    \end{equation}
    gives the Klein--Gordon equation,
    \begin{equation}
        (\dalembertian + m^2)\varphi = 0.
    \end{equation}
    
    The action associated with this Lagrangian is, in \(D\) dimensions,
    \begin{equation}
        S_0[\varphi] = \int \dl{^Dx} \left[ \frac{1}{2}(\partial_\mu\varphi)(\partial^\mu\varphi) - \frac{1}{2}m^2\varphi^2 \right].
    \end{equation}
    There is already some physics going on here.
    The Lagrangian and action tell us what the degrees of freedom of the system are, in this case we just have one, the field, \(\varphi\).
    We can also examine the symmetries of the action.
    Here we have one symmetry, Lorentz invariance.
    Recall that if \(x\) transforms as
    \begin{equation}
        x^\mu \mapsto x'^\mu = \tensor{\Lambda}{^\mu_\nu}x^\nu \iff x \mapsto x' = \Lambda x
    \end{equation}
    for some Lorentz transformation \(\Lambda\) then the field transforms as
    \begin{equation}
        \varphi(x) \mapsto \varphi'(x') = \varphi(x),
    \end{equation}
    since by definition the field is a scalar.
    Writing \(x'\) as \(\Lambda x'\) in the middle of this we have \(\varphi(x) \mapsto \varphi'(\Lambda x)\).
    Redefining our variable so that \(x\) is the position in the new frame we have \(\varphi(x) \mapsto \varphi'(x) = \varphi(\Lambda^{-1}x)\).
    If this is confusing think of a one-dimensional function, \(f \colon \reals \to \reals\).
    If we want to translate this function to the right, in the positive \(x\) direction, by, say \(a\) units, then we transform it as \(f(x) \mapsto f'(x) = f(x - a)\), so we subtract.
    The logic is the same here.
    Our transformation moves the point of evaluation, so we have to move it back.
    
    We define the conjugate momentum as
    \begin{equation}
        \Pi(x) = \diffp{\lagrangianDensity_0}{(\partial_0\varphi)} = \partial_0\varphi(x) = \dot{\varphi}(x).
    \end{equation}
    The Hamiltonian is then
    \begin{align}
        \hamiltonianDensity(\varphi, \Pi) &= \Pi(x)\dot{\varphi}(x) - \lagrangianDensity_0(\varphi, \partial_\mu \varphi)\\
        &= \frac{1}{2}\Pi(x)^2 + \frac{1}{2}\sum_{k = 1}^{3} (\partial_k\varphi(x))^2 + \frac{1}{2} m^2\varphi(x)^2.
    \end{align}
    
    \section{The Path Integral}
    We could derive the path integral in scalar field theory in a very similar way to how we derived it for quantum mechanics, but we won't.
    Instead we'll assume the form is the same after making the usual substitutions which take us from quantum mechanics to quantum field theory.
    These substitutions are as follows:
    \begin{itemize}
        \item \(t \to x\), we change from a single coordinate to \(D\) coordinates;
        \item \(q(t) \to \varphi(x)\), we change from position to the field, \(\varphi\);
        \item \(\operator{Q}(t) \to \operator{\varphi}(x)\), we change from the position operator to the field operator.
    \end{itemize}
    
    Recall the quantum mechanical path integral:
    \begin{equation}
        \braket{q, t}{q', t'}_{f} = \int \DL{q} \, \exp\{i(S_0[q] + qf)\}.
    \end{equation}
    Moving to scalar field theory we replace this with
    \begin{align}
        Z_0[J] \coloneqq \braket{0}{0}_J = \int \DL{\varphi} \, \exp\{i(S_0[\varphi] + J \cdot \varphi)\}
    \end{align}
    where
    \begin{equation}
        J \cdot \varphi \coloneqq \int \dl{^Dx} \, J(x)\varphi(x).
    \end{equation}
    
    This path integral is just a Gaussian integral, the one complication being that it's an infinite dimensional Gaussian integral.
    We can make this more obvious with a bit of rewriting.
    First, consider the action:
    \begin{align}
        S_0[\varphi] \coloneqq \int \dl{^Dx} \, \lagrangianDensity_0 = \frac{1}{2} \dl{^Dx} \, [(\partial_\mu\varphi)(\partial^\mu\varphi) - m^2\varphi^2].
    \end{align}
    Take the first term and integrate by parts to get
    \begin{equation}
        \int \dl{^Dx} \, (\partial_\mu \varphi)(\partial^\mu \varphi) = \int \dl{^Dx} \, [ \partial_\mu(\varphi\partial^\mu \varphi) - \varphi\dalembertian\varphi] = -\int \dl{^Dx} \, \varphi\dalembertian\varphi
    \end{equation}
    where in the last step we assume that the field vanishes sufficiently quickly at infinity that we can ignore the boundary term.
    The action is then
    \begin{equation}
        S_0[\varphi] = \frac{1}{2} \int \dl{^Dx} \, \varphi(x) [-\dalembertian - m^2] \varphi(x).
    \end{equation}
    Now, we insert a factor of \(-i\varepsilon \varphi^2\) for some \(\varepsilon > 0\), which we will later take to zero.
    This ensures convergence.
    The modified action is then
    \begin{equation}
        S_0[\varphi] = \frac{1}{2} \int \dl{^Dx} \, \varphi(x) [-\dalembertian - m^2 - i\varepsilon] \varphi(x).
    \end{equation}
    Now define
    \begin{equation}
        K(x, x') \coloneqq \delta(x - x')[-\dalembertian - m^2].
    \end{equation}
    Then we can write the action as
    \begin{equation}
        S_0[\varphi] = \frac{1}{2} \int \dl{^Dx} \dd{^Dx'} \, \varphi(x) K(x, x') \varphi(x').
    \end{equation}
    Hence,
    \begin{equation}
        Z_0[0] = \int \DL{\varphi} \, \exp\left[ i\frac{1}{2} \int \dl{^Dx} \dd{^Dx'} \, \varphi(x) K(x, x') \varphi(x') \right].
    \end{equation}
    \begin{equation}
        Z_A = \int \dl{^nx} \, \exp\left[ -\frac{1}{2}x^\trans A x \right] = \int \dl{^nx} \, \exp\left[ -\sum_{i}\sum_{j} x_iA_{ij}x_j \right].
    \end{equation}
    We can see that the kernel, \(K(x, x')\), plays the role of \(A\) and our sums become integrals, but other than that not much changes.
    
    It's easier to compute the action if we move to momentum space, since in momentum space \(\dalembertian \varphi\) simply becomes \(-p^2\tilde{\varphi}\).
    To do this we write \(\varphi\) as the inverse transform of \(\tilde{\varphi}\):
    \begin{equation}
        \varphi(x) = \int \frac{\dl{^Dp}}{(2\pi)^D} \e^{-ip\cdot x}\tilde{\varphi}(p).
    \end{equation}
    We then have
    \begin{equation}
        S_0[\varphi] = \frac{1}{2} \int_x \int_{p,p'} \e^{-ip\cdot x} \tilde{\varphi}(p) (p'^2 - m^2 + i\varepsilon)\e^{-ip'\cdot x}\tilde{\varphi}(p').
    \end{equation}
    Here we've introduced the shorthand notation
    \begin{equation}
        \int_x \coloneqq \int \dl{^Dx}, \quad \int_{p} \coloneqq \int \frac{\dl{^Dp}}{(2\pi)^D}, \qand \int_{p,p'} = \int \frac{\dl{^Dp}}{(2\pi)^D} \int \frac{\dl{^Dp'}}{(2\pi)^D}
    \end{equation}
    and so on.
    We can combine the two exponentials into \(\exp[-i(p + p') \cdot x]\), and then performing the \(x\) integral gives us a Dirac delta:
    \begin{equation}
        S_0[\varphi] = \frac{1}{2} \int_{p,p'} \tilde{\varphi}(p)(p'^2 - m^2 + i\varepsilon) \tilde{\varphi}(p') \delta(p + p').
    \end{equation}
    Using the Dirac delta we can perform the \(p'\) integral to get
    \begin{equation}
        S_0[\varphi] = \frac{1}{2} \int_p \tilde{\varphi}(p)(p^2 - m^2 + i\varepsilon) \tilde{\varphi}(-p).
    \end{equation}
    
    We can simplify this slightly using a Fourier transform identity.
    Let \(f \colon \reals \to \reals\) be a real function whose Fourier transform exists.
    Then we have
    \begin{equation}
        \tilde{f}(p) = \int \dl{x} \, \e^{ipx} f(x).
    \end{equation}
    Hence, we have
    \begin{equation}
        \tilde{f}(-p) = \int \dl{x} \, \e^{-ipx} f(x) = \left[ \int \dl{x} \e^{ipx}f(x) \right]^* = \tilde{f}(p)^*.
    \end{equation}
    That is, for a real function, \(f(-p) = f(p)^*\).
    This same result holds in arbitrary dimensions by an identical proof.
    
    Using this we can write the action as
    \begin{equation}
        S_0[\varphi] = \frac{1}{2} \int_p \tilde{\varphi}(p)(p^2 - m^2 + i\varepsilon) \tilde{\varphi}(p)^*.
    \end{equation}
    
    \subsection{With a Source Term}
    Now we want to compute
    \begin{equation}
        Z_0[J] = \int \!\! \DL{\varphi} \, \exp\left[  i\frac{1}{2} \int_p \tilde{\varphi}(p)(p^2 - m^2 + i\varepsilon) \tilde{\varphi}(p)^* + \int \dl{^Dx} \, J(x) \varphi(x) \right].
    \end{equation}
    This is the infinite dimensional analogue of
    \begin{align}
        Z_A(b) &= \int \dl{^nx} \, \exp\left[ -\frac{1}{2}x^\trans A x + b^\trans x \right]\\
        &= \int \dl{^nx} \, \exp\left[ -\frac{1}{2}\sum_i\sum_j x_iA_{ij}x_j + \sum_i b_ix_i \right].
    \end{align}
    The source term simply has the sum turn into an integral.
    To compute this integral we completed the square by making a substitution, \(x = y + \Delta b\), where \(\Delta = A^{-1}\).
    We can do the same here, making the substitution in momentum space:
    \begin{equation}
        \tilde{\chi}(p) = \tilde{\varphi}(p) + \frac{\tilde{J}(p)}{p^2 - m^2 + i\varepsilon}.
    \end{equation}
    Notice that it's much easier to do this in Fourier space since we can literally divide by \(p^2 - m^2 + i\varepsilon\).
    To do this in real space we'd have to somehow invert the operator \(-\dalembertian - m^2 + i\varepsilon\).
    The finite dimensional analogue of this is that going to Fourier space is a basis change into a basis in which \(A\) is diagonal, and so easily inverted.
    In a sense \(\tilde{K}(p, p')\) is \enquote{diagonal} in momentum space, in particular we get independent modes, one for each momentum.
    
    Before we do this we should Fourier transform the source term as well.
    We have
    \begin{equation}
        J(x) = \int \frac{\dl{^Dp}}{(2\pi)^D} \e^{-ip\cdot x} \tilde{J}(p).
    \end{equation}
    Substituting this, and the Fourier transform of the field, into the source term we have
    \begin{align}
        \int_x J(x)\varphi(x) &= \int_x \int_{p,p'} \tilde{J}(p)\tilde{\varphi}(p') \e^{i(p + p') \cdot x}\\
        &= \int_{p,p'} \tilde{J}(p)\tilde{\varphi}(p') (2\pi)^D\delta(p + p')\\
        &= \int_p \tilde{J}(p) \tilde{\varphi}(-p).
    \end{align}
    Hence,
    \begin{multline}
        S_0[\varphi] + J \cdot \varphi =\\
        \frac{1}{2}\int_p \left[ \tilde{\varphi}(p) (p^2 - m^2 + i\varepsilon) \tilde{\varphi}(-p) + \tilde{J}(p)\tilde{\varphi}(-p) + \tilde{J}(-p)\tilde{\varphi}(p) \right].
    \end{multline}
    The seemingly extra term of \(\tilde{J}(-p) \tilde{\varphi}(p)\) comes from factoring out the half, giving two \(\tilde{J}(p)\tilde{\varphi}(-p)\) terms. 
    We then recognise that sine the integration range is symmetric all that matters is that the arguments of \(\tilde{J}\) and \(\tilde{\varphi}\) in this term differ by a sign.
    So, we can make the substitution \(p \to -p\) in one of the factors then giving the final term above.
    
    Making this change of variables we can add the source term to the action:
    \begin{align}
        S_0[\varphi] + J \cdot \varphi &= \frac{1}{2}\int_p \Bigg[ \left( \tilde{\chi}(p) - \frac{\tilde{J}(p)}{p^2 - m^2 + i\varepsilon} \right)\\
        &\qquad\qquad\times (p^2 - m^2 + i\varepsilon) \left( \tilde{\chi}(-p) - \frac{\tilde{J}(-p)}{p^2 - m^2 + i\varepsilon} \right)\\
        &\qquad\qquad+ \tilde{J}(p)\left( \tilde{\chi}(p) - \frac{\tilde{J}(p)}{p^2 - m^2 + i\varepsilon} \right)\\
        &\qquad\qquad+ \tilde{J}(-p)\left( \tilde{\chi}(-p) - \frac{\tilde{J}(-p)}{p^2 - m^2 + i\varepsilon} \right) \Bigg]
    \end{align}
    Expanding this most terms cancel and we're left with
    \begin{multline}
        S_0[\varphi] + J \cdot \varphi =\\
        \frac{1}{2} \int_p \bigg[ \tilde{\chi}(p)(p^2 - m^2 + i\varepsilon)\tilde{\chi}(-p) - \tilde{J}(p) \frac{1}{p^2 - m^2 + i\varepsilon} \tilde{J}(-p)  \bigg].
    \end{multline}
    
    Hence, we have
    \begingroup
    \allowdisplaybreaks
    \begin{align}
        Z_0[J] &= \int \!\! \DL{\mkern-2mu\chi} \exp\Bigg\{ i\frac{1}{2}\int_p \bigg[ \tilde{\chi}(p)(p^2 - m^2 + i\varepsilon)\tilde{\chi}(-p)\\
        &\qquad\qquad\qquad- \tilde{J}(p) \frac{1}{p^2 - m^2 + i\varepsilon}\tilde{J}(-p) \bigg]  \Bigg\}\\
        &= \exp\left[ -\frac{i}{2} \int_p \tilde{J}(p) \frac{1}{p^2 - m^2 + i\varepsilon} \tilde{J}(-p) \right]\\
        &\qquad\qquad\times\int \DL{\mkern-2mu\chi} \exp\left[ \frac{i}{2} \int_p \tilde{\chi}(p)(p^2 - m^2 + i\varepsilon)\tilde{\chi}(-p) \right]\\
        &= \symcal{N} \exp\left[ -\frac{i}{2} \int_p \tilde{J}(p) \frac{1}{p^2 - m^2 + i\varepsilon} \tilde{J}(-p) \right]
    \end{align}
    \endgroup
    where
    \begin{equation}
        \symcal{N} = \int \DL{\mkern-2mu\chi} \exp\left[ \frac{i}{2} \int_p \tilde{\chi}(p)(p^2 - m^2 + i\varepsilon)\tilde{\chi}(-p) \right]
    \end{equation}
    is a normalisation factor.
    This normalisation factor is analgous to the \((\det A)^{-1/2}\) factor in the finite dimensional case.
    Exactly how we're taking the determinant of an infinite dimensional operator and then inverting this is not important, since we always consider ratios, \(Z_0[J]/Z_0[0]\), where this normalisation vanishes.
    
    We can write this result as
    \begin{equation}\label{eqn:Z0[J]}
        Z_0[J] = \symcal{N}\exp\left[ -\frac{i}{2} \int \dl{^Dx} \dd{^Dx'} \, J(x) \Delta(x, x') J(x') \right]
    \end{equation}
    where
    \begin{equation}
        \Delta(x, x') = \Delta(x - x') = \int_p \frac{\e^{-ip\cdot (x - x')}}{p^2 - m^2 + i\varepsilon}
    \end{equation}
    is the \define{Feynman propagator}\index{Feynman propagator!for fermions}.
    We find this by doing the inverse transform
    \begin{equation}
        \tilde{J}(p) = \int \dl{^Dx} \, \e^{ip\cdot x} J(x), \qqand \tilde{J}(-p) = \int \dl{^Dx} \, \e^{-ip\cdot x} J(x),
    \end{equation}
    so
    \begin{align*}
        \int_p \tilde{J}(-p) \frac{1}{p^2 - m^2 + i\varepsilon} \tilde{J}(p) &= \int_p \int_{x,x'} \e^{-ip\cdot x} J(x) \frac{1}{p^2 - m^2 + i\varepsilon} \e^{ip \cdot x'} J(x')\\
        &= \int_p \int_{x,x'} J(x)\frac{1}{p^2 - m^2 + i\varepsilon} J(x') \e^{-ip \cdot (x - x')}.
    \end{align*}
    
    We can show that, in analogy to the finite dimensional integrals, \(\Delta\) is, in a sense, the inverse of \(K\), more specifically,
    \begin{equation}
        I = \int \dl{^Dz} \, K(x, z) \Delta(z, x') = \delta(x - x').
    \end{equation}
    This follows by substituting in the definitions:
    \begin{align}
        I &= \int \dl{^Dz} \int_p \delta(x - z)[-\dalembertian - m^2 - i\varepsilon] \frac{\e^{-ip\cdot (z - x')}}{p^2 - m^2 + i\varepsilon}\\
        &= \int \dl{^Dz} \delta(x - z)[-\dalembertian - m^2 - i\varepsilon] \frac{\delta(z - x')}{p^2 - m^2 + i\varepsilon}
    \end{align}
    The two terms \([-\dalembertian - m^2 - i\varepsilon]\) and \(1/(p^2 - m^2 + i\varepsilon)\) cancel as they are inverses once we move the first to Fourier space, and we are left with the product \(\delta(x - z)\delta(z - x')\), which reduces to \(\delta(x - x')\) when we perform the \(z\) integral.
    
    \subsection{Using the Path Integral}
    We can use the path integral to compute correlators, much like in \cref{sec:sources}.
    We normalise so that \(\symcal{N} = 1\).
    Consider, for example, the vacuum expectation of the time ordered product of two fields:
    \begin{align*}
        \bra{0} &\timeOrdering[\varphi(x_1)\varphi(x_2)] \ket{0} = \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \left( \frac{1}{i} \diffd{}{J(x_2)} \right) Z_0[J] \bigg|_{J = 0}\\
        &= \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \left( \frac{1}{i} \diffd{}{J(x_2)} \right) \exp\left[ -\frac{i}{2} \int_{x,x'} J(x)\Delta(x, x')J(x') \right]\\
        &= \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \left[ -\frac{1}{2}\int_{x'} \Delta(x_2, x')J(x) - \frac{1}{2} \int_x J(x)\Delta(x, x_2)\} \right]\\
        & \qquad \qquad \times \exp\left[ -\frac{i}{2} \int_{x,x'} J(x)\Delta(x, x')J(x') \right]\\
        &= -\frac{1}{i}\frac{1}{2}\{\Delta(x_1, x_2) + \Delta(x_2, x_1)\}\\
        &= i\Delta(x_1, x_2)
    \end{align*}
    where we set \(J = 0\) in the penultimate term and then used the fact that \(\Delta\) is symmetric in its arguments.
    This must be the case since \(\timeOrdering[\varphi(x_1)\varphi(x_2)]\) is symmetric in \(x_1\) and \(x_2\), and we've used this in the past to \emph{define} \(\Delta\) (see \cref{eqn:feynman propagator as vacuum expectation of time ordered product}).
    We can show its the case using the definition here also, since
    \begin{equation}
        \Delta(x) = \int \frac{\dl{^Dp}}{(2\pi)^D} \frac{\e^{-ip\cdot x}}{p^2 - m^2 + i\varepsilon} \stackrel{p \to -p}{=} \int \frac{\dl{^Dp}}{(2\pi)^D} \frac{\e^{ip\cdot x}}{p^2 - m^2 + i\varepsilon} = \Delta(-x).
    \end{equation}
    Note that even if \(D\) is odd we still don't have to change the overall sign since the integration region, \(\reals^D\), is even so we're integrating over all possible value of \(p\) or \(-p\) either way so the change in sign cancels with exchanging the limits.
    
    In terms of physics this symmetry is due to invariance under translations, which means that the propagator can only depend on the \emph{relative} positions of its arguments, not on the actual positions, meaning we can always write it as a function of the difference in positions, and then all that changes is the sign if we swap the two arguments and the argument above tells us the sign is not important.
    
    \chapter{Interacting Scalar Field Theory}
    \section{General Theory}
    In this chapter we consider a scalar field theory with some interaction given by the potential, \(V\), which is a function of the field, \(\varphi\).
    This is described by the Lagrangian
    \begin{align}
        \lagrangianDensity &= \lagrangianDensity_0 + V(\varphi)\\
        &= \frac{1}{2} (\partial_\mu \varphi)(\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2 + V(\varphi).
    \end{align}
    The action for this Lagrangian is then
    \begin{align}
        S[\varphi] &= S_0[\varphi] + S_{\interaction}[\varphi]\\
        &= \int \dl{^Dx} \left[ \frac{1}{2}(\partial_\mu \varphi)(\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2 \right] + \int \dl{^Dx} V(\varphi).
    \end{align}
    
    We will be interested in computing the path integral
    \begin{equation}
        Z[J] = \braket{0}{0}_J = \int \DL{\varphi} \, \exp\left\{ i\left[ S_0[\varphi] + \int \dl{^Dx} \, V(\varphi) + J \cdot \varphi \right] \right\},
    \end{equation}
    where
    \begin{equation}
        J \cdot \varphi = \int \dl{^Dx} \, J(x) \varphi(x).
    \end{equation}
    Now expand \(V\) term in a power series, first write
    \begin{equation}
        Z[J] = \int \DL{\varphi} \exp\left\{ i \int \dl{^Dx} \, V(\varphi) \right\} \exp\left\{ i(S_0[\varphi] + J \cdot \varphi) \right\}.
    \end{equation}
    Then expand the first exponential:
    \begin{equation}
        Z[J] = \int \DL{\varphi} \sum_{n = 0}^{\infty} \frac{1}{n!} \left[ i\int \dl{^Dx} \, V(\varphi(x_n)) \right]^n \exp\{i(S_0[\varphi] + J \cdot \varphi)\}.
    \end{equation}
    Now expand the potential as well, giving
    \begin{equation}
        Z[J] = \int \DL{\varphi} \sum_{n = 0}^{\infty} \frac{1}{n!} \left[ i\int \dl{^Dx} \, \sum_{k=0}^{\infty} \frac{1}{k!} \diffp{V}{\varphi} \varphi^k \right]^n \exp\{i(S_0[\varphi] + J \cdot \varphi)\}.
    \end{equation}
    We can obtain each factor of \(\varphi\) by acting on the exponential with \(\diffd{}/{J(x)}\).
    This also brings down a factor of \(i\), so we include a factor of \(1/i\) for each derivative giving
    \begin{equation*}
        Z[J] = \int \DL{\varphi} \sum_{n = 0}^{\infty} \frac{1}{n!} \left[ i\int \dl{^Dx} \, \sum_{k=0}^{\infty} \frac{1}{k!} \diffp{V}{\varphi} \left( \frac{1}{i}\diffd{}{J(x)} \right)^k \right]^n \exp\{i(S_0[\varphi] + J \cdot \varphi)\}.
    \end{equation*}
    We can package the derivatives up into \(V(-i\diffd{}/{J(x)})\), which is just a formal term standing for exactly the expansion above, giving
    \begin{equation}
        Z[J] = \int \DL{\varphi} \sum_{n = 0}^{\infty} \frac{1}{n!} \left[ i\int \dl{^Dx} \, V\left( \frac{1}{i}\diffd{}/{J(x)} \right) \right]^n \exp\{i(S_0[\varphi] + J \cdot \varphi)\}.
    \end{equation}
    Finally we can package the exponential back up giving
    \begin{equation}
        Z[J] = \int \DL{\varphi} \, \exp\left\{ i \int \dl{^Dx} \, V\left( \frac{1}{i} \diff{}{J(x)} \right) \right\} \exp\{i(S_0[\varphi] + J \cdot \varphi)\}.
    \end{equation}
    Now the first exponential is independent of \(\varphi\), so we can pull it outside of the integral to get
    \begin{align}
        Z[J] &= \exp\left\{ i \int \dl{^Dx} \, V\left( \frac{1}{i} \diffd{}{J(x)} \right) \right\} \int \DL{\varphi} \, \exp\{i(S_0[\varphi] + J \cdot \varphi)\}\\
        &= \exp\left\{ i \int \dl{^Dx} \, V\left( \frac{1}{i} \diffd{}{J(x)} \right) \right\} Z_0[J],
    \end{align}
    where we recognise
    \begin{equation}
        Z_0[J] = \int \DL{\varphi}\, \exp\{i(S_0[\varphi] + J \cdot \varphi)\}
    \end{equation}
    as the amplitude for the noninteracting system.
    
    \section{\texorpdfstring{\(\varphi^3\)}{Phi Cubed} Interactions}
    We will consider as an example the potential
    \begin{equation}
        V(\varphi) = \frac{g}{3!}\varphi^3.
    \end{equation}
    Then we can use
    \begin{equation}
        V\left( \frac{1}{i} \diffd{}{J(x)} \right) = \frac{g}{3!} \left( \frac{1}{i}\diffd{}{J(x)} \right)^3.
    \end{equation}
    Expand the exponential and \(Z_0[J]\) in power series, using \cref{eqn:Z0[J]} for \(Z_0[J]\), this gives
    \begin{equation}\label{eqn:Z[J] in phi cubed}
        Z[J] = \sum_{V = 0}^{\infty} \frac{1}{V!} \left[ i\frac{g}{3!} \int_x \left( \frac{1}{i} \diffd{}{J(x)} \right)^3 \right]^V \sum_{P = 0}^{\infty} \frac{1}{P!} \left[ -\frac{1}{2} \int_{y,z} J(y) i\Delta(y - z) J(z) \right]^P.
    \end{equation}
    This is a perturbative expansion for \(Z[J]\), and is a function of \(J\), all spacetime points, \(x\), \(y\), and \(z\), are integrated over.
    
    Consider what happens for some fixed values of \(V\) and \(P\).
    We have \(2P\) factors of \(J\), and \(3V\) derivatives with respect to \(J\), so we end up with \(E = 2P - 3V\) factors of \(J\).
    
    We will again introduce a diagrammatic notation to write the result of integrals like these.
    First, a propagator is written as a line:
    \begin{equation}
        i\Delta(y - z) = 
        \tikzsetnextfilename{fd-propagator-qft}
        \begin{tikzpicture}[baseline=(x.base)]
            \draw (0, 0) node[left] (x) {\(x\)} -- (1, 0) node[right] {\(y\)};
        \end{tikzpicture}
    \end{equation}
    We denote an integral of \(J(x)\) as solid dot at the end of a line:
    \begin{equation}
        i\int \dl{^Dx} \, J(x) = 
        \tikzsetnextfilename{fd-integrate-J}
        \begin{tikzpicture}[baseline=(x.base)]
            \draw (0, 0) node[left] (x) {\(x\)} -- (1, 0) node[right] {\(y\)};
            \fill (0, 0) circle [radius=0.05cm];
        \end{tikzpicture}
    \end{equation}
    Note that the line also gives a propagator, which we're not worried about here.
    We denote an integral and a factor of \(g\) with a vertex, here with three legs, each of which again represents a propagator which we aren't worried about here:
    \begin{equation}
        ig\int \dl{^D x} = 
        \tikzsetnextfilename{fd-vertex-qft}
        \begin{tikzpicture}[baseline={(0, -0.1)}]
            \draw (0, 0) -- (90:0.5);
            \draw (0, 0) -- (210:0.5);
            \draw (0, 0) -- (330:0.5);
            \node[below] at (0, 0) {\(x\)};
        \end{tikzpicture}
    \end{equation}
    
    We have to carefully count how many times each term contributes to the overall amplitude.
    We need to count the number of ways we can change the term without changing anything.
    We have the following symmetries to account for:
    \begin{itemize}
        \item Permutations of the derivatives with respect to \(J(x)\), evaluated at the same \(x\).
        That is, if we have the \(V = 2\) term
        \begin{equation}
            \frac{1}{2!} \left( i\frac{g}{3!} \int_{x_1} \left( \frac{1}{i}\diffd{}{J(x_1)} \right)^3 \right) \left( i\frac{g}{3!} \int_{x_2} \left( \frac{1}{i}\diffd{}{J(x_2)} \right)^3 \right) \dotsm
        \end{equation}
        then we can exchange two of the derivatives with respect to \(J(x_1)\), but we can't exchange a derivative with respect to \(J(x_1)\) and a derivative with respect to \(J(x_2)\).
        In total this symmetry gives a factor of \((3!)^V\).
        \item Permutations of propagators and relabelling dummy integration variables.
        There are \(P\) propagators (which is why we called this index \(P\) in the first place) and so we have a factor of \(P!\).
        \item Swapping the ends of propagators, since they're symmetric functions, this gives a factor of \(2^P\).
        \item Permuting integrals over \(x\) in the first term, which corresponds to permuting vertices.
        There are \(V\) vertices (which is why we called this index \(V\) in the first place) and so we have a factor of \(V!\).
    \end{itemize}
    
    Notice that \cref{eqn:Z[J] in phi cubed} has factors of \(1/(3!)^V\), \(1/P!\), \(1/2^P\), and \(1/V!\), and so all of these factors cancel out, indeed this is why we choose to have the factor of \(3!\) and \(1/2\) in the initial definitions.
    However, this only works if all of the operations described above are distinct, and this isn't always the case.
    If two of these operations give the same term then we don't want to double count them.
    
    For example, consider the \(V = 1\), \(P = 2\) term:
    \begin{multline*}
        i\frac{g}{3!} \int \dl{^Dx} \left( \frac{1}{i} \diffd{}{J(x)} \right) \left( \frac{1}{i} \diffd{}{J(x)} \right) \left( \frac{1}{i} \diffd{}{J(x)} \right)\\
        \times\frac{1}{2!} \left( -\frac{1}{2} \right)^2 \int_{y_1,z_1} J(y_1) i\Delta(y_1 - z_1)J(z_1) \int_{y_2,z_2} J(y_2) i\Delta(y_2 - z_2)J(z_2).
    \end{multline*}
    Now focus on the contribution from this term when the second derivative acts on the first factor of \(J\) and the third derivative acts on the second factor of \(J\):
    \begin{multline*}
        i\frac{g}{3!} \int \dl{^Dx} \left( \frac{1}{i} \diffd{}{J(x)} \right) \textcolor{Red}{\left( \frac{1}{i} \diffd{}{J(x)} \right)} \textcolor{Purple}{\left( \frac{1}{i} \diffd{}{J(x)} \right)}\\
        \times\frac{1}{2!} \left( -\frac{1}{2} \right)^2 \int_{y_1,z_1} \textcolor{Red}{J(y_1)} i\Delta(y_1 - z_1)\textcolor{Purple}{J(z_1)} \int_{y_2,z_2} J(y_2) i\Delta(y_2 - z_2)J(z_2).
    \end{multline*}
    Now swap the second and third derivative, which changes nothing, but keep track of which \(J\) they act on
    \begin{multline*}
        i\frac{g}{3!} \int \dl{^Dx} \left( \frac{1}{i} \diffd{}{J(x)} \right) \textcolor{Purple}{\left( \frac{1}{i} \diffd{}{J(x)} \right)} \textcolor{Red}{\left( \frac{1}{i} \diffd{}{J(x)} \right)}\\
        \times\frac{1}{2!} \left( -\frac{1}{2} \right)^2 \int_{y_1,z_1} \textcolor{Red}{J(y_1)} i\Delta(y_1 - z_1)\textcolor{Purple}{J(z_1)} \int_{y_2,z_2} J(y_2) i\Delta(y_2 - z_2)J(z_2).
    \end{multline*}
    We can now swap the first two factors of \(J\), and make a change of variables \(y_1 \leftrightarrow z_1\), giving
    \begin{multline*}
        i\frac{g}{3!} \int \dl{^Dx} \left( \frac{1}{i} \diffd{}{J(x)} \right) \textcolor{Purple}{\left( \frac{1}{i} \diffd{}{J(x)} \right)} \textcolor{Red}{\left( \frac{1}{i} \diffd{}{J(x)} \right)}\\
        \times\frac{1}{2!} \left( -\frac{1}{2} \right)^2 \int_{y_1,z_1} \textcolor{Purple}{J(y_1)} i\Delta(y_1 - z_1)\textcolor{Red}{J(z_1)} \int_{y_2,z_2} J(y_2) i\Delta(y_2 - z_2)J(z_2).
    \end{multline*}
    This is just the term we started with but with the colours swapped.
    This means that these two swaps are inverses, and so swapping the derivatives and reverse swapping the \(J\)s (which is just swapping them in this case where there are only 2) are the same operation, and so if we include both the factor of \(P! = 2\) and \((3!)^V = 6\) then we'll be overcounting the contributions from this term.
    
    To see this we can perform three derivatives in this term.
    They will each act on one of four factors of \(J\), specifically one of \(J(y_1)\), \(J(z_1)\), \(J(y_2)\), and \(J(z_2)\).
    The first derivative will have four options for which \(J\) to act on, the second derivative will have three options, and the third will have two, so we include a factor of \(4 \cdot 3 \cdot 2\).
    It doesn't matter which \(J\) we don't act on, since we can always make a change of variables to choose the argument of \(J\) to be any of \(y_1\), \(z_1\), \(y_2\) and \(z_2\).
    So without loss of generality we assume that nothing acts on \(J(y_1)\).
    Then the three other \(J\)s are acted on, and so each gives a Dirac delta, \(\delta(x - \xi)\) for each \(J(\xi)\).
    Hence this term becomes
    \begin{equation}
        i\frac{g}{3!} \frac{1}{2!} \left( -\frac{1}{2} \right)^2 4 \cdot 3 \cdot 2 \int_{x,y_1,z_1,y_2,z_2} J(y_1) \delta(x - z_1) \delta(x - y_2) \delta(x - z_2) i\Delta(y_1 - z_1) i\Delta(y_2 - z_2).
    \end{equation}
    Cancelling some factors and using the Dirac deltas to perform three of the integrals we get
    \begin{equation}
        g\frac{1}{2} \int_{x, y_1} J(y_1) \Delta(y_1 - x) \Delta(x - x).
    \end{equation}
    Now we can draw the diagram representing this integral, we have a single source, so one solid dot, at \(y_1\) this is then connected by a propagator to \(x\), and then we have another propagator from \(x\) to \(x\), which gives a loop:
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-diagram-symmetry-factor-example}
        \begin{tikzpicture}[baseline=(y.base)]
            \draw[looseness=1.5] (0, 0) node[left] (y) {\(y_1\)} -- (1, 0) to[out=40, in=90] (2, 0) to[out=270, in=-40] (1, 0) node [below] {\(x\)};
            \fill (0, 0) circle [radius = 0.05cm];
        \end{tikzpicture}
    \end{equation}
    Notice that this diagram has a single mirror symmetry along the propagator line from \(y_1\) to \(x\).
    This 2-fold symmetry exactly corresponds to the factor of \(1/2\) that we are left with in the integral.
    In general the integral will have a factor of 1 over the order of the symmetry group of the diagram.
    
    \section{Disconnected Diagrams}\label{sec:disconnected diagrams}
    A disconnected diagram is any diagram made from two or more pieces with no propagators connecting them.
    We can consider some arbitrary diagram, \(D\), which may or may not be connected.
    We can always decompose \(D\) as a product of its connected components, which will just be \(D\) itself if it's connected.
    Let \(C_I\) be the \(I\)th connected diagram and suppose it appears \(n_I\) times in \(D\).
    Then diagram \(C_I\) contributes a factor of \(C_I^{n_I}\) to \(D\), up to a symmetry factor.
    We include in \(C_I\) the symmetry factor for this diagram, and so the only symmetry not accounted for is permutations of repeated connected subdiagrams.
    In particular we get a factor of \(n_I!\) for each connected subdiagram.
    Thus, we can express \(D\) as
    \begin{equation}
        D = \frac{1}{S_D} = \prod_I C_I^{n_I}, \qqwhere S_D = \prod_I n_I!.
    \end{equation}
    
    We can then work to all orders in perturbation theory and express \(Z[J]\) as a sum over all diagrams, \(D\), which we can regard as a function of \(\{n_I\}\), the number of times the connected diagram \(C_I\) appears as a subdiagram of \(D\):
    \begin{equation}
        Z[J] = \sum_{\{n_I\}} D(\{n_I\}) = \sum_{\{n_I\}} \prod _I \frac{1}{n_I!} C_I^{n_I}.
    \end{equation}
    We can swap the order of the product and, then the sum is over fixed \(I\) and we take all possible diagrams so \(n_I\) runs from 0 to \(\infty\), giving
    \begin{equation}
        Z[J] = \prod_I \sum_{n_I = 0}^{\infty} \frac{1}{n_I!} C_I^{n_I} = \prod \exp[C_I] = \exp\left[ \sum_I C_I \right].
    \end{equation}
    
    When normalising \(Z[J]\) we choose to have \(Z[0] = 1\), which corresponds to replacing \(Z[J]\) with \(Z[J]/Z[0]\).
    Since \(Z[0]\) consists of all diagrams with no sources we have
    \begin{equation}
        Z[0] = \exp\left[ \sum_{I \mid E = 0} C_I \right]
    \end{equation}
    where the sum is taken over diagrams with \(E - 0\).
    We then have
    \begin{equation}
        \frac{Z[J]}{Z[0]} = \exp\left[ \sum_I C_I \right] \exp\left[ -\sum_{I \mid E = 0} C_I \right] = \exp\left[ \sum_{I \mid E \ne 0} C_I \right] = \exp\left[ {\sum_I}' C_I \right]
    \end{equation}
    where the sum in the last two is taken over all connected diagrams with no sources.
    
    Now consider the log of this value:
    \begin{equation}
        \log\left( \frac{Z[J]}{Z[0]} \right) = \sum_{I \mid E \ne 0} C_I \eqqcolon iW[J].
    \end{equation}
    This is a sum over connected diagrams with no sources.
    The factor of \(i\) is chosen so that \(W[J]\) is real for a real scalar field.
    
    That disconnected diagrams don't contribute to this value proves a similar statement about Gaussian correlators made in \cref{sec:computing log Zlambda/Z0}.
    
    \part{Scalar Field Correlators}
    \chapter{Correlators}
    \section{Correlators}
    The generating functional in \(\varphi^3\) theory is
    \begin{equation*}
        Z[J] = \sum_{V = 0}^{\infty} \frac{1}{V!} \left[ \frac{ig}{3!} \left( \frac{1}{i} \diffd{}{J(x)} \right)^3 \right]^V \sum_{P = 0}^{\infty} \frac{1}{P!} \left[ -\frac{1}{2} \int \!\! \dl{^Dy} \int \!\! \dl{^Dz} \, J(y) i\Delta(y - z) J(z) \right]^P.
    \end{equation*}
    We want to calculate the \(n\)-point correlator
    \begin{align}
        G^{(n)}(x_1, \dotsc, x_n) &\coloneqq \bra{0} \timeOrdering\{\varphi(x_1) \dotsm \varphi(x_n)\} \ket{0}\\
        &\hphantom{:}= \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \dotsm \left( \frac{1}{i} \diffd{}{J(x_n)} \right) Z[J] \bigg|_{J = 0}.
    \end{align}
    Since \(Z[J]\) is just a power series in \(g\) we can also expand \(G^{(n)}\) as a power series in \(g\) with coefficients \(G^{(n, V)}\) defined by
    \begin{equation}
        G^{(n)}(x_1, \dotsc, x_n) = \sum_{V = 0}^{\infty} G^{(n, V)}(x_1, \dotsc, x_n)
    \end{equation}
    
    In order to have a nonzero contribution we need only consider terms with no sources left after performing the derivatives.
    That is, we want \(n = E\), where \(E = 2P - 3V\) is the number of sources.
    
    \section{Two Point Correlator}
    As an example we'll compute the two point correlator, 
    \begin{equation}
        G^{(2)}(x_1, x_2) = \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \left( \frac{1}{i} \diffd{}{J(x_2)} \right) Z[J] \bigg|_{J = 0}.
    \end{equation}
    to order of \(g^2\).
    Since \(n = 2\), and we require \(E = n\) for a nonzero contribution, we need to choose \(P\) to satisfy \(2 = 2P - 3V\).
    
    \subsection{\texorpdfstring{\(V = 0\)}{V = 0}}
    For \(V = 0\) we must have \(P = 1\) for a nonzero contribution.
    We can easily read off the \(V = 0\) and \(P = 1\) contribution to the generating functional, it is
    \begin{equation}
        -\frac{1}{2} \int_{y,z} J(y) i\Delta(y - z) J(z) = 
        \tikzsetnextfilename{fd-correlator-V=0-contribution}
        \begin{tikzpicture}[baseline=(x.base)]
            \draw (0, 0) node[left] (x) {} -- (1, 0);
            \fill (0, 0) circle [radius=0.05cm];
            \fill (1, 0) circle [radius=0.05cm];
        \end{tikzpicture}
    \end{equation}
    We could have gone the other way here.
    If we had first drawn the diagram then we can see from this that there are two sources, the dots, connected by a propagator
    
    So we want to compute
    \begin{align}
        G^{(2, 0)}(x_1, x_2) &= \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \left( \frac{1}{i} \diffd{}{J(x_2)} \right) Z[J] \bigg|_{J = 0}\\
        &= -\frac{1}{2} \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \left( \frac{1}{i} \diffd{}{J(x_2)} \right) \int_{y,z} J(y) i\Delta(y - z) J(z)\\
        &= -\frac{1}{2} \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \bigg[ \int_{y,z} \delta(x_1 - y) \Delta(y - z) J(z)\\
        &\qquad\qquad + \int_{y,z} J(y) i\Delta(y - z)\delta(x_1 - z) \bigg]\\
        &= -\frac{1}{2} \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \bigg[ \int_z \Delta(x_1 - z)J(z)\\
        &\qquad\qquad+ \int_y J(y)\Delta(y - x_1) \bigg]\\
        &= -\left( \frac{1}{i} \diffd{}{J(x_1)} \right) \int_y J(y) \Delta(y - x_1)
    \end{align}
    where in the last step we perform a change of integration variables \(z \to y\) in the first integral and then use the symmetry of \(\Delta\), \(\Delta(a - b) = \Delta(b - a)\), which is a result of translation invariance.
    Performing the last integral we get
    \begin{align}
        G^{(2, 0)}(x_1, x_2) &= i \int_y \delta(x_2 - y) \Delta(y - x_1)\\
        &= i \Delta(x_2 - x_1).
    \end{align}
    We can represent this diagrammatically as
    \begin{equation}
        G^{(2, 0)}(x_1, x_2) = i\Delta(x_1 - x_2) = 
        \tikzsetnextfilename{fd-correlator-V=0-contribution-after-differentiation}
        \begin{tikzpicture}[baseline=(x.base)]
            \draw (0, 0) node[left] (x) {\(x_1\)} -- (1, 0) node[right] {\(x_2\)};
        \end{tikzpicture}
    \end{equation}
    where we now fix the two endpoints and so no longer have the symmetry factor of 2 from swapping the ends of the propagator.
    
    Recall that we can write the propagator as the inverse Fourier transform of its momentum space equivalent:
    \begin{equation}
        G^{(2, 0)}(x_1, x_2) = i\Delta(x_1 - x_2) = \int \frac{\dl{^Dp}}{(2\pi)^D} \e^{ip\cdot (x_1 - x_2)} \frac{i}{p^2 - m^2 + i\varepsilon}.
    \end{equation}
    
    \subsection{\texorpdfstring{\(V = 1\)}{V = 1}}
    For \(V = 1\) we require the equation \(2 = 2P - 3\) to hold, but there is no integer solution for this, and hence there is no nonzero contribution from \(V = 1\).
    This is a peculiarity of \(\varphi^3\) theory, not a general rule.
    We saw this through explicit calculations of the \(V = 1\) terms in \cref{chap:feynman diagrams}.
    
    \subsection{\texorpdfstring{\(V = 2\)}{V = 2}}
    For a nonzero contribution with \(V = 2\) we require the equation \(2 = 2P - 6\) to hold, and it does if \(P = 4\).
    We could go through the exercise of writing out the \(V = 2\) and \(P = 4\) contribution to \(Z[J]\) and then computing the functional derivative with respect to \(J(x_1)\) and \(J(x_2)\), but this isn't that fun.
    We'd have six derivatives to compute, all of which can act on any of eight factors of \(J\), its a mess.
    Instead we can realise that a \(V = 2\) \(P = 4\) contribution consists of a diagram with two vertices and four propagators.
    Before computing the derivative we must have two sources, so that our final result has no sources.
    So, we can simply write down all diagrams with two vertices, four propagators, and two sources.
    It turns out that there are two distinct diagrams satisfying these requirements.
    
    \subsubsection{The First Diagram}
    The following diagram has two vertices, four propagators\footnote{Note that the circle is actually two propagators, one making up the top and the other the bottom.}, and two sources:
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-two-point-two-verticex-a}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (-1.5, 0) node[below] {\scriptsize\(z_1\)} -- (-0.5, 0) node[below left, xshift=0.3] {\scriptsize\(w_1\)};
            \draw (0.5, 0) node[below right, xshift=-0.2] {\scriptsize\(w_2\)} -- (1.5, 0) node[below right] {\scriptsize\(z_2\)};
            \draw (0, 0) circle [radius = 0.5];
            \draw[fill] (-1.5, 0) circle [radius = 0.05];
            \draw[fill] (1.5, 0) circle [radius = 0.05];
        \end{tikzpicture}
    \end{equation}
    Note that the labels are just to assist in constructing the integral.
    
    Let's start with the symmetry factor.
    We have two planes of mirror symmetry, vertical and horizontal, and no other symmetries.
    So the symmetry factor is \(2 \cdot 2 = 4\).
    We then have two source factors, at \(z_1\) and \(z_2\), propagators from \(z_1\) to \(w_1\) and \(w_2\) to \(z_2\), and then two propagators from \(w_1\) to \(w_2\).
    The integral corresponding to this diagram is
    \begin{equation}
        I = \frac{1}{4} \int_{z_1, z_2, w_1, w_2} J(z_1) i\Delta(z_1 - w_1)[i\Delta(w_1 - w_2)]^2 i\Delta(w_2 - z_2) J(z_2).
    \end{equation}
    Notice that there are four factors of \(i\) giving an overall factor of \(1\).
    
    We can compute the first derivative, splitting into two terms with the product rule, then getting Dirac deltas, which we use to remove one integral, and then performing a change of variables to show both terms are the same, so we can recombine them and pick up a factor of 2 giving the result
    \begin{equation}
        \frac{1}{i} \diffd{I}{J(x_2)} = \frac{1}{2i} \int_{z_1,w_1,w_2} J(z_1)\Delta(z_1 - w_1)\Delta(w_1 - w_2)^2\Delta(w_2 - x_2).
    \end{equation} 
    We can perform the next derivative, now with only one factor of \(J\) to act on, and we again get a Dirac delta which we use to remove one of the integrals for the final result
    \begin{align}
        G^{(2, 2)}_a(x_1, x_2) &= \left( \frac{1}{i} \diffd{}{J(x_1)} \right)\left( \frac{1}{i} \diffd{I}{J(x_2)} \right)\\
        &= -\frac{1}{2} \int_{w_1, w_2} \Delta(x_1 - w_1)\Delta(w_1 - w_2)^2\Delta(w_2 - x_2).
    \end{align}
    The subscript \(a\) here simply labels that this is the contribution from the first diagram of this type.
    
    \subsubsection{The Second Diagram}
    The other diagram with two vertices, four propagators\footnote{This time the circle is just a single propagator connecting the vertex to itself.}, and two sources is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-two-point-two-verticex-b}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (-1, 0) node [below] {\scriptsize\(z_1\)} -- (1, 0) node [below] {\scriptsize\(z_2\)};
            \fill (-1, 0) circle [radius = 0.05];
            \fill (1, 0) circle [radius = 0.05];
            \draw (0, 0) node [below] {\scriptsize\(w_1\)} -- (0, 1) node [below right] {\scriptsize\(w_2\)};
            \draw (0, 1.5) circle [radius = 0.5];
        \end{tikzpicture}
    \end{equation}
    Again, the labels are just to help us construct the integral.
    
    Starting with the symmetry factor we have a vertical plane of mirror symmetry, and we can also take the top propagator, forming a circle, swap the ends, and the diagram doesn't change.
    So the symmetry factor is \(2 \cdot 2 = 4\).
    We then have two source factors at \(z_1\) and \(z_2\), propagators from \(z_1\) to \(w_1\), \(w_1\) to \(w_2\), and \(w_1\) to \(z_2\).
    We also have a propagator from \(w_2\) back to \(w_2\), which will give a factor of \(\Delta(0)\).
    The integral corresponding to this diagram is
    \begin{equation}
        I = \frac{1}{4} \int_{z_1,z_2,w_1,w_2} J(z_1) i\Delta(z_1 - w_1) i\Delta(w_1 - w_2) i\Delta(0) i\Delta(w_2 - z_2) J(z_2).
    \end{equation}
    Notice that again the factors of \(i\) cancel.
    
    We can compute the first derivative, much like for the previous diagram, and the result is
    \begin{equation}
        \frac{1}{i} \diffd{I}{J(x_2)} = \frac{1}{2i} \int_{z_1,w_1,w_2} J(z_1)\Delta(z_1 - w_1) \Delta(w_1 - w_2)\Delta(0)\Delta(w_2 - x_2).
    \end{equation}
    Computing the second derivative we have
    \begin{align}
        G^{(2,2)}_b(x_1, x_2) &= \left( \frac{1}{i} \diffd{}{J(x_1)} \right) \left( \frac{1}{i} \diffd{I}{J(x_2)} \right)\\
        &= -\frac{1}{2} \int_{w_1, w_2} \Delta(x_1 - w_1) \Delta(w_1 - w_2) \Delta(0) \Delta(w_1 - x_2).
    \end{align}
    Again, the subscript \(b\) simply labels that this is the contribution from the second diagram of this type.
    
    \section{Momentum Space}
    Its often easier to do computations in Fourier space.
    One example of this is the correlator, which is given by
    \begin{equation}
        \Delta(x) = \int \frac{\dl{^Dp}}{(2\pi)^D} \frac{\e^{-ip\cdot x}}{p^2 - m^2 + i\varepsilon}
    \end{equation}
    in real space, but in Momentum space is simply
    \begin{equation}
        \tilde{\Delta}(p) = \frac{1}{p^2 - m^2 + i\varepsilon}.
    \end{equation}
    
    We can Fourier transform the correlators we have already calculated to work out the correlator in momentum space, but first, consider the Fourier transform of the two-point correlator, \(G^{(n)}\), in a general theory.
    This is given by
    \begin{equation}
        \tilde{G}^{(2)}(p_1, p_2) = \int \dl{^Dx_1} \dd{^Dx_2} \, \e^{ip_1 \cdot x_1} \e^{ip_2 \cdot x_2} G^{(2)}(x_1, x_2).
    \end{equation}
    Now perform a change of variables for the \(x_1\) integral, defining \(\xi = x_1 - x_2\).
    The measure and limits are invariant under translations, as is \(G^{(2)}(x_1, x_2)\).
    This means we can rewrite \(G^{(2)}\) as a function of \(x_1 - x_2 = \xi\), so \(G^{(2)}(x_1 - x_2) = G^{(2)}(\xi)\).
    Doing so we have
    \begin{align}
        \tilde{G}^{(2)}(p_1, p_2) &= \int \dl{^D\xi} \dd{^Dx_2} \, \e^{ip_1\cdot (\xi + x_2)} \e^{ip_2\cdot x_2} G^{(2)}(\xi)\\
        &= \int \dl{^D\xi} \dd{^Dx_2} \, \e^{ip_1\cdot \xi} \e^{i(p_1 + p_2)\cdot x_2} G^{(2)}(\xi)\\
        &= \int \dl{^D\xi} \, \e^{ip_1\cdot \xi} (2\pi)^D\delta(p_1 + p_2) G^{(2)}(\xi)\\
        &= (2\pi)^D\delta(p_1 + p_2) \tilde{G}^{(2)}(p_1),
    \end{align}
    where in the last step we recognise \(\tilde{G}^{(2)}(p_1)\) as the Fourier transform of the single variable function depending on \(x_1 - x_2 = \xi\).
    
    What we see here is that the 2-point correlator depends on only 1 momentum value, the other being fixed by the Dirac delta, so that all momenta sum to zero.
    This is a general fact.
    In momentum spae the \(n\)-point correlator will depend on \(n - 1\) momenta values.
    
    \subsection{\texorpdfstring{\(V = 0\)}{V = 0}}
    We already know that \(G^{(2)}(x_1, x_2) = i\Delta(x_1 - x_2)\), so we trivially have
    \begin{equation}
        \tilde{G}^{(2, 0)}(p) = \frac{i}{p^2 - m^2 + i\varepsilon}.
    \end{equation}
    We represent this diagrammatically as
    \begin{equation}
        \tikzsetnextfilename{fd-correlator-V=0-momentum-space}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (-1, 0) -- (1, 0);
            \draw[->, >=stealth] (-0.4, 0.2) -- (0.4, 0.2) node [midway, above] {\(p\)};
        \end{tikzpicture}
    \end{equation}
    We use arrows to denote the direction of the momentum.
    
    \subsection{\texorpdfstring{\(V = 2\)}{V = 2}}
    \subsubsection{The First Diagram}
    We can similarly compute the contribution from the first diagram in momentum space by computing the Fourier transform of the known result:
    \begin{equation}
        G^{(2, 0)}_a(x_1, x_2) = -\frac{1}{2}\int_{w_1, w_2} \Delta(x_1 - w_1) \Delta(w_1 - w_2)^2 \Delta(w_2 - x_2).
    \end{equation}
    We start by writing the propagators as inverse Fourier transforms of the momentum space propagator, so
    \begin{multline}
        G^{(2, 0)}_a(x_1, x_2) = -\frac{1}{2}\int_{w_1, w_2} \int_{k_1, k_2, k_3, k_4} \frac{\e^{-ik_1 \cdot (x_1 - w_1)}}{k_1^2 - m^2 + i\varepsilon} \frac{\e^{-ik_2 \cdot (w_1 - w_2)}}{k_2^2 - m^2 + i\varepsilon}\\
        \times \frac{\e^{-ik_3 \cdot (w_1 - w_2)}}{k_3^2 - m^2 + i\varepsilon} \frac{\e^{-ik_4 \cdot (w_2 - x_2)}}{k_4^2 - m^2 + i\varepsilon}.
    \end{multline}
    Now comes the actual Fourier transform, we have to Fourier transform both \(x_1\) and \(x_2\), giving
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p_1, p_2) = -\frac{1}{2} \int_{x_1, x_2} \int_{w_1, w_2} \int_{k_1, k_2, k_3, k_4} \e^{-ip_1 \cdot x_1} \e^{-ip_2 \cdot x_2} \frac{\e^{-ik_1 \cdot (x_1 - w_1)}}{k_1^2 - m^2 + i\varepsilon} \\
        \times \frac{\e^{-ik_2 \cdot (w_1 - w_2)}}{k_2^2 - m^2 + i\varepsilon}
        \frac{\e^{-ik_3 \cdot (w_1 - w_2)}}{k_3^2 - m^2 + i\varepsilon} \frac{\e^{-ik_4 \cdot (w_2 - x_2)}}{k_4^2 - m^2 + i\varepsilon}
    \end{multline}
    where the Fourier transform is
    \begin{equation}
        \tilde{f}(p) = \int_x \e^{-ip\cdot x} f(x).
    \end{equation}
    
    Now we can rewrite the exponentials collecting \(x_1\) and \(x_2\) together:
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p_1, p_2) = -\frac{1}{2} \int_{x_1, x_2} \int_{w_1, w_2} \int_{k_1, k_2, k_3, k_4} \e^{-i(p_1 + k_1) \cdot x_1} \e^{-i(p_2 - k_4) \cdot x_2}\\
        \times \frac{\e^{ik_1 \cdot w_1}}{k_1^2 - m^2 + i\varepsilon} \frac{\e^{-ik_2 \cdot (w_1 - w_2)}}{k_2^2 - m^2 + i\varepsilon} \frac{\e^{-ik_3 \cdot (w_1 - w_2)}}{k_3^2 - m^2 + i\varepsilon} \frac{\e^{-ik_4 \cdot w_2}}{k_4^2 - m^2 + i\varepsilon}.
    \end{multline}
    We can now perform the integrals over \(x_1\) and \(x_2\), and we'll get Dirac deltas from these first two exponentials:
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p_1, p_2) = -\frac{1}{2} \int_{w_1, w_2} \int_{k_1, k_2, k_3, k_4} (2\pi)^D\delta(p_1 + k_1) (2\pi)^D(p_2 - k_4)\\
        \times \frac{\e^{ik_1 \cdot w_1}}{k_1^2 - m^2 + i\varepsilon} \frac{\e^{-ik_2 \cdot (w_1 - w_2)}}{k_2^2 - m^2 + i\varepsilon} \frac{\e^{-ik_3 \cdot (w_1 - w_2)}}{k_3^2 - m^2 + i\varepsilon} \frac{\e^{-ik_4 \cdot w_2}}{k_4^2 - m^2 + i\varepsilon}.
    \end{multline}
    The Dirac deltas allow us to easily perform two of the \(k\) integrals, setting \(k_1 = -p_1\) and \(k_4 = p_2\).
    Note that the \(k\) integrals, being momentum integrals, come with implicit factors of \(1/(2\pi)^D\), and so the \(k_1\) and \(k_4\) integrals cancel the factor of \((2\pi)^{2D}\):
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p_1, p_2) = -\frac{1}{2} \int_{w_1, w_2} \int_{k_2, k_3}
        \frac{\e^{-ip_2 \cdot w_1}}{p_2^2 - m^2 + i\varepsilon} \frac{\e^{-ik_2 \cdot (w_1 - w_2)}}{k_2^2 - m^2 + i\varepsilon}\\
        \times \frac{\e^{-ik_3 \cdot (w_1 - w_2)}}{k_3^2 - m^2 + i\varepsilon} \frac{\e^{-ip_1 \cdot w_2}}{p_1^2 - m^2 + i\varepsilon}.
    \end{multline}
    Next rewrite the exponentials again, this time collecting factors of \(w_1\) and \(w_2\).
    We get
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p_1, p_2) = -\frac{1}{2} \int_{w_1, w_2} \int_{k_2, k_3} \e^{-i(p_2 + k_2 + k_3) \cdot w_1} \e^{-i(-k_2 - k_3 + p_1) \cdot w_2}\\
        \times \frac{1}{p_2^2 - m^2 + i\varepsilon} \frac{1}{k_2^2 - m^2 + i\varepsilon} \frac{1}{k_3^2 - m^2 + i\varepsilon} \frac{1}{p_1^2 - m^2 + i\varepsilon}.
    \end{multline}
    We can then perform the \(w_1\) and \(w_2\) integrals, and we get another two Dirac deltas:
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p_1, p_2) = -\frac{1}{2} \int_{k_2, k_3} (2\pi)^D\delta(p_2 + k_2 + k_3) (2\pi)^D\delta(-k_2 - k_3 + p_1)\\
        \times \frac{1}{p_2^2 - m^2 + i\varepsilon} \frac{1}{k_2^2 - m^2 + i\varepsilon} \frac{1}{k_3^2 - m^2 + i\varepsilon} \frac{1}{p_1^2 - m^2 + i\varepsilon}.
    \end{multline}
    We can then perform the \(k_2\) integral using the first Dirac delta setting \(k_2 = -p_2 - k_3\), and again cancelling one of the \((2\pi)^D\) factors with one implicit in the \(k_2\) integral, giving
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p_1, p_2) = -\frac{1}{2} \int_{k_3} (2\pi)^D\delta(p_2 + k_3 - k_3 + p_1) \frac{1}{p_2^2 - m^2 + i\varepsilon}\\
        \times \frac{1}{(p_2 + k_3)^2 - m^2 + i\varepsilon} \frac{1}{k_3^2 - m^2 + i\varepsilon} \frac{1}{p_1^2 - m^2 + i\varepsilon}.
    \end{multline}
    Note that we use the square to get rid of the negatives in the second denominator.
    We can rename \(k_3 \to k\) and rewrite this as
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p_1, p_2) = -\frac{1}{2} (2\pi)^D\delta(p_1 + p_2) \frac{1}{p_2^2 - m^2 + i\varepsilon}\\
        \times \bigg\{ \int_{k} \frac{1}{(p_2 + k)^2 - m^2 + i\varepsilon} \frac{1}{k^2 - m^2 + i\varepsilon} \bigg\} \frac{1}{p_1^2 - m^2 + i\varepsilon}.
    \end{multline}
    Notice that the Dirac delta means we can set \(p_2 = -p_1\) and then we have \((-p_1 + k)^2 = (p_1 - k)^2\) in the first denominator in the integral.
    Then the result can be written as a function of \(p = p_1\) alone, with the only \(p_2\) term being in the Dirac delta:
    \begin{multline}
        \tilde{G}^{(2, 0)}_a(p) = -\frac{1}{2} (2\pi)^D\delta(p + p_2) \frac{1}{p^2 - m^2 + i\varepsilon}\\
        \times \bigg\{ \int \frac{\dl{^Dk}}{(2\pi)^D} \frac{1}{(p - k)^2 - m^2 + i\varepsilon} \frac{1}{k^2 - m^2 + i\varepsilon} \bigg\} \frac{1}{p^2 - m^2 + i\varepsilon}.
    \end{multline}
    We can also express this in terms of a momentum space diagram:
    \begin{equation}
        \tikzsetnextfilename{fd-correlator-V=2-momentum-space-a}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (-1.5, 0) -- (-0.5, 0);
            \draw (0.5, 0) -- (1.5, 0);
            \draw[->, >=stealth] (-1.2, 0.2) -- (-0.8, 0.2) node [midway, above] {\(p\)};
            \draw[->, >=stealth] (1.2, 0.2) -- (0.8, 0.2) node [midway, above] {\(p_2\)};
            \draw (0, 0) circle [radius = 0.5];
            \draw[->, >=stealth] (120:0.7) arc (120:60:0.7) node [midway, above] {\(k\)};
            \draw[->, >=stealth] (300:0.7) arc (300:240:0.7) node [midway, below] {\(p - k\)};
        \end{tikzpicture}
    \end{equation}
    Notice that at each vertex the total momentum is conserved, that is the sum of the momenta on the arrows pointing towards the vertex is equal to the sum of the momenta on the arrows pointing out of the vertex.
    
    \section{Feynman Rules}\label{sec:feynman rules scalar correlator momentum space}
    Rather than go through the integrals and Fourier transforms every time we can just start with a diagram in momentum space and use the following Feynman rules to turn it into the momentum space propagator:
    \begin{itemize}
        \item An \(n\)-point correlator has \(n\) lines.
        \item We leave one end of each external line free, and attach the others to vertices.
        \item The \(i\)th external line has momentum \(p_i\), which we take as incoming momentum, pointing towards the vertex, we denote this with an arrow along the line, labelled with the four-momentum.
        \item Four-momenta flow along the arrows and the total momentum is conserved at each vertex, with arrows pointing in taken as positive and arrows pointing out as negative.
        Ensuring this will fix all but \(L\) momenta in a diagram with \(L\) loops.
        \item The value of the diagram is given by the product of the factors
        \begin{itemize}
            \item \(i/(p^2 - m^2 + i\varepsilon)\) for each line with momentum \(p\),
            \item \(1/i\) for each external end of a line (corresponding to the \(1/i\) factor that comes with each \(\diffd{}/{J(x)}\) factor),
            \item \(ig(1/i)^{N}\) for each vertex connecting \(N\) lines,
            \item A diagram with \(L\) loops will have \(L\) internal momenta not fixed by momentum conservation.
            We integrate over each unfixed momentum with measure \(\dl{^Dp}/(2\pi)^D\).
        \item \(1/S\) where \(S\) is the symmetry factor of the diagram given by fixing external lines but allowing internal propagators to vary.
        \end{itemize}
    \end{itemize}
    
    \chapter{Polology}
    \section{Physical States}
    The eigenstates of the Hamiltonian form a complete set of states.
    They can be classified into three types:
    \begin{itemize}
        \item The vacuum state, or ground state, \(\ket{0}\), is the lowest energy eigenstate of the Hamiltonian.
        It corresponds to a state with no particles.
        \item The single particle states, \(\ket{\vv{p}, \sigma}\), are classified by their spatial momentum, \(\vv{p}\), and \(\sigma\), which represents some collection of quantum numbers.
        The energy of these particles is given by the relativistic dispersion relation\footnote{Note that we've been calling this quantity \(\omega(\vv{p})\) in the first half of the course.}
        \begin{equation}
            E_p = \sqrt{\vv{p}^2 + m^2_{\phys}}.
        \end{equation}
        Here \(m_{\phys}\) is the physical mass of the state.
        Note that this is \emph{not} necessarily the same as the mass appearing in the Lagrangian.
        We choose normalisation such that\footnote{Note that we've been calling this quantity \(\delta_{\sigma\sigma'}\bardelta(\vv{p} - \vv{p})\) in the first half off the course.}
        \begin{equation}
            \braket{\vv{p}, \sigma}{\vv{p}', \sigma'} = \delta_{\sigma\sigma'} 2E_p(2\pi)^{D-1}\delta^{(D-1)}(\vv{p} - \vv{p}').
        \end{equation}
        \item The multiparticle states, \(\ket{\vv{P}, n}\), are classified by the total spatial momentum, \(\vv{P}\), and a collection of other quantum numbers, \(n\).
        The energy of the multiparticle state is \(\sqrt{\vv{P}^2 + M^2}\) where \(M^2\) is a parameter included in \(n\), and doesn't necessarily correspond to the mass of the particles.
    \end{itemize}
    
    The completeness relation for the eigenstates of the Hamiltonian is
    \begin{equation}
        \ident = \ket{0} \bra{0} + \sum_\sigma \int \dl{\Omega_p} \, \ket{\vv{p}, \sigma} \bra{\vv{p}, \sigma} + \sum_n \int \dl{\Omega_P} \, \ket{\vv{P}, n} \bra{\vv{P}, n}
    \end{equation}
    where\footnote{Note that we've been calling this quantity \(\invariantmeasure{p}\) in the first half of the course.}
    \begin{equation}
        \dl{\Omega_p} = \frac{\dl{^{D-1}p}}{(2\pi)^{D-1}} \frac{1}{2E_p}.
    \end{equation}
    The sums over \(n\) and \(\sigma\) are just a shorthand meaning we should sum over all discrete parameters and integrate over all continuous parameters.
    
    \section{Correlators in Momentum Space}\label{sec:correlators in momentum space}
    We want to compute the correlator
    \begin{equation*}
        \tilde{G}^{(n)}(p_1, \dotsc, p_n) = \int \dl{^Dx_1} \dotsm \dl{^Dx_n} \, \e^{ip_1\cdot x_1} \dotsm \e^{ip_n\cdot x_n} \bra{0} \timeOrdering[\varphi(x_1) \dotsm \varphi(x_n)] \ket{0}.
    \end{equation*}
    To do so we consider the sector of the \(nD\)-dimensional integration region where, for some \(r\), we have \(x_1^0\) through \(x_r^0\) greater than \(x_{r+1}^0\) through \(x_n^{0}\).
    That is,
    \begin{equation}
        \min\{x_1^0, \dotsc, x_r^0\} \ge \max\{x_{r+1}^0, x_n^0\}.
    \end{equation}
    We can then split the time ordered product:
    \begin{equation}
        \timeOrdering[\varphi(x_1) \dotsm \varphi(x_n)] = \timeOrdering[\varphi(x_1) \dotsm \varphi(x_r)] \timeOrdering[\varphi(x_{r+1}) \dotsm \varphi(x_n)].
    \end{equation}
    
    The contribution from this particular time ordering is
    \begin{multline}\label{eqn:contribution to correlator}
        I = \int_{x_1, \dotsc, x_n} \exp[i(p_1 \cdot x_1 + \dotsb + p_n \cdot x_n)] \heaviside(\min\{x_1^0, \dotsc, x_r^0\} - \max\{x_{r+1}^0, \dotsc, x_n^0\})\\
        \times\bra{0} \timeOrdering[\varphi(x_1) \dotsm \varphi(x_r)] \timeOrdering[\varphi(x_{r+1}) \dotsm \varphi(x_n)] \ket{0}.
    \end{multline}
    We're going to insert a complete set of states between the two time ordered products, but first, we do some rewriting.
    
    Suppose we have an analytic function, \(f\), depending on some value \(z\).
    Consider the following expression, where \(p\) is the momentum operator in the \(z\) direction:
    \begin{equation}
        \exp[ipa] = \exp\left[ a\diffp{}{z} \right] f(z) = \sum_{n = 0}^{\infty} \frac{1}{n!} a^n \diffp[n]{}{z} f(z) = f(z + a).
    \end{equation}
    This is what we mean when we say that momentum is the generator of translations\footnote{For technical details see \course{Symmetries of Particles and Fields}}, exponentiating the momentum operator, up to a factor of \(i\), gives a translation.
    This works in an arbitrary number of dimensions, so we have
    \begin{equation}
        \e^{ip \cdot a} f(z) = f(z + a).
    \end{equation}
    So, the operator \(\exp[i p \cdot a]\) acts on a function to produce a translation.
    Similarly it acts on an operator to produce a translation.
    If \(F(z)\) is an operator then we have
    \begin{equation}
        \e^{ip \cdot a} F(z) \e^{-ip\cdot a} = F(z + a),
    \end{equation}
    where as usual we act on operators by sandwiching them between the operator and its inverse.
    
    We can use this to rewrite our time ordered products.
    Define new variables, \(y_i\), for \(i = 1, \dotsc, r\) such that
    \begin{equation}
        x_i = x_1 + y_i.
    \end{equation}
    Note that \(y_1 = 0\).
    Then we have
    \begin{align}
        \varphi(x_1) \dotsm \varphi(x_r)& = \varphi(x_1) \varphi(x_1 + y_2) \dotsm \varphi(x_1 + y_r)\\
        &= \e^{ip \cdot x_1} \varphi(0) \e^{-ip\cdot x_1} \e^{ip \cdot x_1} \varphi(y_2) \e^{-ip\cdot x_1} \dotsm \e^{ip \cdot x_1} \varphi(y_r) \e^{-ip\cdot x_1}\notag\\
        &= \e^{ip \cdot x_1} \varphi(0)\varphi(y_2) \dotsm \varphi(y_r) \e^{-ip\cdot x_1}.
    \end{align}
    
    We'll focus on the single particle term after inserting the complete set of states, this term has
    \begin{equation}
        \bra{0} \timeOrdering[\varphi(x_1) \dotsm \varphi(x_r)] \ket{\vv{p}, \sigma} \bra{\vv{p}, \sigma} \timeOrdering[\varphi(x_{r+1}) \dotsm \varphi(x_n)].
    \end{equation}
    So, for the first time ordered product we need to compute
    \begin{align}
        \bra{0} \timeOrdering[\varphi(x_1) \dotsm \varphi(x_r)] \ket{\vv{p}, \sigma} &= \bra{0} \e^{ip \cdot x_1} \timeOrdering[\varphi(0)\varphi(y_2) \dotsm \varphi(y_r)] \e^{-ip\cdot x_1} \ket{\vv{p}, \sigma} \notag\\
        &= \e^{-ip\cdot x_1} \bra{0} \timeOrdering[\varphi(0)\varphi(y_2) \dotsm \varphi(y_r)] \ket{\vv{p}, \sigma}.
    \end{align}
    To achieve this last equality we act on the single particle state with \(\exp[-ip \cdot x]\), resulting in the same value, but now \(p\) is the four-momentum of the single particle state rather than the momentum operator, and so this exponential is a scalar.
    We similarly act with \(\exp[ip\cdot x]\) on the vacuum, giving \(\exp[0] = 1\).
    
    We can do the same analysis with the second time ordered product, defining \(y_i\) for \(i = r + 1, \dotsc, n\) such that 
    \begin{equation}
        x_i = x_{r + 1} + y_i.
    \end{equation}
    We then have
    \begin{equation}
        \varphi(x_{r + 1}) \dotsm \varphi(x_n) = \e^{ip\cdot x_{r+1}} \varphi(0)\varphi(y_{r+2}) \dotsm \varphi(y_n) \e^{-ip\cdot x_{r+1}},
    \end{equation}
    and so
    \begin{equation}
        \bra{\vv{p}, \sigma} \timeOrdering[\varphi(x_{r+1}) \dotsm \varphi(x_n)] \ket{0} = \e^{ip\cdot x_{r+1}} \bra{0} \timeOrdering[\varphi(0) \varphi(y_{r+2}) \dotsm \varphi(y_{n})] \ket{0}.
    \end{equation}
    
    Now consider the argument to the step function.
    If we shift all arguments to a \(\min\)/\(\max\) by a fixed amount then the minimum/maximum is found at the shift value plus the minimum/maximum size of the shift, and hence we have
    \begin{multline}
        \min\{x_1^0, \dotsc, x_r^0\} - \max\{x_{r+1}^0, \dotsc, x_n^0\}\\
        = x_1^0 - x_{r+1}^0 + \min\{0, y_2, \dotsc, y_r\} - \max\{0, y_{r+2}, \dotsc, y_n\}.
    \end{multline}
    
    We now use the integral representation of the step function:
    \begin{equation}
        \heaviside(\tau) = -\frac{1}{2\pi i} \int_{-\infty}^{\infty} \dl{\omega} \, \frac{\e^{-i\tau \omega}}{\omega + i\varepsilon}.
    \end{equation}
    For some real value \(\varepsilon > 0\) which we will take to zero after computing the integral.
    Suppose \(\omega = a + ib\).
    Then \(-i\tau\omega = \tau(-ia + b)\).
    In order to close the contour and compute the integral we want to choose the contour such that the exponential goes to zero as \(\omega\) goes to infinity in some way, so that if we close with an arc to complete a semicircle the value of the integral along the arc will vanish by Jordan's lemma\footnote{see \course{Methods of Theoretical Physics} or \course{Methods of Mathematical Physics} for details.}.
    If \(\tau > 0\) then the integral along the arc vanishes when \(b\) goes to \(-\infty\), and so we close in the lower half plane, containing the pole at \(\omega = -i\varepsilon\).
    In this case going anticlockwise about the contour means going from \(\infty\) to \(-\infty\), flipping the limits of the integral, so we include an extra factor of \(-1\) to account for this.
    We can then use the residue theorem to compute the value of the integral:
    \begin{equation}
        -2\pi i \Res\left( \frac{\e^{-i\tau \omega}}{\omega + i\varepsilon}, \omega = -i\varepsilon \right) = -2\pi i\e^{-i\tau (-i\varepsilon)} = -2\pi i\e^{-\varepsilon \tau}.
    \end{equation}
    Taking \(\varepsilon \to 0\), and so taking the exponential term to 1, we have
    \begin{equation}
        \heaviside(\tau) = -\frac{1}{2\pi i} (-2\pi i) = 1.
    \end{equation}
    
    On the other hand, if \(\tau < 0\) then the integral along the arc vanishes when \(b\) goes to \(\infty\), and so we close in the upper half plane, containing no poles, and so by Cauchy's theorem the integral vanishes and
    \begin{equation}
        \heaviside(\tau) = 0.
    \end{equation}
    So, this integral does indeed represent the Heaviside step function.
    
    Using this, and the shifted minimum and maximum, we can rewrite the Heaviside step function in our integral as
    \begin{align}
        &\heaviside(\min\{x_1^-, \dotsc, x_r^0\} - \max\{x_{r+1}^0, \dotsc, x_n^0\})\\
        &= -\frac{1}{2\pi i}\int_{-\infty}^{\infty} \exp[-i\omega(x_1^0 - x_{r+1}^0 + \min\{0, y_2, \dotsc, y_r\} - \max\{0, y_{r+2}, \dotsc, y_n\})]. \notag
    \end{align}
    Now consider \cref{eqn:contribution to correlator}.
    We can write out the part of the integrand with dependence on \(x_1\) and \(x_{r+1}\) as
    \begin{equation*}\label{eqn:x1 and xr+1 dependence of correlator contribution}
        \exp\left[ i\left( \sum_{j = 1}^{r} p_j \cdot x_1 - \omega x_1^0 - p \cdot x_1 \right) \right] \exp\left[ i\left( \sum_{j=r+1}^{n} p_j \cdot x_{r+1} + \omega x_{r+1}^0 + p \cdot x_{r+1} \right) \right].
    \end{equation*}
    
    The sums over \(p_j \cdot x_1\) and \(p_j\cdot x_{r+1}\) come from the exponentials as part of the definition of the Fourier transform after our change of variables, for example we have
    \begin{alignat}{2}
        \e^{ip_2 \cdot x_2} &= \e^{ip_2\cdot(x_1 + y_2)} &&= \e^{ip_2\cdot x_1} \e^{ip_2 \cdot y_2},\\
        \e^{ip_n \cdot x_n} &= \e^{ip_n\cdot(x_{r+1} + y_n)} &&= \e^{ip_n\cdot x_{r+1}} \e^{ip_n \cdot y_n},
    \end{alignat}
    and we aren't considering the \(\exp[-ip_2 \cdot y_2]\) or \(\exp[-ip_n \cdot y_n]\) factors as we're only interested in \(x_1\) and \(x_{r+1}\) dependence here.
    
    The \(\omega x_1^0\) and \(\omega x_{r+1}^0\) terms come from the \(x_1^0\) and \(x_{r^+1}\) terms in the integral representation of the Heaviside step function.
    
    Finally, we have the \(p \cdot x_1\) and \(p \cdot x_{r+1}\) terms from acting on \(\ket{\vv{p}, \sigma}\) with \(\exp[-ip\cdot x_1]\) and \(\exp[ip\cdot x_{r+1}]\).
    
    Note that in \cref{eqn:x1 and xr+1 dependence of correlator contribution} we will be integrating over \(x_1\) and \(x_{r + 1}\) from the Fourier transform, \(\omega\) from the integral representation of the Heaviside step function, and \(p\) from the integral over the momenta of single particle states in the completeness relation.
    
    Computing the integral over this term with all \(x_1\) and \(x_{r+1}\) dependence we get Dirac deltas.
    Splitting into the space and time components we get
    \begin{multline}
        (2\pi)^D\delta^{(D-1)}\bigg( \vv{p} - \sum_{j=1}^{r} \vv{p}_j \bigg) \delta\bigg( E_p + \omega - \sum_{j=1}^{r} p_j^0 \bigg)\\
        \times (2\pi)^D \delta^{(D-1)}\bigg( \vv{p} + \sum_{j=r+1}^{n} \vv{p}_j \bigg) \delta\bigg( E_p + \omega + \sum_{j = r+1}^{n} p_j^0 \bigg)
    \end{multline}
    where \(E_p = p^0\).
    
    Taking this we can compute the single particle contribution to \cref{eqn:contribution to correlator} is
    \begin{align}
        &\int_{y_2, \dotsc y_r, y_{r+2}, \dotsc y_n} \exp[i(p_2 \cdot y_2 + \dotsb + p_r \cdot y_r + p_{r+2} \cdot y_{r+2} + \dotsb + p_n \cdot y_n)] \notag\\
        &\times \left( -\frac{1}{2\pi i} \right) \int \dl{\omega} \, \frac{1}{\omega + i\varepsilon} \exp[-i\omega(\min\{0, y_2, \dotsc, y_r\} - \max\{0, y_{r+2}, \dotsc, y_{n}\})]\notag\\
        &\times \sum_\sigma \int \dl{\Omega_p} \, \bra{0} \timeOrdering[\varphi(0) \varphi(y_2) \dotsm \varphi(y_r)] \ket{\vv{p}, \sigma} \bra{\vv{p}, \sigma} \timeOrdering[\varphi(0) \varphi(y_{r+2} \dotsm \varphi(y_n))] \ket{0}\notag\\
        &\times (2\pi)^D \delta^{(D-1)} \bigg( \vv{p} - \sum_{j=1}^{r} \vv{p}_j \bigg) \delta\bigg(E_p + \omega - \sum_{j=1}^{r} p_j^0 \bigg)\notag\\
        &\times (2\pi)^D \delta^{(D-1)} \bigg( \vv{p} + \sum_{j=r+1}^{n} \vv{p}_j \bigg) \delta\bigg( E_p + \omega + \sum_{j = r+1}^{n} p_j^0 \bigg).
    \end{align}
    While this looks daunting we can simplify it greatly using the Dirac deltas.
    Performing the integral over \(p\), with the measure \(\dl{\Omega_p}\) we set
    \begin{equation}
        \vv{p} = \sum_{j=1}^{r}\vv{p}_j = - \sum_{r+1}^{n} \vv{p}_j \eqqcolon \vv{q},
    \end{equation}
    and performing the \(\omega\) integral we set
    \begin{equation}
        \omega = \sum_{j=1}^{r} p_j^0 - E_p = -\sum_{j=r+1}^{n} p_j^0 - E_p \coloneqq q^0 - E_p.
    \end{equation}
    That is, we define the four-momentum
    \begin{equation}
        q = \sum_{j=1}^{r} p_j = -\sum_{j=r+1}^{n} p_j.
    \end{equation}
    We can also cancel one of the factors of \((2\pi)^D\) with the factor in the integration measure \(\dl{\Omega_p}\).
    We are then left with a factor of \(1/(2E_p)\) from this measure.
    
    The result of these steps is
    \begin{align}
        &(2\pi)^D \delta^{(D)}\bigg( \sum_{j=1}^{n} p_j \bigg) \left( -\frac{1}{2\pi i} \right) \frac{1}{q^0 - E_p + i\varepsilon} \frac{1}{2E_p}\\
        &\times \int_{y_2, \dotsc, y_r, y_{r+2}, \dotsc y_n} \exp[i(p_2 \cdot y_2 + \dotsb + p_r \cdot y_r + p_{r+2} \cdot y_{r+2} + \dotsb + p_n \cdot y_n)] \notag\\
        &\times \exp[-i(q_0 - E_p)(\min\{0, y_2, \dotsc, y_r\} - \max\{0, y_{r+2}, \dotsc, y_{n}\})]\notag\\
        &\times \sum_\sigma \bra{0} \timeOrdering[\varphi(0) \varphi(y_2) \dotsm \varphi(y_r)] \ket{\vv{q}, \sigma} \bra{\vv{q}, \sigma} \timeOrdering[\varphi(0) \varphi(y_{r+2}) \dotsm \varphi(y_n)] \ket{0} \notag
    \end{align}
    
    Notice that we have a pole when \(q^0 - E_p + i\varepsilon = 0\).
    This means there is a pole whenever some sum of \(r\) momenta is on shell.
    The residue at this pole will be the matrix elements,
    \begin{equation}
        \bra{0} \timeOrdering[\varphi(0) \varphi(y_2) \dotsm \varphi(y_r)] \ket{\vv{q}, \sigma} \bra{\vv{q}, \sigma} \timeOrdering[\varphi(0) \varphi(y_{r+2}) \dotsm \varphi(y_n)] \ket{0},
    \end{equation}
    times the constant factors.
    
    This result can be written more compactly as
    \begin{multline}
        (2\pi)^D \delta(p_1 + \dotsb + p_n) \frac{1}{q^2 - m_{\phys}^2 + i\varepsilon}\\
        \times \sum_{\sigma} M_{0|q\sigma}(p_2, \dotsc, p_r) M_{q\sigma|0}(p_{r+2}, \dotsc, p_n)
    \end{multline}
    where
    \begin{align}
        M_{0|q\sigma}(p_2, \dotsc, p_r) &= \int \dl{^Dy_2} \dotsm \dl{^Dy_r} \, \e^{ip_2 \cdot y_2} \dotsm \e^{ip_r \cdot y_r}\notag\\
        &\qquad\qquad\times \bra{0} \timeOrdering[\varphi(0) \varphi(y_2) \dotsm \varphi(y_r)] \ket{q, \sigma},\\
        M_{q\sigma|0}(p_{r + 2}, \dotsc, p_n) &= \int \dl{^Dy_{r+1}} \dotsm \dl{^Dy_n} \, \e^{ip_{r+2} \cdot x_{r+2}} \dotsm \e^{ip_n \cdot y_n} \notag\\
        &\qquad\qquad\times \bra{q, \sigma} \timeOrdering[\varphi(0) \varphi(y_{r+2}) \dotsm \varphi(y_n)] \ket{0}.
    \end{align}
    
    To see how we go from \(q^0 - E_p + i\varepsilon\) to \(q^2 - m_{\phys}^2 + i\varepsilon\) consider the following limit:
    \begin{align}
        \lim_{q^0 \to E_p} \frac{1}{q^0 - E_p} \frac{1}{2E_p} &= \lim_{q^0 \to E_p} \frac{1}{q^0 - E_p} \frac{1}{E_p + E_p}\\
        &= \lim_{q^0 \to E_p} \frac{1}{q^0 - E_p} \frac{1}{q^0 + E_p}\\
        &= \lim_{q^0 \to E_p} \frac{1}{(q^0)^2 - E_p^2}\\
        &= \lim_{q^0 \to E_p} \frac{1}{(q^0)^2 - \vv{q}^2 - m_{\phys}^2}\\
        &= \lim_{q^0 \to E_p} \frac{1}{q^2 - m_{\phys}^2}.
    \end{align}
    So, when \(q^0 \approx E_p\), so near the pole, we can change \(q^0 - E_p\) to \(q^2 - m_{\phys}^2\).
    This makes it more obvious that the pole occurs when some sum of momenta goes on-shell.
    
    \chapter{K\"all\'en--Lehmann Representation}
    \section{Motivation}
    In the previous chapter we didn't consider the contribution due to multiparticle states, we just looked at single particle states.
    In this chapter we'll look specifically at the two-point correlation function and we'll consider multiparticle states.
    We'll also drop the dependence on other quantum numbers, \(\sigma\), for the single particle state.
    
    The two point correlator is defined as
    \begin{equation}
        G^{(2)}(x, y) \coloneqq \bra{0} \timeOrdering[\varphi(x)\varphi(y)] \ket{0} = i\Delta_{\feynman}(x - y).
    \end{equation}
    The Fourier transform of this is
    \begin{align}
        \tilde{G}^{(2)}(p_1, p_2) &= \int \dl{^Dx}\dd{^Dy} \, \e^{ip_1 \cdot x} \e^{ip_2 \cdot y} i\Delta(x - y)\\
        &= \int \dl{^D\xi} \dd{^Dy} \e^{ip_1 \cdot \xi} \e^{i(p_1 + p_2) \cdot y} i\Delta(\xi)\\
        &= (2\pi)^D \delta(p_1 + p_2) \int \dl{^D\xi} \e^{ip\cdot \xi} i\Delta(\xi)
    \end{align}
    where we've made the substitution \(\xi = x - y\).
    
    We make the normalisation choice that
    \begin{equation}
        \bra{0} \varphi(x)\ket{0} = 0, \qqand \bra{\vv{p}} \varphi(0) \ket{0} = 1,
    \end{equation}
    where \(\ket{\vv{p}}\) is the single particle state.
    With these choices the free theory result is that the Fourier transformed propagator is
    \begin{equation}
        \tilde{\Delta}_{\feynman}(p) = \frac{i}{p^2 - m^2 + i\varepsilon} = \hbox{\includegraphics{tikz-external/fd-correlator-V=0-momentum-space}}
    \end{equation}
    That is, we have \(m_{\phys} = m\), where \(m_{\phys}\) is the mass of the particles and \(m\) is the mass parameter appearing in the Lagrangian.
    
    \section{Interacting Theory}
    Consider the case when \(x^0 > y^0\), so \(\timeOrdering[\varphi(x)\varphi(y)] = \varphi(x)\varphi(y)\).
    Then we can insert the identity in the form of the complete set of states summing over the vacuum, single particle states, and multiparticle states, and we get
    \begin{align}
        \bra{0} \varphi(x)\varphi(y) \ket{0} &= \bra{0} \varphi(x) \ket{0} \bra{0} \varphi(y) \ket{0} + \int \dl{\Omega_p} \bra{0} \varphi(x) \ket{\vv{p}} \bra{\vv{p}} \varphi(y) \ket{0} \notag\\
        &\qquad + \sum_n \int \dl{\Omega_P} \bra{0} \varphi(x) \ket{\vv{P}, n} \bra{\vv{P}, n} \varphi(y) \ket{0}.
    \end{align}
    Notice that the completeness relation is defined by summing over all physical states, that means all states we integrate over are on-shell.
    In particular, if we have a single particle state with momentum \(p^\mu = (p^0, \vv{p})\) then we have \(E_p = p^0 = \sqrt{\vv{p}^2 + m^2}\) and so we must have \(p^2 = m_{\phys}^2\), since \(E_p = \sqrt{\vv{p}^2 + m_{\phys}^2}\) by definition.
    Similarly, we have \(P^0 = \sqrt{\vv{P}^2 + M^2}\), and so \(P^2 = M^2\).
    
    Consider the field \(\varphi(x)\).
    We can write this in terms of \(\varphi(0)\) by introducing the translation operator, \(\exp[i\operator{P} \cdot x]\), and we then have
    \begin{equation}
        \varphi(x) = \e^{i\operator{P} \cdot x} \varphi(0) \e^{-i\operator{P} \cdot x},
    \end{equation}
    and we can do the same for \(\varphi(y)\).
    We then have
    \begin{equation}
        \bra{0} \varphi(x) \ket{\vv{p}} = \bra{0} \varphi(0) \e^{-ip \cdot x} \ket{\vv{p}} = \bra{0} \varphi(0) \e^{-ip \cdot x} \ket{\vv{p}}
    \end{equation}
    and this exponential is just a numerical factor.
    Similarly, we have
    \begin{equation}
        \bra{\vv{p}} \varphi(y) \ket{0} = \bra{\vv{p}} \e^{i\operator{P} \cdot y} \varphi(0) \e^{-i\operator{P} \cdot y} \ket{0} = \bra{\vv{p}} \e^{ip \cdot y} \varphi(0) \ket{0}.
    \end{equation}
    We can also do the same in the multiparticle state, and we just replace \(p\) with \(\vv{P}\) and \(\ket{\vv{p}}\) with \(\ket{\vv{P}, n}\).
    Doing the same in the vacuum term leaves us with factors of \(\ket{0} \varphi(0) \bra{0} = 0\), so this term vanishes.
    Furthermore, for the single particle state we have \(\bra{0} \varphi(0) \ket{\vv{p}} = 1\), and so the single particle state simply becomes an integral over an exponential factor, leaving us with
    \begin{equation}
        \bra{0} \varphi(x) \varphi(y) \ket{0} = \int \dl{\Omega_p} \, \e^{-ip \cdot (x - y)} + \sum_n \int \dl{\Omega_P} \, \e^{-iP\cdot (x - y)} \abs{\bra{0} \varphi(0) \ket{\vv{P}, n}}^2.
    \end{equation}
    It would be tempting to compute the integral now, but we have to be careful as the integral is over states which are on-shell, and whether a state is on-shell depends on \(M^2\), which is one of the parameters that we are summing over with \(\sum_n\).
    
    We define the \defineindex{spectral density}, \(\rho\), to be
    \begin{equation}
        \rho(s) \coloneqq \sum_n \abs{\bra{0} \varphi(0) \ket{\vv{P}, n}}^2 \delta(s - M^2).
    \end{equation}
    This is a function which is given by a sum over matrix elements when \(s = M^2\).
    This allows us to turn a sum over \(n\) into an integral over \(s\), in a density-of-states type argument.
    We then have
    \begin{equation}
        \bra{0} \varphi(x) \varphi(y) \ket{0} = \int \dl{\Omega_p} \, \e^{-ip\cdot (x - y)} + \int_{4m_{\phys}^2}^{\infty} \!\! \dl{s} \, \rho(s) \int \dl{\Omega_P} \e^{-iP \cdot (x - y)}.
    \end{equation}
    We start the integral at \(4m_{\phys}^2\) since we must have \(M \ge 2m\) in order for a two-point process to occur, and so \(M^2 \ge 4m_{\phys}^2\).
    
    If \(x^0 < y^0\) then we simply swap \(x\) and \(y\):
    \begin{align}
        \bra{0} \timeOrdering[\varphi(x) \varphi(y)]\ket{0} &= \bra{0} \varphi(y) \varphi(x) \ket{0}\\
        &= \int \dl{\Omega_p} \, \e^{-ip\cdot(y - x)} + \int_{4m_{\phys}^2}^{\infty} \!\! \dl{s} \, \rho(s) \int \dl{\Omega_P} \e^{-iP\cdot (y - x)}.\notag
    \end{align}
    In general we then have
    \begin{equation}
        \bra{0} \timeOrdering[\varphi(x) \varphi(y)] \ket{0} = \heaviside(x^0 - y^0) \bra{0} \varphi(x) \varphi(y) \ket{0} + \heaviside(y^0 - x^-) \bra{0} \varphi(y) \varphi(x) \ket{0}.
    \end{equation}
    
    We can then evaluate this using the contour representation of the propagator:
    \begin{align}
        \Delta_{\feynman}(x - y) &= \int \frac{\dl{^Dp}}{(2\pi)^D} \frac{\e^{-ip\cdot (x - y)}}{p^2 - m_{\phys}^2 + i\varepsilon}\\
        &= \int \frac{\dl{^{D-1}p}}{(2\pi)^{D-1}} \e^{i\vv{p} \cdot (\vv{x} - \vv{y})} \int \frac{\dl{p^0}}{2\pi} \frac{\e^{-ip^0(x^0 - y^0)}}{p^2 - m_{\phys}^2 + i\varepsilon}.
    \end{align}
    We integrate along the real axis in the complex \(p^0\) plane.
    There are poles just below the real axis at \(E_p\) and just above the real axis at \(-E_p\).
    First suppose \(x^0 > y^0\), as can be seen in \cref{fig:contour for feynman propagator with i epsilon prescription}.
    Then \(\exp[-ip^0(x^0 - y^0)]\) vanishes as \(p^0 \to \infty\), so we close the contour with a semicircle in the upper half plane.
    The integral along this semicircle vanishes by Jordan's lemma and we're left with just the integral along the real axis.
    Using the residue theorem the value is then given by the residue at the pole positioned just above \(-E_p\).
    Focussing just on the \(p^0\) integral we have
    \begin{align}
        I &= \int \frac{\dl{p^0}}{2\pi} \frac{\e^{-ip^0(x^0 - y^0)}}{p^2 - m_{\phys}^2 + i\varepsilon}\\
        &= \int \frac{\dl{p^0}}{2\pi} \frac{\e^{-ip^0(x^0 - y^0)}}{(p^0 - E_p + i\varepsilon)(p^0 + E_p - i\varepsilon)}\\
        &= 2\pi i \Res\left( \frac{\e^{-ip^0(x^0 - y^0)}}{(p^0 - E_p + i\varepsilon)(p^0 + E_p - i\varepsilon)}, p^0 = -E_p + i\varepsilon \right)\\
        &= 2\pi i \frac{\e^{-ip^0(x^0 - y^0)}}{p^0 + E_p - i\varepsilon} \bigg|_{p_0 = -E_p + i\varepsilon}.
    \end{align}
    Note that the \(i\varepsilon\) appearing in the second line is \emph{not} the same as the \(i\varepsilon\) appearing in the first line.
    We could compute some function of \(i\varepsilon\) and \(E_p\) such that this is the case, but all that really matters is that we have a small positive \(\varepsilon > 0\) such that the \(i\varepsilon\) allows us to avoid the contours, so we simply rename \(\varepsilon\) as we go.
    The full integral is then
    \begin{equation}
        \Delta_{\feynman}(x - y) = \int \dl{\Omega_p} \e^{-ip \cdot (x - y)}
    \end{equation}
    for \(x^0 > y^0\), and we can do something similar if \(x^0 < y^0\) closing instead in the lower half plane.
    
    The result of this computation is then
    \begin{align}
        &\bra{0} \timeOrdering[\varphi(x)\varphi(y)] \ket{0}\\
        &\qquad = \int \frac{\dl{^Dp}}{(2\pi)^D} \e^{-ip\cdot(x - y)} \left[ \frac{i}{p^2 - m_{\phys}^2 + i\varepsilon} + \int_{4m_{\phys}^2}^{\infty} \!\! \dl{s} \, \rho(s) \frac{i}{p^2 - s + i\varepsilon} \right]. \notag
    \end{align}
    The propagator in momentum space is then given by
    \begin{equation}
        \tilde{\Delta}_{\feynman}(p) = \frac{i}{p^2 - m_{\phys}^2 + i\varepsilon} + \int_{4m_{\phys}^2}^{\infty} \!\! \dl{s} \, \rho(s) \frac{i}{p^2 - s + i\varepsilon}.
    \end{equation}
    
    \chapter{LSZ, Optical Theorem, and Ward Identities}
    \section{The \texorpdfstring{\(S\)}{S} Matrix}
    In a scattering process with an arbitrary number of initial and final particles we need to compute the overlap of a state in the distant past, called the in state, and a state in the distant future, called the out state.
    The most common example is a \(2 \to n\) scattering, where we start with two particles of momenta \(\vv{k_1}\) and \(\vv{k_2}\), and end with \(n\) particles of momenta \(\vv{p_1}, \dotsc, \vv{p_n}\).
    We then want to compute
    \begin{equation}
        \braket{\vv{p_1}, \dotsc, \vv{p_n}; \text{out}}{\vv{k_1}, \vv{k_2}; \text{in}},
    \end{equation}
    where \(\ket{\vv{k_1}, \vv{k_2}; \text{in}}\) is considered to be the state \(\ket{\vv{k_1}, \vv{k_2}, t = -\infty}\) and \(\ket{\vv{p_1}, \dotsc, \vv{p_n}; \text{out}}\) is the state \(\ket{\vv{p_1}, \dotsc, \vv{p_n}, t = +\infty}\).
    The \(S\) matrix is defined to be the operator such that we can take the states at any time and compute the matrix element of \(S\) and get the result above, that is
    \begin{equation}
        \braket{\vv{p_1}, \dotsc, \vv{p_n}; \text{out}}{\vv{k_1}, \vv{k_2}; \text{in}} = \bra{\vv{p_1}, \dotsc, \vv{p_n}} S \ket{\vv{k_1}, \vv{k_2}}.
    \end{equation}
    We usually write the \(S\) matrix as
    \begin{equation}
        S = \ident + iT,
    \end{equation}
    where \(T\) is called the \defineindex{transition matrix}.
    This corresponds to the two things that can happen in a scattering process, either nothing happens, corresponding to \(\ident\), or we have some sort of transition from the input state to another state, so we get end up with the state \(iT\ket{\vv{k_1}, \vv{k_2}}\).
    We then compute the overlap of this with the desired final state to work out the probability of a particular outcome.
    
    \section{LSZ Reduction}
    \defineindex{LSZ reduction}, named for Harry Lehmann, Kurt Symanzik, and Wolfhart Zimmermann, is the result of setting \(r = 1\) in \cref{sec:correlators in momentum space}.
    This means there will be a pole in the momentum space correlator whenever a single field is on-shell.
    The \(n\)-point correlation function then has, at least, \(n\) poles, each corresponding to one of the momenta \(p_i^2 = m_{\phys}^2\).
    Note that we count higher order poles as multiple poles here.
    The residue at this multiple pole point gives the \(S\) matrix for an \(n\) particle scattering process.
    Suppose we have \(m\) particles in the initial state and \(m'\) particles in the final state, with \(m + m' = n\).
    Then we consider the residue at the pole given by taking \(m\) of the initial particles, with momenta \(p_j\), to have \(p_j^2 = m_{\phys}^2\), and \(m'\) of the final particles, with momenta \(p'_k\), to have \(p_k^{\prime 2} = m_{\phys}^2\).
    This gives
    \begin{align}
        \braket{p_1', \dotsc, p'_{m'}; \text{out}}{p_1, \dotsc, p_m; \text{in}} &= \bra{p'_1, \dotsc, p'_m} S \ket{p_1, \dotsc, p_m}\\
        &\mkern-210mu= \lim_{p_j^2, p_k^{\prime 2} \to m_{\phys}^2} \prod_{k=1}^{m'} (p_k^{\prime 2} - m_{\phys}^2 + i\varepsilon) \prod_{j=1}^{m} (p_j^2 - m_{\phys}^2 + i\varepsilon) \notag\\
        &\times \tilde{G}^{(n)}(p_1, \dotsc, p_m, -p'_1, \dotsc, -p'_{m'}).
    \end{align}
    This holds when
    \begin{equation}
        \bra{\vv{p}} \varphi(0) \ket{0} = 1.
    \end{equation}
    
    The LSZ reduction formula above gives a way to represent quantum amplitudes with Feynman diagrams in momentum space.
    We can compute the correlator by adapting the Feynman rules in \cref{sec:feynman rules scalar correlator momentum space} with the following modifications:
    \begin{itemize}
        \item We associate an outgoing momentum to the external lines that correspond to particles in the final state, giving a relative minus sign compared to the normal Feynman rules when all momenta are taken to be inwards.
        \item We multiply each external line by a factor of \(-i(p^2 - m_{\phys}^2 + i\varepsilon)\), this is part of computing the residue.
        Correlators multiplied by these factors are called \define{truncated correlators}\index{truncated correlator|see{amputated correlator}} or \define{amputated correlators}\index{amputated correlator}
    \end{itemize}
    
    \section{Optical Theorem}
    Physical constraints are expressed by relations between correlators.
    One such physical constraint is the unitarity of the \(S\) matrix, corresponding to conservation of probability.
    If we have \(S^\hermit S = \ident\) and \(S = \ident + iT\) then
    \begin{equation}
        (\ident + iT)^\hermit (\ident + iT) = (\ident - iT^\hermit)(\ident + iT) = \ident -iT^\hermit + iT^\hermit + T^\hermit T
    \end{equation}
    and so we must have
    \begin{equation}
        -iT^\hermit + iT^\hermit + T^\hermit T = 0 \implies -i(T - T^\hermit) = T^\hermit T.
    \end{equation}
    Consider a matrix element of this equation between some initial state, \(\ket{a}\), and some final state, \(\ket{b}\).
    We also factor out a Dirac delta corresponding to conservation of total momentum, so we have
    \begin{equation}
        \bra{b} T \ket{a} = (2\pi)^D\delta(P_a - P_b) \amplitude(a \to b)
    \end{equation}
    where \(P_a\) and \(P_b\) are the total momenta of the states \(\ket{a}\) and \(\ket{b}\).
    This equation defines \(\amplitude(a \to b)\), the amplitude for the scattering process where we start in state \(\ket{a}\) and end in state \(\ket{b}\).
    The left hand side gives us
    \begin{align}
        -i\bra{b}T\ket{a} + i\bra{b}T^\hermit\ket{a} &= -i\bra{b}T\ket{a} + i \bra{a}T\ket{a}^*\\
        &= -i(2\pi)^D \delta(P_a - P_b) [\amplitude(a \to b) - \amplitude(b \to a)^*].
    \end{align}
    On the right hand side if we insert a complete set of states, \(\{\ket{f}\}\), summed over quantum numbers, \(f\) and integrated over momenta, which we also denote by \(f\), then we have
    \begin{align}
        \bra{b}T^\hermit T\ket{a} &= \sum_{f} \int \dl{\Omega_f} \, \bra{b}T^\hermit\ket{f} \bra{f} T\ket{a}\\
        &\mkern-50mu= \sum_{f} \int \dl{\Omega_f} \, (2\pi)^D\delta(P_b - P_f) \amplitude(b \to f)^* (2\pi)^D \delta(P_f - P_a) \amplitude(a \to f) \notag\\
        &\mkern-50mu= (2\pi)^D \delta(P_a - P_b) \sum_f \int \dl{\Omega_f} \, (2\pi)^D \delta(P_a - P_f) \amplitude(b \to f)^* \amplitude(a \to f) \notag
    \end{align}
    where we use the product of Dirac deltas identity \(\delta(x - y)\delta(y - z) = \delta(x - z) \delta(y - z)\), essentially, setting \(x\) to \(y\) then \(y\) to \(z\) is the same as setting \(x\) to \(z\) and \(y\) to \(z\).
    
    Now consider the special case where \(a = b\), then the left hand side gives us
    \begin{equation}
        -i(2\pi)^D \delta(0) [\amplitude(a \to a) - \amplitude(a \to a)^*] = -i(2\pi)^D \delta(0) 2i\Im[\amplitude(a \to a)],
    \end{equation}
    where we've used \(z - z^* = x + iy - (x - iy) = 2iy\).
    The right hand side gives
    \begin{align}
        \bra{b} T^\hermit T\ket{a} &= (2\pi)^D \delta(0) \sum_f \int \dl{\Omega_f} \, (2\pi)^D \delta(P_a - P_f) \amplitude(a \to f)^* \amplitude(a \to f) \notag\\
        &= (2\pi)^D \delta(0) \sum_f \int \dl{\Omega_f} \, (2\pi)^D \delta(P_a - P_f) \abs{\amplitude(a \to f)}^2.
    \end{align}
    Equating these we have
    \begin{equation}
        2\Im[\amplitude(a \to a)] = \sum_f \int \dl{\Omega_f} \, (2\pi)^D \delta(P_a - P_f) \abs{\amplitude(a \to f)}^2.
    \end{equation}
    This is the \defineindex{optical theorem}.
    It relates the imaginary part of the amplitude for \(a \to a\) to the total cross section for \(a \to f\) summed over all final states.
    
    \section{Ward Identities}
    Another example of physical constraints giving relationships between correlators are the Ward identities.
    These are equalities between field correlators due to symmetries of the system.
    In classical mechanics symmetries correspond to conserved currents, by Noether's theorem.
    The Ward identities are the quantum field theory analogue of this.
    
    Start by considering the symmetry transformation of the field
    \begin{equation}
        \varphi(x) \mapsto \varphi'(x) = \varphi(x) + \varepsilon \delta\varphi(x)
    \end{equation}
    such that for some constant \(\varepsilon > 0\) the action is unchanged.
    If we then allow \(\varepsilon\) to depend on position the variation in the action is given by
    \begin{equation}
        \delta S = \int \dl{^Dx} \diffd{S}{\varphi(x)} \varepsilon(x) \delta\varphi(x) = -\int \dl{^Dx} \, \varepsilon(x) \partial_\mu j^\mu(x)
    \end{equation}
    where the last step is to integrate by parts.
    Here \(j^\mu\) is exactly the conserved current from Noether's theorem.
    The transformation to perform a change of integration variables in the following path integral:
    \begin{equation}
        \int \DL{\varphi} \, \e^{iS[\varphi]} O(\varphi) = \int \DL{\varphi'} \, \e^{iS[\varphi']}O(\varphi')
    \end{equation}
    where \(O\) is some generic function of the field.
    Expanding the right hand side to order \(\varepsilon\) we have
    \begin{equation}
        \int \DL{\varphi} \, \e^{iS[\varphi]} O(\varphi) = \int \DL{\varphi} \,  e^{iS[\varphi]} (1 + i\delta S[\varphi]) (O(\varphi) + \delta O).
    \end{equation}
    Notice that taking the \(1\) term from \(1 + i\delta S[\varphi]\) we just get the left hand side, so we can cancel these and consider just the \(i\delta S[\varphi]\) term.
    We can substitute in the expression for \(\delta S\) from before and a similar expression for the variation in \(O\), that is \(\delta O\), giving:
    \begin{equation}
        \int \DL{\varphi} \, \e^{iS[\varphi]} \left[ -i \int \dl{TDx}\varepsilon(x) \partial_\mu j^\mu(x) O(\varphi) + \int \dl{^Dx} \diff{O(\varphi)}{\varphi(x)} \varepsilon(x) \delta\varphi(x) \right] = 0.
    \end{equation}
    We can rearrange this and write it as
    \begin{equation}
        \int \dl{^Dx} \, \varepsilon(x) \left[ -i \expected{\partial_\mu j^\mu(x) O(\varphi)} + \expected*{\diffd{O(\varphi)}{\varphi(x)} \delta\varphi(x)} \right] = 0.
    \end{equation}
    This is sometimes called the \defineindex{integrated Ward identity}.
    It holds for all \(\varepsilon\), vanishing on the boundary, and so we get the \defineindex{Ward identity}
    \begin{equation}
        -i\expected{\partial_\mu j^\mu(x)O(x)} + \expected*{\diffd{O(\varphi)}{\varphi(x)} \delta \varphi(x)} = 0.
    \end{equation}
    
    This result encodes two important physical statements:
    \begin{itemize}
        \item Symmetry in QFT becomes relations between correlators. This concept holds beyond perturbation theory and is important when doing renormalisation.
        \item Current conservation in QFT is realised at the level of the insertion of \(\partial_\mu j^\mu\) in field correlators, up to terms from the variation of \(O\).
        If \(O\) is a product of local fields then this variation is localised in spacetime.
        The contributions are then all proportional to Dirac deltas.
        These terms are called \define{contact terms}\index{contact term}.
    \end{itemize}
    Note that we assumed invariance of the integration measure under this transformation, that is \(\DL{\varphi} = \DL{\varphi'}\).
    If this isn't the case we get extra terms in the Ward identities, which we call the \defineindex{anomalous Ward identities}\index{Ward identity!anomalous}.
    
    \part{Fermion Fields}
    \chapter{Fermion Fields}
    \section{Fermion Fields as Grassmann Variables}
    The operators corresponding to fermionic fields anticommute.
    However, in a path integral we don't have operators.
    Instead we treat the fermionic fields, \(\psi\) and \(\diracadjoint{\psi}\), as independent Grassmann variables.
    See \cref{app:grasmmann variables} for a more detailed explanation, but essentially a Grassmann algebra consists of mutually anticommuting generators, \(\vartheta_i\) and \(\diracadjoint{\vartheta}_i\), such that
    \begin{equation}
        \anticommutator{\vartheta_i}{\vartheta_j} = \vartheta_i\vartheta_j + \vartheta_j\vartheta_i = 0.
    \end{equation}
    Notice that this implies \(\vartheta_i^2 = 0\), since we have \(\vartheta_i\vartheta_i = -\vartheta_i\vartheta_i\), where we've swapped \(\vartheta_i\) and \(\vartheta_i\) on the right hand side, you just can't tell.
    All terms in the Grassmann algebra are then linear combinations of products of these generators where each appears at most once in the product.
    
    So, if \(\psi\) is a fermionic field, that is a spinor, we have the anticommutation relation
    \begin{equation}
        \anticommutator{\psi_\alpha(x)}{\psi_\beta(y)} = 0.
    \end{equation}
    Here \(\alpha\) and \(\beta\) are spinor indices.
    In order to compute anything in the path integral formalism we need to be able to take functional derivatives.
    For this to work consistently with the Grassmann algebra we need the functional derivative with respect to a Grassmann variable to be another Grassmann variable.
    This means that if we apply these to some functional, \(F\), we have
    \begin{equation}
        \diffd{F}{\psi_\alpha(x),\psi_\beta(y)} = -\diffd{F}{\psi_\beta(x)\psi_\alpha(x)},
    \end{equation}
    and in particular this implies
    \begin{equation}
        \diffd{F}{\psi_\alpha(x),\psi_\alpha(x)} = 0.
    \end{equation}
    
    We will have terms of the form
    \begin{equation}
        \int \dl{^Dy} \, [\diracadjoint{\eta}(y) \psi(y) + \diracadjoint{\psi}(y)\eta(y)]
    \end{equation}
    appearing in the (exponent in the) path integral.
    Here \(\eta\) and \(\diracadjoint{\eta}\) are two independent source terms/
    As with the scalar case we take derivatives with respect to the source terms to generate factors of the fields.
    We can then use
    \begin{align}
        I_1 &= \diffd{}{\diracadjoint{\eta}(x)} \int\dl{^Dy} \, \diracadjoint{\eta}(y)\psi(y)\\
        &= \int\dl{^Dy} \, \diffd{\diracadjoint{\eta}(y)}{\diracadjoint{\eta}(x)} \psi(y)\\
        &= \int\dl{^Dy} \, \delta(y - x) \psi(y)\\
        &= \psi(x).
    \end{align}
    Similarly, being cautious about commuting Grassmann variables and picking up a negative, we have
    \begin{align}
        I_2 &= \diffd{}{\eta(x)} \int\dl{^Dy} \, \diracadjoint{\psi}(y)\eta(y)\\
        &= \int \dl{^Dy} \, \diffd{}{\eta(x)} \diracadjoint{\psi}(y) \eta(y)\\
        &= - \int \dl{^Dy} \, \diracadjoint{\psi}(y) \diffd{}{\eta(x)} \eta(y)\\
        &= - \int \dl{^Dy} \, \diracadjoint{\psi}(y) \delta(x - y)\\
        &= - \diracadjoint{\psi}(x).
    \end{align}
    
    \section{Free Theory}
    In a free theory for fermions the action is given by
    \begin{equation}
        S_0[\psi, \diracadjoint{\psi}] = \int \dl{^Dx} \, \diracadjoint{\psi}(x) (i\slashed{\partial} - m)\psi(x).
    \end{equation}
    Note that the operator \(i\slashed{\partial} - m\) is an operator both on the spacetime, through the derivative, but also on the spinor space, through the \(\gamma^\mu\) hidden in the \(\slashed{\partial} = \gamma^\mu \partial_\mu\).
    
    Roughly we can, when \(D = 4\), think of \(i\slashed{\partial} - m\) as a \(4 \times 4\) matrix on spinor space, made up of derivatives.
    Note that the 4 in \(D = 4\) is the spacetime dimension and the 4 in \(4 \times 4\) is the dimension of the spinor space.
    These values are related, but not necessarily equal, their equality here is a consequence of the fact that\footnote{see \course{Symmetries of Particles and Fields}} \(2 \times 2 = 2 + 2 = 4\).
    
    We can rewrite the action with spinor indices as follows:
    \begin{equation}
        S_0[\psi, \diracadjoint{\psi}] = \int \dl{^Dx} \, \diracadjoint{\psi}_\alpha(x) (i\slashed{\partial} - m)_{\alpha\beta} \psi_\beta(x).
    \end{equation}
    We can then expand upon what we mean by the operator \((i\slashed{\partial} - m)_{\alpha\beta}\) by noticing that there is an implicit identity multiplied by \(m\), and so
    \begin{equation}
        (i\slashed{\partial} - m)_{\alpha\beta} = (i\gamma^\mu\partial_\mu - mI)_{\alpha\beta} = i\gamma^\mu_{\alpha\beta}\partial_\mu - m\delta_{\alpha\beta}.
    \end{equation}
    
    Taking the derivative of the action with respect to \(\diracadjoint{\psi}\) we get the classical equations of motion
    \begin{equation}
        \diffd{}{\diracadjoint{\psi}(x)} S_0[\psi, \diracadjoint{\psi}] = 0 \implies (i\slashed{\partial} - m)\psi(x) = 0.
    \end{equation}
    That is, the classical equation of motion is simply Dirac's equation.
    This means that in the classical limit the path integral will be dominated by the solutions to the Dirac equation.
    
    Like the scalar case we can write a generating functional for the free theory by introducing two independent sources, \(\eta\) and \(\diracadjoint{\eta}\), and defining
    \begin{equation}
        Z_0[\eta, \diracadjoint{\eta}] \coloneqq \int \DL{\psi} \DD{\diracadjoint{\psi}} \, \exp\{i (S_0[\psi, \diracadjoint{\psi}] + \diracadjoint{\eta} \cdot \psi + \diracadjoint{\psi} \cdot \eta)\}
    \end{equation}
    where
    \begin{equation}
        \diracadjoint{\eta} \cdot \psi \coloneqq \int \dl{^Dy} \, \diracadjoint{\eta}(y) \psi(y), \qqand \diracadjoint{\psi} \cdot \eta \coloneqq \int \dl{^Dy} \, \diracadjoint{\psi}(y) \eta(y).
    \end{equation}
    Notice that the exponent is just a quadratic form in \(\psi\) and \(\diracadjoint{\psi}\), and so this generating functional is just a Gaussian, which we can integrate to give
    \begin{equation}
        Z_0[\eta, \diracadjoint{\eta}] = \exp\left\{ -\int\!\!\dl{^Dx} \int\!\!\dl{^Dy} \, \diracadjoint{\eta}(x) S(x - y) \eta(y) \right\}
    \end{equation}
    where \(S\) is the \define{Feynman propagator}\index{Feynman propagator!for fermions}, and is given by
    \begin{equation}
        S_{\alpha\beta}(x - y) = \int_p \e^{-ip\cdot(x - y)} \frac{i(\slashed{p} + m)_{\alpha\beta}}{p^2 - m^2 + i\varepsilon}.
    \end{equation}
    As with the finite dimensional case for Gaussians \(S\) is the inverse of the kernel of the quadratic form, which is a fancy way fo saying that \(S(x - y)\) is the inverse of \(i\slashed{\partial} - m\).
    
    In the free theory the propagator is given by the two point correlator:
    \begin{equation}
        \bra{0} \timeOrdering[\psi_\alpha(x)\diracadjoint{\psi}_\beta(y)] \ket{0}_0 = S_{\alpha\beta}(x - y).
    \end{equation}
    As with the scalar case we can use functional derivatives acting on the generating functional to calculate this result.
    However, due to the extra negative sign instead of considering \(1/i\) for the derivatives with respect to \(\eta\) we just take \(i\):
    \begin{align}
        \frac{1}{i} &\diffd{}{\diracadjoint{\eta}(x)} i \diffd{}{\eta(y)} Z_0[\eta, \diracadjoint{\eta}]\bigg|_{\eta = \diracadjoint{\eta} = 0}\\
        &= \diffd{}{\diracadjoint{\eta}(x)} \diffd{}{\eta(y)} \exp\left\{ -\int_{w,z} \diracadjoint{\eta}(w) S(w - z) \eta(z) \right\} \bigg|_{\eta = \diracadjoint{\eta} = 0} \notag\\
        &= \diffd{}{\diracadjoint{\eta}(x)} \left( \int_{w,z} \diracadjoint{\eta}(w) S(w - z)\delta(z - y) \right) \notag\\
        &\qquad\qquad \times \exp\left\{ -\int_{w,z} \diracadjoint{\eta}(w) S(w - z) \eta(z) \right\} \bigg|_{\eta = \diracadjoint{\eta} = 0} \notag\\
        \intertext{note that we picked up a minus from the exponent, and a minus for commuting the derivative with \(\diracadjoint{\eta}(w)\)}
        &= \diffd{}{\diracadjoint{\eta}(x)} \left( \int_{w} \diracadjoint{\eta}(w) S(w - y) \right) \notag\\
        &\qquad\qquad \times \exp\left\{ -\int_{w,z} \diracadjoint{\eta}(w) S(w - z) \eta(z) \right\} \bigg|_{\eta = \diracadjoint{\eta} = 0} \notag\\
        &= \left( \int_w \delta(w - x) S(w - y) \right) \exp\left\{ -\int_{w,z} \diracadjoint{\eta}(w) S(w - z) \eta(z) \right\} \notag\\
        &\qquad\qquad \times \left( \int_{w} \diracadjoint{\eta}(w) S(w - y) \right) \left( -\int_{w,z} \delta(w - z)S(w - z)\eta(w - z) \right) \bigg|_{\eta = \diracadjoint{\eta} = 0} \notag\\
        &= S(x - y),
    \end{align}
    setting \(\eta = \diracadjoint{\eta} = 0\) in the last step.
    
    Notice that the linear term in \(p\) means the propagator is no longer symmetric under exchanging \(x\) and \(y\), and so in the diagrammatic representation we now add a line to the propagator to show this:
    \begin{equation}
        S(x - y) = x
        \tikzsetnextfilename{fd-fermion-propagator}
        \feynmandiagram[small, horizontal=i to o]{i -- [fermion] o}; y, \qqand S_{\alpha\beta}(x - y) = x, \alpha \vcenter{\hbox{\includegraphics{tikz-external/fd-fermion-propagator}}} y, \beta.
    \end{equation}
    
    We can compute a general correlator using Wick's theorem modified for Fermions:
    \begin{align}
        &\bra{0} \timeOrdering[\psi_{\alpha_1}(x_1) \dotsm \psi_{\alpha_n}(x_n) \diracadjoint{\psi}_{\beta_1}(y_1) \dotsm \diracadjoint{\psi}_{\beta_n}(y_n)] \ket{0}_0\\
        &= \left( \frac{1}{i} \diffd{}{\diracadjoint{\eta}_{\alpha_1}(x_1)} \right) \dotsm \left( \frac{1}{i} \diffd{}{\diracadjoint{\eta}_{\alpha_n}(x_n)} \right) \left( i \diffd{}{\eta_{\beta_n}(y_n)} \right) \left( i \diffd{}{\eta_{\beta_1}(y_n)} \right) Z_0[\eta, \diracadjoint{\eta}] \bigg|_{\eta = \diracadjoint{\eta} = 0}\\
        &= \sum_{\sigma \in S_n} \sgn(\sigma) S_{\alpha_1\beta_{\sigma(1)}}(x_1 - y_{\sigma(1)})
    \end{align}
    where \(S_n\) is the symmetric group on \(n\) objects, that is the group of all permutations of \(\{1, \dotsc, n\}\), and \(\sigma\) is a permutation sending \(1\) to \(\sigma(1)\) with \(\sgn(\sigma)\) being the number of swaps required to reorder \(\sigma(1), \sigma(2), \dotsc, \sigma(n)\) to \(1,2, \dotsc, n\).
    
    \section{Interacting Theory}
    If interactions occur through a potential \(V(\psi, \diracadjoint{\psi})\) then the action is given by
    \begin{equation}
        S[\psi, \diracadjoint{\psi}] = S_0[\psi, \diracadjoint{\psi}] + \int \dl{^Dx} \, V(\psi, \diracadjoint{\psi}).
    \end{equation}
    The generating functional for the interacting theory is
    \begin{align}
        Z[\eta, \diracadjoint{\eta}] &= \int \DL{\psi} \DD{\diracadjoint{\psi}} \exp\{i (S[\psi, \diracadjoint{\psi}] + \diracadjoint{\eta} \cdot \psi + \diracadjoint{\psi} \cdot \eta)\}\\
        &= \exp\left\{ i \int \dl{^Dx} \, V\left( \frac{1}{i} \diffd{}{\diracadjoint{\eta}(x)}, i \diffd{}{\eta(x)} \right) \right\} Z_{0}[\eta, \diracadjoint{\eta}]\\
        &= \sum_{V = 0}^{\infty} \frac{1}{V!} \left[ i \int \dl{^Dx} \, V\left( \frac{1}{i} \diffd{}{\diracadjoint{\eta}(x)}, i \diffd{}{\eta(x)} \right) \right]^V\\
        &\qquad \times \sum_{P = 0}^{\infty} \frac{1}{P!} \left[ -\int \dl{^Dy} \int\dl{^Dz} \, \diracadjoint{\eta}(y) S(y - z) \eta(z) \right]^P.
    \end{align}
    Here we factor out the interaction from the generating functional leaving the free generating functional.
    We then do the usual trick of writing a function, here the potential, as a Taylor series in \(\psi\) and \(\diracadjoint{\psi}\), and then replacing the fields with derivatives acting on the free generating functional.
    In the last step we expand the two exponentials as power series.
    We choose to normalise such that
    \begin{equation}
        Z[0, 0] = 1.
    \end{equation}
    
    Now consider a the particular potential
    \begin{equation}
        V(\psi, \diracadjoint{\psi}) = \frac{g}{2} \diracadjoint{\psi}(x) \psi(x) \diracadjoint{\psi}(x) \psi(x).
    \end{equation}
    Then, we have
    \begin{align}
        Z[\eta, \diracadjoint{\eta}] &= \sum_{V = 0}^{\infty} \frac{1}{V!} \left[ i \frac{g}{2} \int \dl{^Dx} \, \diffd{}{\diracadjoint{\eta}(x)} \diffd{}{\eta(x)} \diffd{}{\diracadjoint{\eta}(x)} \diffd{}{\eta(x)} \right]^V\\
        &\qquad \times \sum_{P = 0}^{\infty} \frac{1}{P!} \left[ -\int \dl{^Dy} \int\dl{^Dz} \, \diracadjoint{\eta}(y) S(y - z) \eta(z) \right]^P.
    \end{align}
    Consider a term with some fixed values of \(V\) and \(P\).
    We will have \(P\) factors of \(\eta\) to start with in this term, and we remove \(2V\) by differentiating.
    So, we have \(P - 2V\) factors of \(\eta\), and similarly we'll have \(P - 2V\) factors of \(\diracadjoint{\eta}\).
    
    Consider an interaction represented by
    \begin{equation}
        x, \alpha
        \tikzsetnextfilename{fd-generic-1-1-interaction}
        \begin{tikzpicture}[baseline=-3pt]
            \begin{feynman}
                \vertex (i);
                \vertex (o) at (4, 0);
                \vertex (v) at (2, 0);
                \diagram*[small]{i -- [fermion] v [blob] -- [fermion] o};
            \end{feynman}
        \end{tikzpicture}
        y, \beta
    \end{equation}
    where the blob in the middle represents some interaction process with one fermion entering and one fermion leaving.
    We must have one factor of \(\eta\) and one factor of \(\diracadjoint{\eta}\) for the incoming and outgoing fermions.
    We'll consider only connected diagrams.
    
    The \(V = 0\) case must then have \(P = 1\) in order to have \(P - 2V = 1\).
    This is just the propagator term though, corresponding to
    \begin{equation}
        S_{\alpha\beta}(x - y) = x, \alpha \vcenter{\hbox{\includegraphics{tikz-external/fd-fermion-propagator}}} y, \beta.
    \end{equation}
    
    The \(V = 1\) term must have \(P = 3\).
    This means we have three propagators and one vertex.
    Since this is a quartic interaction we'll have four legs connected at a vertex.
    As usual we have to compute a constant factor, lets start with the terms coming from the expansion.
    For \(V = 1\) we'll have one factor of \(i\) and one factor of \(g/2\).
    For \(P = 3\) we have a factor of \(1/3!\).
    Next we have symmetry factors.
    Note that most of the operations that left a diagram invariant in the scalar case are no longer valid, as the propagators are no longer symmetric.
    There are still some choices that we can make when constructing the diagram though.
    First, we have to choose one end before the arrow of the three propagators to correspond to the incoming particle at \(x\).
    We then have to choose one end after of the arrow of the two remaining propagators to correspond to the outgoing particle at \(y\).
    This gives a factor of \(3 \cdot 2\).
    There is one final step.
    We must have two incoming and two outgoing lines at the vertex, that is we must have two propagators start at the vertex and two propagators end at the vertex.
    We've fixed the start of one propagator as \(x\) and the end of another propagator as \(y\).
    We must then take one of the two unfixed starts and fix it as one of the outgoing particles, this gives a factor of 2.
    There are two options here, either we picked the propagator with the other end unfixed, in which case since it can't be external it must loop back around to the vertex, leaving only one way to connect the two propagators with a single fixed end, or we fixed the start of the propagator with the other end fixed at \(y\), in which case we have to connect the unfixed propagator in a loop and the unfixed end of the other propagator goes into the vertex.
    
    The overall factor is
    \begin{equation}
        ig \frac{3\cdot 2 \cdot 2}{3! \cdot 2} = ig.
    \end{equation}
    This shouldn't be a surprising result at this point.
    
    The diagram corresponding to this term is
    \begin{equation}
        \tikzsetnextfilename{fd-fermion-quartic-example}
        \begin{tikzpicture}
            \draw[-{Latex[width=1.5mm]}] (0, 0) -- (1, 0);
            \draw[-{Latex[width=1.5mm]}] (2, 0) node[below] {\(z, \gamma\)} -- (3, 0);
            \draw (0, 0) node [left] {\(x, \alpha\)} -- (4, 0) node [right] {\(y, \beta\)};
            \draw (2, 0) to[out=135, in=180] (2, 1.5);
            \draw (2, 0) to[out=45, in=0] (2, 1.5);
            \draw[-{Latex[width=1.5mm]}] (2.07, 1.5) -- ++ (0.01, 0);
        \end{tikzpicture}
    \end{equation}
    
    The symmetry factor of this diagram is 1, since the arrows mean we can't do any of the swapping or mirroring we did for scalar diagrams.
    The overall value of the diagram is
    \begin{equation}
        ig \int_{x, y} \diracadjoint{\eta}_\alpha(x) S_{\alpha\gamma}(x - z) S_{\gamma\gamma}(z - z) S_{\gamma\beta}(z - y) \eta_\beta(y).
    \end{equation}
    
    
    
    %   Appdendix
    \appendixpage
    \begin{appendices}
        \include{parts/prelim}
        \include{parts/hamiltonian-calculation}
        \chapter{Grassmann Variables}
        \label{app:grasmmann variables}
        \section{What Are They}
        \begin{dfn}{Grassmann Algebra}{}
            Let \(V\) be an \(n\)-dimensional vector space over a field, \(\field\), with a basis \(\{\vartheta_i\}\) for \(i =1, \dotsc, n\).
            The \define{Grassmann algebra}\index{Grassmann!algebra} is defined to be the exterior algebra of \(V\):
            \begin{equation}
                \symcal{A} = \bigwedge V \coloneqq \complex \oplus V \oplus (V \wedge V) \oplus (V \wedge V \wedge V) \oplus \dotsb
            \end{equation}
            where \(\wedge\) is the totally antisymmetric exterior product.
            The elements of this algebra are called \define{Grassmann numbers}\index{Grassmann!number} or \define{Grassmann variables}\index{Grassmann!variable}.
        \end{dfn}
        
        Less abstractly, a Grassmann algebra, \(\symcal{A}\), over \(\reals\) or \(\complex\), is constructed from a set, \(\{\vartheta_i\}\), which can be added and scaled like vectors, and we define an antisymmetric product on these elements, so \(\vartheta_i \vartheta_j = -\vartheta_j\vartheta_i\), or
        \begin{equation}
            \anticommutator{\vartheta_i}{\vartheta_j} = 0.
        \end{equation}
        
        A general element of this algebra is of the form
        \begin{equation}
            z = z_0 + \sum_{k=1}^{n} z_{i_1,i_2,\dotsc,i_k}\vartheta_{i_1}\vartheta_{i_2}\dotsm\vartheta_{i_k},
        \end{equation}
        for \(z_0, z_{i_1, \dotsc, i_k} \in \field\) and \(\vartheta_{i_j}\) the basis vectors of \(V\).
        That is, a general element is a polynomial in the basis vectors such that each basis vector appears at most once in each product, since antisymmetry requires that any term with a repeated basis vector vanishes.
        
        If we have \(n\) generators then the Grassmann algebra is a vector space of dimension \(2^n\).
        We can always pick some canonical ordering for basis vectors, say \(\vartheta_1\), \(\vartheta_2\), and so on, and reorder any term to be in this ordering, up to a sign change.
        The basis for the Grassmann algebra is formed of products \(\vartheta_{i_1} \vartheta_{i_2} \dotsm \vartheta_{i_k}\) with \(1 \le k \le n\), such that \(i_{j} < i_{j+1}\) for all \(j = 1, \dotsc, n\).
        There are \(2^n\) such objects, since there are \(\binom{n}{k}\) terms with \(k\) generators, and
        \begin{equation}
            \sum_{k = 1}^{n} \binom{n}{k} = 2^n
        \end{equation}
        is a well known identity.
        
        \section{Parity}
        \begin{dfn}{Parity}{}
            Let \(\symcal{A}\) be a Grassmann algebra.
            \define{parity}\index{Grassmann!parity} is defined as an automorphism, \(P \colon \symcal{A} \to \symcal{A}\) defined on a single generator by
            \begin{equation}
                P(\vartheta_i) = -\vartheta_i
            \end{equation}
            and as an algebra automorphism we have \(P(\vartheta_i\vartheta_j) = P(\vartheta_i)P(\vartheta_j)\), and so on, and we then extend \(P\) linearly to be defined on all of \(\symcal{A}\).
            
            Parity acting on a monomial gives
            \begin{equation}
                P(\vartheta_{i_1} \dotsm \vartheta_{i_k}) = (-1)^{k} \vartheta_{i_1} \dotsm \vartheta_{i_k}.
            \end{equation}
        \end{dfn}
        
        Parity can be thought of as a generalisation of reflection through the origin.
        As such it defines two eigenspaces, \(\symcal{A}^{\pm}\).
        If \(a \in \symcal{A}^+\) then \(P(a) = a\), and if \(a \in \symcal{A}^-\) we have \(P(a) = -a\).
        Note that a general sum of Grassmann variables may not be in either of these eigenspaces, for example, \(\vartheta_1 + \vartheta_2\vartheta_3\) maps to \(-\vartheta_1 + \vartheta_2\vartheta_3\) under parity.
        
        \section{Grassmann Differentiation}
        \begin{dfn}{Grassmann Differentiation}{}
            For a Grassmann algebra \(\symcal{A}\) we can define \define{Grassmann differentiation}\index{Grassmann!differentation} to be a linear mapping, \(D \colon \symcal{A} \to \symcal{A}\) satisfying the modified product rule
            \begin{equation}
                D(a_1a_2) = P(a_1)D(a_2) + D(a_1)a_2.
            \end{equation}
        \end{dfn}
        This product rule is such that
        \begin{equation}
            DP + PD = 0,
        \end{equation}
        that is differentiation and parity anticommute.
        We can think of the parity operation as arising because we have to commute \(D\) past \(a_1\) in order to take the derivative of \(a_2\).
        
        Note that if \(a \in \symcal{A}^\pm\) then \(D(a) \in \symcal{A}^{\mp}\), that is, the derivative changes the parity of a product of Grassmann variables.
        
        We can introduce differential operators, \(\diffp{}/{\vartheta_i}\), with respect ot a Grassmann variable, \(\vartheta_i\), by requiring that the identity
        \begin{equation}
            \diffp{\vartheta_j}{\vartheta_i} = \delta_{ij}
        \end{equation}
        holds.
        These then satisfy the anticommutation relations
        \begin{equation}
            \diffp{}{\vartheta_i} \diffp{}{\vartheta_j} + \diffp{}{\vartheta_j}\diffp{}{\vartheta_i} = 0, \qqand \vartheta_i \diffp{}{\vartheta_j} + \diffp{}{\vartheta_j}\vartheta_i = 0,
        \end{equation}
        in other words we can define the derivatives and generators as operators on \(\symcal{A}\) satisfying the above anticommutation relations.
        
        Consider some functions \(\sigma \colon \symcal{A} \to \symcal{A}^-\) and \(x \colon \symcal{A} \to \symcal{A}^+\), and another function \(f \colon \symcal{A}^- \times \symcal{A}^+ \to \symcal{A}\).
        Then we have the chain rule
        \begin{equation}
            \diffp{}{\vartheta} f(\sigma, x) = \diffp{\sigma}{\vartheta}\diffp{f}{\sigma} + \diffp{x}{\vartheta}\diffp{f}{x}.
        \end{equation}
        
        Suppose now that \(F \colon \complex \to \complex\) is an analytic function, at least in the region of interest.
        Then this function has a Taylor series at \(0\) given by
        \begin{equation}
            F(x) = F(0) + x\diffp{F}{x}\bigg|_{x=0} + \order(x^2).
        \end{equation}
        We can extend \(F\) to a function of Grassmann variables through its Taylor series, and this is particularly simple, since we have
        \begin{equation}
            F(\vartheta) = F(0) + \vartheta\diffp{F}{x}\bigg|_{x=0}
        \end{equation}
        exactly, since higher powers vanish by antisymmetry.
        
        \section{Grassmann Integration}
        \begin{dfn}{Grassmann Integration}{}
            Let \(\symcal{A}\) be a Grassmann algebra over a field, \(\field\), and \(D \colon \symcal{A} \to \symcal{A}\) a differential operator on \(\symcal{A}\).
            We can define another linear operator \(I \colon \symcal{A} \to \symcal{A}\) called \define{Grassmann integration}\index{Grassmann!integration}.
            It is defined by requiring the following properties:
            \begin{itemize}
                \item \(I\) is linear:
                \begin{equation}
                    I(\lambda_1 a_1 + \lambda_2 a_2) = \lambda_1 I(a_1) + \lambda_2 I(a_2)
                \end{equation}
                for all \(\lambda_1, \lambda_2 \in \field\) and \(a_1, a_2 \in \symcal{A}\).
                \item The derivative anticommutes with parity:
                \begin{equation}
                    PI + IP = 0.
                \end{equation}
                \item \(DI = ID = 0\).
                \item If \(D(a) = 0\) for some \(a \in \symcal{A}\) then \(I(ba) = I(b)a\) for all \(b \in \symcal{A}\).
            \end{itemize}
            Note that all of these properties are satisfied by \(D\), so we can define integration and differentiation to be the same:
            \begin{equation}
                \int \dl{\vartheta} \, a = \diffp{}{\vartheta}a
            \end{equation}
            for all \(\vartheta, a \in \symcal{A}\).
        \end{dfn}
        
        The requirement that \(ID = 0\) is the requirement that the integral of a total derivative vanishes, which we often require for integration by parts, and that an derivative of an integral vanishes is then just a statement that we want to have \(DI + ID = 0\).
        
        This definition gives
        \begin{equation}
            \int \dl{\vartheta} \, f(\vartheta) = \frac{1}{\lambda} \int \dl{\vartheta'} \, f(\lambda \vartheta + \mu)
        \end{equation}
        for \(\lambda, \mu \in \field\).
        This follows by writing \(\vartheta\) as a function of \(\vartheta'\), namely \(\vartheta(\vartheta') = \lambda \vartheta' + \mu\).
        We then have
        \begin{align}
            \int \dl{\vartheta} \, f(\vartheta) &= \diffp*{f(\vartheta)}{\vartheta}\\
            &= \left( \diffp*{f(\vartheta(\vartheta'))}{\vartheta'} \right) \diffp{\vartheta'}{\vartheta} \\
            &= \left( \diffp*{f(\vartheta(\vartheta'))}{\vartheta'} \right) \left( \diffp{\vartheta}{\vartheta'} \right)^{-1} \\
            &= \left( \diffp*{f(\vartheta(\vartheta'))}{\vartheta'} \right) \frac{1}{\lambda}\\
            &= \frac{1}{\lambda} \int \dl{\vartheta'} \, f(\vartheta(\vartheta'))\\
            &= \frac{1}{\lambda} \int \dl{\vartheta'} \, f(\lambda\vartheta' + \mu).
        \end{align}
        This generalises further to
        \begin{equation}
            \int \dl{\vartheta_1} \dotsm \dl{\vartheta_k} = \int \dl{\vartheta'_1} \dotsm \dl{\vartheta'_k} \, J(\vartheta')
        \end{equation}
        where \(J\) is the \emph{inverse} of the Jacobian for normal integration, meaning
        \begin{equation}
            J^{-1} = \det \diffp{\vartheta_i}{\vartheta_j'}.
        \end{equation}
        
        \begin{dfn}{Complex Conjugation}{}
            Let \(\symcal{A}\) be a Grassmann algebra generated by \(\{\vartheta_i\}\) with \(i = 1, \dotsc, n\).
            We can define another Grassmann algebra with \(2n\) generators, \(\{\vartheta_i, \overbar{\vartheta}_i\}\).
            We think of \(\overbar{\vartheta}_i\) as the \define{complex conjugate}\index{Grassmann!complex conjugate} of \(\vartheta_i\).
        \end{dfn}
        This process of taking complex conjugates acts much like the Hermitian conjugate, in particular, \(\overbar{\overbar{\vartheta}}_i = \vartheta_i\), \(\overline{(\lambda a + \mu b)} = \lambda^* \overbar{a} + \mu^* \overbar{b}\) for \(\lambda, \mu \in \field\) and \(a, b \in \symcal{A}\), and \(\overline{ab} = \overbar{b}\overbar{a}\) for \(a, b \in \symcal{A}\).
        
        This can be used to define a Grassmann Gaussian exponential:
        \begin{equation}
            \exp\left[ \sum_{i, j=1}^n \overbar{\vartheta}_i M_{ij} \vartheta_j \right].
        \end{equation}
        This is defined through the Taylor series, which vanishes beyond the first order term, so we can first pull out one of the sums giving
        \begin{equation}
            \prod_{i = 1}^n \exp\left[ \overbar{\vartheta}_i \sum_{j=1}^n M_{ij} \vartheta_j \right],
        \end{equation}
        and then expand the exponential giving
        \begin{equation}
            \prod_{i=1}^n \left( 1 + \overbar{\vartheta}_i \sum_{j=1}^n M_{ij}\vartheta_j \right).
        \end{equation}
    \end{appendices}
    
    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}
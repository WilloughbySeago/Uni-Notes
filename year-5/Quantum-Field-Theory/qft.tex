% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

%% Packages
\usepackage{tensor}
\usepackage{csquotes}
\usepackage{scalerel,stackengine}  % needed for d with slash
\usepackage{simpler-wick}
\usepackage{siunitx}

% Tikz stuff
\usepackage{tikz}
%\tikzset{>=latex}
% External
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
% Other libraries
\usetikzlibrary{calc}

\usepackage[compat=1.1.0]{tikz-feynman}

% References, should be last things loaded
\usepackage[pdfauthor={Willoughby Seago},pdftitle={Quantum Field Theory},pdfkeywords={quantum field theory, QFT, quantisation, perturbation theory, Feynman diagrams, QED, QCD, standard model, path integral, renormalisation},pdfsubject={Quantum Field Theory}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}

% Highlight colour
\definecolor{Yellow}{HTML}{F9C80E}
\definecolor{Orange}{HTML}{F86624}
\definecolor{Red}{HTML}{EA3546}
\definecolor{Purple}{HTML}{662E9B}
\definecolor{Blue}{HTML}{43BCCD}

\colorlet{highlight}{Red}

% Title page info
\title{Quantum Field Theory}
\author{Willoughby Seago}
\date{}
% \subtitle{}
% \subsubtitle{}

% Commands
% Text
\newcommand*{\course}[1]{\textit{#1}}
% Maths
\newcommand{\minkowskiMetric}{\eta}
\newcommand{\dalembertian}{\partial^2}
\newcommand{\e}{\symrm{e}}
\newcommand{\lagrangian}{L}
\newcommand{\lagrangianDensity}{\symcal{L}}
\newcommand{\hamiltonianDensity}{\symcal{H}}
\DeclarePairedDelimiterX{\poissonBracket}[2]{\{}{\}}{#1, #2}
\newcommand{\parity}{\symcal{P}}
\newcommand{\chargeConjugation}{\symcal{C}}
\newcommand{\timeReversal}{\symcal{T}}
\newcommand{\hermit}{{\dagger}}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1, #2}
\newcommand{\ident}{\symbb{1}}
\newcommand\dbar{\ThisStyle{\ensurestackMath{%
            \stackengine{-.3\LMpt}{\SavedStyle \symrm{d}}{\SavedStyle\overbar{}%
                \mkern2.25mu}{O}{r}{F}{F}{L}}}}
\newcommand{\invariantmeasure}[1]{\dbar #1}
\newcommand\bardelta{\ThisStyle{\ensurestackMath{%
            \stackengine{-.3\LMpt}{\SavedStyle \delta}{\SavedStyle\overbar{}%
                \mkern4.5mu}{O}{r}{F}{F}{L}}}}
\newcommand{\hilbertSpace}{\symbb{H}}
\newcommand{\normalordering}[1]{\mathopen{\vcentcolon}{#1}\mathclose{\vcentcolon}}
\AtBeginDocument{
    \let\Re\relax
    \let\Im\relax
    \DeclareMathOperator{\Re}{Re}
    \DeclareMathOperator{\Im}{Im}
}
\DeclareMathOperator{\Res}{Res}
\newcommand{\trans}{{\top}}
\newcommand{\EM}{\text{em}}
\newcommand{\interaction}{\symrm{I}}
\newcommand{\probability}{\symbb{P}}
\newcommand{\heaviside}{\theta}
\DeclareMathOperator{\timeOrdering}{T}
\newcommand{\feynman}{\symrm{F}}
\newcommand{\amplitude}{\symcal{M}}
\DeclarePairedDelimiterX{\anticommutator}[2]{\{}{\}}{#1, #2}
\DeclareMathOperator{\tr}{tr}
\newcommand{\order}{\symcal{O}}
\newcommand{\phaseSpaceMeasure}[1][n]{(\dl{\symrm{PS}})_{#1}}
\DeclarePairedDelimiterX{\innerproduct}[2]{\langle}{\rangle}{#1 , #2}
\newcommand{\DL}[1]{\symcal{D}#1}
\newcommand{\DD}[1]{\,\symcal{D}#1}

\includeonly{parts/prelim, parts/hamiltonian-calculation}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{}
    \tableofcontents
    \listoffigures
    \mainmatter
    
    \chapter{Introduction}
    \section{Course Overview}
    \define{Quantum field theory (QFT)}\index{quantum field theory}\glossary[acronym]{QFT}{Quantum Field Theory} is combination of relativity and quantum mechanics.
    It will be our topic of study in this course.
    We will focus mostly on a single quantum field theory, namely \define{quantum electrodynamics (QED)}\index{quantum electrodynamics}\glossary[acronym]{QED}{Quantum Electrodynamics}, which is the result of combining electrodynamics, which is inherently relativistic, and quantum mechanics.
    
    There are various courses leading into this course, some of the important ones for which I have notes are
    \begin{itemize}
        \item \course{Principles of Quantum Mechanics};
        \item \course{Quantum Theory};
        \item \course{Classical Electrodynamics}; and
        \item \course{Symmetries of Quantum Mechanics}.
    \end{itemize}
    This course has a companion course, \course{Symmetries of Particles and Fields}, which focuses on the mathematical abstraction of symmetry.
    Ideas from this companion course, and the related course \course{Symmetries of Quantum Mechanics} will occur throughout this course.
    Another related course, focused more on the experimental side of this field, is \course{Particle Physics}.
    I have notes for both \course{Symmetries of Particles and Fields} and \course{Particle Physics}.
    
    This course, and its companion, lead naturally into the course \course{Gauge Theories in Particle Physics}, for which I also have notes.
    Many ideas in this course will be expanded upon in the gauge theories course.
    The gauge theories course will also treat other quantum field theories, such as \defineindex{electroweak} interactions (a combination of electromagnetism and the weak interactions), and \define{quantum chromodynamics (QCD)}\index{quantum chromodynamics}\glossary[acronym]{QCD}{Quantum Chromodynamics} (the quantum field theory of the strong force).
    The course also briefly touches on lattice gauge theory, a particular way of doing calculations in nonperturbative QCD.
    
    This course is roughly divided into four sections:
    \begin{itemize}
        \item Canonical QFT: Here we deal with operators and quantisation, familiar from traditional quantum mechanics, such as in \course{Principles of Quantum Mechanics}.
        \item QED: We will use this as an example, to allow us to work in a concrete setting rather than in the abstract.
        This is in many ways the simplest of quantum field theories, we study mostly interactions of photons and electrons, and since photons aren't charged the interactions are about as simple as they get.
        Mathematically this \enquote{simplicity} is due to the underlying \(\unitary(1)\) symmetry, which is an Abelian gauge group, and is significantly easier to work with than, say, the \(\specialUnitary(3)\) symmetry of QCD.
        \item Path integral formalism: This is a more abstract, but often more powerful, yet equivalent formulation of quantum field theory.
        This formalism leads itself to gauge theories, and so will be used heavily in \course{Gauge Theories in Particle Physics}.
        Nonrelativistic path integrals were treated in \course{Quantum Theory}.
        \item Renormalisation: When we do QFT calculations we often get infinite results.
        Renormalisation is the process of removing these infinities and extracting meaningful results.
        The renormalisation of QED will be treated in more detail in \course{Gauge Theories}.
    \end{itemize}
    There is some concurrent teaching of these topics, but in these notes I separate them out, so if something's not making sense maybe check to see if its been explained in more detail in a different section.
    
    \section{Conventions}
    Quantum field theory is full of conventions.
    One convention that almost everyone follows is that we work in natural units, where \(c = \hbar = 1\).
    This makes the formulas look much simpler and less cluttered, and we can put \(c\) and \(\hbar\) back in by dimensional analysis.
    
    The second convention is unfortunately much more varied, its the choice of metric.
    We will use the \(({+}{-}{-}{-})\) metric,
    \begin{equation}
        \minkowskiMetric_{\mu\nu} = 
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & -1 & 0 & 0\\
            0 & 0 & -1 & 0\\
            0 & 0 & 0 & -1
        \end{pmatrix}
        .
    \end{equation}
    The most common alternative to this is \(({-}{+}{+}{+})\), where the diagonal has a single negative 1 and the rest is made of positive 1s.
    There are some even rarer choices to use an imaginary metric, \((i{+}{+}{+})\), which still gives the same relative minus sign when we square each element in the inner product.
    
    Throughout the course, unless specified otherwise, we will use the Einstein summation convention.
    Specifically, if an index appears exactly twice in a term, once in a raised position and once in a lowered position, then we sum over all values of that index.
    We also follow the convention where Greek indices, such as \(\mu\) and \(\nu\), run from \(0\) to the number of dimensions minus one, most commonly meaning \(\mu = 0, 1, 2, 3\).
    On the other hand, Latin indices, such as \(i\) and \(j\), run from \(1\) to the number of dimensions minus one, most commonly, \(i = 1, 2, 3\).
    This means that Greek indices are summed over all components, whereas Latin indices are summed only over spatial components.
    
    We follow the convention that the electromagnetic field strength tensor is
    \begin{equation}
        F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu.
    \end{equation}
    An alternative convention is
    \begin{equation}
        F^{\mu\nu} = \partial^\nu A^\mu - \partial^\mu A^\nu,
    \end{equation}
    which differs from our choice by an overall minus sign.
    For example, this results in Maxwell's equations being written as \(\partial_\mu F^{\mu\nu} = -j^\nu\), instead of \(\partial_\mu F^{\mu\nu} = j^\nu\), which is what we'll be using.
    
    \part{Canonical Quantisation}
    \chapter{Classical Fields}
    \section{Relativity Basics}
    In order to understand quantum field theory we will need to quantise fields.
    In order to quantise fields we should be sure that we understand classical fields, so this is where we start the course.
    We will briefly recap ideas mostly from the \course{Classical Electrodynamics} and \course{Quantum Theory} courses, so check the notes for this course for more details.
    
    \subsection{Four-Vectors}
    We can consider a generic four-vector,
    \begin{equation}
        a^\mu = (a^0, \vv{a}) = (a^0, a^1, a^2, a^3).
    \end{equation}
    Using the metric, \(\minkowskiMetric_{\mu\nu} \coloneqq \diag(1,-1,-1,-1)\), we can lower the indices to get a covariant four-vector:
    \begin{equation}
        a_\nu \coloneqq \minkowskiMetric_{\mu\nu}a^\mu = (a^0, -\vv{a}) = (a^0, -a^1, -a^2, -a^3) = (a_0, a_1, a_2, a_3).
    \end{equation}

    Given two four vectors, \(a^\mu = (a^0, \vv{a})\) and \(b = (b^0, \vv{b})\), we can take the inner product,
    \begin{equation}
        a \cdot b \coloneqq a^\mu b_\mu = a_\mu b^\mu = \minkowskiMetric_{\mu\nu} a^\mu b^\nu = a^0b^0 - \vv{a} \cdot \vv{b}.
    \end{equation}
    Here \(\vv{a} \cdot \vv{b} = a^ib^i = a^1b^1 + a^2b^2 + a^3b^3\) is the standard dot product for three-vectors.
    
    For the specific case of the inner product of a four-vector with itself we use the shorthand
    \begin{equation}
        a^2 = a \cdot a = a^0 a^0 - \vv{a} \cdot \vv{a} = (a^0)^2 - \vv{a}^2 .
    \end{equation}
    
    Four vectors transform under Lorentz transformations.
    The broadest class of such transformations is the \defineindex{Lorentz group}, \(\orthogonal(1, 3)\).
    This includes all boosts and rotations, including those that invert the directions of space or time.
    In this course we only consider \define{proper orthochronous Lorentz transformations}\index{proper orthochronous Lorentz transformation}, that is Lorentz transformations preserving the orientation of space (proper) and time (orthochronous).
    These form the \defineindex{proper orthochronous Lorentz group} \(\specialOrthogonal^+(1, 3)\).
    From now on if we say Lorentz transformation we mean \emph{proper orthochronous} Lorentz transformation.
    
    \subsection{Specific-Four Vectors}
    The position is a four-vector, \(x^\mu = (t, \vv{x})\), where \(t\) is the time coordinate and \(\vv{x}\) is the spatial position.
    Note that in SI units this would be \(x^\mu = (ct, \vv{x})\), since the components of a four-vector must have the same dimensions.
    We can also construct a covariant position four-vector, \(x_\mu = (t, -\vv{x})\).
    
    The momentum is a four-vector, \(p^\mu = (E, \vv{p})\), where \(E\) is the energy, and \(\vv{p}\) is the relativistic three-momentum.
    Note that in SI units this would be \(p^\mu = (E/c, \vv{p})\).
    We can also construct a covariant momentum four-vector, \(p_\mu = (E, -\vv{p})\).
    
    For a (real\footnote{as in, not a virtual particle}) particle of mass \(m\) the four-momentum squares to the mass squared, that is
    \begin{equation}
        m^2 = p^2 = E^2 - \vv{p}^2.
    \end{equation}
    Rearranging this gives us
    \begin{equation}
        E^2 = \vv{p}^2 + m^2.
    \end{equation}
    This is the relativistic \defineindex{energy-momentum relation}, it's perhaps more familiar if we reinstate the factors of \(c\):
    \begin{equation}
        E^2 = \vv{p}^2c^2 + m^2c^4,
    \end{equation}
    and is most famous in the case where \(\vv{p} = \vv{0}\):
    \begin{equation}
        E = mc^2.
    \end{equation}
    
    We can construct a four-vector derivative by defining the derivative with respect to \(x^\mu\):
    \begin{equation}
        \partial_\mu \coloneqq \diffp{}{x^\mu} = \left( \diffp{}{t}, \grad \right).
    \end{equation}
    Note that this is a covariant vector, since the contravariant quantity, \(x^\mu\), appears in the denominator, so the derivative transforms in the opposite way to how it would if \(x^\mu\) were in the numerator.
    We can similarly define a contravariant operator,
    \begin{equation}
        \partial^\mu = \diffp{}{x_\mu} = \left( \diffp{}{t}, -\grad \right).
    \end{equation}
    The square of these operators is common enough to be given it's own name, it's called the \defineindex{d'Alembert operator},
    \begin{equation}
        \dalembertian = \partial^\mu \partial_\mu = \diffp[2]{}{t} - \laplacian.
    \end{equation}
    This same quantity is also denoted \(\square^2\), as a sort of four-dimensional (note four sides) Lorentzian-manifold analogue of the Laplacian, \(\laplacian\), and also confusingly sometimes denoted \(\square\), without the superscript 2.
    We also call this the wave operator, since if \(f\) is a field satisfying \(\dalembertian f = 0\) then expanding and rearranging this we have
    \begin{equation}
        \diffp[2]{f}{t} = \diffp[2]{f}{x},
    \end{equation}
    which is the equation of a wave travelling at the speed of light.
    
    \section{Nonrelativistic Particle and Wave}
    A nonrelativistic particle of mass \(m\) has the energy-momentum relation
    \begin{equation}
        E = \frac{\vv{p}^2}{2m} = \frac{1}{2}m\vv{v}^2.
    \end{equation}
    To get an equation for this particle we substitute in the usual operators,
    \begin{equation}
        E \to i\diffp{}{t}, \qand \vv{p} \to -i\grad.
    \end{equation}
    Acting on an arbitrary state, \(\psi\), we then get
    \begin{equation}
        i\diffp{\psi}{t} = \frac{(-i\grad)^2}{2m} \psi = -\frac{1}{2m}\laplacian\psi.
    \end{equation}
    This is exactly the \define{Schrödinger equation}\index{Schrödinger!equation} for a free particle.
    
    One solution to this is a plane wave,
    \begin{equation}
        \psi(x) = \e^{-iEt + i\vv{p} + \vv{x}} = \e^{-ip\cdot x}.
    \end{equation}
    This is only a solution if \(E^2 = \vv{p}^2/2m\).
    Notice that while we can write the exponent with the relativistic four-vectors this is still nonrelativistic since the energy-momentum is not relativistic.
    In particular, we have a single energy value for each possible momentum, as seen in \cref{fig:energy-momentum relation nonrelativistic}.
    
    \begin{figure}
        \tikzsetnextfilename{energy-momentum-nonrelativistic}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [below] {\(\vv{p}\)};
            \draw[thick, ->] (0, 0) -- (0, 3) node [left] {\(E\)};
            \draw[very thick, highlight, domain=-3:3, samples=500] plot (\x, \x*\x/3);
        \end{tikzpicture}
        \caption{The nonrelativistic energy-momentum relation assigns a single energy to each three-momentum. Note that this is really just a slice through a four-dimensional plot, but all that really matters is the magnitude of the three-momentum.}
        \label{fig:energy-momentum relation nonrelativistic}
    \end{figure}
    
    We know that solutions to the Schrödinger equation can be interpreted as amplitudes, which we then take the modulus square of, \(\abs{\psi}^2\), to get a probability density function.
    We call a function with this property a \defineindex{wave function}.
    
    \section{Relativistic Particle and Wave}
    A relativistic particle of mass \(m\) has the energy-momentum relation
    \begin{equation}
        E^2 = \vv{p}^2 + m^2.
    \end{equation}
    Making the operator substitutions and acting on an arbitrary state, \(\varphi\), we get
    \begin{equation}
        \left( i\diffp{}{t} \right)^2\varphi = (-i\grad)^2\varphi + m^2\varphi.
    \end{equation}
    Rearranging this we get
    \begin{equation}
        (\dalembertian + m^2)\varphi = 0.
    \end{equation}
    This is the \defineindex{Klein--Gordon equation}.
    
    One solution to this is a plane wave,
    \begin{equation}
        \varphi(x) = \e^{-ip \cdot x}.
    \end{equation}
    This is a solution provided that \(p^2 = m^2\).
    If \(p^2 = m^2\) then we say that \(p^\mu\) is \defineindex{on-shell}.
    
    If \(e^{-ip\cdot x}\) is a solution then so is
    \begin{equation}
        \varphi^* = \e^{ip\cdot x}.
    \end{equation}
    We see that we get pairs of solutions.
    These correspond to the pairs of possible energy values, \(E = \pm \sqrt{\vv{p}^2 + m^2}\), as shown in \cref{fig:energy-momentum relation relativistic}.
    
    \begin{figure}
        \tikzsetnextfilename{energy-momentum-relation}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [below] {\(\vv{p}\)};
            \draw[thick, ->] (0, -3) -- (0, 3) node [left] {\(E\)};
            \clip (-3, -3) rectangle (3, 3);
            \draw[very thick, highlight, domain=-3:3, samples=500] plot ({sinh(\x)}, {cosh(\x)});
            \draw[very thick, highlight, domain=-3:3, samples=500] plot ({sinh(\x)}, {-cosh(\x)});
            \draw[dashed, thick, black!50] (-3, -3) -- (3, 3);
            \draw[dashed, thick, black!50] (-3, 3) -- (3, -3);
            \node[above left] at (0, 1) {\(m\)};
        \end{tikzpicture}
        \caption{The relativistic energy momentum assigns two energies to a each three-momentum. The curve is a hyperbola. Notice that at zero three-momentum there we don't have zero energy, instead we have energy \(\pm m\), since we are counting the mass towards the energy in the relativistic formulation. The hyperbola here are the \enquote{shell} referred to in \enquote{on-shell}.}
        \label{fig:energy-momentum relation relativistic}
    \end{figure}
    
    The negative energy solutions to the Klein--Gordon equation pose a problem, in particular, it is possible for \(\varphi^* \varphi\) to be negative, which means there is no probabilistic interpretation of \(\varphi\), and so \(\varphi\) is \emph{not} a wave function.
    Instead, we just call \(\varphi\) a field.
    
    States of a system obeying the Klein--Gordon equation correspond to relativistic free particles (plural).
    
    \section{Maxwell Fields}
    The most familiar fields are electromagnetic fields.
    In this section we will briefly recap a relativistic treatment of these fields.
    For more details see the \course{Classical Electrodynamics} course.
    
    The electromagnetic \enquote{potential} is
    \begin{equation}
        A^\mu(x) = (\varphi(x), \vv{A}(x)),
    \end{equation}
    where \(\varphi\) is the electric potential and \(\vv{A}\) is the electromagnetic vector potential.
    The electric \enquote{field} is then
    \begin{equation}
        F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu.
    \end{equation}
    \begin{wrn}
        An alternative convention,
        \begin{equation}
            F^{\mu\nu} = \partial^\nu A^\mu - \partial^\mu A^\nu,
        \end{equation}
        differs by a sign.
    \end{wrn}
    The electromagnetic current density is
    \begin{equation}
        j^\mu = (\rho, \vv{j}).
    \end{equation}
    This current is conserved, mathematically this is expressed as its four-divergence vanishing,
    \begin{equation}
        \partial_\mu j^\mu = \diffp{\rho}{t} + \div\vv{j} = 0.
    \end{equation}
    This is a continuity equation.
    
    Maxwell's equation is
    \begin{equation}
        \partial_\mu F^{\mu\nu} = j^\nu.
    \end{equation}
    Writing \(F^{\mu\nu}\) in terms of \(A^\mu\) we get
    \begin{equation}
        \partial_\mu F^{\mu\nu} = \partial_\mu \partial^\mu A^\nu - \partial_\mu \partial^\nu A^\mu = \dalembertian A^\nu - \partial^\nu \partial_\mu A^\mu.
    \end{equation}
    The \defineindex{Lorenz gauge} condition is to choose \(A^\mu\) such that its four-divergence vanishes, \(\partial_\mu A^\mu\).
    Then we have \(\partial_\mu F^{\mu\nu} = \dalembertian A^\nu\), so we can write Maxwell's equation as
    \begin{equation}
        \dalembertian A^\nu = j^\nu.
    \end{equation}
    
    In QFT we call \(A^\mu\) the \defineindex{electromagnetic field} and we call \(F^{\mu\nu}\) the \defineindex{electromagnetic field strength}.
    This is partly an annoying historical artefact and partly because \(A^\mu\) is the field appearing in our equations in an analogous way to \(\varphi\) in the Klein--Gordon equation.
    
    In free space, that is when \(j^\mu = 0\), Maxwell's equation is
    \begin{equation}
        \dalembertian A^\mu = 0.
    \end{equation}
    Notice that this is similar to the Klein--Gordon equation with massless particles, \(m = 0\).
    This is good, because we would like electromagnetism to work with photons, which are massless.
    The one difference from the Klein--Gordon equation is that \(A^\mu\) is a four-vector, whereas \(\varphi\) is a scalar.
    The only change in the solutions is that we, in theory, get a different solution for each component of \(A^\mu\).
    The way that this manifests in the solutions is we get an extra vector out front called the \defineindex{polarisation vector}, \(\varepsilon^\mu\):
    \begin{equation}
        A^\mu = \varepsilon^\mu \e^{-ip\cdot x}.
    \end{equation}
    Assuming the momentum is on-shell, that is \(p^2 = 0\) in this massless case.
    This means the momentum is light-like, more good news for electromagnetism explaining photons.
    
    It can be shown that\footnote{See the \course{Classical Electrodynamics} course, note that there we work with \(k^\mu = \hbar p^\mu\).} \(\varepsilon_\mu p^\mu = 0\).
    We are also free to choose \(\varepsilon^\mu\) such that \(\varepsilon^0 = 0\), then the condition \(\varepsilon_\mu p^\mu = 0\) becomes \(\vv{\varepsilon} \cdot \vv{p} = 0\).
    This means that electromagnetic waves are \defineindex{transverse}, since their polarisation vector, \(\vv{\varepsilon}\), is perpendicular to their direction of travel, \(\vv{p}\).
    
    Since the mathematics of the vector field \(A^\mu\) is so similar to the mathematics of the scalar field \(\varphi\) we will mostly deal with scalar fields in this course to develop our theory, then work with other fields once we have a good understanding off the maths.
    Each type of field corresponds to a different spin.
    Scalar fields describe spin zero particles, like the Higgs boson, and vector fields describe spin one particles, like the photon.
    Spin \(1/2\) particles are described by spinors.
    
    \chapter{Lagrangians and Hamiltonians}
    \section{Lagrangian Dynamics of a Particle}
    \epigraph{That's all there is to a Lagrangian dynamics course, the rest is just examples.}{Richard Ball}
    \begin{rmk}
        See the \course{Lagrangian Dynamics} course for more details.
    \end{rmk}
    Consider a particle of mass \(m\) moving in one dimension in a conservative force field.
    We can define its position with a \define{generalised coordinate}\index{generalised!coordinate}, \(q\), which gives us a \define{generalised velocity}\index{generalised!velocity}, \(\dot{q} \coloneqq \diff{q}/{t}\).
    We can then define the \defineindex{Lagrangian}:
    \begin{equation}
        \lagrangian(q, \dot{q}) = T(\dot{q}) - V(q).
    \end{equation}
    Here \(T(\dot{q})\) is the kinetic energy of the particle, which we assume depends only on the velocity for simplicity, and \(V(q)\) is the potential energy of the particle, which must depend only on the position.
    The Lagrangian is a function of the generalised position and velocity.
    
    The \defineindex{action} of the particle between the times \(t_1\) and \(t_2\) is defined to be
    \begin{equation}
        S[q(t)] \coloneqq \int_{t_1}^{t_2} \dl{t} \, \lagrangian(q, \dot{q}).
    \end{equation}
    \defineindex{Hamilton's principle} states that the path the particle takes, \(q(t)\), is such that the action is extremised, that is the variation, \(\delta S\), given by varying \(q \to q + \delta q\) vanishes.
    We can compute the variation in a general action, \(S\), by computing the variation in the Lagrangian:
    \begin{equation}
        \delta \lagrangian = \diffp{\lagrangian}{q} \delta q + \diffp{\lagrangian}{\dot{q}} \delta \dot{q}.
    \end{equation}
    For a smooth variation, \(\delta q\), we have
    \begin{equation}
        \delta \dot{q} = \diffp{}{t}(\delta q),
    \end{equation}
    and so
    \begin{equation}
        \delta \lagrangian = \diffp{\lagrangian}{q} \delta q + \diffp{\lagrangian}{\dot{q}} \diff{}{t} (\delta q)
    \end{equation}
    Integrating this the variation in the action is
    \begin{equation}
        \delta S = \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} \delta q + \diffp{\lagrangian}{\dot{q}} \diff{}{t} (\delta q) \right].
    \end{equation}
    The product rule tells us that
    \begin{equation}
        \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \delta q \right) = \diffp{\lagrangian}{\dot{q}} \diff{}{t} (\delta \dot{q}) + \diff{}{t} \left( \diffp{\lagrangian}{\dot{q}} \right) \delta q.
    \end{equation}
    Rearranging this we can rewrite the second term in \(\delta S\) as a total derivative minus a term:
    \begin{align}
        \delta S &= \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} \delta q + \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \delta q \right) - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \delta q \right]\\
        &= \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \right] \delta q + \int_{t_1}^{t_2} \dl{t} \, \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \delta q \right)\\
        &= \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \right] \delta q + \left[ \diffp{\lagrangian}{\dot{q}} \delta q \right]_{t_1}^{t_2}.
    \end{align}
    Now suppose that \(\delta q\) vanishes at \(t_1\) and \(t_2\), so \(\delta q(t_1) = \delta q(t_2) = 0\), then the last term above vanishes and we have
    \begin{equation}
        \delta S = \int_{t_1}^{t_2} \dl{t} \left[ \diffp{\lagrangian}{q} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \right]\delta q.
    \end{equation}
    If \(\delta S\) is to vanish for all variations \(\delta q\) then we must have
    \begin{equation}
        \diffp{\lagrangian}{q} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) = 0.
    \end{equation}
    This is the \defineindex{Euler--Lagrange equation} for a single particle in one dimension.
    
    This generalises to \(n\)-dimensional space by replacing \(q\) with \(n\) coordinates \(q_i\) and \(\dot{q}\) with \(\dot{q}_i\).
    The Lagrangian is then
    \begin{equation}
        \lagrangian(q_i, \dot{q}_i) = T(\dot{q}_i) - V(q_i),
    \end{equation}
    and the action is
    \begin{equation}
        S[q_i(t)] = \int_{t_1}^{t_2} \dl{t} \, \lagrangian(q_i, \dot{q}_i).
    \end{equation}
    We then get \(n\) Euler--Lagrange equations:
    \begin{equation}
        \diffp{\lagrangian}{q_i} - \diff{}{t} \left( \diffp{\lagrangian}{\dot{q}_i} \right) = 0.
    \end{equation}
    
    \begin{exm}{}{}
        The Lagrangian for a simple harmonic oscillator is
        \begin{equation}
            \lagrangian = \frac{1}{2} m \dot{x}^2 - \frac{1}{2} m\omega^2 x^2.
        \end{equation}
        We have
        \begin{equation}
            \diffp{\lagrangian}{x} = -m\omega^2 x, \qand \diffp{\lagrangian}{\dot{x}} = m\dot{x} \implies \diff{}{t}\left( \diffp{\lagrangian}{\dot{x}} \right) = m\ddot{x}.
        \end{equation}
        Hence,
        \begin{equation}
            m\ddot{x} = -m\omega^2 x \implies \ddot{x} = -\omega^2 x
        \end{equation}
        is the equation of motion for the simple harmonic oscillator, which is exactly what we would expect.
    \end{exm}
    
    \section{Hamiltonian Dynamics of a Particle}
    Let's go back to the one-dimensional case.
    We can define the \defineindex{canonical momentum} of the particle as
    \begin{equation}
        p \coloneqq \diffp{\lagrangian}{\dot{q}}.
    \end{equation}
    Note that, in general, this is \emph{not} the normal momentum.
    
    We then define the \defineindex{Hamiltonian}, \(H\), as a function of the generalised position and canonical momentum:
    \begin{equation}
        H(q, p) \coloneqq p\dot{q} - \lagrangian(q, \dot{q}).
    \end{equation}
    Note that we must eliminate any \(\dot{q}\) remaining in the expression for the Hamiltonian, this can be done by solving the defining relation \(p = \diff{\lagrangian}/{\dot{q}}\) for \(\dot{q}\) and then substituting in the result.
    
    Now consider what happens when we vary \(H\).
    Varying the left hand side we get
    \begin{equation}
        \dl{H} = \diffp{H}{q} \dd{q} + \diffp{H}{p} \dd{p}.
    \end{equation}
    Varying the right hand side we have
    \begin{equation}
        \dl{H} = p \dd{q} + q \dd{p} - \diffp{\lagrangian}{q} \dd{q} - \diffp{\lagrangian}{\dot{q}} \dd{\dot{q}}.
    \end{equation}
    We can rewrite the last term in terms of the canonical momentum:
    \begin{equation}
        \dl{H} = p \dd{q} + q \dd{p} - \diffp{\lagrangian}{q} \dd{q} - p \dd{\dot{q}} =  q \dd{p} - \diffp{\lagrangian}{q} \dd{q}.
    \end{equation}
    Using the Euler--Lagrange equations the second term can be rewritten to give
    \begin{equation}
        \dl{H} =  q \dd{p} - \diff{}{t}\left( \diffp{\lagrangian}{\dot{q}} \right) \dd{q} = q \dd{p} - \dot{p} \dd{q}.
    \end{equation}
    Comparing this with the result for the left hand side variation we can read off
    \begin{equation}
        \dot{q} = \diffp{H}{p}, \qand \dot{p} = - \diffp{H}{q}.
    \end{equation}
    These are \defineindex{Hamilton's equations}.
    
    This all generalises to \(n\) dimensions.
    First replace \(q\) with \(q_i\), and \(p\) with \(p_i \coloneqq \diffp{\lagrangian}/{q_i}\).
    Then the Hamiltonian is
    \begin{equation}
        H(q_i, p_i) = \sum_i p_i \dot{q}_i - \lagrangian(q_i, \dot{q}_i).
    \end{equation}
    Hamilton's equations are
    \begin{equation}
        \dot{q}_i = \diffp{H}{p_i}, \qand \dot{p}_i = -\diffp{H}{q_i}.
    \end{equation}
    
    It can be useful when doing Hamiltonian mechanics to define the \defineindex{Poisson bracket} of two functions, \(A\) and \(B\), depending on position, \(q_i\), and momentum, \(p_i\):
    \begin{equation}
        \poissonBracket{A, B} \coloneqq \sum_i \left( \diffp{A}{p_i}\diffp{B}{q_i} - \diffp{A}{q_i}\diffp{B}{p_i} \right).
    \end{equation}
    This has the advantage of allowing us to express Hamilton's equations in the more symmetric form
    \begin{equation}\label{eqn:hamilton's equations with poisson brackets}
        \dot{q}_i = \poissonBracket{H}{q_i}, \qand \dot{p}_i = \poissonBracket{H}{p_i}.
    \end{equation}
    
    Note the similarity to Heisenberg's operator equation of motion for a time independent operator, \(A\):
    \begin{equation}
        \dot{A}(t) = i [H, A].
    \end{equation}
    Replacing Poisson brackets with \(i\) times the commutator can get us quite a long way in quantum mechanics.
    
    The Hamiltonian is conserved.
    To show this consider the time derivative:
    \begin{equation}
        \diff{H}{t} = \diffp{H}{p} \dot{p} + \diffp{H}{q} \dot{q} = \dot{q}\dot{p} - \dot{p}\dot{q} = 0.
    \end{equation}
    We can often identify the Hamiltonian with the total energy when this is also conserved.
    
    \begin{exm}{}{}
        The canonical momentum for a simple harmonic oscillator is
        \begin{equation}
            p = \diffp{\lagrangian}{\dot{x}} = m\dot{x},
        \end{equation}
        which is just the normal momentum in this case.
        The Hamiltonian is then
        \begin{equation}
            H = p\dot{q} - \lagrangian = m\dot{x}^2 - \frac{1}{2}m\dot{x}^2 + \frac{1}{2}m\omega^2x^2 = \frac{1}{2}m\dot{x}^2 + \frac{1}{2}m\omega^2x^2,
        \end{equation}
        which is exactly the energy of a simple harmonic oscillator.
    \end{exm}
    
    \section{Lagrangian Dynamics of a Field}
    Lagrangian dynamics doesn't change that much when we work with fields.
    We start by replacing the generalised coordinate \(q(t)\) with the field, \(\varphi(x)\), which now depends on a position in spacetime, rather than just a time.
    Instead of a Lagrangian we work with a \define{Lagrangian density}\index{Lagrangian!density}, \(\lagrangianDensity(\varphi, \partial_\mu \varphi)\), replacing the time derivative of the coordinate with a four-vector derivative.
    We then have to integrate over a region of spacetime to get the action:
    \begin{equation}
        S[\varphi(x)] \coloneqq \int \dl{^4x} \, \lagrangianDensity(\varphi, \partial_\mu \varphi).
    \end{equation}
    The integral should be taken over some arbitrary region of spacetime.
    Note that we can write this as
    \begin{equation}
        S[\varphi(x)] = \int \dl{t} \int \dl{^3\vv{x}} \, \lagrangianDensity(\varphi, \partial_\mu \varphi) = \int \dl{t} \, \lagrangian(\varphi, \partial_\mu \varphi)
    \end{equation}
    where
    \begin{equation}
        \lagrangian(\varphi, \partial_\mu \varphi) \coloneqq \int \dl{^3\vv{x}} \, \lagrangianDensity(\varphi, \partial_\mu \varphi).
    \end{equation}
    So, integrating \(\lagrangianDensity\) over space gives \(\lagrangian\), which we call the Lagrangian, which explains the interpretation of \(\lagrangianDensity\) as a Lagrangian \emph{density}.
    While this distinction is important the Lagrangian doesn't actually appear that much in QFT, so people often just call \(\lagrangianDensity\) the Lagrangian, leaving the density part implicit.
    
    The Euler--Lagrange equations for the Lagrangian density don't change much, and can be derived in a very similar way.
    We start by varying the field, \(\varphi \to \varphi + \delta \varphi\), and looking at the resulting variation in the Lagrangian density:
    \begin{equation}
        \delta\lagrangianDensity = \diffp{\lagrangianDensity}{\varphi} \delta \varphi + \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta (\partial_\mu \varphi).
    \end{equation}
    For a smooth variation we have
    \begin{equation}
        \delta (\partial_\mu \varphi) = \partial_\mu (\delta \varphi)
    \end{equation}
    and so
    \begin{equation}
        \delta\lagrangianDensity = \diffp{\lagrangianDensity}{\varphi} \delta \varphi + \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \partial_\mu (\delta \varphi).
    \end{equation}
    We can use the product rule,
    \begin{equation}
        \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right) = \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi + \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \partial_\mu (\delta \varphi),
    \end{equation}
    to write the last term as a total derivative minus a term:
    \begin{equation}
        \delta\lagrangianDensity = \diffp{\lagrangianDensity}{\varphi} \delta \varphi + \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right) - \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi.
    \end{equation}
    Integrating over some spacetime region, \(\Omega\), to get the variation in the action we have
    \begin{align}
        \delta S &= \int_\Omega \dl{^4x} \left[ \diffp{\lagrangianDensity}{\varphi} \delta \varphi + \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right) - \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi \right]\\
        &= \int_\Omega \dl{^4x} \left[ \diffp{\lagrangianDensity}{\varphi} \delta \varphi - \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi \right] + \int_\Omega \dl{^4x} \, \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right).
    \end{align}
    Applying the divergence theorem to the second integral we can replace it with an integral of \((\diffp{\lagrangianDensity}/{(\partial_\mu \varphi)}) \delta \varphi\) over the boundary, \(\partial \Omega\).
    Choosing a variation, \(\delta \varphi\), which vanishes on the boundary this term will vanish, and we will be left with
    \begin{equation}
        \delta S = \int_\Omega \dl{^4x} \left[ \diffp{\lagrangianDensity}{\varphi} \delta \varphi - \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) \delta \varphi \right].
    \end{equation}
    For this to vanish for all variations of the field we require that
    \begin{equation}
        \diffp{\lagrangianDensity}{\varphi} - \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) = 0.
    \end{equation}
    These are the \define{Euler--Lagrange equations}\index{Euler--Lagrange equation} for a scalar field.
    
    \section{Hamiltonian Dynamics of a Field}
    Following the same logic as for a single particle we can define the \defineindex{canonical momentum},
    \begin{equation}
        \pi(x) \coloneqq \diffp{\lagrangianDensity}{\dot{\varphi}}.
    \end{equation}
    This is \emph{not} the same as the actual momentum.
    Notice that this definition treats time differently to position, this will result in the Hamiltonian formulation not being Lorentz invariant, although the results we get still are, the steps between are just frame dependent.
    This corresponds to the non-Lorentz invariance of the energy, the energy of a particle of mass \(m\) is \(\gamma m\).
    
    We then define the Hamiltonian, \(H\), to be
    \begin{equation}
        H \coloneqq \int \pi \dot{\varphi} \dd{^4x} - L = \int \dd{^4x} \, (\pi \dot{\varphi} - \lagrangianDensity) = \int \dl{^3\vv{x}} \, \hamiltonianDensity
    \end{equation}
    where
    \begin{equation}
        \hamiltonianDensity \coloneqq \pi \dot{\varphi} - \lagrangianDensity
    \end{equation}
    is the \defineindex{Hamiltonian density}.
    Both the Hamiltonian and Hamiltonian density must be functions of the field, \(\varphi\), and the canonical momentum, \(\pi\), which is just another field.
    
    The same derivation as before works without modification, so we won't repeat it here, the result is \defineindex{Hamilton's equations} for a field:
    \begin{equation}
        \dot{\varphi} = \diffp{\hamiltonianDensity}{\pi}, \qand \dot{\pi} = - \diffp{\hamiltonianDensity}{\varphi}.
    \end{equation}
    
    As before, the Hamiltonian density is conserved:
    \begin{align}
        \diff{\hamiltonianDensity}{t} &= \int \dl{^3\vv{x}} \left( \diffp{\hamiltonianDensity}{\varphi} \dot{\varphi} + \diffp{\hamiltonianDensity}{\pi} \dot{\pi} \right)\\
        &= \int \dl{^3\vv{x}} (-\dot{\pi} \dot{\varphi} + \dot{\varphi} \dot{\pi})\\
        &= 0.
    \end{align}
    Again, when energy is conserved we can often identify it with the Hamiltonian.
    
    \section{Klein--Gordon Equation}
    \epigraph{We have to learn to run before we walk.}{Richard Ball}
    The Lagrangian for a scalar field, \(\varphi\), is
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi)(\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2.
    \end{equation}
    Clearly we have
    \begin{equation}
        \diffp{\lagrangianDensity}{\varphi} = -m^2\varphi.
    \end{equation}
    We can also compute the derivative with respect to \(\partial_\mu \varphi\), note that \((\partial_\mu \varphi)(\partial^\mu \varphi)\) is just \((\partial \varphi)^2\), so we can treat this just like the derivative of a quantity squared, giving
    \begin{equation}
        \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} = \partial^\mu \varphi.
    \end{equation}
    We end with an upper index, since the left hand side has a lower index in the denominator, which is an upper index in the numerator.
    We then have
    \begin{equation}
        \partial_\mu \left( \diffp{\lagrangianDensity}{\varphi} \right) = \partial_\mu \partial^\mu \varphi = \dalembertian \varphi.
    \end{equation}
    Using the Euler--Lagrange equations we then have
    \begin{equation}
        \dalembertian \varphi + m^2 \varphi = 0,
    \end{equation}
    which is just the Klein--Gordon equation.
    
    The canonical momentum associated with this Lagrangian is
    \begin{equation}
        \pi = \diffp{\lagrangianDensity}{\dot{\varphi}} = \diffp{\lagrangianDensity}{(\partial_0 \varphi)}  = \partial^0 \varphi = \dot{\varphi}.
    \end{equation}
    The Hamiltonian is then
    \begin{equation}
        \hamiltonianDensity = \pi \dot{\varphi} - \lagrangianDensity = \frac{1}{2}(\pi^2 + (\grad \varphi)^2 + m^2\varphi^2).
    \end{equation}
    The \(\grad\varphi\) term comes from
    \begin{align}
        \pi^2 - \frac{1}{2} (\partial_\mu\varphi)(\partial^\mu\varphi) &= \pi^2 - \frac{1}{2} [(\partial_0 \varphi)(\partial^0 \varphi) - (\partial_i \varphi)(\partial^i \varphi)]\\
        &= \pi^2 - \frac{1}{2}[\dot{\varphi}^2 - (\grad\varphi)^2] = \frac{1}{2}[\pi^2 - (\grad\varphi)^2]
    \end{align}
    where we've used \(\dot{\varphi} = \pi\).
    
    Compare the Hamiltonian density for the Klein--Gordon equation with the Hamiltonian for the harmonic oscillator:
    \begin{equation}
        H = \frac{1}{2m} (p^2 + m^2\omega^2 x^2).
    \end{equation}
    Up to a factor of \(\omega^2\) and \(1/m\) this is pretty much the same as the Klein--Gordon Hamiltonian density, minus the \((\grad \varphi)^2\) term.
    Similarly the Klein--Gordon Lagrangian density is very similar to the harmonic oscillator Lagrangian.
    This suggests that we should study the harmonic oscillator, and indeed we shall soon.
    
    \begin{exm}{Electromagnetism}{}
        The Lagrangian density for the electromagnetic field, \(A^\mu\), is
        \begin{equation}
            \lagrangianDensity = -\frac{1}{4}F^{\mu\nu}F_{\mu\nu}.
        \end{equation}
        We can insert the definition of \(F^{\mu\nu}\) and expand this out to get
        \begin{align}
            \lagrangianDensity &= -\frac{1}{4}(\partial^\mu A^\nu - \partial^\nu A^\mu)(\partial_\mu A_\nu - \partial_\nu A_\mu)\\
            &= -\frac{1}{4}[(\partial^\mu A^\nu)(\partial_\mu A_\nu) - (\partial^\nu A^\mu)(\partial_\mu A_\nu)\\
            &\qquad- (\partial^\mu A^\nu)(\partial_\nu A_\mu) + (\partial^\nu A^\mu)(\partial_\nu A_\mu)]\\
            &= -\frac{1}{2}(\partial^\mu A^\nu)(\partial_\mu A_\nu) + \frac{1}{2}(\partial^\mu A^\nu)(\partial_\nu A_\mu).
        \end{align}
        
        We treat each component of \(A_\nu\) as an independent field.
        We then have
        \begin{equation}
            \diffp{\lagrangianDensity}{A_\nu} = 0, \qand \diffp{\lagrangianDensity}{(\partial_\mu A_\nu)} = -\partial^\mu A^\nu + \partial^\nu A^\mu = -F^{\mu\nu}.
        \end{equation}
        Hence, the Euler--Lagrange equations give
        \begin{equation}
            \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu A_\nu)} \right) = -\partial_\mu \partial^\mu A^\nu + \partial^\nu \partial_\mu A^\mu = 0.
        \end{equation}
        That is,
        \begin{equation}
            \dalembertian A^\nu - \partial^\nu \partial_\mu A^\mu = 0 \iff -\partial_\mu F^{\mu\nu} = 0
        \end{equation}
        This is Maxwell's equation in a vacuum in an arbitrary gauge.
        
        The canonical momentum is
        \begin{equation}
            \pi^\mu(x) = \diffp{\lagrangianDensity}{\dot{A}_\mu} = \diffp{\lagrangianDensity}{(\partial_0 A_\mu)} = -\partial^\mu A^0 + \partial^0 A^\mu = -F^{0\mu}.
        \end{equation}
        Antisymmetry of \(F^{\mu\nu}\) implies that \(\pi^\mu(x) = 0\), and so \(\pi^\mu(x) = (0, -\vv{E}(x))\), where \(\vv{E}(x)\) is the electric field.
        
        The fact that \(\pi^0(x) = 0\) will cause problems later when we try to quantise the electromagnetic field.
        
        In the presence of sources the Lagrangian density is instead
        \begin{equation}
            \lagrangianDensity = -\frac{1}{4}F^{\mu\nu}F_{\mu\nu} - J^\mu A_\mu.
        \end{equation}
        This doesn't change the term given by differentiating with respect to \(\partial_\mu A_\nu\), all that changes is we now have
        \begin{equation}
            \diffp{\lagrangianDensity}{A_\mu} = -J^\mu,
        \end{equation}
        and so the result of applying the Euler--Lagrange equations is now
        \begin{equation}
            \dalembertian A^\nu - \partial^\nu \partial_\mu A^\mu = J^\nu,
        \end{equation}
        which is Maxwell's equation in the presence of a source.
    \end{exm}
    
    \section{Symmetries}
    \subsection{Discrete Symmetries}
    There are three discrete symmetries of particular interest in quantum field theory, they are
    \begin{itemize}
        \item \defineindex{parity}: \(\parity \colon \vv{x} \mapsto -\vv{x}\), \(\parity \colon x^\mu = (x^0, x^i) \mapsto (x^0, -x^i) = (x_0, x_i) = x_\mu\), so parity acts to swap raised and lowered indices of positions;
        \item \defineindex{time reversal}: \(\timeReversal \colon t \mapsto t\), \(\timeReversal \colon x^\mu = (x^0, x^i) \mapsto (-x^0, x^i) = -(x^0, -x^i) = -(x_0, x_i) = -x_\mu\), so time reversal acts to swap raised and lowered indices and negate positions;
        \item \defineindex{charge conjugation}: \(\chargeConjugation \colon e \mapsto -e\), where \(e\) is the charge of the particle.
    \end{itemize}
    
    Consider, for example, the Lagrangian of the Klein--Gordon equation:
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial^\mu \varphi)(\partial_\mu\varphi) - \frac{1}{2}m^2 \varphi^2.
    \end{equation}
    Under parity the scalar field is unchanged, and \(\partial^\mu \leftrightarrow \partial^\mu\), so the Lagrangian is unchanged under parity.
    Under time reversal the scalar field is unchanged, and \(\partial^\mu \leftrightarrow -\partial_\mu\), so the Lagrangian is unchanged under time reversal.
    Since this Lagrangian has no charges involved it is trivially invariant under charge conjugation.
    
    Now consider the Electromagnetic Lagrangian,
    \begin{equation}
        \lagrangianDensity = -\frac{1}{4}F^{\mu\nu}F_{\mu\nu} - J^\mu A_\mu.
    \end{equation}
    Under parity transformations \(F^{\mu\nu} \leftrightarrow F_{\mu\nu}\), \(J^\mu \to J_\mu\), and \(A_\mu \to A^\mu\), so
    \begin{equation}
        \parity\lagrangianDensity = -\frac{1}{4}F_{\mu\nu}F^{\mu\nu} - J_\mu A^\mu = \lagrangianDensity,
    \end{equation}
    so the Lagrangian is unchanged by parity.
    Under time reversal transformations \(F^{\mu\nu} \leftrightarrow -F_{\mu\nu}\) since the derivatives in the definition of \(F^{\mu\nu}\) transform as \(\partial^\mu \to -\partial_\mu\) under time reversal.
    On the other hand, \(J^\mu \to J^\mu\), and \(A_\mu \to A_\mu\), since time reversal doesn't effect the current or potential.
    Hence,
    \begin{equation}
        \timeReversal\lagrangianDensity = -\frac{1}{4}(-F_{\mu\nu})(-F^{\mu\nu}) - J^\mu A_\mu = \lagrangianDensity,
    \end{equation}
    so the Lagrangian is unchanged under time reversal.
    Under charge conjugation \(F^{\mu\nu} \leftrightarrow -F_{\mu\nu}\), \(J^\mu \to -J_\mu\), and \(A^\mu \to -J_\mu\), so
    \begin{equation}
        \chargeConjugation\lagrangianDensity = -\frac{1}{4}(-F_{\mu\nu})(-F^{\mu\nu}) - (-J_\mu)(-A^\mu) = \lagrangianDensity,
    \end{equation}
    so the Lagrangian is invariant under charge conjugation.
    
    Invariance under each of these symmetries means that both of these Lagrangians are invariant under the combined \(\chargeConjugation\parity\timeReversal\) symmetry, given by \(x^\mu \mapsto -x^\mu\) and \(e \mapsto -e\).
    
    \subsection{Continuous Symmetries}
    \begin{rmk}
        For more details see the \course{Symmetries of Particles and Fields} course.
    \end{rmk}
    If the action is invariant under a group of continuous symmetries then we have a conserved quantity.
    This is the essence of \defineindex{Noether's theorem}.
    The most common examples being
    \begin{itemize}
        \item invariance under translations in time leading to energy conservation;
        \item invariance under spatial translations leading to momentum conservation;
        \item invariance under rotation leading to angular momentum conservation.
    \end{itemize}
    
    The simplest case is, perhaps, when the Lagrangian is invariant under translations, then the action is also necessarily translation invariant.
    Suppose we have a translation
    \begin{equation}
        \varphi(x) \to \varphi'(x) = \varphi(x) + \delta \varphi(x).
    \end{equation}
    Note that the translation is allowed to depend on \(x\).
    The variation in \(\lagrangianDensity\) is
    \begin{align}
        \delta \lagrangianDensity &= \diffp{\lagrangianDensity}{\varphi} \delta\varphi + \diffp{\lagrangianDensity}{(\partial_\mu)} \delta(\partial_\mu \varphi)\\
        &= \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu\varphi)} \right) \delta \varphi + \diffp{\lagrangianDensity}{(\partial_\mu\varphi)} \delta(\partial_\mu\varphi)\\
        &= \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi \right).
    \end{align}
    The second equality is simply an application of the Euler--Lagrange equations to rewrite the first term.
    The last equality is just recognising the product rule.
    
    Imposing that \(\lagrangianDensity\) is invariant under translations like this we have \(\delta \lagrangianDensity = 0\), which gives us the continuity equation
    \begin{equation}
        \partial_\mu J^\mu(x) = 0,
    \end{equation}
    where
    \begin{equation}
        J^\mu(x) \coloneqq \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta \varphi
    \end{equation}
    is the conserved current, a generalisation of the normal current in electrodynamics.
    There is also a corresponding conserved \enquote{charge}:
    \begin{equation}
        Q = \int\dl{^3\vv{x}} \, J^0(x).
    \end{equation}
    To see that this quantity is conserved consider
    \begin{equation}
        \dot{Q} = \int \dl{^3\vv{x}} \, \partial_0 J^0(x) = -\int \dl{^3\vv{x}} \, \partial_i J^i(x) = 0
    \end{equation}
    where in the last step we've used the divergence theorem to rewrite the last term as a surface integral, then take this surface to be at infinity, and made the usual assumption that fields vanish sufficiently quickly at infinity.
    
    \begin{exm}{Probability Current}{}
        Consider the Lagrangian
        \begin{equation}
            \lagrangianDensity = i\psi^* \partial_t \psi - \frac{1}{2m} (\grad\psi^*) \cdot (\grad\psi) - V(\vv{x}, t)\psi^* \psi.
        \end{equation}
        If we treat \(\psi\) and \(\psi^*\) as separate fields then varying one of them gives the Schrödinger equation.
        This Lagrangian is invariant under the transformation
        \begin{equation}
            \psi \to \e^{i\alpha}\psi, \qand \psi^* \to \e^{-i\alpha}\psi^*
        \end{equation}
        where \(\alpha\) is a constant.
        Linearising we get the transformation
        \begin{equation}
            \psi \to \psi(1 + i\alpha), \qand \psi^* \to \psi^*(1 - i\alpha).
        \end{equation}
        Since we have two fields, \(\psi\) and \(\psi^*\), the conserved current is a sum over field:
        \begin{equation}
            J^\mu = \diffp{\lagrangianDensity}{(\partial_\mu \psi)} \delta \psi + \diffp{\lagrangianDensity}{(\partial_\mu \psi^*)} \delta \psi^*.
        \end{equation}
        Computing the derivatives we have
        \begin{align}
            \diffp{\lagrangianDensity}{(\partial_t \psi)} &= i\psi^*, \qquad & \diffp{\lagrangianDensity}{(\partial_i \psi)} = -\frac{1}{2m}\partial^i \psi^*,\\
            \diffp{\lagrangianDensity}{(\partial_t \psi^*)} &= 0, \qquad & \diffp{\lagrangianDensity}{(\partial_i \psi^*)} = -\frac{1}{2m}\partial^i \psi.
        \end{align}
        Hence, using the variations
        \begin{equation}
            \delta \psi = i\alpha \psi, \qand \delta \psi^* = -i\alpha \psi^*
        \end{equation}
        we have the time component of the conserved current:
        \begin{equation}
            J^0 = \rho = -\alpha \psi^*\psi,
        \end{equation}
        and the position components:
        \begin{align}
            J^i &= \frac{i\alpha}{2m}(\psi^* \partial^i \psi - \psi \partial^i \psi^*)\\
            \vv{J} &= \frac{i\alpha}{2m}(\psi \grad \psi^* - \psi^* \grad \psi).
        \end{align}
        We can then identify \(J^\mu\) as, up to a constant, the conserved probability current of \cref{sec:continuity equation}.
    \end{exm}
    
    It can be shown that the Euler--Lagrange equations are invariant under the addition of a four-divergence, 
    \begin{equation}
        \lagrangianDensity \to \lagrangianDensity + \partial_\mu \Lambda^\mu
    \end{equation}
    for some four-vector field \(\Lambda^\mu\) with a sufficiently smooth derivative.
    The same derivation as before, but setting \(\delta\lagrangianDensity = \partial_\mu \Lambda^\mu\) instead of zero, then gives a conserved current
    \begin{equation}
        j^\mu = \diffp{\lagrangianDensity}{(\partial_\mu\varphi)} \delta\varphi - \Lambda^\mu.
    \end{equation}
    
    As well as changing the Lagrangian directly we can instead act on spacetime, for example, by translation:
    \begin{equation}
        x^\mu \to x'^\mu = x^\mu + a^\mu
    \end{equation}
    for some infinitesimal four-vector \(a^\mu\).
    This induces a change in the scalar field, given by Taylor expanding:
    \begin{equation}
        \varphi(x) \to \varphi(x + a) = \varphi(x) + a^\mu \partial_\mu \varphi(x).
    \end{equation}
    Since the Lagrangian density is also a scalar it must transform the same way:
    \begin{equation}
        \lagrangianDensity \to \lagrangianDensity + a^\mu \partial_\mu \lagrangianDensity = \lagrangianDensity + a^\nu \partial_\mu (\tensor{\delta}{^\mu_\nu} \lagrangianDensity).
    \end{equation}
    Comparing this to the transformation \(\lagrangianDensity \to \lagrangianDensity + \partial_\mu \Lambda^\mu\) we can identify
    \begin{equation}
        \partial_\mu \Lambda^\mu = a^\nu \partial_\mu (\partial^\mu_\nu \lagrangianDensity).
    \end{equation}
    We then have four conserved currents, one for each value of \(\nu\), such as
    \begin{equation}
        J^\mu(x) = \diffp{\lagrangianDensity}{(\partial_\mu \varphi)}a^0\partial_0\varphi - a^0 \tensor{\delta}{^\mu_0}\lagrangianDensity,
    \end{equation}
    which comes from setting \(\nu = 0\).
    We can combine these four currents into a single rank 2 tensor, scaling out the \(a^\nu\) parameter, to get
    \begin{equation}
        \tensor{T}{^\mu_\nu} = \diffp{\lagrangianDensity}{(\partial_\mu \lagrangianDensity)}\partial_\nu \varphi - \lagrangianDensity \tensor{\delta}{^\mu_\nu}.
    \end{equation}
    Raising the index we get
    \begin{equation}
        T^{\mu\nu} = \diffp{\lagrangianDensity}{(\partial_\mu \varphi)}\partial^\nu \varphi - \lagrangianDensity \minkowskiMetric^{\mu\nu}.
    \end{equation}
    This is the \defineindex{energy-momentum tensor}, also called the \define{stress-energy tensor}\index{stress-energy tensor|see{energy-momentum tensor}}\footnote{Strictly this is the energy-momentum density tensor, but very rarely does anyone bother to make that distinction.}.
    
    The conserved \enquote{charge} associated with the \(\nu = 0\) component is
    \begin{equation}
        \int \dl{^3\vv{x}} \, T^{00} = \int \left( \diffp{\lagrangianDensity}{(\partial_0\varphi)} \partial^0\varphi - \lagrangianDensity \right) = \int \dl{^3\vv{x}} \, \hamiltonianDensity = H,
    \end{equation}
    so this is an expression of energy conservation.
    This shouldn't be surprising as translations by \(a^0\) correspond to time translations.
    
    Similarly the conserved charges associated with the \(\nu = i\) components are
    \begin{equation}
        P^i = \int \dl{^3\vv{x}} \, T^{0i} = \int \dl{^3\vv{x}} \, \pi \partial^i \varphi.
    \end{equation}
    Since we expect these three components to form a four-vector with time component given by \(H\) we interpret them as the actual momentum of the particle, as opposed to the conjugate momentum, \(\pi\).
    We can then interpret \(\pi\partial_i\varphi\) as the momentum density.
    
    \chapter{Quantised Fields}
    Quantum field theory is a quantum theory of fields.
    We've seen classical fields, now we need some quantum mechanics before we can start quantising fields.
    So, in this section we'll give a quick recap of quantum mechanics.
    For more details see the \course{Principles of Quantum Mechanics} and \course{Quantum Theory} courses.
    
    \section{Quantum Mechanics}
    \epigraph{It's slightly disturbing that people believe it.}{Richard Ball, on quantum mechanics}
    \subsection{The Basics}
    In quantum mechanics we start with coordinates \(q_i\), and a Lagrangian, \(\lagrangian(q_i, \dot{q}_i)\).
    We then define canonical momenta,
    \begin{equation}
        p_i \coloneqq \diffp{\lagrangian}{q_i}.
    \end{equation}
    From this we can find a Hamiltonian,
    \begin{equation}
        H = \sum_i p+i\dot{q}_i - \lagrangian.
    \end{equation}

    So far this could all be classical.
    The quantum mechanics begins when we interpret \(q_i\), \(p_i\), and \(H\) as \define{Hermitian operators}\index{Hermitian!operator}\footnote{An operator, \(A\), is Hermitian if \(A^\hermit = A\), where \(A^\hermit\) is the \define{Hermitian conjugate}\index{Hermitian!conjugate}, defined such that \(\bra{\psi}A\ket{\varphi} = \bra{\varphi}A^\hermit\ket{\psi}^*\).} acting on a Hilbert space of states.
    The Hermitian requirement is so that these operators have real eigenvalues, which we can then interpret as the results of measurements.
    For example, the average energy is given by
    \begin{equation}
        E = \bra{\psi, t} H \ket{\psi, t},
    \end{equation}
    where \(\ket{\psi, t}\) is the state of the particle, that is a vector in the Hilbert space, and \(\bra{\psi, t} = \ket{\psi, t}^\hermit\) is the corresponding vector in the dual space.
    
    The next step is to realise that operators don't necessarily commute, and so we impose the \define{canonical commutation relation (CCR)}\index{canonical commutation relation}\glossary[acronym]{CCR}{Canonical Commutation Relation}:
    \begin{equation}
        \commutator{q_i}{p_i} \coloneqq i\delta_{ij},
    \end{equation}
    where \(\commutator{A}{B} \coloneqq AB - BA\) is the \defineindex{commutator}.
    As well as this we impose that the positions commute, and the momenta commute, so
    \begin{equation}
        \commutator{q_i}{q_j} = \commutator{p_i}{p_j} = 0.
    \end{equation}
    
    \subsection{Time Dependence}
    The time evolution of a state, \(\ket{\psi, t}\), under a system with Hamiltonian \(H\) is given by the \define{Schrödinger equation}\index{Schrödinger!equation}\index{time dependent Schrödinger equation}
    \begin{equation}
        i \diff{}{t} \ket{\psi, t} = H\ket{\psi, t}.
    \end{equation}
    This is a fairly simple differential equation, if we ignore the fact that \(H\) is an operator and \(\ket{\psi, t}\) is a vector, the solution is just
    \begin{equation}\label{eqn:time evolution of Schrödinger equation}
        \ket{\psi, t} = \e^{-iHt}\ket{\psi}
    \end{equation}
    where \(\ket{\psi} = \ket{\psi, 0}\) is the initial state of the system.
    Fortunately this is perfectly valid, even though \(H\) \emph{is} an operator and \(\ket{\psi, t}\) \emph{is} vector, we just have to interpret the exponential through its power series:
    \begin{equation}
        \e^{-iHt} \coloneqq \sum_{n = 0}^{\infty} \frac{1}{n!}(-iHt)^n.
    \end{equation}
    
    This scheme with which we have worked so far, where operators are time independent and states are time dependent, is called the \define{Schrödinger picture}\index{Schrödinger!picture}.
    It turns out that its actually easier to do QFT by interpreting operators to be time dependent and states to be time independent, called the \define{Heisenberg picture}\index{Heisenberg!picture}.
    We distinguish between these two pictures by either including \(t\) or not, as appropriate in our operators and states.
    A state, \(\ket{\psi}\), in the Heisenberg picture is related to the state \(\ket{\psi, t}\) in the Schrödinger picture by
    \begin{equation}
        \ket{\psi} = \e^{iHt}\ket{\psi, t},
    \end{equation}
    which is just what we get rearranging \cref{eqn:time evolution of Schrödinger equation}.
    An operator, \(A(t)\), in the Heisenberg picture is related to the operator \(A\) in the Schrödinger picture by
    \begin{equation}
        A(t) = \e^{iHt} A \e^{-iHt}.
    \end{equation}
    
    Importantly, expectation values are the same in both pictures, and so both pictures describe the same physics:
    \begin{align}
        \expected{A}_{\symrm{S}} &= \bra{\psi, t} A \ket{\psi, t}\\
        &= \bra{\psi} \e^{iHt} A \e^{-iHt}\\
        &= \bra{\psi} A(t) \ket{\psi}\\
        &= \expected{A}_{\symrm{H}},
    \end{align}
    where the subscripts refer to the picture in which we interpret the expectation value.
    
    This picture changing is the same for all operators, so in particular, the position and momentum in the Heisenberg picture are given by
    \begin{equation}
        q_i(t) = \e^{iHt}q_i\e^{-iHt}, \qqand p_i(t) = \e^{iHt}p_i\e^{-iHt}.
    \end{equation}
    We also have
    \begin{equation}
        H(t) = \e^{iHt} H \e^{-iHt} = \e^{iHt}\e^{-iHt} H = H,
    \end{equation}
    which works since \(H\) commutes with itself, and the exponentials are just power series in \(H\), so also commute with \(H\).
    This shows that the Hamiltonian is time independent in both pictures, which is a statement of energy conservation.
    
    Consider two operators, \(A(t)\) and \(B(t)\), in the Heisenberg picture.
    If we know their commutator, \(\commutator{A}{B}\), in the Schrödinger picture then we can also compute it in the Heisenberg picture:
    \begin{align}
        \commutator{A(t)}{B(t)} &= \braket{\e^{iHt}A\e^{-iHt}}{\e^{iHt}B\e^{-Ht}}\\
        &= \e^{iHt}A\e^{-iHt}\e^{iHt}B\e^{-iHt} - \e^{iHt}B\e^{-iHt}\e^{iHt}A\e^{iHt}\\
        &= \e^{iHt}AB\e^{-iHt} - \e^{iHt}BA\e^{-iHt}\\
        &= \e^{iHt}(AB - BA)\e^{-iHt}\\
        &= \e^{iHt}\commutator{A}{B}\e^{iHt}.
    \end{align}
    In particular,
    \begin{equation}
        \commutator{q_i(t)}{p_j(t)} = \e^{iHt}\commutator{q_i}{p_j}\e^{-iHt} = \e^{iHt}i\delta_{ij}\e^{-iHt} = i\delta_{ij}\e^{iHt}\e^{-iHt} = i\delta_{ij},
    \end{equation}
    so the canonical commutation relations hold if both operators are evaluated at the same time.
    Similarly,
    \begin{equation}
        \commutator{q_i(t)}{q_j(t)} = \commutator{p_i(t)}{p_j(t)} = 0.
    \end{equation}
    
    In the Heisenberg picture instead of the Schrödinger equation we have the \define{Heisenberg equation}\index{Heisenberg!equation}, which we can derive by considering the time derivative of a generic operator, \(A(t)\):
    \begin{align}
        \diff{}{t}A(t) &= \diff{}{t}(\e^{iHt}A\e^{-iHt})\\
        &= iH\e^{iHt}A\e^{-iHt} + \e^{iHt}A(-iH)\e^{-iHt}\\
        &= i\e^{iHt}HA\e^{-iHt} - i\e^{iHt}AH\e^{-iHt}\\
        &= i\e^{iHt}\commutator{H}{A}\e^{-iHt}\\
        &= i\commutator{H}{A(t)}.
    \end{align}
    That is,
    \begin{equation}
        \diff{}{t}A(t) = i\commutator{H}{A(t)}.
    \end{equation}
    
    The Heisenberg equation applied to the position and momentum gives
    \begin{equation}
        \diff{}{t}q(t) = i\commutator{H}{q(t)}, \qand \diff{}{t}p(t) = i\commutator{H}{p(t)}.
    \end{equation}
    Note the similarity to \cref{eqn:hamilton's equations with poisson brackets}.
    In general when we work in the Heisenberg picture things can look pretty similar to classical mechanics, but with \(i\) times the commutator in place of Poisson brackets.
    
    \begin{exm}{}{}
        Consider the Lagrangian for a particle of mass \(m\) in a position dependent potential, \(V\), in one dimension:
        \begin{equation}
            L = \frac{1}{2}m\dot{q}^2 - V(q).
        \end{equation}
        The Hamiltonian for this system is
        \begin{equation}
            H = \frac{p^2}{2m} + V(q).
        \end{equation}
        We then have
        \begin{equation}
            \dot{q} = i\commutator{H}{q(t)} = \frac{i}{2m}\commutator{p^2}{q}.
        \end{equation}
        Now, consider three operators, \(A\), \(B\), and \(C\).
        We have
        \begin{align}
            \commutator{AB}{C} &= ABC - CBA\\
            &= ABC - ACB + ACB - CBA\\
            &= A\commutator{B}{C} + \commutator{A}{C}B.
        \end{align}
        Using this with \(A = B = p\) and \(C = q\) gives
        \begin{equation}
            \commutator{p^2}{q} = p\commutator{p}{q} + \commutator{p}{q}p = -2ip,
        \end{equation}
        where the negative comes from the antisymmetry of the commutator, \(\commutator{p}{q} = -\commutator{q}{p} = -i\).
        Hence,
        \begin{equation}
            \dot{q} = \frac{p}{2m}.
        \end{equation}
        Classically we would have \(p = mv = m\dot{q}\), and this is just the quantum analogue of the classical nonrelativistic momentum.
        
        We also have
        \begin{equation}
            \dot{p} = i\commutator{H}{p(t)} = i\commutator{V(q)}{p}.
        \end{equation}
        We can expand \(V(q)\) as a power series in \(q\), we also have the identity
        \begin{equation}
            \commutator{q^n}{p} = inq^{n-1}.
        \end{equation}
        This can be proven with induction on \(n\).
        First, take \(n = 0\), then
        \begin{equation}
            \commutator{q^0}{p} = \commutator{\ident}{p} = 0.
        \end{equation}
        Now suppose that
        \begin{equation}
            \commutator{q^k}{p} = ikq^{k-1}
        \end{equation}
        for some nonnegative integer \(k\).
        Then
        \begin{multline}
            \commutator{q^{k+1}}{p} = \commutator{qq^k}{p} = q\commutator{q^k}{p} + \commutator{q}{p}q^k\\
            = q ikq^{k-1} + iq^k = i(k + 1)q^k,
        \end{multline}
        and so by induction the identity holds for all natural numbers, \(n\).
        
        Expanding \(V\) as a Taylor series we have
        \begin{equation}
            V(q) = \sum_{n = 0}^{\infty} \frac{q^n}{n!} \diffp[n]{V}{q}\bigg|_{q = 0}.
        \end{equation}
        Then we have
        \begin{align}
            \commutator{V(q)}{p} &= \sum_{n = 0}^{\infty} \frac{1}{n!} \diffp[n]{V}{q}\bigg|_{q = 0} \commutator{q^n}{p}\\
            &= \sum_{n = 0}^{\infty} \frac{1}{n!} \diffp[n]{V}{q}\bigg|_{q = 0} in q^{n - 1}\\
            &= \sum_{n = 1}^{\infty} \frac{iq^{n - 1}}{(n - 1)!} \diffp[n]{V}{q}\bigg|_{q = 0}.
        \end{align}
        To first order we then have
        \begin{equation}
            \commutator{V(q)}{p} = i \diffp[n]{V}{q}\bigg|_{q = 0}
        \end{equation}
        and so
        \begin{equation}
            \dot{p} = i\commutator{V(q)}{p} = -\diffp{V}{q}.
        \end{equation}
        Compare this to the classical nonrelativistic equation
        \begin{equation}
            F = -\diffp{V}{x},
        \end{equation}
        expressing the force due to a potential.
        
        Both of these two analogies, known as \defineindex{Ehrenfest's theorem}, between classical mechanics and quantum mechanics are really due to the similarity between the classical and quantum equations of motion in the Heisenberg picture.
    \end{exm}
    
    \section{Quantum Fields}
    \epigraph{Fundamentally silly}{Richard Ball}
    The procedure for quantising fields is almost identical to the procedure for quantising position and momentum.
    Instead of the position and momentum we have the fields \(\varphi\) and \(\pi\).
    We interpret these as operators in the Heisenberg picture.
    Since in field theory we deal with densities instead to be integrated over, such as \(\lagrangianDensity\) instead of \(\lagrangian\), we replace \(\delta_{ij}\) in the canonical commutation relations with \(\delta^3(\vv{x} - \vv{x}')\), note that this treats time differently to space.
    We postulate that the at some time, \(t\), we have
    \begin{equation}
        \commutator{\varphi(t, \vv{x})}{\pi(t, \vv{x}')} = i\delta^3(\vv{x} - \vv{x}')
    \end{equation}
    and the fields commute with themselves at all positions:
    \begin{equation}
        \commutator{\varphi(t, \vv{x})}{\varphi(t, \vv{x}')} = \commutator{\pi(t, \vv{x})}{\pi(t, \vv{x}')} = 0.
    \end{equation}
    This is called the \define{equal time commutation relation (ETCR)}\index{equal time commutation relation}\glossary[acronym]{ETCR}{Equal Time Commutation Relation}.
    
    In quantum mechanics as done in the previous section we treat \(t\) as a parameter and \(\vv{x}\) as an operator.
    This is clearly not Lorentz covariant.
    There are also other problems, such as having only a single \(\vv{x}\) operator which only allows us to talk of a single particle.
    In quantum field theory we interpret both \(t\) and \(\vv{x}\) as parameters and \(\varphi\) as an operator.
    We can then make this Lorentz covariant and treat systems with multiple particles.
    
    \subsection{Equations of Motion}
    The Heisenberg equation,
    \begin{equation}
        \dot{A}(t, \vv{x}) = i \commutator{H}{A(t, \vv{x})},
    \end{equation}
    applies to any operator, \(A\), and hence applies to \(\varphi\).
    We then have
    \begin{equation}
        \dot{\varphi}(t, \vv{x}) = i \commutator{H}{\varphi(t, \vv{x})} = i \int \dl{^3\vv{x}'} \, \commutator{\hamiltonianDensity(t, \vv{x}')}{\varphi(t, \vv{x})}
    \end{equation}
    We've replaced the Hamiltonian with an integral over the Hamiltonian density.
    We'll now introduce a shorthand notation where primes denote quantities evaluated at \((t, \vv{x}')\) and an absence of a prime is a quantity evaluated at \((t, \vv{x})\), so \(\varphi' = \varphi(t, \vv{x}')\) and \(\varphi = \varphi(t, \vv{x})\).
    The Hamiltonian density is
    \begin{equation}
        \hamiltonianDensity' = \frac{1}{2}(\pi'^2 + (\grad'\varphi')^2 + m^2\varphi'^).
    \end{equation}
    The commutator then becomes
    \begin{equation}
        \commutator{\hamiltonianDensity'}{\varphi} = \frac{1}{2}\left( \commutator{\pi'^2}{\varphi} + \commutator{(\grad'\varphi')^2}{\varphi} + m^2\commutator{\varphi'^2}{\varphi} \right).
    \end{equation}
    The last two terms vanish, since the fields commute with themselves at all positions.
    The first term isn't too bad if we use
    \begin{equation}
        \commutator{AB}{C} = A \commutator{B}{C} + \commutator{A}{C}B,
    \end{equation}
    we have
    \begin{align}
        \frac{1}{2}\commutator{\pi'^2}{\varphi} &= \frac{1}{2}\pi'\commutator{\pi'}{\varphi} + \frac{1}{2}\commutator{\pi'}{\varphi}\pi'\\
        &= -\frac{1}{2}\pi'\commutator{\varphi}{\pi'} - \frac{1}{2}\commutator{\varphi}{\pi'}\pi'\\
        &= -\frac{1}{2} \pi' i\delta^3(\vv{x} - \vv{x}') - \frac{1}{2} i\delta^3(\vv{x} - \vv{x}') \pi'\\
        &= -i\pi' \delta^3(\vv{x} - \vv{x}').
    \end{align}
    Hence, we have the equation of motion
    \begin{align}
        \dot{\varphi}(t, \vv{x}) &= i \int \dl{^3\vv{x}'} \commutator{\hamiltonianDensity(t, \vv{x}')}{\varphi(t, \vv{x})}\\
        &= i \int \dl{^3\vv{x}'} (-i\pi(t, \vv{x}') \delta^3(\vv{x} - \vv{x}'))\\
        &= \pi(t, \vv{x}).
    \end{align}
    So, \(\dot{\varphi} = \pi\).
    This should be no surprise since we found this same relationship for \(\varphi\) satisfying the Klein--Gordon equation before we started quantising.
    
    Similarly we can find the equation of motion for \(\pi\):
    \begin{equation}
        \dot{\pi}(t, \vv{x}) = i\commutator{H}{\pi(t, \vv{x})} = i\int \dl{^3\vv{x}'} \commutator{\hamiltonianDensity(t, \vv{x}')}{\pi(t, \vv{x})}.
    \end{equation}
    The commutator in this case is
    \begin{equation}
        \commutator{\hamiltonianDensity'}{\pi} = \frac{1}{2}(\commutator{\pi'^2}{\pi} + \commutator{(\grad'\varphi')^2}{\pi} + m^2\commutator{\varphi'^2}{\pi}).
    \end{equation}
    The first term vanishes this time.
    The last term can be computed easily:
    \begin{align}
        \frac{1}{2}m^2\commutator{\varphi'^2}{\pi} &= \frac{1}{2}m^2 \varphi'^2\commutator{\varphi'}{\pi} + \frac{1}{2}m^2\commutator{\varphi'}{\pi}\varphi'\\
        &= \frac{1}{2}m^2 i\delta^3(\vv{x} - \vv{x}') + \frac{1}{2}m^2 \varphi' i\delta^3(\vv{x} - \vv{x}')\\
        &= im^2 \delta^3(\vv{x} - \vv{x}').
    \end{align}
    The second term isn't too bad, we just have to notice that \(\grad'\) acts on \(\vv{x}'\) and not on \(\vv{x}\), allowing us to pull it outside of commutators like \(\commutator{\grad'\varphi'}{\pi} = \grad'\commutator{\varphi'}{\pi}\).
    Proceeding as before we then have
    \begin{align}
        \frac{1}{2}\commutator{(\grad'\varphi')^2}{\pi} &= \frac{1}{2} (\grad'\varphi') \cdot \commutator{\grad'\varphi'}{\pi} + \frac{1}{2} \commutator{\grad'\varphi'}{\pi} \cdot (\grad'\varphi')\\
        &= \frac{1}{2}(\grad'\varphi') \cdot (\grad'\commutator{\varphi'}{\pi}) + \frac{1}{2}(\grad'\commutator{\varphi'}{\pi}) \cdot (\grad'\varphi')\\
        &= \frac{1}{2}(\grad'\varphi') \cdot (\grad'i\delta^3(\vv{x} - \vv{x}')) + \frac{1}{2}(\grad'i\delta^3(\vv{x} - \vv{x}')) \cdot (\grad'\varphi')\notag\\
        &= i(\grad'\varphi') \cdot (\grad'\delta^3(\vv{x} - \vv{x}')).
    \end{align}
    We can deal with the gradient of the delta distribution by integrating by parts, and assuming that fields vanish sufficiently quickly at infinity:
    \begin{align}
        \int \dl{^3\vv{x}'} (\grad'\varphi) \cdot (\grad'\delta^3(\vv{x} - \vv{x}')) &= \int \dl{^3\vv{x}'} \, \grad' \cdot [\delta^3(\vv{x} - \vv{x}') (\grad'\varphi')]\\
        &\qquad- \int \dl{^3\vv{x}'} \, (\grad'^2\varphi') \delta^3(\vv{x} - \vv{x}')\\
        &= \int_{\partial V} [\delta^3(\vv{x} - \vv{x}') \grad'\varphi']\\
        &\qquad- \int \dl{^3\vv{x}'} \, (\grad'^2\varphi') \delta^3(\vv{x} - \vv{x}')\\
        &= - \int \dl{^3\vv{x}'} \, (\grad'^2\varphi') \delta^3(\vv{x} - \vv{x}')
    \end{align}
    where as usual the boundary term vanishes.
    This means that, as distributions,
    \begin{equation}
        i(\grad'\varphi') \cdot (\grad'\delta^3(\vv{x} - \vv{x}')) = -i(\grad'^2\varphi')\delta^3(\vv{x} - \vv{x}').
    \end{equation}
    Hence, we have
    \begin{align}
        \dot{\pi}(t, \vv{x}) &= i \int \dl{^3\vv{x}'} \commutator{\hamiltonianDensity(t, \vv{x}')}{\varphi(t, \vv{x})}\\
        &= i \int \dl{^3\vv{x}'} \left[ -i(\grad'^2\varphi') \delta^3(\vv{x} - \vv{x}') + im^2 \delta^3(\vv{x} - \vv{x}')\right]\\
        &= \laplacian\varphi - m^2\varphi.
    \end{align}
    
    Now, we have \(\pi = \dot{\varphi}\), so \(\dot{\pi} = \ddot{\varphi} = \partial_0\partial^0 \varphi\).
    Hence,
    \begin{equation}
        \partial_0\partial^0 \varphi = \laplacian\varphi - m^2\varphi = \partial_i\partial^i \varphi - m^2\varphi.
    \end{equation}
    Rearranging this we get
    \begin{equation}
        0 = (\partial_0\partial^0 - \partial_i\partial^i)\varphi + m^2\varphi = (\dalembertian + m^2)\varphi,
    \end{equation}
    so the quantised fields satisfy the Klein--Gordon equation.
    This means that the Heisenberg equations for field operators look a lost like classical field equations with operators replacing the fields and \(i\) times the commutator replacing Poisson brackets.
    
    Even though the Hamiltonian formulation and the equal time commutation relations aren't Lorentz covariant the final result is, and that's all that we care about.
    The steps in the derivation, however, are frame dependent.
    This is an inescapable problem with canonical quantisation.
    
    \chapter{Quantum Harmonic Oscillator}
    \epigraph{By understanding the harmonic oscillator you understand everything! Well, accept from all the things we can't solve.}{Richard ball}
    \epigraph{Surely the hydrogen atom isn't a harmonic oscillator? Well it is!}{Richard Ball}
    The quantum harmonic oscillator is one of the most important systems in quantum mechanics.
    It's everywhere, we use it to model atoms and phonons, molecules, and pretty much anything else.
    Even other systems we study in quantum mechanics, such as the hydrogen atom or pairs of particles, can be interpreted as a harmonic oscillator by a change of coordinates.
    After all, any potential in physics can always be Taylor expanded to second order at a minimum to get a harmonic potential.
    Part of the reason why we study the harmonic oscillator is that its very nice.
    We can solve it exactly.
    The results are also nice, evenly spaced energy levels, bounded below.
    
    The harmonic oscillator is the system with the Hamiltonian
    \begin{equation}
        H = \frac{p^2}{2m} + \frac{1}{2}m\omega^2 q^2.
    \end{equation}
    We have previously remarked on the similarity of this and the Klein--Gordon Hamiltonian density, and suggested that study of the harmonic oscillator could be enlightening for the Klein--Gordon equation.
    In this section we'll study the harmonic oscillator, computing the spectrum of the Hamiltonian, and discussing various interpretations.
    In the next chapter we'll go back through the same steps but for fields obeying the Klein--Gordon and the Hamiltonian density.
    
    \section{Solution}
    There are two ways to solve the harmonic oscillator: the analytic way, involving Hermite polynomials, which is frankly, horrible, and the algebraic way, which is maybe more mysterious but is much more inline with the quantum field theory interpretation, so this is the way we'll proceed.
    
    Start by defining a new operators from the position and momentum operators:
    \begin{align}
        a \coloneqq \sqrt{\frac{m\omega}{2}} q + \frac{i}{\sqrt{2m\omega}} p.
    \end{align}
    The factors are chosen so that our final statements have a simple form, note that \(a\) is dimensionless.
    The Hermitian conjugate of this is
    \begin{equation}
        a^\hermit = \sqrt{\frac{m\omega}{2}} q - \frac{i}{\sqrt{2m\omega}} p.
    \end{equation}
    Note that \(a\) is not Hermitian, it's not an observable, for now we can just think of it as a mathematical trick.
    We'll see later that combinations of \(a\) and \(a^\hermit\) can be treated as observables with meaningful values.
    The motivation for this definition is that if \(a\) and \(a^\hermit\) commuted (they don't, as we'll see soon) we could factorise the Hamiltonian as
    \begin{equation}
        \omega aa^\hermit.
    \end{equation}
    
    A bit of algebra allows us to rewrite \(q\) and \(p\) in terms of \(a\) and \(a^\hermit\):
    \begin{equation}
        q = \frac{1}{2m\omega} (a + a^\dagger), \qqand p = -i\sqrt{\frac{m\omega}{2}} (a - a^\hermit).
    \end{equation}
    We can now compute the commutation relations for \(a\) and \(a^\hermit\) from the canonical commutation relation:
    \begin{align}
        i &= \commutator{q}{p}\\
        &= -\frac{i}{2}\commutator{a + a^\hermit}{a - a^\hermit}\\
        &= -\frac{i}{2}\left( \commutator{a}{a} - \commutator{a}{a^\hermit} + \commutator{a^\hermit}{a} - \commutator{a^\hermit}{a^\hermit} \right)\\
        &= i\commutator{a}{a^\hermit}.
    \end{align}
    Hence, we have that
    \begin{equation}
        \commutator{a}{a^\hermit} = 1.
    \end{equation}
    
    An alternative approach to quantum mechanics is to start with \(a\) and \(a^\hermit\), motivating them starting with the interpretation we'll see later, and then show that the canonical commutation relations are what they are as a consequence of this choice.
    
    We can write the Hamiltonian in terms of \(a\) and \(a^\hermit\):
    \begin{align}
        H &= -\frac{\omega}{4}(a - a^\hermit)^2 + \frac{\omega}{4}(a + a^\hermit)^2\\
        &= -\frac{\omega}{4}(a^2 - aa^\hermit - a^\hermit a + (a^\hermit)^2) + \frac{\omega}{4}(a^2 + aa^\hermit + a^\hermit a + (a^\hermit)^2)\\
        &= \frac{\omega}{2}(aa^\hermit + a^\hermit a).
    \end{align}
    Now use
    \begin{equation}
        1 = \commutator{a}{a^\hermit} = aa^\hermit - a^\hermit a \implies aa^\hermit = a^\hermit a + 1
    \end{equation}
    and we get
    \begin{equation}
        H = \frac{\omega}{2}(aa^\hermit + a^\hermit a) = \frac{\omega}{2}(a^\hermit a + 1 + a^\hermit a) = \omega\left( a^\hermit a + \frac{1}{2} \right).
    \end{equation}
    
    To compute the spectrum of the Hamiltonian we work with the operator \(N \coloneqq a^\hermit a\).
    This \emph{is} a Hermitian operator, \(N^\hermit = (a^\hermit a) = a^\hermit (a^\hermit)^\hermit = a^\hermit a = N\).
    We can then write the Hamiltonian as
    \begin{equation}
        H = \omega\left( N + \frac{1}{2} \right),
    \end{equation}
    if you remember what the energy eigenvalues of the harmonic oscillator are you may see where this is going.
    
    Let \(\ket{n}\) be an eigenstate of \(N\) with eigenvalue \(n\), so \(N\ket{n} = n\ket{n}\).
    For now all we know is that \(n \in \reals\), since \(N\) has real eigenvalues as a Hermitian operator.
    We choose to normalise these states so that
    \begin{equation}
        \braket{n}{m} = \delta_{nm}
    \end{equation}
    The use of \(n\) and \(m\) as labels, as well as the Dirac delta hints at something we will show later, that \(n\) is an integer, but for now we just treat \(n\) as a real number.
    
    The expectation value of \(N\) in the state \(\ket{n}\) is
    \begin{equation}
        \expected{N} = \bra{n}N\ket{n} = n\braket{n}{n} = n.
    \end{equation}
    Using \(N = a^\hermit a\) we can calculate this expectation value a different way:
    \begin{equation}
        n = \bra{n} N \ket{n} = \bra{n} a^\hermit a \ket{n} = \norm{a \ket{n}}^2,
    \end{equation}
    and since \(\norm{a \ket{n}}^2 \ge 0\) we have that \(n \ge 0\).
    
    Now consider commutators of \(N\) with \(a^\hermit\) and \(a\):
    \begin{align}
        \commutator{N}{a^\hermit} = \commutator{a^\hermit a}{a^\hermit} = a^\hermit \commutator{a}{a^\hermit} + \commutator{a^\hermit}{a^\hermit}a = a^\hermit.
    \end{align}
    Similarly,
    \begin{equation}
        \commutator{N}{a} = \commutator{a^\hermit a}{a} = a^\hermit\commutator{a}{a} + \commutator{a^\hermit}{a}a = -a.
    \end{equation}
    
    Suppose we have an eigenstate, \(\ket{n}\).
    What happens when we act on this with \(N\)?
    Well, it turns out that we get another eigenstate:
    \begin{align}
        N a^\hermit \ket{n} &= (\commutator{N}{a^\hermit} + a^\hermit N)\ket{n}\\
        &= (a^\hermit + a^\hermit N)\ket{n}\\
        &= a^\hermit \ket{n} + a^\hermit N \ket{n}\\
        &= a^\hermit \ket{n} + a^\hermit n \ket{n}\\
        &= (n + 1)a^\hermit \ket{n}.
    \end{align}
    So, \(a^\hermit \ket{n}\) is an eigenstate of \(N\) with eigenvalue \(n + 1\).
    That is, \(a^\hermit \ket{n} \propto \ket{n + 1}\).
    We can fairly easily figure out the constant of proportionality by considering \(\norm{a^\hermit\ket{n}}^2\):
    \begin{align}
        \norm{a^\hermit \ket{n}}^2 &= \bra{n} aa^\hermit \ket{n}\\
        &= \bra{n} (a^\hermit a + \commutator{a}{a^\hermit}) \ket{n}\\
        &= \bra{n} (a^\hermit a + 1) \ket{n}\\
        &= \bra{n} a^\hermit a \ket{n} + \braket{n}{n}\\
        &= \bra{n} N \ket{n} + 1\\
        &= n + 1.
    \end{align}
    Since the overall phase of a state has no physical significance we are free to choose the phase of the proportionality constant, so we make the sensible choice that its real and positive.
    That is, we choose the positive square root, \(\norm{a^\hermit \ket{n}} = \sqrt{n + 1}\), and we then have
    \begin{equation}
        a^\hermit \ket{n} = \sqrt{n + 1}\ket{n}.
    \end{equation}
    
    We can proceed similarly for \(a\ket{n}\), we have
    \begin{align}
        N a \ket{n} &= (\commutator{N}{a} + aN)\ket{n}\\
        &= (-a + aN)\ket{n}\\
        &= (n - 1)a \ket{n}.
    \end{align}
    So \(a\ket{n}\) is an eigenstate of \(N\) with eigenvalue \(n - 1\), so \(a\ket{n} \propto \ket{n - 1}\).
    We can, again, work out the constant of proportionality:
    \begin{equation}
        \norm{a\ket{n}}^2 = \bra{n} a^\hermit a \ket{n} = \bra{n} N \ket{n} = n.
    \end{equation}
    So, again choosing a real, positive, solution we have
    \begin{equation}
        a\ket{n} = 
        \begin{cases}
            \sqrt{n}\ket{n - 1} & n > 0,\\
            0 & n = 0,
        \end{cases}
    \end{equation}
    the reason for the careful distinction when \(n = 0\) is that the state \(\ket{0 - 1} = \ket{-1}\) is not defined, as the eigenvalues of \(N\) are nonnegative.
    The reason we don't have to exclude, say, \(n = 0.5\), which would have \(\ket{0.5 - 1} = \ket{-0.5}\), is that, as we will now show, \(n\) is a nonnegative integer, that is \(n \in \naturals = \{0, 1, 2, \dotsc\}\).
    
    Suppose that \(n \notin \naturals\).
    Consider some \(m \in \naturals \setminus \{0\}\).
    Then we can apply \(a\) \(m\) times to \(\ket{n}\):
    \begin{multline}
        a^m \ket{n} = \sqrt{n} a^{m - 1}\ket{n} = \sqrt{n}\sqrt{n - 1} a^{m - 2} \ket{n}\\
        = \dotsb = \sqrt{n} \sqrt{n - 1} \dotsm \sqrt{n - m + 1} \ket{n - m}.
    \end{multline}
    For sufficiently large \(m\) we will have \(n - m < 0\).
    However, \(n - m\) is an eigenvalue and all eigenvalues of \(N\) are nonnegative.
    This is a contradiction and so we must have \(n \in m\).
    In this case one of the square roots will vanish before we get to negative eigenvalues and we'll avoid the contradiction.
    
    Since \(H = \omega(N + 1/2)\) we can see that the eigenvalues of the Hamiltonian are
    \begin{equation}
        \left( n + \frac{1}{2} \right)\omega \qqwhere n \in \naturals.
    \end{equation}
    The eigenvectors of the Hamiltonian can all be written as
    \begin{equation}
        \ket{n} = \frac{(a^\hermit)^n}{\sqrt{n!}} \ket{0}
    \end{equation}
    where \(\ket{0}\) is such that when \(a\) acts on it we get zero:
    \begin{equation}
        a\ket{0} \coloneqq 0.
    \end{equation}
    Note that \(\ket{0}\) is an eigenstate with eigenvalue 0, and \(0\) is the zero vector, these are not the same.
    The factor of \(1/\sqrt{n!}\) is just normalisation, if \(\ket{0}\) is normalised then \(a^\hermit \ket{0} = \ket{1} = \sqrt{1}\ket{1}\), then \((a^\hermit)^2\ket{0} = \sqrt{1}a^\hermit \ket{1} = \sqrt{1} \sqrt{2} \ket{2}\), and so on, so we can get rid of all these factors with \(1/\sqrt{n!}\), and we're left with the normalised states \(\ket{n}\).
    
    The most important thing about the harmonic oscillator is that the energy eigenvalues are equally spaced.
    This makes harmonic oscillators very nice.
    Another nice property is that the energy eigenvalues are nonnegative.
    This means that the energy is bounded below, which is good as if it wasn't then systems would quickly decay to having negative infinite energies.
    The equally spaced eigenvalues is what makes all of quantum field theory possible.
    
    \section{Interpretations}
    \epigraph{It's fine, but in a sense, it's just wrong.}{Richard Ball}
    There are two ways to interpret the harmonic oscillator.
    Historically the first we discuss here was Schrödinger's interpretation, and actually came a year after the second interpretation we'll discuss, which is due to Heisenberg.
    However, for a long time Schrödinger's interpretation was preferred, mostly because it uses more familiar mathematics, like differential equations, and also because it aligns more similarly with the notion of a classical harmonic oscillator.
    
    \subsection{Wave Function Interpretation}
    \epigraph{This is the way you were taught the harmonic oscillator in kindergarten I guess.}{Richard Ball}
    The quantum harmonic oscillator is a quantised classical harmonic oscillator.
    We take the normal harmonic oscillator Hamiltonian, replace the position and momentum with operators, and solve the equations of motion.
    The picture that people then have in their head is a harmonic potential with equally spaced energy levels, as seen in \cref{fig:harmonic potential and energy levels}.
    
    \begin{figure}
        \tikzsetnextfilename{HO-wave-function-interpretation-energy-levels}
        \begin{tikzpicture}
            \draw[->, thick] (-2, 0) -- (2, 0) node [below] {\(q\)};
            \draw[->, thick] (0, 0) -- (0, 3) node [left] {\(E\)};
            \draw[highlight, very thick, domain={-sqrt(3)}:{sqrt(3)}] plot (\x, \x*\x);
            \foreach \y in {0, 0.5, ..., 2.5} {
                \draw[Blue, very thick] (-1, \y) -- (1, \y);
            }
        \end{tikzpicture}
        \caption{The harmonic potential and the evenly spaced energy levels it produces.}
        \label{fig:harmonic potential and energy levels}
    \end{figure}
    
    The physical variables are \(q\) and \(p\), with \(q = x\) the position operator, and \(p\) chosen to satisfy the canonical commutation relations, which it can be shown is the case if \(p = -i\diff{}/{x}\).
    We solve the equation \(a \ket{0} = 0\) to find the vacuum state \(\ket{0}\), and it's wave function \(\psi_0(x) = \braket{x}{0}\):
    \begin{equation}
        \psi_0(x) = \sqrt{\frac{m\omega}{2\pi}} \exp\left[ -\frac{1}{2}m\omega x^2 \right].
    \end{equation}
    We then find the wave equation for the \(n\)th excited state, \(\sqrt{n!}\psi_n(x) = \bra{x}(a^\hermit)^n \ket{0}\):
    \begin{equation}
        \psi_n(x) = \frac{1}{\sqrt{n!}} \left( 0\frac{i}{\sqrt{m\omega}} \diff{}{x} + i\sqrt{\frac{m\omega}{2}} x \right)^n \psi_0(x)
    \end{equation}
    where the expression in brackets is just \(a^\hermit = i/\sqrt{2m\omega} p + \sqrt{m\omega/2} q\) with the operators substituted in.
    After working through lots of derivatives of Gaussians we will find that the solutions are Gaussians times a Hermite polynomial, \(H_n\):
    \begin{equation}
        \psi_n(x) = \frac{1}{\sqrt{2^n n!}} \left( \frac{m\omega}{\pi} \right)^{1/4} \exp\left[ -\frac{1}{2}m\omega x^2 \right] H_n\left( \sqrt{m\omega}x \right).
    \end{equation}
    
    This interpretation is called \defineindex{first quantisation}, presumably because it is the first way that most people are taught to interpret the harmonic oscillator.
    
    \subsection{Quanta}
    The energy eigenvalues of the harmonic oscillator are evenly spaced.
    This allows us to interpret the state \(\ket{n}\) as consisting of \(n\) identical quanta, all of the same energy, \(\omega\).
    The state \(\ket{0}\) is then the vacuum state, where there are no quanta, and we have the vacuum energy \(\omega/2\).
    
    Rather than try to interpret what a harmonic oscillator is we focus on what it does.
    Think of it as a black box which can absorb a quanta of energy, so it has \(n + 1\) quanta, or it can emit a quanta, so it has \(n - 1\) quanta.
    Clearly these occurrences align with the action of \(\ket{a}^\hermit\) and \(\ket{a}\) respectively.
    This leads to us interpreting \(a^\hermit\) as a \defineindex{creation operator}, producing a quanta of energy in the black box, and \(a\) as a \defineindex{annihilation operator}, destroying a quanta of energy in the black box.
    Of course this energy must come from somewhere and go somewhere, but just within the black box it may as well be appearing and vanishing.
    
    This interpretation, while a bit odd at first, is actually closer to what we do in experiments.
    We scatter particles, essentially adding them to the black box, and see what comes out, that is, what leaves the black box.
    This interpretation also works well when considering atomic spectra, which occur in much the same way.
    
    This interpretation is intrinsically multiparticle, which is good for quantum field theory.
    This interpretation is called \defineindex{second quantisation}, presumably because it is the second way that most people are taught to interpret the harmonic oscillator.
    
    \section{Harmonic Oscillator with Time Dependence}
    If we include time dependence in the Harmonic oscillator the, working in the Heisenberg picture, this corresponds to replacing \(q\) and \(p\) with time dependent operators, which in term means introducing a time dependence to \(a\) and \(a^\hermit\).
    This time dependence doesn't change the first step of solving the Harmonic oscillator, which is finding \(q\) and \(p\) in terms of \(a\) and \(a^\hermit\), which gives us
    \begin{align}
        q(t) &= \frac{1}{\sqrt{2m\omega}} [a(t) + a^\hermit(t)],\\
        p(t) &= i\sqrt{\frac{m\omega}{2}} [a(t) - a^\hermit(t)].
    \end{align}
    The commutation relations for \(a(t)\), \(a^\hermit(t)\), and \(H\) are \(\commutator{a(t)}{a^\hermit(t)} = 1\), \(\commutator{H}{a(t)} = -\omega a(t)\), and \(\commutator{H}{a^\hermit(t)} = \omega a^\hermit(t)\), which are the same as if we replace \(H\) with \(N\), since \(H = \omega(N + 1/2)\).
    The time dependence doesn't change the commutators, which we can check quite easily:
    \begin{align}
        \commutator{a(t)}{a^\hermit(t)} &= \commutator{\e^{-iHt}a\e^{iHt}}{\e^{-iHt}a^\hermit\e^{iHt}}\\
        &= \e^{-iHt}a\e^{iHt}\e^{-iHt}a^\hermit\e^{iHt} - \e^{-iHt}a^\hermit\e^{iHt}\e^{-iHt}a\e^{iHt}\\
        &= \e^{-iHt}aa^\hermit\e^{iHt} - \e^{-iHt}a^\hermit a\e^{iHt}\\
        &= \e^{-iHt} \commutator{a}{a^\hermit}\e^{iHt}\\
        &= \e^{-iHt} 1 \e^{iHt}\\
        &= 1.
    \end{align}
    Likewise, the commutators with the Hamiltonian are unchanged:
    \begin{align}
        \commutator{H}{a^\hermit(t)} &= \commutator{H}{\e^{-iHt}a^\hermit \e^{iHt}}\\
        &= H\e^{-iHt}a^\hermit\e^{iHt} - \e^{-iHt}a^\hermit\e^{iHt} H\\
        &= \e^{-iHt} Ha^\hermit \e^{iHt} - \e^{-iHt} a^\hermit H \e^{iHt}\\
        &= \e^{-iHt} \commutator{H}{a^\hermit} \e^{iHt}\\
        &= \e^{-iHt} \omega a^\hermit \e^{iHt}\\
        &= \omega a^\hermit(t).
    \end{align}
    An similarly for \(\commutator{H}{a(t)}\).
    
    We can then solve the Heisenberg equation for the creation and annihilation operators:
    \begin{align}
        \dot{a}(t) = i \commutator{H}{a(t)} = -i \omega a(t).
    \end{align}
    This very simple differential equation, \(\dot{a}(t) = -i\omega a(t)\), has a plane wave solution,
    \begin{equation}
        a(t) = a \e^{-i\omega t},
    \end{equation}
    where \(a = a(0)\) is the annihilation operator in the Schrödinger picture.
    Taking the Hermitian conjugate of this we get
    \begin{equation}
        \dot{a}^\hermit(t) = a^\hermit \e^{i\omega t}.
    \end{equation}
    The position and momentum operators are then
    \begin{align}
        q(t) &= \frac{1}{\sqrt{2m\omega}} [a \e^{-i\omega t} + a^\hermit \e^{i\omega t}],\\
        p(t) &= i\sqrt{\frac{m\omega}{2}} [a \e^{-i\omega t} - a^\hermit \e^{i\omega t}].
    \end{align}
    
    \chapter{Mode Expansions}
    \section{Expanding the Field}
    The field \(\varphi(x)\) satisfies the Klein--Gordon equations.
    We've seen that this allows plane wave solutions, \(\e^{\pm i p \cdot x}\).
    This suggests that we can write a general solution as a superposition of wave solutions, which is just a Fourier transform.
    Since \(\varphi\) is a function of \(x\) this is really an inverse Fourier transform.
    For comparison consider the inverse transform of a single variable, real, one-dimensional function, \(f\):
    \begin{equation}
        f(x) = \int \frac{\dl{p}}{2\pi} (\tilde{f}(p) \e^{-ip x} + \tilde{f}^*(p) \e^{ip x}).
    \end{equation}
    Here \(\tilde{f}(p)\) is the Fourier transformed function, which acts as a Fourier coefficient here.
    We need both the \(\e^{ipx}\) and conjugate \(\e^{-ipx}\) terms to get a real function.
    The factor of \(2\pi\) is part of the normalisation, but exactly where it appears between the forward and inverse is just convention.
    
    We want our field to have on-shell momentum, that is \(p^2 = m^2\).
    To enforce this we include a factor of \(2\pi \delta(p^2 - m^2)\), where the \(2\pi\) is factor is from the integral representation of the Dirac delta, as the Fourier transform of one:
    \begin{equation}
        \int \dl{x} \, \e^{-ip \cdot x} = 2\pi \delta(p).
    \end{equation}
    We also want to restrict ourselves to positive energy solutions, so \(p_0 > 0\), we can enforce this with a factor of \(\heaviside(p_0)\), where \(\heaviside\)\index{\(\heaviside\), Heaviside step function} is the \defineindex{Heaviside step function} defined by
    \begin{equation}
        \heaviside(u) = 
        \begin{cases}
            1 & u > 0,\\
            1/2 & u = 0,\\
            0 & u < 0.
        \end{cases}
    \end{equation}
    
    The \defineindex{mode expansion} of \(\varphi\) is then
    \begin{equation}
        \varphi(x) = \int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p_0) [ a(\vv{p}) \e^{-ip\cdot x} + a^\hermit(\vv{p}) \e^{i p \cdot x} ].
    \end{equation}
    Here \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) are Fourier coefficients, we'll see later that they are the quantum field theory analogues of the creation and annihilation operators for the harmonic oscillator.
    They depend only on \(\vv{p}\), rather than the four-vector \(p\), since we can take \(\delta(p^2 - m^2)\) as fixing the value of \(p_0\).
    
    Define \(\omega(\vv{p}) \coloneqq +\sqrt{\vv{p}^2 + m^2}\), which is always nonnegative.
    This allows us to rewrite the argument of the Dirac delta:
    \begin{equation}
        (p_0 - \omega(\vv{p}))(p_0 + \omega(\vv{p})) = p_0p_0 - \omega(\vv{p})^2 = p_0p_0 - \vv{p}^2 -m^2 = p^2 - m^2.
    \end{equation}
    So, now we have \(\delta(\omega(\vv{p}))\) in our integral.
    Recall the distribution identity
    \begin{equation}
        \delta(f(x)) = \sum_i \frac{\delta(x - a_i)}{\abs{f'(a_i)}}
    \end{equation}
    where \(a_i\) are all the zeros of \(f\) lying in the range of integration.
    Since in the integral we are enforcing \(p_0 \ge 0\) the only time \(p^2 - m^2 = (p_0 - \omega(\vv{p}))(p_0 + \omega(\vv{p}))\) vanishes is when \(p_0 = \omega(\vv{p})\), and
    \begin{equation}
        \diffp{}{p} (p^2 - m^2) \bigg|_{p_0 = \omega(\vv{p})} = 2\omega(\vv{p})
    \end{equation}
    so, as distributions, we have
    \begin{equation}
        \delta(p^2 - m^2)\heaviside(p_0) = \frac{1}{2\omega(\vv{p})} \delta(p^0 - \omega(\vv{p})).
    \end{equation}
    
    This lets us write \(\varphi(x)\) as
    \begin{equation}
        \varphi(x) = \int \frac{\dl{^4 p}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} \delta(p^0 - \omega(\vv{p})) [ a(\vv{p}) \e^{-ip\cdot x} + a^\hermit(\vv{p}) \e^{i p \cdot x} ].
    \end{equation}
    Performing the \(p^0\) integral using the sifting property of the Dirac delta we have
    \begin{equation}
        \varphi(x) = \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} [ a(\vv{p}) \e^{-ip\cdot x} + a^\hermit(\vv{p}) \e^{i p \cdot x} ] \, \bigg|_{p_0 = \omega(\vv{p})}.
    \end{equation}
    
    At this point we make some remarks on this expansion:
    \begin{itemize}
        \item The expansion is manifestly Lorentz invariant, while the presence of \(p^0\) may be concerning orthochronous Lorentz transformations (the only type we consider in this course) don't change the sign of \(p^0\), and so \(\heaviside(p^0)\) is Lorentz invariant.
        \item Since \(p^0 = \omega(\vv{p})\) the energy is always positive, and so is bounded below.
        \item The integral over \(\delta(p^0 - \omega(\vv{p}))\) fixes \(p^0 = \omega(\vv{p})\), and from now on this will be implicit in the integrals, instead of writing \(|_{p_0 = \omega(\vv{p})}\).
        \item The operator \(\varphi(x)\) is in the Heisenberg picture, but the time dependence comes from the exponential factors, both \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) are time independent operators.
        \item We need both the \(a\) and \(a^\hermit\) terms for \(\varphi\) to be Hermitian.
        \item The measure
        \begin{equation}
            \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} = \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p^0)
        \end{equation}
        is Lorentz invariant, but takes a lot of writing, we will use the shorthand notation
        \begin{equation}
            \invariantmeasure{p} = \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} = \int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p^0).
        \end{equation}
    \end{itemize}
    
    Now that we've expanded the field \(\varphi\) it is easy to expand the conjugate field, \(\pi\), using \(\pi = \dot{\varphi}\):
    \begin{align}
        \pi(x) = \dot{\varphi}(x) &= -\frac{i}{2} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} [a(\vv{p}) \e^{-ip\cdot x} - a^\hermit(\vv{p}) \e^{ip \cdot x}]\\
        &= -\frac{i}{2} \int \invariantmeasure{p} [a(\vv{p}) \e^{-ip\cdot x} - a^\hermit(\vv{p}) \e^{ip \cdot x}].
    \end{align}
    Here we've used
    \begin{multline}
        \diffp{}{x^0} \e^{\pm ip \cdot x} = \diffp{}{x^0} \exp[\pm ip^0x_0 \mp p^ix_i]\\
        = \pm ip^0 \exp[\pm ip^0x_0 \mp p^ix_i] = \pm i p^0 \exp[\pm ip \cdot x]
    \end{multline}
    and then set \(p^0 = \omega(\vv{p})\).
    
    \section{The Fourier Coefficients}
    As with the Harmonic oscillator we now want to invert these relations to find expressions for \(a(\vv{p})\) and \(a^\hermit(\vv{p})\).
    Since these are inverse Fourier transformations the inversion happens through Fourier transformation.
    For a single variable, real, one-dimensional function, \(f\), the Fourier transform is
    \begin{equation}
        \tilde{f}(p) = \int \dl{x} \, f(x) \e^{ip \cdot x}.
    \end{equation}
    Using this, but in three dimensions, we have
    \begin{align*}
        \int \dl{^3\vv{x}} \, \e^{ip' \cdot x} \varphi(x) &= \int \! \invariantmeasure{p} \frac{1}{2\omega(\vv{p})} \int \! \dl{^3\vv{x}} \, \big[a(\vv{p}) \e^{i(p' - p) \cdot x} + a^\hermit \e^{i(p' + p)\cdot x}\big]\\
        &= \int \! \invariantmeasure{p} \, (2\pi)^3 \big[a(\vv{p}) \delta^3(\vv{p}' - \vv{p})\e^{i(p'^0 - p^0)t}\\
        &\qquad\qquad\qquad+ a^\hermit(\vv{p}) \delta^3(\vv{p}' + \vv{p})\e^{i(p'^0 + p^0)t} \big]\\
        &= \int \frac{\dl{^3\vv{p}}}{2\omega(\vv{p})} \big[a(\vv{p}) \delta^3(\vv{p}' - \vv{p})\e^{i(p'^0 - p^0)t}\\
        &\qquad\qquad\qquad+ a^\hermit(\vv{p}) \delta^3(\vv{p}' + \vv{p})\e^{i(p'^0 + p^0)t}\big].
    \end{align*}
    Here we've recognised the integral representation of the Dirac delta:
    \begin{equation}
        \int \dl{^3\vv{x}} \, \e^{-ip\cdot x} = (2\pi)^3 \delta^3(\vv{x}) \e^{ip^0t},
    \end{equation}
    and made the usual assumption that everything is sufficiently convergent for us to be able to swap the order of integrals.
    Performing the integrals now using the Dirac deltas, and the fact that \(\omega(-\vv{p}) = \omega(\vv{p})\), we have
    \begin{align}
        \int \dl{^3\vv{x}} \, \e^{ip' \cdot x} \varphi(x) &= \frac{1}{2\omega(\vv{p}')}a(\vv{p}') + \frac{1}{2\omega(-\vv{p}')} a^\hermit(-\vv{p}') \e^{2ip'^0t}\\
        &= \frac{1}{2\omega(\vv{p}')} \big[a(\vv{p}') + a^\hermit(-\vv{p}') \e^{2i\omega(\vv{p}')t}\big].
    \end{align}
    Here we've used the Dirac delta's to set \(\vv{p} = \pm\vv{p}'\) in each term as appropriate.
    Restricting ourselves to on-shell, positive energy, solutions this then fixes the value of \(p^0\) to \(\sqrt{\vv{p}'^2 + m^2} = \omega(\vv{p}')\).
    
    We can do exactly the same for \(\pi\), the only difference is signs and a factor of \(1/\omega(\vv{p})\), so we get
    \begin{equation}
        \int \dl{^3\vv{x}} \, \e^{ip'\cdot x}  \pi(x) = -\frac{i}{2}[a(\vv{p}') - a(-\vv{p}')^\hermit \e^{2i\omega(\vv{p}')t}].
    \end{equation}
    Adding \(i\) times this to \(\omega(\vv{p})\) times the result for \(\varphi\) we get
    \begin{equation}
        a(\vv{p}) = \int \dl{^3\vv{x}} \, \e^{ip \cdot x} [\omega(\vv{p}) \varphi(t, \vv{x}) + i\pi(t, \vv{x})].
    \end{equation}
    Taking the conjugate we get
    \begin{equation}
        a^\hermit(\vv{p}) = \int \dl{^3\vv{x}} \, \e^{-ip \cdot x} [\omega(\vv{p}) \varphi(t, \vv{x}) - i\pi(t, \vv{x})].
    \end{equation}
    
    Now we have expressions for \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) we can compute their commutator:
    \begin{align}
        \commutator{a(\vv{p})}{a^\hermit(\vv{p}')} &= \int \! \dl{^3\vv{x}} \! \int \dl{^3\vv{x}'} \, \e^{ip\cdot x - ip'\cdot x'}\\
        &\qquad\qquad\commutator{\omega(\vv{p}) \varphi(t, \vv{x}) + i\pi(t, \vv{x})}{\omega(\vv{p}')\varphi(t, \vv{x}') - i\pi(t, \vv{x}')}. \notag
    \end{align}
    Using the equal time commutation relations only the cross terms don't vanish, and so this becomes
    \begin{align}
        &\commutator{a(\vv{p})}{a^\hermit(\vv{p}')} = \int \! \dl{^3\vv{x}} \! \int \dl{^3\vv{x}'} \, \e^{ip\cdot x - ip'\cdot x'} \big(-i\omega(\vv{p})\commutator{\varphi(t, \vv{x})}{\pi(t, \vv{x}')}\\
        &\hspace{17.5em}+ i\omega(\vv{p}')\commutator{\pi(t, \vv{x})}{\varphi(t, \vv{x}')}\big). \notag\\
        &= \int \! \dl{^3\vv{x}} \! \int \dl{^3\vv{x}'} \, \e^{ip\cdot x - ip'\cdot x'} (\omega(\vv{p}) \delta^3(\vv{x} - \vv{x}') + \omega(\vv{p}') \delta^3(\vv{x} - \vv{x}'))\\
        &= \int \dl{^3\vv{x}} \, \e^{i(p - p')\cdot x} (\omega(\vv{p}) + \omega(\vv{p}'))\\
        &= (2\pi)^3 \delta(\vv{p} - \vv{p}') 2\omega(\vv{p})
    \end{align}
    where again we've recognised the integral representation of the Dirac delta, and made use of the resulting Dirac delta to set \(\vv{p} = \pm \vv{p}'\), and hence \(p^0 = p'^0\) choosing to stay on-shell and positive energy.
    We introduce the shorthand notation,
    \begin{equation}
        (2\pi)^3 \delta(\vv{p} - \vv{p}') 2\omega(\vv{p}) = \bardelta(\vv{p} - \vv{p}').
    \end{equation}
    This allows us to write
    \begin{equation}
        \commutator{a(\vv{p})}{a^\hermit(\vv{p}')} = \bardelta(\vv{p} - \vv{p}').
    \end{equation}
    Compare this to \(\commutator{a}{a^\hermit} = 1\) for the harmonic oscillator.
    We also have
    \begin{equation}
        \commutator{a(\vv{p})}{a(\vv{p}')} = \commutator{a^\hermit(\vv{p}')}{a^\hermit(\vv{p}')} = 0.
    \end{equation}
    
    Note that \(\bardelta(\vv{p} - \vv{p}')\) is just a number, not an operator.
    This notation is nice, since if we combine the invariant measure, \(\invariantmeasure{p}\), and \(\bardelta(\vv{p} - \vv{p}')\) then we can just pretend we have \(\dl{p}\) and \(\delta^3(\vv{p} - \vv{p})\):
    \begin{equation}
        \int \invariantmeasure{p} \, f(\vv{p}) \bardelta(\vv{p} - \vv{p}') = f(\vv{p}').
    \end{equation}
    
    \section{The Hamiltonian}\label{sec:the hamiltonian}
    To complete the analogy with the harmonic oscillator we need to find the Hamiltonian in terms of \(a(\vv{p})\) and \(a^\hermit(\vv{p})\).
    Starting with the hamiltonian
    \begin{equation}
        H = \frac{1}{2} \int \dl{^3\vv{x}} \left[ \pi(x)^2 + (\grad \varphi(x))^2 + m^2 \varphi(x)^2 \right]
    \end{equation}
    we can split this into three components,
    \begin{align}
        H_1 &= \frac{1}{2}\int \dl{^3\vv{x}} \, \pi(x)^2,\\
        H_2 &= \frac{1}{2}\int \dl{^3\vv{x}} \, (\grad\varphi(x))^2,\\
        H_3 &= \frac{1}{2}\int \dl{^3\vv{x}} \, m^2\varphi(x)^2,\\
    \end{align}
    We'll then treat each of these separately.
    
    Start with the first term.
    Substituting in the expansion for \(\pi(x)\), being careful to use different integration variables for each factor of \(\pi\), we get
    \begin{multline}
        H_1 = -\frac{1}{8} \int \!\! \dl{^3\vv{x}} \int \! \frac{\dl{^3\vv{p}}}{(2\pi)^3} \left[ a(\vv{p}) \e^{-ip\cdot x} - a^\hermit(\vv{p}) \e^{ip\cdot x} \right] \\
        \times\int \! \frac{\dl{^3\vv{p}'}}{(2\pi)^3} \left[ a(\vv{p}') \e^{-ip'\cdot x} - a^\hermit(\vv{p}') \e^{ip'\cdot x} \right].
    \end{multline}
    Expanding this all the integrand becomes
    \begin{multline}
        a(\vv{p}) a(\vv{p}') \e^{-i(p + p')\cdot x} - a(\vv{p}) a^\hermit(\vv{p}') \e^{-i(p - p')\cdot x}\\
        - a^\hermit(\vv{p})a(\vv{p}')\e^{i(p - p') \cdot x} + a^\hermit(\vv{p}) a^\hermit(\vv{p}') \e^{i(p + p')\cdot x}.
    \end{multline}
    We can perform the integration over position first, and, recalling that \(p^0 = \omega(\vv{p})\), we get two types of terms, time dependent and time independent, these come from recognising the integral representation of the Dirac delta:
    \begin{align}
        \int \dl{^3\vv{x}} \, \e^{\pm i(p + p') \cdot x} &= (2\pi)^3 \delta(\vv{p} + \vv{p}') \e^{\pm 2i\omega(\vv{p})t},\\
        \int \dl{^3\vv{x}} \, \e^{\pm i(p - p') \cdot x} &= (2\pi)^3 \delta(\vv{p} - \vv{p}').
    \end{align}
    We should expect that the time dependent terms in \(H\) will all cancel, since \(H\) is time independent.
    
    After performing the integral over positions we are left with the following integrand
    \begin{multline*}
        (2\pi)^3 a(\vv{p}) a(\vv{p}') \delta(\vv{p} - \vv{p}')\e^{-2i\omega(\vv{p})t} - (2\pi)^3 a(\vv{p}) a^\hermit(\vv{p}') \delta(\vv{p} - \vv{p}')\\
        - (2\pi)^3a^\hermit(\vv{p})a(\vv{p}')\delta(\vv{p} - \vv{p}') + (2\pi)^3a^\hermit(\vv{p}) a^\hermit(\vv{p}') \delta(\vv{p} + \vv{p}') \e^{2i\omega(\vv{p})t}.
    \end{multline*}
    Performing the integral over \(\vv{p}'\), and cancelling one factor of \((2\pi)^3\) we are left with
    \begin{align}
        H_1 &= -\frac{1}{8} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \big[ a(\vv{p}) a(\vv{p}) \e^{-2i\omega(\vv{p})t}  - a(\vv{p}) a^\hermit(\vv{p})\\
        &\qquad\qquad\qquad\qquad- a^\hermit(\vv{p}) a(\vv{p}) + a^\hermit(\vv{p})a^\hermit(\vv{p}) \e^{2i\omega(\vv{p})t} \big].\notag
    \end{align}
    Keeping only the time independent terms, since the time independent terms will cancel with terms in \(H_2\) and \(H_3\), we get the contribution from the first term:
    \begin{equation}
        H_1 \to \frac{1}{8} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \big[ a(\vv{p}) a^\hermit(\vv{p})+ a^\hermit(\vv{p}) a(\vv{p}) \big].
    \end{equation}
    Doing the same for the other two terms we get their time independent contributions:
    \begin{align}
        H_2 &\to \frac{1}{8} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{\vv{p}^2}{\omega(\vv{p})^2} \big[ a(\vv{p}) a^\hermit(\vv{p}) + a^\hermit(\vv{p}) a(\vv{p}) \big],\\
        H_3 &\to \frac{1}{8} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{m^2}{\omega(\vv{p})^2} \big[ a(\vv{p}) a^\hermit(\vv{p}) + a^\hermit(\vv{p}) a(\vv{p}) \big].
    \end{align}
    See \cref{app:hamiltonian mode expansion} for the full calculation.
    Noticing that \(\vv{p}^2 + m^2\) cancels with \(1/\omega(\vv{p})^2\) in the full expression we get
    \begin{equation}
        H = \frac{1}{4} \int \frac{\dl{^3p}}{(2\pi)^3} \big[ a(\vv{p})a^\hermit(\vv{p}) + a(\vv{p})a^\hermit(\vv{p}) \big].
    \end{equation}
    Compare this to the Harmonic oscillator result,
    \begin{equation}
        H = \frac{1}{2}\omega(aa^\hermit + a^\hermit a).
    \end{equation}
    
    We can now compute the commutators of \(H\) with \(a(\vv{p})\) and \(a^\hermit(\vv{p})\):
    \begin{align}
        \commutator{H}{a^\hermit(\vv{p})} &= \frac{1}{4} \int \frac{\dl{^3p'}}{(2\pi)^3} \commutator{a(\vv{p}')a^\hermit(\vv{p}') + a^\hermit(\vv{p}')a(\vv{p}')}{a^\hermit(\vv{p})}\\
        &= \frac{1}{4} \int \frac{\dl{^3p'}}{(2\pi)^3} \big( \commutator{a(\vv{p}')a^\hermit(\vv{p}')}{a^\hermit(\vv{p})} + \commutator{a^\hermit(\vv{p}')a(\vv{p}')}{a^\hermit(\vv{p})}  \big) \notag\\
        &= \frac{1}{4} \int \frac{\dl{^3p'}}{(2\pi)^3} \big( a(\vv{p}')\commutator{a^\hermit(\vv{p}')}{a^\hermit(\vv{p})} + \commutator{a(\vv{p}')}{a^\hermit(\vv{p})}a^\hermit(\vv{p}') \notag\\
        &\qquad\qquad\qquad+ a^\hermit(\vv{p}')\commutator{a(\vv{p}')}{a^\hermit(\vv{p})} + \commutator{a^\hermit(\vv{p}')}{a^\hermit(\vv{p})} \big)\\
        &= \frac{1}{4} \int \dl{^3p'} \, 2\omega(\vv{p}) [ \delta(\vv{p} - \vv{p}')a^\hermit(\vv{p}') + a^\hermit(\vv{p}') \delta(\vv{p}' - \vv{p}) ]\\
        &= \omega(\vv{p})a^\hermit(\vv{p}).
    \end{align}
    
    Similarly, with \(a(\vv{p})\) we get
    \begin{equation}
        \commutator{H}{a(\vv{p})} = -\omega(\vv{p})a(\vv{p}).
    \end{equation}
    Compare these to the harmonic oscillator results, \(\commutator{H}{a^\hermit} = \omega a^\hermit\) and \(\commutator{H}{a} = -\omega a\).
    
    We can view the quantum fields as being a quantum harmonic oscillator of energy \(\omega(\vv{p}) = \sqrt{\vv{p}^2 + m^2}\) for every Fourier mode, \(\vv{p}\).
    
    \section{Particle Interpretation}
    Consider an energy eigenstate, \(\ket{E}\), with eigenvalue \(E\), so \(H\ket{E} = E\ket{E}\).
    Then the state \(a^\hermit(\vv{p})\ket{E}\) is also an energy eigenstate, with energy \(E + \omega(\vv{p})\):
    \begin{align}
        Ha^\hermit(\vv{p})\ket{E} &= \left( \commutator{H}{a^\hermit(\vv{p})} + a^\hermit(\vv{p})H \right)\ket{E}\\
        &= (\omega(\vv{p})a^\hermit(\vv{p}) + a^\hermit(\vv{p})E)\\
        &= (\omega(\vv{p}) + E)a^\hermit(\vv{p})\ket{E}.
    \end{align}
    Similarly, \(a(\vv{p})\ket{E}\) is an energy eigenstate with energy \(E - \omega(\vv{p})\).
    
    This suggests that, following the interpretation of the harmonic oscillator, \(a^\hermit(\vv{p})\) is a \defineindex{creation operator}, creating a quantum with energy \(\omega(\vv{p}) = \sqrt{\vv{p}^2 + m^2}\), and \(a(\vv{p})\) is an \defineindex{annihilation operator}, destroying a quantum with energy \(\omega(\vv{p})\).
    Note that all quanta in this interpretation have positive energy.
    There is another interpretation, which we'll discuss later, where \(a^\hermit(\vv{p})\) creates a quantum with energy \(\omega(\vv{p})\) and \(a(\vv{p})\) creates a quantum with energy \(-\omega(\vv{p})\).
    
    Since the Hamiltonian is nonnegative, its defined as a sum of squares of real quantities, there must be some lowest energy state, \(\ket{0}\), which we call the \defineindex{vacuum state}.
    This state is such that any annihilation operator acting on it gives the zero vector:
    \begin{equation}
        a(\vv{p}) \ket{0} = 0 \quad \forall \vv{p}.
    \end{equation}
    
    Starting with \(\ket{0}\) we can act with creation operators to make new states.
    Each creation operator, \(a^\hermit(\vv{p})\), makes a new particle with energy \(\omega(\vv{p})\) and momentum \(\vv{p}\).
    For example, 
    \begin{equation}
        \ket{\psi} = a^\hermit(\vv{p}) a^\hermit(\vv{q}) a^\hermit(\vv{r}) \ket{0}
    \end{equation}
    is a state with three particles with total energy \(\omega(\vv{p}) + \omega(\vv{q}) + \omega(\vv{r})\), and total momentum \(\vv{p} + \vv{q} + \vv{r}\).
    
    Since all \(a^\hermit(\vv{p})\) commute the state
    \begin{equation}
        a^\hermit(\vv{p}) a^\hermit(\vv{q}) a^\hermit(\vv{r}) \ket{0}
    \end{equation}
    is the same as \(\ket{\psi}\).
    We interpret this as meaning that the particles are bosons, swapping any two particles leaves the state unchanged.
    This means the particles follow Bose--Einstein statistics.
    
    The space of all states which can be constructed in this way, called \defineindex{Fock space}, is the space given by all complex-linear combinations of
    \begin{equation}
        a^\hermit(\vv{p_1}) \dotsm a^\hermit(\vv{p_n})\ket{0}
    \end{equation}
    for any values of \(n \in \naturals\).
    
    Formally, the Fock space (for bosons) is defined to be
    \begin{equation}
        F(\hilbertSpace) = \overline{\bigoplus_{n = 0}^{\infty} S \hilbertSpace^{\otimes n}} = \overline{\complex \oplus \hilbertSpace \oplus S(\hilbertSpace \otimes \hilbertSpace) \oplus S(\hilbertSpace \otimes \hilbertSpace \otimes \hilbertSpace) \oplus \dotsb}
    \end{equation}
    where \(\hilbertSpace\) is the single particle Hilbert space,
    \begin{equation}
        \hilbertSpace^{\otimes n} \coloneqq \bigotimes_{i = 1}^{n}\hilbertSpace = \underbrace{\hilbertSpace \otimes \dotsb \otimes \hilbertSpace}_{n \text{ times}},
    \end{equation}
    the operator \(S\) symmetrises all tensors, the direct sum, \(\oplus\), runs from \(n = 0\), for states with no particles, to infinity, with each term representing an \(n\)-particle state space, and the line represents that we complete the space to form a Hilbert space, this entails adding in the values of all absolutely convergent series.
    
    We make the normalisation choice that \(\braket{0}{0} = 1\), then, if \(\ket{\vv{p}} = a^\hermit(\vv{p})\ket{\vv{p}}\) is a single particle state we have
    \begin{align}
        \braket{\vv{p}}{\vv{q}} &= \bra{0} a(\vv{p}) a^\hermit(\vv{q})\ket{0}\\
        &= \bra{0} \commutator{a(\vv{p})}{a^\hermit(\vv{q})}\ket{0} + \bra{0} a^\hermit(\vv{q}) a(\vv{p})\ket{0}\\
        &= \bardelta(\vv{p} - \vv{q}).
    \end{align}
    Note that \(a(\vv{p})\ket{0}\) and \(\bra{0}a^\hermit(\vv{q}) = [a(\vv{q}) \ket{0}]^\hermit\) both vanish, since they are annihilators acting on the vacuum state.
    
    As with the harmonic oscillator we can define a \defineindex{number density operator}, \(N(\vv{p}) \coloneqq a^\hermit(\vv{p})a(\vv{p})\), the eigenvalues of which integrate to give the number of particles in the state with momentum \(\vv{p}\).
    
    \section{Ground State Energy}
    We can rewrite the Hamiltonian in terms of the number density operator:
    \begin{align}
        H &= \frac{1}{4} \int \invariantmeasure{p} \, \omega(\vv{p}) \big( a^\hermit(\vv{p}) a(\vv{p}) + a(\vv{p})a^\hermit(\vv{p}) \big)\\
        &= \frac{1}{2} \int \invariantmeasure{p} \, \omega(\vv{p}) \left( a^\hermit(\vv{p})a(\vv{p}) + \frac{1}{2} \commutator{a(\vv{p})}{a^\hermit(\vv{p})} \right)\\
        &= \frac{1}{2} \int \invariantmeasure{p} \, \omega(\vv{p}) \left( N(\vv{p}) + \omega(\vv{p})(2\pi)^3 \delta^3(\vv{p} - \vv{p}) \right)\\
        &= \frac{1}{2} \int \invariantmeasure{p} \, \omega(\vv{p}) \left( N(\vv{p}) + \omega(\vv{p})(2\pi)^3 \delta^3(\vv{0}) \right).
    \end{align}
    
    Clearly we have \(\bra{0}N(\vv{p})\ket{0} = \bra{0} a^\hermit(\vv{p})a(\vv{p})\ket{0} = 0\), and so
    \begin{equation}
        \bra{0}H\ket{0} = \frac{1}{2} \int \dl{^3\vv{p}} \, \omega(\vv{p}) \delta^3(\vv{0}).
    \end{equation}
    This is infinite.
    In fact, its \emph{very} infinite.
    Each \(\delta(0)\) is infinite, and we have three of them multiplied together, then we integrate over all momentum space, which is infinite, to make matters worse \(\omega(\vv{p})\) grows arbitrarily large with \(\abs{\vv{p}}\).
    
    The solution, as with so many similar problems in physics, is just not to think about it.
    All that we can measure is the energy of particles absorbed and emitted by the system.
    We cannot measure the ground state energy, so it doesn't matter that our theory gives a nonsensical answer when we try to compute the expected ground state energy.
    
    \subsection{Normal Ordering}
    One solution to the infinity arising in the ground state energy is to be careful about how we order \(a(\vv{p})\) and \(a^\hermit(\vv{p})\).
    If we keep all \(a^\hermit(\vv{p})\) on the left of all \(a(\vv{p})\) then, when we take expectation values, one of our \(a(\vv{p})\) will act on \(\ket{0}\) giving the zero vector, and one of our \(a^\hermit(\vv{p})\) will act on \(\bra{0}\), also giving the zero vector.
    
    Mathematically what happens is that when we change to order of operators to have all \(a^\hermit(\vv{p})\) on the left we'll get commutators of \(a(\vv{p})\) and \(a^\hermit(\vv{p}')\), which give rise to Dirac deltas which cancel with the existing ones, to leave a finite result.
    
    This idea that careful ordering of operators can avoid infinities goes beyond the expectation value of the ground state energy.
    Consider our field, \(\varphi(x)\).
    We can split this into parts,
    \begin{equation}
        \varphi(x) = \varphi^+(x) + \varphi^-(x)
    \end{equation}
    where
    \begin{equation}\label{eqn:phi +}
        \varphi^+(x) = \int \invariantmeasure{p} \, a(\vv{p}) \e^{-ip\cdot x},
    \end{equation}
    and
    \begin{equation}\label{eqn:phi -}
        \varphi^-(x) = \int \invariantmeasure{p} \, a^\hermit(\vv{p}) \e^{ip\cdot x} = (\varphi^+)^\hermit.
    \end{equation}
    If these were single particle states in quantum mechanics then they would correspond to positive and negative energy states respectively, but they aren't, although we'll see an interpretation along these lines later.
    
    Now suppose we want to consider the product \(\varphi(x)\varphi(y)\) for two arbitrary spacetime points, \(x\) and \(y\).
    We can expand this as follows:
    \begin{align}
        \varphi(x)\varphi(y) &= [\varphi^+(x) + \varphi^-(x)][\varphi^+(y) + \varphi^-(y)]\\
        &= \varphi^+(x)\varphi^+(y) + \varphi^+(x)\varphi^-(y) + \varphi^-(x)\varphi^+(y) + \varphi^-(x)\varphi^-(y).
    \end{align}
    The \defineindex{normal ordering} of \(\varphi(x)\varphi(y)\), denoted \(\normalordering{\varphi(x)\varphi(y)}\), is defined as the result of taking this product, but swapping terms so that all \(a^\hermit(\vv{p})\) appear on the left of all \(a(\vv{p}')\), that is all \(\varphi^+\) appear on the left of all \(\varphi^-\), so
    \begin{equation}
        \normalordering{\varphi(x)\varphi(y)} = \varphi^+(x)\varphi^+(y) + \varphi^+(x)\varphi^-(y) + \textcolor{highlight}{\varphi^+(y)\varphi^-(x)} + \varphi^-(x)\varphi^-(y).
    \end{equation}
    Then we have
    \begin{equation}
        \bra{0} \normalordering{\varphi(x)\varphi(y)} \ket{0} = 0.
    \end{equation}
    In general, if \(X\) is any nontrivial\footnote{That is, \(X\) is not just a scalar, in which case \(\bra{0}\normalordering{X}\ket{0} = X\braket{0}{0} = X\).} polynomial in \(a^\hermit(\vv{p})\) and \(a(\vv{p})\) we'll have
    \begin{equation}
        \bra{0} \normalordering{X} \ket{0} = 0.
    \end{equation}
    
    Subtracting the zero point energy of \(H\) from \(H\) is equivalent to normal ordering
    \begin{equation}
        \normalordering{H} = H - \bra{0} H \ket{0},
    \end{equation}
    since by normal ordering \(H\) we swap exactly those terms which give infinity, and all others give zero, so we can think of normal ordering as subtracting them off.
    
    From now on we'll assume that \(H\) and \(\vv{p}\) are normal ordered, often without writing it.
    
    \section{Ground State Momentum}
    The physical momentum operator is given by
    \begin{equation}
        \vv{P} = \int \dl{^3 \vv{x}} \, \pi(x) \grad\varphi(x).
    \end{equation}
    Recall that this comes from the \(T^{i0}\) component of the energy-momentum tensor.
    Expressing the fields in terms of \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) we get
    \begin{equation}
        \vv{P} = \frac{1}{2} \invariantmeasure{p} \, \vv{p} (a(\vv{p}) a^\hermit(\vv{p}) + a^\hermit(\vv{p})a(\vv{p}))
    \end{equation}
    where the factor of \(\vv{p}\) comes from computing \(\grad\varphi\).
    Rewriting this using the commutator of \(a(\vv{p})\) and \(a^\hermit(\vv{p})\) we get
    \begin{equation}
        \vv{P} = \int \invariantmeasure{p} \, \vv{p}(N(\vv{p}) + \omega(\vv{p})(2\pi)^3\delta(\vv{0})).
    \end{equation}
    
    Unlike with the ground state energy the \(\delta(\vv{0})\) factor doesn't cause a problem.
    Integrating over all modes in the modes with momenta \(\vv{p}\) and \(-\vv{p}\) cancel, since \(\omega(\vv{p}) = \omega(-\vv{p})\).
    So, we have
    \begin{equation}
        \bra{0} \vv{P} \ket{0} = \bra{0} \normalordering{\vv{P}} \ket{0} = 0,
    \end{equation}
    that is, the vacuum state has no momentum, which really ought to be the case.
    
    \chapter{Covariant Commutators}
    \section{What are They}
    The equal time commutation relations treat time specially.
    This is generally undesirable in relativity.
    So, in this section we derive a commutator, \(\commutator{\varphi(x)}{\varphi(y)}\) at two arbitrary spacetime points, \(x\) and \(y\).
    We start by writing
    \begin{equation}
        \varphi(x) = \varphi^+(x) + \varphi^-(x),
    \end{equation}
    where \(\varphi^+\) and \(\varphi^-\) are as in \cref{eqn:phi +,eqn:phi -}.
    Since \(\varphi^+\) only consists of annihilation operators and \(\varphi^-\) only consists of creation operators we have
    \begin{equation}
        \commutator{\varphi^+(x)}{\varphi^+(y)} = \commutator{\varphi^-(x)}{\varphi^-(y)} = 0.
    \end{equation}
    Hence, expanding out the commutator we have
    \begin{equation}
        \commutator{\varphi(x)}{\varphi(y)} = \commutator{\varphi^-(x)}{\varphi^+(y)} + \commutator{\varphi^-(x)}{\varphi^+(y)}.
    \end{equation}
    
    Consider the first of these,
    \begin{align}
        \commutator{\varphi^+(x)}{\varphi^-(y)} &= \int \invariantmeasure{p} \int \invariantmeasure{p'} \, \commutator{a(\vv{p})}{a^\hermit(\vv{p}')} \e^{-ip\cdot x + ip'\cdot y}\\
        &= \int \invariantmeasure{p} \int \invariantmeasure{p'} \, \bardelta(\vv{p} - \vv{p}') \e^{-ip\cdot x + ip' \cdot y}\\
        &= \int \invariantmeasure{p} \, \e^{-ip \cdot (x - y)}\\
        &\eqqcolon i\Delta^+(x - y),
    \end{align}
    where the last line defines \(\Delta^+\):
    \begin{equation}
        \Delta^+(x) \coloneqq i \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} \e^{ip\cdot x}
    \end{equation}
    The factor of \(i\) is a convention to make \(\Delta^+\) a real function.
    
    The second commutator can now easily be worked out using antisymmetry:
    \begin{equation}
        \commutator{\varphi^-(x)}{\varphi^+(y)} = -\commutator{\varphi^+(y)}{\varphi^-(x)} = -i\Delta^+(y - x) \eqqcolon i\Delta^-(x - y),
    \end{equation}
    where the last equality defines \(\Delta^-\).
    
    Combining these results we have
    \begin{align}
        \commutator{\varphi(x)}{\varphi(y)} = i\Delta^+(x - y) + i\Delta^-(x - y) \eqqcolon i\Delta(x - y)
    \end{align}
    where
    \begin{align}
        \Delta(x) &\coloneqq \Delta^+(x) + \Delta^-(x)\\
        &\hphantom{:}= i \int \invariantmeasure{p} (\e^{ip\cdot x} - \e^{-ip\cdot x})\\
        &\hphantom{:}= -2\int \invariantmeasure{p} \, \sin (p \cdot x).
    \end{align}
    Note that \(\Delta\) is a real (\(\Delta(x) = \Delta(x)^*\)), odd (\(\Delta(-x) = -\Delta(x)\)) function.
    This is what we would expect since the field is real and the commutator is antisymmetric.
    
    Consider the full form of \(\Delta^+\):
    \begin{equation}
        i\Delta^+(x) = \int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p_0) \e^{-ip\cdot x}.
    \end{equation}
    We can also write \(\Delta^-\) similarly:
    \begin{equation}
        i\Delta^-(x) = -\int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \heaviside(p_0) \e^{ip\cdot x}.
    \end{equation}
    If we make a change of variables, \(p \to -p\), then \(\dl{^4p} \to \dl{^4p}\), and
    \begin{equation}
        i\Delta^-(x) = -\int \frac{\dl{^4p}}{(2\pi)^4} 2\pi\delta(p^2 - m^2) \heaviside(-p_0) \e^{-ip\cdot x}.
    \end{equation}
    This allows us to write \(\Delta\) as
    \begin{equation}
        i\Delta(x) = \int \frac{\dl{^4p}}{(2\pi)^4} 2\pi \delta(p^2 - m^2) \varepsilon(p_0) \e^{-ip\cdot x}
    \end{equation}
    where
    \begin{equation}
        \varepsilon(p_0) \coloneqq \heaviside(p_0) - \heaviside(-p_0) = 
        \begin{cases}
            + 1 & p_0 > 0,\\
            0 & p_0 = 0,\\
            -1 & p_0 < 0,
        \end{cases}
    \end{equation}
    is the sign function.
    
    Note that for proper orthochronous Lorentz transformations \(\Delta\) is Lorentz invariant, since the sign of \(p_0\) doesn't change and all other terms are manifestly Lorentz invariant.
    
    \section{Microcausality}
    Suppose \(x\) is space-like, that is \(x^2 < 0\).
    Then there exists some proper orthochronous Lorentz transformation, \(\Lambda\), taking \(x\) to \(-x\).
    Space-like vectors lie outside the light cone, and we can view this transformation as rotating around the light cone.
    Since \(\Delta^{\pm}(x)\) are individually Lorentz invariant we can apply a Lorentz transformation to one of them and not the other when computing \(\Delta\) and get the same result, so
    \begin{align}
        \Delta(x) &= \Delta^+(x) + \Delta^-(x)\\
        &= \Delta^+(x) + \Delta^-(\Lambda x)\\
        &= \Delta^+(x) + \Delta^-(-x)\\
        &= \Delta^+(x) - \Delta^+(x)\\
        &= 0,
    \end{align}
    so \(\Delta\) vanishes for all space-like points.
    Hence, \(\commutator{\varphi(x)}{\varphi(y)}\) vanishes for all space-like separations, \(x - y\).
    
    On the other hand no such transformation exists for time-like or light-like points, since it would have to map from one half of the light cone to the other, which is a discontinuous transformation.
    
    We can interpret this as a requirement for causality, specifically for a nonzero commutator between fields at space-like separated points which happen to have the same time coordinate we'd need some form of communication at this time between these points, and that can't happen between space-like points at the same time without violating causality.
    
    This analysis tells us that \(\Delta\) is not analytic, it has a sudden jump from zero to nonzero at light-like points.
    
    \section{Equal Time Commutation Relations}
    The equal time commutation relations can now be viewed as a special case of the covariant commutation relations.
    Since we have \(\pi = \dot{\varphi}\) we want to compute \(\commutator{\varphi(t, \vv{x})}{\pi(t, \vv{x}')} = \commutator{\varphi(t, \vv{x})}{\dot{\varphi}(t, \vv{x}')}\).
    To do this take the time derivative of \(\Delta\):
    \begin{align}
        \diffp{}{x^0} \Delta(x - y) &= i \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} \diffp{}{x^0} \left[ \e^{ip\cdot(x - y)} - \e^{-ip\cdot(x - y)} \right]\\
        &=i \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} i\omega(\vv{p}) \left[ \e^{ip\cdot(x - y)} + \e^{-ip\cdot(x - y)} \right]\\
        &= -\frac{1}{2} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \left[ \e^{ip\cdot(x - y)} + \e^{-ip\cdot(x - y)} \right].
    \end{align}
    For the equal time commutation relations we have \(x^0 = y^0\) and so
    \begin{equation}
        p \cdot (x - y) = \vv{p} \cdot (\vv{x} - \vv{y}).
    \end{equation}
    Hence,
    \begin{align}
        \diffp{}{x^0} \Delta(x - y) &= -\frac{1}{2} \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \left[ \e^{i\vv{p}\cdot(\vv{x} - \vv{y})} + \e^{-i\vv{p}\cdot(\vv{x} - \vv{y})} \right]\\
        &= -\delta^3(\vv{x} - \vv{y}),
    \end{align}
    where we've recognised the integral representation of the Dirac delta:
    \begin{equation}
        \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \e^{\pm i\vv{p} \cdot (\vv{x} - \vv{y})} = \delta^3(\vv{x} - \vv{y}).
    \end{equation}
    
    We then have
    \begin{align}
        \diffp{}{x^0} [i\Delta(x - y)] \bigg|_{x^0 = y^0 = t} &= \diffp{}{x^0} \commutator{\varphi(t, \vv{x})}{\varphi(t, \vv{y})}\\
        &= \commutator{\dot{\varphi}(t, \vv{x})}{\varphi(t, \vv{y})}\\
        &= \commutator{\pi(t, \vv{x})}{\varphi(t, \vv{y})}\\
        &= -i\delta^3(\vv{x} - \vv{y})
    \end{align}
    Hence,
    \begin{equation}
        \commutator{\varphi(t, \vv{x})}{\pi(t, \vv{y})} = i\delta^3(\vv{x} - \vv{y}).
    \end{equation}
    So, we can view the equal time commutation relations as a specific case of the covariant commutation relations.
    This is why the equal time commutation relations can still produce Lorentz invariant physics.
    
    \section{Contour Representation}\label{sec:contour representation}
    \begin{rmk}
        This section makes use of some complex analysis, in particular contour integrals and the residue theorem.
        See either \course{Methods of Theoretical Physics} or \course{Methods of Mathematical Physics} for details.
    \end{rmk}
    
    The contour representation of \(\Delta^{\pm}\) is
    \begin{equation}
        \Delta^{\pm}(x) = - \int_{C^{\pm}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2}.
    \end{equation}
    The contours \(C^{\pm}\) are anticlockwise circles containing the poles at \(\pm\omega(\vv{p})\).
    This is shown in \cref{fig:contours for Delta}.
    
    \begin{figure}
        \tikzsetnextfilename{contours-for-Delta}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [right] {\(\Re(p_0)\)};
            \draw[thick, ->] (0, -2) -- (0, 2) node [above] {\(\Im(p_0)\)};
            \fill[Red] (1, 0) circle [radius = 0.05cm] node [below, text=black] {\(\omega(\vv{p})\)};
            \fill[Red] (-1, 0) circle [radius = 0.05cm] node [below, text=black] {\(-\omega(\vv{p})\)};
            \draw[very thick, Blue] (1, 0) circle [radius = 0.8cm];
            \draw[very thick, Blue] (-1, 0) circle [radius = 0.8cm];
            \draw[very thick, Blue, ->] (1.01, 0.8) -- ++ (-0.02, 0);
            \draw[very thick, Blue, ->] (-1.01, 0.8) -- ++ (-0.02, 0);
            \node[below] at (1, -0.8) {\(C^+\)};
            \node[below] at (-1, -0.8) {\(C^-\)};
            \draw[very thick, Purple] (0, 0) circle [x radius = 2.2cm, y radius = 1.6 cm];
            \draw[very thick, Purple, ->] (0.01, 1.6) -- ++ (-0.02, 0);
            \node[below right] at (0, -1.6) {\(C^+ + C^-\)};
        \end{tikzpicture}
        \caption{The contours used in the contour representation of \(\Delta^{\pm}\) and \(\Delta\).}
        \label{fig:contours for Delta}
    \end{figure}
    
    To see why this works start with
    \begin{equation}
        p^2 - m^2 = (p_0 + \omega(\vv{p})) (p_0 - \omega(\vv{p})),
    \end{equation}
    which shows there are poles at \(p_0 = \pm \omega(\vv{p})\).
    We can then apply the residue theorem, which says that 
    \begin{equation}
        \oint_\gamma f(z) \dd{z} = 2\pi i \sum_i \Res(f, a_i)
    \end{equation}
    where \(\gamma\) is a closed contour, \(f\) is analytic on and inside \(\gamma\), \(a_i\) are the poles of \(f\) inside \(\gamma\), and \(\Res(f, a_i)\) is the residue of \(f\) at \(a_i\).
    
    We have a particularly easy case of two simple poles.
    In the case of a simple pole the residue at \(a_i\) is given by
    \begin{equation}
        \Res(f, a_i) = (z - a_i)f(z)|_{z = a_i},
    \end{equation}
    so
    \begin{equation}
        \Res\left( \frac{\e^{-ip\cdot x}}{(p_0 + \omega(\vv{p}))(p_0 - \omega(\vv{p}))}, \pm \omega(\vv{p}) \right) = \frac{\e^{-ip\cdot x}}{p_0 \pm \omega(\vv{p})} \bigg|_{p_0 = \mp \omega(\vv{p})} = \mp \frac{\e^{-ip\cdot x}}{2\omega(\vv{p})}.
    \end{equation}
    Hence,
    \begin{equation}
        -\int_{C^+} \frac{\dl{p_0}}{2\pi} \frac{\e^{-ip\cdot x}}{p^2 - m^2} = -i \frac{\e^{-ip\cdot x}}{2\omega(\vv{p})}\bigg|_{p_0 = \omega(\vv{p})},
    \end{equation}
    and
    \begin{equation}
        -\int_{C^+} \frac{\dl{p_0}}{2\pi} \frac{\e^{-ip\cdot x}}{p^2 - m^2} = i \frac{\e^{ip\cdot x}}{2\omega(\vv{p})}\bigg|_{p_0 = -\omega(\vv{p})}.
    \end{equation}
    That is,
    \begin{equation}
        -\int_{C^{\pm}} \frac{\dl{p_0}}{2\pi} \frac{\e^{-ip\cdot x}}{p^2 - m^2} = \mp i \frac{\e^{\mp ip\cdot x}}{2\omega(\vv{p})}\bigg|_{p_0 = \omega(\vv{p})}.
    \end{equation}
    Having performed the integral over \(p_0\) we are left with an integral over \(\vv{p}\) so
    \begin{equation}
        -\int_{C^{\pm}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2} = \mp i \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{\e^{mp ip \cdot x}}{2\omega(\vv{p})} = \Delta^{\pm}(x).
    \end{equation}
    
    The contour representation of \(\Delta\) is then simply given by
    \begin{equation}
        \Delta(x) = - \int_{C} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2}
    \end{equation}
    where \(C = C^+ + C^-\) is the combined contour, with the middle parts cancelling out, to give a contour containing both poles as shown in \cref{fig:contours for Delta}.
    
    \section{\texorpdfstring{\(\Delta\)}{Delta} as Green's Functions}
    To interpret \(\Delta\) and \(\Delta^{\pm}\) consider the Klein--Gordon equation,
    \begin{equation}
        0 = (\dalembertian + m^2)\varphi.
    \end{equation}
    Taking the Fourier transform and using the rule that \(\partial_x \to -ip\) we have
    \begin{equation}
        0 = ((-ip)^2 + m^2)\tilde{\varphi} \implies (p^2 - m^2)\tilde{\varphi} = 0.
    \end{equation}
    Transforming back then gives us the contour representation of \(\Delta\) or \(\Delta^{\pm}\), depending on the boundary conditions.
    
    This means we can interpret \(\Delta\) and \(\Delta^{\pm}\) as classical Green's functions for the Klein--Gordon equation, so
    \begin{equation}
        (\dalembertian + m^2)\Delta(x) = \delta^4(x).
    \end{equation}
    
    We call \(\Delta\) and \(\Delta^{\pm}\) propagators, since they tell us how a quantum propagates between states.
    
    \part{Interactions}
    \chapter{Scattering}
    \section{Interactions}
    \epigraph{Quantum things are generally more difficult than classical things.}{Richard Ball}
    So far, we have only considered Lagrangians which are quadratic in the field, we call these \define{free field Lagrangians}\index{free field Lagrangian}, since quadratic Lagrangians give linear equations of motion with plane wave solutions, which we interpret as many independent harmonic oscillators, each corresponding to a noninteracting particle.
    In order to have our particles do anything, such as be measured, we need interactions.
    This means introducing a nonquadratic term to our Lagrangian.
    
    \subsection{\texorpdfstring{\(\varphi^3\)}{Phi Cubed} Theory}
    The simplest nonquadratic term that we can add to our Lagrangian is a cubic term, giving the Lagrangian
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi) (\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2 - \frac{1}{6} g \varphi^3.
    \end{equation}
    The first two terms are just the usual Klein--Gordon Lagrangian, call it \(\lagrangianDensity\).
    We can treat this as the Lagrangian for a free particle.
    The extra term, \(-g\varphi^3/6\), is our interaction term, call it \(\lagrangianDensity_{\interaction}\).
    We call a theory with a cubic term like this a \define{\(\symbf{\varphi^3}\) theory}\index{\(\varphi^3\) theory}
    The factor of \(1/6\) in this factor is just conventional, its there to make equations simpler later.
    A \(\varphi^n\) theory would equivalently have a factor of \(1/n!\), each time we differentiate this factor becomes closer to 1.
    The factor of \(g\) is called the \defineindex{coupling constant}.
    It measures how strong the interaction is.
    
    We can easily apply the Euler--Lagrange equations to this modified Lagrangian, without much changing.
    We have
    \begin{equation}
        \partial_\mu \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} = \dalembertian\varphi, \qqand \diffp{\lagrangianDensity}{\varphi} = -m^2\varphi - \frac{1}{3}g \varphi^2.
    \end{equation}
    Hence, we have the equations of motion
    \begin{equation}
        (\dalembertian + m^2)\varphi = -\frac{1}{2}g \varphi^2.
    \end{equation}
    This is just the Klein--Gordon equation with some sort of source term.
    
    This is now a nonlinear differential equation.
    These are, in general, hard to solve.
    There is no general theory and we are usually restricted to specific examples with solutions.
    Because of this we assume that the interaction isn't very strong, that is that \(g \ll 1\), and then we solve by expanding in \(g\) and truncating at some appropriate point in our calculations.
    
    \section{The \texorpdfstring{\(S\)}{S} Matrix}
    Most processes in physics are, when it comes down to it, scattering.
    We see light after it scatters off an object, two objects collide in classical mechanics, two protons collide in the LHC, a whole bunch of particles collide all over the place and a statistical study of this gives statistical mechanics, and so on.
    Scattering is a very simple process, there are three steps:
    \begin{itemize}
        \item We start in some initial state with free particles of definite momentum, spin, etc.
        \item The particles undergo some scattering process, which we treat as a black box.
        \item We finish in some final state with free particles of definite momentum, spin, etc.
    \end{itemize}
    
    In order to allow us to consider free particles we assume that the initial state happens a long long time before the scattering, taking \(t \to -\infty\).
    Similarly we assume the final state is a long long time after the scattering, taking \(t \to +\infty\).
    Call the initial state \(\ket{\Psi, -\infty}\) and the final state \(\ket{\Psi, +\infty}\).
    Then an in between state at time \(t\) is \(\ket{\Psi, t}\).
    
    The initial and final state must somehow be related if we are to pass from one to the other.
    We take this relation to be of the form
    \begin{equation}
        \ket{\Psi, +\infty} = S\ket{\Psi, -\infty}
    \end{equation}
    where \(S\), known as the \define{\(\symbf{S}\) matrix}\index{S matrix@\(S\) matrix}, is the operator describing the scattering process.
    
    Now suppose that we start with some fixed, known initial state, \(\ket{i}\), so \(\ket{\Psi, -\infty} = \ket{i}\).
    We can choose some complete orthonormal basis of possible final states, \(\{\ket{f}\}\), which allows us to expand the final state as
    \begin{equation}
        \ket{\Psi, +\infty} = \sum_f \ket{f}\braket{f}{\Psi, +\infty}.
    \end{equation}
    We interpret
    \begin{equation}
        \abs{\braket{f}{\Psi, +\infty}}^2 = \probability(i \to f)
    \end{equation}
    as the probability that when we start in state \(\ket{i}\) our scattering process finishes in state \(\ket{f}\).
    
    Using the definition of \(S\) we can write the amplitude as
    \begin{equation}
        \braket{f}{\Psi, +\infty} = \bra{f} S \ket{\Psi, -\infty} = \bra{f} S \ket{i} \eqqcolon S_{fi},
    \end{equation}
    where \(S_{fi} \coloneqq \bra{f}S\ket{i}\) is a matrix element of the scattering operator.
    So the matrix elements of the scattering operator tell us the amplitude for a particular scattering process.
    
    We can expand the final state as
    \begin{equation}
        \ket{\Psi, +\infty} = \sum_f \ket{f} S_{fi},
    \end{equation}
    and take the conjugate to get
    \begin{equation}
        \bra{\Psi, +\infty} = \sum_f S_{if}^*\bra{f}.
    \end{equation}
    Combining this, and working with normalised states, we get
    \begin{equation}
        1 = \braket{\Psi, +\infty}{\Psi, +\infty} = \sum_{f, f'} S_{fi}S_{if'}^{*} \braket{f}{f'} = \sum_{f, f'} S_{fi}S_{if'}^*\delta_{ff'} = \sum_{f} S_{if}^*S_{fi}
    \end{equation}
    or in terms of operators,
    \begin{equation}
        \ident = S^\hermit S,
    \end{equation}
    so the scattering operator is unitary.
    
    We can interpret this as conservation of probability, since \(\{\ket{f}\}\) is a complete set of states we must have 
    \begin{equation}
        \sum_{f} \probability(i \to f) = 1,
    \end{equation}
    and hence if we have a definite start state that is also a complete set, so the evolution must be unitary.
    
    Note that the notion of the \(S\) matrix as discussed here is more general than in relativistic quantum mechanics since in QFT we can create and destroy particles, so our scattering can include processes like annihilation or pair production.
    
    \section{Interaction Picture}
    We now introduce the third, and final, picture of quantum mechanics, called the \defineindex{interaction picture}, or \define{Dirac picture}\index{Dirac picture|see{interaction picture}}.
    Suppose we have a Hamiltonian
    \begin{equation}
        H = H_0 + H_{\interaction}
    \end{equation}
    where \(H_0\) describes a free field and \(H_{\interaction}\) an interaction.
    
    In the Dirac picture both operators and states have time dependence.
    The states evolve as if they were in the Schrödinger picture, but under only the free Hamiltonian, so
    \begin{equation}
        \ket{\psi, t}_{\symrm{D}} \coloneqq \e^{iH_0t} \ket{\psi, t}_{\symrm{S}} = \e^{iH_0t} \e^{-iHt}\ket{\psi}_{\symrm{H}}
    \end{equation}
    where subscripts \(\symrm{D}\), \(\symrm{S}\), and \(\symrm{H}\) denote the Dirac, Schrödinger, and interaction pictures respectively.
    The first equality is the definition of a state in the Dirac picture, and the second comes from inverting the definition of a state in the Heisenberg picture:
    \begin{equation}
        \ket{\psi}_{\symrm{H}} \coloneqq \e^{iHt}\ket{\psi, t}_{\symrm{S}}.
    \end{equation}
    
    Operators in the Dirac picture transform as if they were in the Heisenberg picture, but only under the free Hamiltonian, so
    \begin{equation}
        A_{\symrm{D}}(t) = \e^{iH_0t} A_{\symrm{S}} \e^{-iH_0t}.
    \end{equation}
    
    Take the derivative of the equation defining a state in the Dirac picture, this gives
    \begin{equation}
        i\diffp{}{t} \ket{\psi, t}_{\symrm{D}} = -H_0\e^{iH_0t}\ket{\psi, t}_{\symrm{S}} + \e^{iH_0t} i\diffp{}{t}\ket{\psi, t}_{\symrm{S}}.
    \end{equation}
    Now recognise the final term as \(\e^{iH_0t}\) times one side of the Schrödinger equation:
    \begin{equation}
        i\diffp{}{t} \ket{\psi, t}_{\symrm{S}} = H\ket{\psi, t}_{\symrm{S}} = H_0 \ket{\psi, t}_{\symrm{S}} + H_{\interaction} \ket{\psi, t}_{\symrm{S}}.
    \end{equation}
    Substituting this into the previous equation the first term cancels leaving us with
    \begin{equation}
        i\diffp{}{t} \ket{\psi, t}_{\symrm{D}} = \e^{iH_0t} H_{\interaction} \ket{\psi, t}_{\symrm{S}} = \e^{iH_0t} H_{\interaction} \e^{-iH_0t}\ket{\psi, t}_{\symrm{D}} = H_{\interaction}^{\symrm{D}}\ket{\psi, t}_{\symrm{D}}.
    \end{equation}
    This shows that states evolve in time according to the free Hamiltonian, \(H_0\).
    The time evolution due to the interaction is exhibited by the operators.
    
    We derived the Heisenberg equation by differentiating an arbitrary operator in both pictures.
    Doing the same here, between the Dirac and Schrödinger picture gives rise to the equation of motion
    \begin{equation}
        \diffp{}{t} A_{\symrm{D}}(t) = i\commutator{H_0}{A_{\symrm{D}}(t)}.
    \end{equation}
    Note that \(H_0^{\symrm{S}} = H_0^{\symrm{D}} = H_0^{\symrm{H}}\), but in general \(H_{\interaction}^{\symrm{D}} \ne H_{\interaction}^{\symrm{S}}\), which comes from the general fact that \(\commutator{H_0}{H_{\interaction}} \ne 0\).
    
    From now on we will work entirely in the Dirac picture.
    Accordingly we will drop the labels \(\symrm{D}\), \(\symrm{S}\), and \(\symrm{H}\).
    
    \section{Dyson Series}
    The \defineindex{Dyson series} is a formal solution to the evolution equation
    \begin{equation}
        i\diffp{}{t}\ket{\Psi, t} = H_{\interaction}(t) \ket{\Psi, t}.
    \end{equation}
    Now we make the time dependence of the interaction Hamiltonian explicit.
    The solution to this is
    \begin{equation}\label{eqn:to iterate for dyson series}
        \ket{\Psi, t} = \ket{\Psi, -\infty} - i \int_{-\infty}^{t} \dl{t_1} \, H_{\interaction}(t_1) \ket{\Psi, t_1}.
    \end{equation}
    To see this just take the derivative:
    \begin{equation}
        i\diffp{}{t}\ket{\Psi, t} = i\diffp{}{t}\ket{\Psi, -\infty} + \diffp{}{t}\int_{-\infty}^{t} \dl{t_1} \, H_{\interaction}(t_1)\ket{\Psi, t_1} = H_{\interaction}(t) \ket{\Psi, t}.
    \end{equation}
    Here the first term vanishes as it is constant and the second term is easily computed by an application of the fundamental theorem of calculus.
    
    The problem is that this solution for \(\ket{\Psi, t}\) has \(\ket{\Psi, t_1}\) in it, in particular we integrate up to \(t_1 = t\).
    This means that we need to know the state at time \(t\) in order to evaluate the state at time \(t\).
    This seems like a problem.
    The solution is that we are assuming weak interactions.
    This means that the operator \(H_{\interaction}(t)\) is, in some sense, \enquote{small}, and the operator \(H_{\interaction}(t)H_{\interaction}(t_1)\) is \enquote{smaller}.
    Of course, operators don't have sizes in this way, but we can think about specific matrix elements and it all works out.
    This suggests that we can solve this problem recursively with each level of recursion being a smaller and smaller correction.
    The first level of recursion is to substitute \(\ket{\Psi, t_1}\) with the entire right hand side of \cref{eqn:to iterate for dyson series}, being careful to avoid reusing integration variables, giving
    \begin{equation}
        \ket{\Psi, t} = \ket{\Psi, -\infty} - i \int_{-\infty}^{t} \dl{t_1} \, H_{\interaction}(t_1) \left[ \ket{\Psi, -\infty} - i\int_{-\infty}^{t_1} \dl{t_2} \, H_{\interaction}(t_2) \ket{\Psi, t_2} \right].
    \end{equation}
    Note that the second integral is quadratic in the Hamiltonian, so is \enquote{smaller} than the rest of the terms.
    We can repeat this process, writing \(\ket{i} = \ket{\Psi, -\infty}\) for compactness, and we get
    \begin{equation*}
        \ket{\Psi, t} = \ket{i} - i \int_{-\infty}^{t} \dl{t_1} \, H_{\interaction}(t_1) \left[ \ket{i} - i \int_{-\infty}^{t_1} \dl{t_2} \, H_{\interaction}(t_2) \left\{ \ket{i} - i \int_{-\infty}^{t_2} \dl{t_3} \, H_{\interaction}(t) \ket{\Psi, t_3} \right\} \right].
    \end{equation*}
    
    Continuing on like this we have a series
    \begin{multline}
        \ket{\Psi, t} = \ket{i} + (-i) \int_{-\infty}^{t} \!\! \dl{t_1} \, H_{\interaction}(t_1) \ket{i} + (-i)^2 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} H_{\interaction}(t_1) H_{\interaction}(t_2) \ket{i}\\
        + (-i)^3 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} \int_{-\infty}^{t_2} \!\! \dl{t_3} \, H_{\interaction}(t_1) H_{\interaction}(t_2) H_{\interaction}(t_3) \ket{i} + \dotsb.
    \end{multline}
    Each term in this series is \enquote{smaller} than the previous term and so we can truncate at some point once we've achieved the required level of accuracy.
    
    Taking \(t\) to infinity we get
    \begin{multline}
        \ket{\Psi, +\infty} = \ket{i} + (-i) \int_{-\infty}^{\infty} \!\! \dl{t_1} \, H_{\interaction}(t_1) \ket{i} + (-i)^2 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} H_{\interaction}(t_1) H_{\interaction}(t_2) \ket{i}\\
        + (-i)^3 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} \int_{-\infty}^{t_2} \!\! \dl{t_3} \, H_{\interaction}(t_1) H_{\interaction}(t_2) H_{\interaction}(t_3) \ket{i} + \dotsb.
    \end{multline}
    Recognising that this has the form of an operator, albeit an infinite series of integrals, acting on the initial state to give the final state we can identify this operator as the \(S\) matrix:
    \begin{multline}
        S = \ident + (-i) \int_{-\infty}^{\infty} \!\! \dl{t_1} \, H_{\interaction}(t_1) + (-i)^2 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} H_{\interaction}(t_1) H_{\interaction}(t_2)\\
        + (-i)^3 \int_{-\infty}^{t} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} \int_{-\infty}^{t_2} \!\! \dl{t_3} \, H_{\interaction}(t_1) H_{\interaction}(t_2) H_{\interaction}(t_3) + \dotsb.
    \end{multline}
    Or, more compactly,
    \begin{equation}
        S = \sum_{n = 0}^{\infty} (-i)^n \int_{-\infty}^{\infty} \!\! \dl{t_1} \int_{-\infty}^{t_1} \!\! \dl{t_2} \dotsm \int_{-\infty}^{t_{n-1}} \!\! \dl{t_n} \, H_{\interaction}(t_1) H_{\interaction}(t_2) \dotsm H_{\interaction}(t_n).
    \end{equation}
    Notice that the times, \(t_1\), \(t_2\), and so on appear in increasing order, that is in the integral \(t_1 < t_2 < \dotsb < t_n\), we can neglect the endpoints of the integration ranges where we may have equality since individual points don't contribute to an integral.
    
    To proceed we make use of the following identity:
    \begin{multline}
        \int_{t_a}^{t_b} \!\! \dl{t_1} \int_{t_a}^{t_1} \!\! \dl{t_2} \int_{t_a}^{t_2} \dotsm \int_{t_a}^{t_{n-1}} \!\! \dl{t_n} \, A(t_1)A(t_2) \dotsb A(t_n)\\
        = \frac{1}{n!} \int_{t_a}^{t_b} \!\! \dl{t_1} \int_{t_a}^{t_b} \dl{t_2} \dotsm \int_{t_a}^{t_b} \!\! \dl{t_n} \timeOrdering[A(t_1)A(t_2) \dotsm A(t_n)].
    \end{multline}
    Here \(t_a\) and \(t_b\) are arbitrary initial and final times with \(t_a < t_b\), and \(A\) is an arbitrary operator.
    The operator \(\timeOrdering\) is called the \defineindex{time ordering}.
    We take the arguments, which are operators, and order according to evaluation time in order of increasing time from left to right, so that the first operator to act on some state is the operator evaluated at the earliest time.
    That is, for two operators
    \begin{align}
        \timeOrdering[A(t_1), B(t_2)] &\coloneqq
        \begin{cases}
            A(t_1)B(t_2) & t_1 > t_2\\
            B(t_2)A(t_1) & t_2 > t_1
        \end{cases}
        \\
        &\hphantom{:}= \heaviside(t_1 - t_2) A(t_1)B(t_2) + \heaviside(t_2 - t_1) B(t_2)A(t_1).
    \end{align}
    This can then be extended to \(n\) operators in the obvious way, although it becomes harder to write in terms of Heaviside step functions.
    
    We'll show how this works for the \(n = 2\) case, and then the general case follows by induction.
    Start with the right hand side of the identity.
    For \(n = 2\) we have
    \begin{equation}
        I = \int_{t_a}^{t_b} \dl{t_1} \int_{t_a}^{t_b} \dl{t_2} \, \timeOrdering[A(t_1)A(t_2)].
    \end{equation}
    We can split the integral over \(t_2\) into two parts, one running from \(t_a\) to \(t_1\), and the other from \(t_1\) to \(t_b\):
    \begin{equation}
        I = \int_{t_a}^{t_b} \dl{t_1} \left[ \int_{t_a}^{t_1} \dl{t_2} \, \timeOrdering[A(t_1)A(t_2)] + \int_{t_1}^{t_b} \dl{t_2} \, \timeOrdering[A(t_1) A(t_2)] \right].
    \end{equation}
    Consider the second term,
    \begin{equation}
        \int_{t_a}^{t_b} \dl{t_1} \int_{t_2}^{t_b} \dl{t_2} \, \timeOrdering[A(t_1) A(t_2)].
    \end{equation}
    This is a double integral.
    This corresponds to integrating over the triangle in the plane bounded by \(t_1 = t_2\), \(t_1 = t_a\), and \(t_2 = t_b\).
    We can integrate over the same region in a different way.
    Instead integrate along \(t_2\) from \(t_a\) to \(t_1\) and then along \(t_1\) from \(t_a\) to \(t_b\).
    This corresponds to the integral
    \begin{equation}
        \int_{t_a}^{t_b} \dl{t_2} \int_{t_a}^{t_2} \dl{t_1} \, \timeOrdering[A(t_1)A(t_2)].
    \end{equation}
    Now, \(t_1\) and \(t_2\) are just integration variables, and the integrand is symmetric in \(t_1\) and \(t_2\), as \(\timeOrdering[A(t_1)A(t_2)] = \timeOrdering[A(t_2)A(t_1)]\), the ordering of operators at different times within a time ordering is not important, since we're going to time order them.
    We are then free to rename \(t_1\) and \(t_2\), and in particular swap them without changing anything else,
    \begin{equation}
        \int_{t_a}^{t_b} \dl{t_1} \int_{t_a}^{t_1} \dl{t_2} \, \timeOrdering[A(t_1) A(t_2)].
    \end{equation}
    Combing this back with the first term we get
    \begin{equation}
        I = 2 \int_{t_a}^{t_b} \dl{t_1} \int_{t_a}^{t_1} \dl{t_2} \, \timeOrdering[A(t_1)A(t_2)].
    \end{equation}
    Noticing with these integration ranges the operators are already time ordered, so we can drop the explicit time ordering, and that \(2 = 2!\) and dividing through by 2 we have proven the identity for \(n = 2\).
    
    Now, suppose that the identity holds for \(k\) integrals, and we have \(k + 1\) integrals.
    We can use the induction hypothesis to swap \(k\) integrals, and then we pairwise swap integrals as above before the unswapped integral has also been swapped.
    
    \begin{figure}
        \tikzsetnextfilename{region-of-integration-for-identity}
        \begin{tikzpicture}
            \draw[very thick, <->] (0, 4) node[above] {\(t_2\)} -- (0, 0) -- (4, 0) node[right] {\(t_1\)};
            \draw[highlight, very thick] (0, 0) -- (4, 4);
            \coordinate (t1=t2) at (3.53, 3.8);
            \node[rotate around={45:(t1=t2)}] at (t1=t2) {\(t_1 = t_2\)};
            \node[below] at (1, 0) {\(t_a\)};
            \node[left] at (0, 1) {\(t_a\)};
            \node[below] at (3, 0) {\(t_b\)};
            \node[left] at (0, 3) {\(t_b\)};
            \draw[highlight] (1, 0) -- ++ (0, 1);
            \draw[highlight] (3, 0) -- ++ (0, 3);
            \draw[highlight] (0, 1) -- ++ (1, 0);
            \draw[highlight] (0, 3) -- ++ (3, 0);
            \draw[very thick, highlight, fill=highlight!50] (1, 1) -- (3, 3) -- (1, 3) -- cycle;
        \end{tikzpicture}
        \caption{The region of integration for the \(n = 2\) case of the identity.}
    \end{figure}
    
    We can use this identity to write
    \begin{equation}
        S = \sum_{n = 0}^{\infty} \frac{(-i)^n}{n!} \int_{-\infty}^{\infty} \!\! \dl{t_1} \int_{-\infty}^{\infty} \!\! \dl{t_2} \dotsm \int_{-\infty}^{\infty} \!\! \dl{t_n} \, \timeOrdering[H_{\interaction}(t_1) H_{\interaction}(t_2) \dotsm H_{\interaction}(t_n)].
    \end{equation}
    We can now replace \(H_{\interaction}\) with an integral over the Hamiltonian density giving
    \begin{equation}
        S = \sum_{n = 0}^{\infty} \frac{(-i)^n}{n!} \int \!\! \dl{x_1} \int \!\! \dl{x_2} \dotsm \int \!\! \dl{x_n} \, \timeOrdering[\hamiltonianDensity_{\interaction}(x_1) \hamiltonianDensity_{\interaction}(x_2) \dotsm \hamiltonianDensity_{\interaction}(x_n)].
    \end{equation}
    This is called the \defineindex{Dyson series}.
    
    Note that the interaction Hamiltonian density is given by \(\hamiltonianDensity_{\interaction} = -\lagrangianDensity_{\interaction}\).
    This follows directly from the definition
    \begin{equation}
        \hamiltonianDensity = \hamiltonianDensity_0 + \hamiltonianDensity_{\interaction} = \pi \dot{\varphi} - \lagrangianDensity = \pi \dot{\varphi} - \lagrangianDensity_0 - \lagrangianDensity_{\interaction}
    \end{equation}
    where the \(\pi\dot{\varphi} - \lagrangianDensity_0\) term is the unperturbed Hamiltonian density.
    Using this we can write
    \begin{align}
        S &= \sum_{n = 0}^{\infty} \frac{i^n}{n!} \int \!\! \dl{x_1} \int \!\! \dl{x_2} \dotsm \int \!\! \dl{x_n} \, \timeOrdering[\lagrangianDensity{\interaction}(x_1) \lagrangianDensity{\interaction}(x_2) \dotsm \lagrangianDensity{\interaction}(x_n)]\\
        &= \exp\left[ i\int \dl{^4x} \lagrangianDensity_{\interaction} \right].
    \end{align}
    So the \(S\) matrix can be viewed as the phase factor given by the interaction action.
    This last step is simply a formal rewriting recognising the exponential series.
    When expanding make sure to have each integral in each power be over a different variable, and to time order the integrand.
    From this form we can see that, assuming the action is Lorentz invariant, then so is the \(S\) matrix.
    This works because time ordering is Lorentz invariant, because it can be done using only terms like \(\heaviside(t - t')\), and each such term is invariant under proper orthochronous Lorentz transformations.
    
    \section{Interpretation}
    For a given initial state, \(\ket{i}\), and final state, \(\ket{f}\), we need to find terms in the interaction Lagrangian, \(\lagrangianDensity_{\interaction}\) which give a nonzero transition amplitude for \(\ket{i}\) to \(\ket{f}\).
    The entire effect of the interaction is in \(\lagrangianDensity_{\interaction}\), which is an operator given by a polynomial in the fields.
    This means that really it's just a collection of creation and annihilation operators.
    Therefore the entire effect of the interaction is to create and destroy particles.
    We then look for terms with
    \begin{itemize}
        \item annihilation operators destroying the initial state,
        \item creation operators creating the final state,
        \item paired creation/annihilation operators creating and then destroying intermediate states.
    \end{itemize}

    In terms of particles, we look for terms which destroy the incoming particles, potentially create and then destroy one or more new particles, then create some particles which form the final state.
    The initial and final particles must be on-shell for us to observe them.
    The intermediate particles however needn't be on-shell, since we cannot observe them.
    These are called \define{virtual particles}\index{virtual particle}.
    Don't put too much stock in the existence and off-shell nature of these virtual particles, they're really just a mathematical construct with different combinations of intermediate particles corresponding to different terms in the Dyson series.
    
    In the interaction picture the fields satisfy the free field equation, and therefore have the usual creation and annihilation operators.
    We can therefore use normal ordering, placing all creation operators on the left and all annihilation operators on the right, in order to ensure that incoming particles are destroyed and then new particles are created, rather than the other way round.
    
    \section{\texorpdfstring{\(\varphi^3\)}{Phi Cubed} Interactions}
    In \(\varphi^3\) theory the interaction Hamiltonian is
    \begin{equation}
        \hamiltonianDensity_{\interaction} = -\lagrangianDensity_{\interaction} = \frac{1}{6}g\normalordering{\varphi^3},
    \end{equation}
    where we now normal order the \(\varphi^3\) factor in order to destroy existing particles and then create new particles.
    
    We will consider 2--2 scattering, that is two particles come in, interact in some way, and two particles leave.
    This can be represented by a diagram like
    \vspace{2.4cm}
    \begin{equation}
        \tikzsetnextfilename{fd-2-2-phi-cubed-scattering-general-diagram}
        \smash{\rotatebox{45}{
            \feynmandiagram[inline=(v)]{
                {a, b, c, d} -- v [blob]
            };
        }}
    \end{equation}
    Here the blob represents some unknown black box type interaction.
    Time flows, in our convention, from left to right, so the left two legs represent the incoming particles, and the right two the outgoing particles.
    In order to destroy the incoming particles we start with a \(\varphi^+\varphi^+\) term.
    Then to create two new particles we need a \(\varphi^-\varphi^-\) term.
    Recall that we order operators from right to left, as we want the \(\varphi^+\varphi^+\) term to act on the initial before the \(\varphi^-\varphi^-\) term.
    We could therefore look for a
    \begin{equation}
        \varphi^-\varphi^-\varphi^+\varphi^+
    \end{equation}
    term in the series.
    However, we won't find such a term since in \(\varphi^3\) theory we can only have terms with a multiple of 3 factors of \(\varphi^{\pm}\).
    This means that there are no direct 2--2 scattering events in \(\varphi^3\) theory, such events must always go via at least one intermediate virtual particle state.
    So, next we look at the term in the series which is quadratic in the Hamiltonian, and hence has six factors of \(\varphi^{\pm}\).
    There are two possible orderings of terms with the required starting point of \(\varphi^+\varphi^+\) to annihilate the initial state and end point of \(\varphi^-\varphi^-\) to create the final state.
    They are
    \begin{equation}
        \varphi^-\varphi^- \varphi^- \varphi^+ \varphi^+ \varphi^+, \qqand \varphi^- \varphi^- \varphi^+ \varphi^- \varphi^+ \varphi^+.
    \end{equation}
    When acting on the initial state the the first vanishes, since after annihilating the initial state we are left with the vacuum state, which we then act on with the annihilator \(\varphi^+\).
    So, the only nonzero second term quadratic in the Hamiltonian is
    \begin{equation}
        \varphi^-\varphi^-\varphi^+\varphi^-\varphi^+\varphi^+.
    \end{equation}
    We interpret this as annihilating the initial state, creating a new particle, annihilating it, and then destroying it again.
    The amplitude for this particular term is proportional to \(g^2\), since each Hamiltonian introduces a factor of \(g\).
    Further, by assuming that energy is conserved and everything is local when we annihilate the two initial particles we must immediately create the virtual particle, and all of this must happen at the same position, so this is a single spacetime event, \(x_2\).
    Then when the virtual particle is annihilated we must immediately, and at the same location, create the new particles, so this is a second spacetime event, \(x_1\).
    These correspond to the two points at which the interaction Hamiltonians are evaluated
    so we'll have a nonzero term
    \begin{equation}
        -\frac{g^2}{36}\int_{-\infty}^{\infty} \dl^4{x_1} \int_{-\infty}^{\infty} \dl{^4x_2} \, \varphi^-(x_2)\varphi^-(x_2)\varphi^+(x_1)\varphi^-(x_2)\varphi^+(x_2)\varphi^+(x_2).
    \end{equation}
    Pictorially we can represent this as
    \begin{equation}
        \tikzsetnextfilename{fd-2-2-phi-cubed-single-virtual-particle}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[small, horizontal=v1 to v2] {
                    {i1, i2} -- v1 -- v2 -- {o1, o2}
                };
                \node[left] at (v1) {\(g\)};
                \node[right] at (v2) {\(g\)};
                \node[above, xshift=0.05cm] at (v1) {\(x_2\)};
                \node[above, xshift=-0.05cm] at (v2) {\(x_1\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Here the left and right legs represent the incoming and outgoing particles, and the single line in the middle is the virtual particle.
    
    Next, we would consider terms cubic in the Hamiltonian.
    However, things quickly get out of hand if we proceed as we did here.
    The cubic term will have nine factors of \(\varphi\).
    So, we want a way to do this all more systematically.
    This is the theory we will develop in the next chapter.
    
    \chapter{Wick's Theorem}
    To proceed we need to be able to deal with time orderings of operators.
    Our interpretation, of particles being created and destroyed, works best if we normal order things, placing all creation operators on the left and annihilation operators on the right.
    So, we look for a way to rewrite a time ordering in terms of normal orderings, and other quantities which can be calculated more easily.
    We proceed with the case of two operators, then we introduce a general theorem central to much of quantum field theory.
    
    \section{Two Operator Case}
    Suppose we have two operator valued fields, \(A\) and \(B\), which we can write as
    \begin{equation}
        A = A^+ + A^-, \qqand B = B^+ + B^-,
    \end{equation}
    where \(A^+\) and \(B^+\) are expressions in the annihilation operators and \(A^-\) and \(B^-\) are expressions in the creation operators.
    Consider the product \(\normalordering{A(x)B(x')}\).
    Notice that the normal ordering of a sum is the sum of the normal ordering of the individual terms, that is \(\normalordering{C + D} = \normalordering{C} + \normalordering{D}\).
    Expanding in terms of \(A^{\pm}\) and \(B^{\pm}\) we have
    \begin{align*}
        \normalordering{A(x)B(x')} &= \normalordering{[A^+(x) + A^-(x')][B^+(x') + B^-(x')]}\\
        &= \normalordering{A^+(x)B^+(x') + A^+(x)B^-(x') + A^-(x)B^+(x') + A^-(x)B^-(x')}\\
        &= A^+(x)B^+(x') + B^-(x')A^+(x) + A^-(x)B^+(x') + A^-(x)B^-(x').
    \end{align*}
    
    Now, consider what happens when we subtract this result from \(A(x)B(x')\) without the normal ordering, that is
    \begin{equation}
        A(x)B(x') = A^+(x)B^+(x') + A^+(x)B^-(x') + A^-(x)B^+(x') + A^-(x)B^-(x').
    \end{equation}
    The only term that doesn't cancel is the second term where we have to swap the ordering, so
    \begin{equation}
        A(x)B(x') - \normalordering{A(x)B(x')} = A^+(x)B^-(x') - B^-(x')A^+(x) = \commutator{A^+(x)}{B^-(x')}.
    \end{equation}
    
    The commutator of creation and annihilation operators is just a number, or more accurately a Dirac delta, and so it doesn't act on states, in particular
    \begin{equation}
        \bra{0} \commutator{A^+(x)}{B^-(x')}\ket{0} = \braket{0}{0} \commutator{A^+(x)}{B^-(x')} = \commutator{A^+(x)}{B^-(x')},
    \end{equation}
    since we choose to normalise the vacuum state.
    We also have
    \begin{equation}
        \bra{0} \commutator{A^+(x)}{B^-(x')}\ket{0} = \bra{0} A^+(x)B^-(x')\ket{0} - \bra{0} B^-(x')A^+(x)\ket{0} = \bra{0} A^+(x)B^-(x')\ket{0},
    \end{equation}
    where the second term vanishes since the annihilation operator \(A^+(x)\) acts on the vacuum.
    
    We then have
    \begin{equation}\label{eqn:almost wicks theorem for two operators}
        A(x)B(x') = \normalordering{A(x)B(x')} + \bra{0} A(x) B(x') \ket{0}.
    \end{equation}
    We're nearly there, recall that the definition of time ordering is
    \begin{equation}
        \timeOrdering[A(x)B(x')] = \heaviside(t - t')A(x)B(x') + \heaviside(t' - t)B(x')A(x),
    \end{equation}
    where \(t\) is the time component of \(x\) and \(t'\) is the time component of \(x'\).
    
    At this point we make the observation that it doesn't matter what order we write operators in if we are normal ordering them, since we're going to change the order anyway, so
    \begin{equation}
        \normalordering{A(x)B(x')} = \normalordering{B(x')A(x)}.
    \end{equation}
    Hence, if we normal order the time ordering we have
    \begin{align}
        \normalordering{\timeOrdering[A(x)B(x')]} &= \heaviside(t - t')\normalordering{A(x)B(x')} + \heaviside(t' - t)\normalordering{B(x')A(x)}\\
        &= \heaviside(t - t')\normalordering{A(x)B(x')} + \heaviside(t' - t)\normalordering{A(x)B(x')}\\
        &= \normalordering{A(x)B(x')}.
    \end{align}
    So, the normal ordering of a time ordering is just the normal ordering.
    
    Consider \cref{eqn:almost wicks theorem for two operators}.
    This holds for any product of two operators, in particular if we time order the operators it still works, since we can just separate into the \(t < t'\) and \(t > t'\) cases and then proceed without the time ordering.
    Hence,
    \begin{equation}
        \timeOrdering[A(x)B(x')] = \normalordering{A(x)B(x')} + \bra{0} \timeOrdering[A(x)B(x')] \ket{0}.
    \end{equation}
    Here we use \(\normalordering{\timeOrdering[A(x)B(x')]} = \normalordering{A(x)B(x')}\) to drop the time ordering in the first term on the right.
    
    This equation, and others like it, are very important in quantum field theory, so important that we introduce a notation to simplify it.
    We define the \defineindex{Wick contraction} of two operators, \(A\) and \(B\) to be
    \begin{equation}
        \wick{\c A(x) \c B(x')} \coloneqq \bra{0} \timeOrdering[A(x)B(x')] \ket{0}.
    \end{equation}
    Note that this is just a number, not an operator.
    This allows us to state Wick's theorem for two operators.
    
    \begin{lma}{}{lma:wick's theorem for two operators}
        For two operators, \(A\) and \(B\), such that \(A = A^+ + A^-\) and \(B = B^+ + B^-\) where \(A^+\) and \(B^+\) are expressions in the annihilation operator and \(A^-\) and \(B^-\) are expressions in the creation operators we have
        \begin{equation}
            \timeOrdering[A(x)B(x')] = \normalordering{A(x)B(x')} + \wick{\c A(x) \c B(x')}.
        \end{equation}
        \begin{proof}
            See above work.
        \end{proof}
    \end{lma}
    
    \section{Three Operator Case}
    The proof of Wick's theorem, the generalisation of the previous lemma to an arbitrary number of operators, proceeds inductively.
    It will be simpler to prove if we first prove the three operator case to see how the inductive logic works in detail.
    As before our goal is to express a time ordered product of operators in terms of their normal ordering and contractions.
    We now have an extra operator, \(C\), which we again assume can be written as \(C = C^+ + C^-\) where \(C^+\) is an expression in the annihilation operators and \(C^-\) an expression in the creation operators.
    
    We want to evaluate \(\timeOrdering[A(x_1)B(x_2)C(x_2)]\) where \(x_i\) has time component \(t_i\).
    For simplicity, and without loss of generality, we assume that \(t_1 > t_2 > t_3\).
    If this isn't the case we can reorder freely within the time ordering and then relabel our operators so that it is.
    We don't consider here the case where the operators are evaluated at the same time, we'll do this later.
    From now on we suppress the arguments to our operators, but be aware that they are not all evaluated at the same point.
    
    Since, by assumption, \(A\) is the first operator evaluated we can move it outside of the time ordering, since it will always appear on the left of any time ordered product of operators:
    \begin{equation}
        \timeOrdering[ABC] = A\timeOrdering[BC].
    \end{equation}
    Now we can apply \cref{lma:wick's theorem for two operators} to the time ordering of \(B\) and \(C\), giving
    \begin{equation}
        \timeOrdering[ABC] = A\normalordering{BC} + A\wick{\c B \c C}.
    \end{equation}
    Now express \(A\) as \(A^+ + A^-\), so
    \begin{align}
        \timeOrdering[ABC] &= (A^+ + A^-)\normalordering{BC} + \wick{\c B \c C}\\
        &= A^+ \normalordering{BC} + A^+\wick{\c B \c C} + A^- \normalordering{BC} + A^- \wick{\c B \c C}.
    \end{align}
    Notice that since \(A^-\) is a creation operator and is on the left of \(\normalordering{BC}\) we can move it into the normal ordering without changing anything, so
    \begin{equation}
        \timeOrdering[ABC] = A^+ \normalordering{BC} + A^+\wick{\c B \c C} + \normalordering{A^-BC} + A^- \wick{\c B \c C}.
    \end{equation}
    
    Things aren't quite so simple for \(A^+\normalordering{BC}\).
    In order to bring \(A^+\), which is an annihilation operator, into the normal ordering we need it to be on the right.
    We can achieve this at the cost of a commutator:
    \begin{equation}
        A^+ \normalordering{BC} = \normalordering{BC}A^+ + \commutator{A^+}{\normalordering{BC}} = \normalordering{BCA^+} + \commutator{A^+}{\normalordering{BC}}.
    \end{equation}
    Now that \(A^+\) has been brought into the normal ordering we can move it around within the ordering, so
    \begin{equation}
        A^+ \normalordering{BC} = \normalordering{A^+BC} + \commutator{A^+}{\normalordering{BC}}.
    \end{equation}
    The commutator can be evaluated with the identity
    \begin{equation}\label{eqn:commutator identity x and yz}
        \commutator{X}{YZ} = XYZ - YZX = XYZ - YXZ + YXZ - YZX = \commutator{X}{Y}Z + Y\commutator{X}{Z}.
    \end{equation}
    Since the commutator is just a number the normal ordering doesn't effect it, and so we have
    \begin{equation}
        A^+ \normalordering{BC} = \normalordering{A^+BC} + \normalordering{\commutator{A^+}{B^-}C} + \normalordering{B\commutator{A^+}{C^-}}.
    \end{equation}
    We've also used here that \(\commutator{X^+}{Y} = \commutator{X^+}{Y^-}\), since \(X^+\) commutes with \(Y^+\), as both are just formed from annihilation operators.
    
    Exactly as in the two operator case we can replace the commutator with its vacuum expectation value, and then drop the commutator, these are just numbers, so can be brought out of the normal ordering.
    \begin{align}
        A^+ \normalordering{BC} &= \normalordering{A^+BC} + \commutator{A^+}{B^-}\normalordering{C} + \commutator{A^+}{C^-}\normalordering{B}\\
        &= \normalordering{A^+BC} + \bra{0}\commutator{A^+}{B^-}\ket{0}\normalordering{C} + \bra{0}\commutator{A^+}{C^-}\ket{0}\normalordering{B}\\
        &= \normalordering{A^+BC} + \bra{0}A^+B^-\ket{0}\normalordering{C} + \bra{0}A^+C^-\ket{0}\normalordering{B}.
    \end{align}
    Notice that since we're normal ordering a single term which decomposes as the sum of creation and annihilation operators we don't really need the normal orderings any more, but we keep them so our result fits the general result.
    
    Going back to our starting point we now have
    \begin{align}
        \timeOrdering[ABC] &= A^+ \normalordering{BC} + A^+\wick{\c B \c C} + \normalordering{A^-BC} + A^- \wick{\c B \c C}\\
        &= \normalordering{ABC} + \bra{0}\timeOrdering[A^+B^-]\ket{0}\normalordering{C} + \bra{0}\timeOrdering[A^+C^-]\ket{0}\normalordering{B} + A\wick{\c B \c C} \notag\\
        &= \normalordering{ABC} + \wick{\c A \c B}\normalordering{C} + \wick{\c A \c C}\normalordering{B}.
    \end{align}
    Here we use
    \begin{equation}
        A^+\wick{\c B \c C} + A^- \wick{\c B \c C} = (A^+ + A^-) \wick{\c B \c C} = A\wick{\c B \c C},
    \end{equation}
    and
    \begin{equation}
        \normalordering{A^+ BC} + \normalordering{A^- BC} = \normalordering{(A^+ + A^-)BC} = \normalordering{ABC}
    \end{equation}
    
    The result here is that the time ordering is given by the normal ordering plus all pairs of contractions, normal ordering anything that isn't contracted.
    
    \section{General Theorem}
    We can now prove the general theorem relating time orderings of an arbitrary number of operators and their normal orderings and contractions.
    
    \begin{thm}{Wick's Theorem}{thm:wick}\index{Wick's theorem}
        Given operators, \(A, B, C, D, \dotsc\), which can all be written as a sum of an annihilator and creation operators.
        Suppose \(A\) is evaluated at \(x_1\), \(B\) at \(x_2\), \(C\) at \(x_3\), \(D\) at \(x_4\), and so on, and \(t_i\) is the time component of \(x_i\).
        Then if none of the \(t_i\) are equal we have
        \begin{align}
            \timeOrdering[ABCD\dotsm] &= \normalordering{ABCD\dotsm}\\
            &+ \wick{\c A \c B}\normalordering{CD\dotsm} + \wick{\c A \c C} \normalordering{BD\dotsm} + \wick{\c A \c D} \normalordering{BC} + \dotsb\\
            &+ \wick{\c B \c C} \normalordering{AD \dotsm } + \wick{\c B \c D} \normalordering{AC\dotsm} + \dotsb\\
            &\vdotswithin{+}\\
            &+ \wick{\c A \c B} \wick{\c C \c D} \normalordering{ \dotsm } + \wick{\c A \c C} \wick{\c B \c D} \normalordering{ \dotsm } + \wick{\c A \c D} \wick{\c B \c C} \normalordering{ \dotsm } + \dotsb\\
            &+ \text{all other possible contractions}.
        \end{align}
        That is, the time ordering is given by the normal ordering, plus the normal ordering contracting each pair of operators, plus the normal ordering contracting every possible pair of pairs of operators, and so on, doing every single contraction possible and normal ordering anything not contracted.
        
        \begin{proof}
            We proceed by strong induction on the number of operators.
            The one operator case is trivial and acts as the basis case, equivalently the two operator case can be the basis case and we then use \cref{lma:wick's theorem for two operators} as the proof of our basis case.
            
            Now, suppose that The hypothesis holds for \(k - 1\) operators for some positive integer \(k\).
            Take \(k\) operators \(A, B, C, D \dotsb\), evaluated at \(x_1, x_2, x_3, x_4, \dotsc\) respectively, with \(t_i\) the time component of \(x_i\).
            Without loss of generality we assume \(t_1 > t_2 > \dotsb > t_k\), if this isn't the case we can reorder within the time ordering and relabel such that it is the case.
            
            We can pull \(A\) outside of the time ordering, since it is evaluated last:
            \begin{equation}
                \timeOrdering[ABCD\dotsm] = A\timeOrdering[BCD\dotsm].
            \end{equation}
            Writing \(A = A^+ + A^-\) where \(A^+\) and \(A^-\) are expressions in the annihilation and creation operators respectively we have
            \begin{equation}
                \timeOrdering[ABCD\dotsm] = (A^+ + A^-)\timeOrdering[BCD\dotsm].
            \end{equation}
            The time ordering on the right has \(k - 1\) operators, so the induction hypothesis allows us to write this as
            \begin{align}
                \timeOrdering[ABCD\dotsm] &= (A^+ + A^-)(\normalordering{BCD\dotsm} + \wick{\c B \c C}\normalordering{B \dotsm}\\
                &\qquad+ \text{all contractions not involving \(A\)}).
            \end{align}
            We can trivially put \(A^-\) into all of the normal orderings, since it is a creation operator and is on the left.
            Therefore consider the \(A^+\) terms.
            We have
            \begin{align}
                A^+ \normalordering{BCD\dotsm} &= \normalordering{BCD\dotsm}A^+ + \commutator{A^+}{\normalordering{BCD\dotsm}}\\
                &= \normalordering{BCD\dotsm A^+} + \commutator{A^+}{\normalordering{BCD\dotsm}}\\
                &= \normalordering{A^+BCD\dotsm} + \commutator{A^+}{\normalordering{BCD\dotsm}},
            \end{align}
            Iteratively applying the identity in \cref{eqn:commutator identity x and yz} we have
            \begin{multline}
                A^+\normalordering{BCD\dotsm} = \normalordering{A^+BCD\dotsm} + \commutator{A^+}{B^-}\normalordering{CD\dotsm}\\
                + \commutator{A^+}{C^-} \normalordering{BD\dotsm} + \commutator{A^+}{D^-}\normalordering{BC\dotsm} + \dotsb.
            \end{multline}
            As with the two and three operator cases we can replace the commutators with contractions, giving
            \begin{multline}
                A^+\normalordering{BCD\dotsm} = \normalordering{A^+BCD} + \wick{\c A \c B}\normalordering{CD\dotsm}\\
                + \wick{\c A \c C}\normalordering{BD\dotsm} +  \wick{\c A \c D}\normalordering{BC\dotsm} + \dotsb.
            \end{multline}
            So we end up with all contractions containing \(A\) and some other operator, normal ordering all other operators.
            
            The exact same logic works for other terms, like
            \begin{equation}
                A^+ \wick{\c B \c C} \normalordering{D \dotsm} = \wick{\c A \c D} \wick{\c B \c C} \normalordering{\dotsm} + \dotsb,
            \end{equation}
            where we have all pairs of pairs of contractions with one pair being \(B\) and \(C\) and the other containing \(A\).
            Considering all such terms we see that we get all possible contractions involving \(A\).
            Going back to our result for \(\timeOrdering[ABCD\dotsm]\) as containing all contractions not involving \(A\), and recombining \(A^+\) and \(A^-\) to get \(A\) we prove the theorem.
        \end{proof}
    \end{thm}
    
    \begin{crl}{Wick's Theorem for Mixed Time Products}{}
        Suppose that some of the operators in the statement of \cref{thm:wick} are evaluated at the same time.
        Then the time ordering is instead given by all contractions not involving contractions of equal time operators and normal ordering everything left over.
        \begin{proof}
            This follows by applying Wick's theorem at unequal times, and then setting some times equal and seeing that contractions of equal time terms vanish, since
            \begin{equation}
                \timeOrdering(\normalordering{A(t, \vv{x})B(t, \vv{x}')}) = \normalordering{A(t, \vv{x})B(t, \vv{x}')}
            \end{equation}
            so
            \begin{equation}
                \wick{\c A \c B} = \timeOrdering[\normalordering{AB}] - \normalordering{AB} = 0.
            \end{equation}
        \end{proof}
    \end{crl}

    \section{Feynman Propagator}
    Consider the contraction
    \begin{align}
        \wick{\c \varphi(x) \c \varphi(x')} &= \bra{0} \timeOrdering[\varphi(x)\varphi(x')]\ket{0}\\
        &= \heaviside(t - t') \bra{0}  \varphi(x) \varphi(x')\ket{0} + \heaviside(t' - t) \bra{0} \varphi(x')\varphi(x)\ket{0}
    \end{align}
    where \(t\) is the time component of \(x\) and \(t'\) is the time component of \(x'\).
    
    We've seen that
    \begin{equation}
        i\Delta^+(x - x') = \commutator{\varphi^+(x)}{\varphi^-(x')}
    \end{equation}
    is just a number, and so
    \begin{equation}
        \bra{0} \commutator{\varphi^+(x)}{\varphi^-(x')} \ket{0} = \commutator{\varphi^+(x)}{\varphi^-(x')} \braket{0}{0} = \commutator{\varphi^+(x)}{\varphi^-(x')}.
    \end{equation}
    Hence, we have
    \begin{align}
        i\Delta^+(x - x') &= \bra{0} \commutator{\varphi^+(x)}{\varphi^-(x')} \ket{0}\\
        &= \bra{0} \varphi^+(x)\varphi^-(x') \ket{0}\\
        &= \bra{0} \varphi(x) \varphi(x') \ket{0},
    \end{align}
    where expanding the commutator the \(\varphi^-(x')\varphi^+(x)\) term annihilates the vacuum, and so vanishes, and in the last part expanding \(\varphi\) using \(\varphi^{\pm}\) shows that all terms apart from the \(\varphi^+(x)\varphi^-(x')\) term vanish as they involve annihilating the vacuum:
    \begin{align}
        \varphi(x)\varphi(x') &= (\varphi^+(x) + \varphi^-(x))(\varphi^+(x') + \varphi^-(x'))\\
        &= \varphi^+(x)\varphi^+(x') + \varphi^+(x)\varphi^-(x') + \varphi^-(x)\varphi^+(x') + \varphi^-(x)\varphi^-(x') \notag
    \end{align}
    
    Similarly, we have
    \begin{equation}
        i\Delta^-(x - x') = -\bra{0} \commutator{\varphi(x')}{\varphi(x)}\ket{0}.
    \end{equation}
    This just follows from \(\Delta^+(x) = -\Delta^-(-x)\).
    We can then define
    \begin{align}
        i\Delta_{\feynman} (x - x') &\coloneqq \wick{\c \varphi(x) \c \varphi(x')}\\
        &\hphantom{:}= \bra{0} \timeOrdering[\varphi(x)\varphi(x')]\ket{0}\\
        &= \heaviside(t - t') i\Delta^+(x - x') - \heaviside(t' - t)i\Delta^-(x - x').
    \end{align}
    This is called the \defineindex{Feynman propagator}.
    Without the factor of \(i\) it's defined as
    \begin{equation}
        \Delta_{\feynman}(x) = \heaviside(t)\Delta^+(x) - \heaviside(-t)\Delta^-(x).
    \end{equation}
    The interpretation is as follow:
    \begin{itemize}
        \item For \(t > t'\) we have a positive energy, that is \(\Delta^+\), mode propagating from \(x'\) to \(x\).
        \item For \(t < t'\) we have a negative energy, that is \(\Delta^-\), mode propagating from \(x'\)  to \(x\).
    \end{itemize}
    
    Notice that we can rewrite the Feynman propagator as
    \begin{equation}
        \Delta_{\feynman}(x - x') = \heaviside(t - t')\Delta^+(x - x') + \heaviside(t' - t)\Delta^+(x' - x).
    \end{equation}
    This is has the same value but a different interpretation:
    \begin{itemize}
        \item For \(t > t'\) we have a positive energy, that is \(\Delta^+\), mode propagating from \(x'\) to \(x\).
        \item For \(t < t'\) we have a positive energy, that is \(\Delta^+\), mode propagating from \(x\) to \(x'\).
    \end{itemize}
    
    Comparing these interpretations we see that a negative energy mode (which we'll later associate with antiparticles) is equivalent to a positive energy mode travelling in the opposite direction.
    
    \subsection{Contour Representation}
    As well as this interpretation the Feynman propagator has the nice property of automatically encoding the time ordering.
    This isn't that useful in the current form, since we have explicit Heaviside step functions doing the encoding.
    However, the Feynman propagator has a contour representation which makes the time ordering less obvious.
    
    The contour representation of \(\Delta_{\feynman}\) is
    \begin{equation}
        \Delta_{\feynman}(x) = \int_{C_{\feynman}} \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2}
    \end{equation}
    where \(C_{\feynman}\) is the contour given by the real axis in the \(p_0\) plane with a slight dip below the pole at \(-\omega(\vv{p})\) and a slight dip above the pole at \(\omega(\vv{p})\).
    This is shown in \cref{fig:contour for feynman propagator}.
    
    \begin{figure}
        \tikzsetnextfilename{contour-for-feynman-propagator}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [right] {\(\Re(p_0)\)};
            \draw[thick, ->] (0, -2) -- (0, 2) node [above] {\(\Im(p_0)\)};
            \fill[Red] (1, 0) circle [radius = 0.05cm] node [below, text=black] {\(\omega(\vv{p})\)};
            \fill[Red] (-1, 0) circle [radius = 0.05cm] node [above, text=black] {\(-\omega(\vv{p})\)};
            \draw[very thick, Blue] (-3, 0) -- (-1.5, 0) arc (-180:0:0.5) -- (0.5, 0) arc(180:0:0.5) -- (3, 0);
            \draw[very thick, Blue, ->] (-3, 0) -- (-2.25, 0) node [below, text=black, yshift=-0.05cm] {\(C_{\symrm{F}}\)};
            \draw[very thick, Blue, ->] (1.5, 0) -- (2.25, 0);
        \end{tikzpicture}
        \caption{The contour used to compute the Feynman propagator.}
        \label{fig:contour for feynman propagator}
    \end{figure}
    
    To see why this works we consider two cases, first, suppose that \(x_0 > 0\).
    Then close the contour in the lower half plane, so that \(p_0\) has a large, negative, imaginary component and overall \(-ip\cdot x\) has a large, negative, real component.
    Closing with a semicircle Jordan's lemma tells us that the integral along this arc vanishes.
    Hence, the value of the integral along the \(C_{\feynman}\) contour is equal to the value of the integral around this closed loop.
    We can apply the residue theorem to the contour about the loop.
    There is a single pole enclosed in this loop, the one at \(\omega(\vv{p})\).
    However, we've already computed the integral of this function around a contour containing the pole \(\omega(\vv{p})\) in \cref{sec:contour representation}, the only difference is a minus sign not in the Feynman propagator contour representation, so we can conclude that the result is \(\Delta^+(x)\).
    
    Similarly, if \(x_0 < 0\) we close the contour in the upper half plane.
    Again the integral vanishes along this arc and we can then treat this as a contour containing only the pole at \(-\omega(\vv{p})\), the result of which tells us that this integral gives \(-\Delta^-(x)\).
    So, combining these results we see that we get the expected result of \(\pm \Delta^{\pm}(x)\) depending on the sign of \(x_0\).
    
    \subsubsection{The \texorpdfstring{\(i\varepsilon\)}{i epsilon} Prescription}
    This contour integral is not particular nice to evaluate, we have to awkwardly detour around the poles on the real axis.
    Instead we can slightly modify the integrand to give the representation
    \begin{equation}
        \Delta_{\feynman}(x) = \int \frac{\dl{^4p}}{(2\pi)^4} \frac{\e^{-ip\cdot x}}{p^2 - m^2 + i\varepsilon}.
    \end{equation}
    This moves the two poles slightly off the real axis.
    Let \(\varepsilon > 0\) be a small positive real number, then
    \begin{equation}
        p^2 - m^2 + i\varepsilon = p_0^2 - \left( \omega(\vv{p}) - \frac{i\varepsilon}{2\omega(\vv{p})} \right)^2,
    \end{equation}
    just expand this out to check, and drop terms of order \(\varepsilon^2\).
    The poles are then shifted by \(i\varepsilon/[2\omega(\vv{p})]\), so that the pole which was at \(-\omega(\vv{p})\) is now at \(-\omega(\vv{p}) + i\varepsilon/[2\omega(\vv{p})]\), and the pole which was at \(\omega(\vv{p})\) is now at \(\omega(\vv{p}) - i\varepsilon/[2\omega(\vv{p})]\).
    This is shown in \cref{fig:contour for feynman propagator with i epsilon prescription}.
    We can then integrate along the real axis as normal, and then take \(\varepsilon \to 0\) after our computations have finished.
    
    \begin{figure}
        \tikzsetnextfilename{contour-for-feynman-propagator-with-i-epsilon-prescription}
        \begin{tikzpicture}
            \draw[thick, ->] (-3, 0) -- (3, 0) node [right] {\(\Re(p_0)\)};
            \draw[thick, ->] (0, -2) -- (0, 2) node [above] {\(\Im(p_0)\)};
            \fill[Red] (1, -0.3) circle [radius = 0.05cm] node [below, text=black, xshift=0.5cm] {\(\omega(\vv{p}) + i\varepsilon/[2\omega(\vv{p})]\)};
            \fill[Red] (-1, 0.3) circle [radius = 0.05cm] node [above, text=black, xshift=-0.55cm] {\(-\omega(\vv{p}) - i\varepsilon/[2\omega(\vv{p})]\)};
            \draw[very thick, Blue] (-3, 0) -- (3, 0);
            \draw[very thick, Blue, ->] (-3, 0) -- (-2.25, 0) node [below, text=black, yshift=-0.05cm] {\(C_{\symrm{F}}\)};
            \draw[very thick, Blue, ->] (1.5, 0) -- (2.25, 0);
        \end{tikzpicture}
        \caption{The contour for calculating the Feynman propagator using the \(i\varepsilon\) prescription.}
        \label{fig:contour for feynman propagator with i epsilon prescription}
    \end{figure}
    
    \subsection{Feynman Propagator as a Green's Function}
    Like the other propagators the Feynman propagator is a Green's function of the Klein--Gordon equation, that is
    \begin{equation}
        (\dalembertian + m^2) \Delta_{\feynman}(x) = \delta^4(x).
    \end{equation}
    The difference is just in the boundary conditions.
    
    The classical retarded and advanced Green's functions, as well as the \(\Delta^{\pm}\) propagators are Green's functions for an equation with Neumann boundary conditions, that is, the boundary conditions fix initial conditions on a function and its derivative, in our case we have an initial condition on \(\varphi\) and \(\pi = \dot{\varphi}\).
    This is a classical way of thinking, we have some initial state and we let it evolve according to some equation.
    The problem is that the assumption is we can know both \(\varphi\) and \(\pi\) at the same time, this is not possible due to the uncertainty principle.
    
    The Feynman propagator on the other hand corresponds to Dirichlet boundary conditions, where we set initial and final conditions on some function, in this case \(\varphi\).
    This is much closer to how we think in quantum mechanics, where we consider an initial and final state and transitions between them.
    
    \chapter{Feynman Diagrams}
    \epigraph{This is a great course because there are loads and loads of integrals to do but they're all really easy, so you don't feel any pain but you get to feel proud after.}{Richard Ball}
    In this section we develop the theory of Feynman diagrams.
    A diagrammatic representation of interactions which have the benefit of greatly simplifying the maths and also seeming quite physical.
    We stress however, that any one diagram is not a true representation of a physical process, rather it corresponds to a single term in an expansion describing a physical process.
    
    We start from the Dyson expansion,
    \begin{equation}
        S = \sum_{n = 0}^{\infty} S^{(n)},
    \end{equation}
    where
    \begin{equation}
        S^{(n)} = \frac{(-i)^n}{n!} \int \!\! \dl{^4x_1} \dotsm \int \!\! \dl{^4x_n} \, \timeOrdering[\hamiltonianDensity_{\interaction}(x_1) \dotsm \hamiltonianDensity_{\interaction}(x_n)].
    \end{equation}

    We will use \(\varphi^3\) theory as an example, but the process is basically the same, but usually with harder integrals, for other theories.
    This means that
    \begin{equation}
        \hamiltonianDensity_{\interaction}(x) = \frac{g}{3!} \normalordering{\varphi(x)^3} = \frac{g}{3!}\normalordering{(\varphi^+(x) + \varphi^-(x))^3}.
    \end{equation}
    We consider low order terms in this series, and develop Feynman diagrams as we go along.
    
    \section{Evaluating Low Order Terms}
    \subsection{\texorpdfstring{\(n = 0\)}{n = 0}}
    The \(n = 0\) term, \(S^{(0)}\), is just 1.
    This represents the case where nothing happens, our initial state just remains and is equal to the final state.
    
    \subsection{\texorpdfstring{\(n = 1\)}{n = 1}}
    The \(n = 1\) term is
    \begin{equation}
        S^{(1)} = \frac{-ig}{3!} \int \dl{^4x} \, ({\varphi^+}^3 + 3{\varphi^-}^2\varphi^+ + 3\varphi^-{\varphi^+}^2 + {\varphi^-}^3).
    \end{equation}
    We can consider each term in the integrand separately.
    
    The first term represents three annihilators, therefore this corresponds to three particles in the initial state entering, being destroyed, and then a vacuum final state.
    As a diagram we represent this as
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S1-3-0}
        \begin{tikzpicture}[baseline=(v)]
            \begin{feynman}
                \diagram[horizontal=i2 to v, small, layered layout]{
                    {i1, i2, i3} -- v
                };
                \node[right] at (v) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This interaction doesn't conserve momentum, so it must give zero contribution to the overall value of \(S^{(1)}\).
    
    The second term represents an annihilator and two creators, so this corresponds to a single particle entering, being destroyed, and then two new particles being created.
    Since all three factors of \(\varphi\) are evaluated at the same spacetime point, \(x\), this occurs all at once, which we draw as
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S1-1-2}
        \begin{tikzpicture}[baseline=(v)]
            \begin{feynman}
                \diagram[horizontal=i to v, small]{
                    i -- v,
                    v -- {o1, o2}
                };
                \node[below left] at (v) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    It's not immediately obvious, but this term cannot actually conserve momentum in \(\varphi^3\) theory, since all three particles have the same mass, and so it gives zero, which we'll show later.
    
    The third term is two annihilators and one creation operator, it corresponds to two particles being annihilated and then one new particle being created.
    As a diagram it is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S1-2-1}
        \begin{tikzpicture}[baseline=(v)]
            \begin{feynman}
                \diagram[horizontal=v to o, small]{
                    {i1, i2} -- v,
                    v -- o
                };
                \node[below right] at (v) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Like the previous term this vanishes in \(\varphi^3\) theory.
    
    The fourth term is three creation operators, so represents three particles being created from an initial vacuum state.
    Diagrammatically it is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S1-0-3}
        \begin{tikzpicture}[baseline=(v)]
            \begin{feynman}
                \diagram[horizontal=v to o2, small, layered layout]{
                    v -- {o1, o2, o3}
                };
                \node[left] at (v) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Like the first term this one doesn't conserve momentum and so must have no net contribution to \(S^{(1)}\).
    
    \subsection{\texorpdfstring{\(n = 2\)}{n = 2}}
    The \(n = 2\) term is
    \begin{equation}
        S^{(2)} = \frac{g^2}{2!(3!)^2} \int \dl{^4x} \int \dl{^4y} \timeOrdering(\normalordering{\varphi(x)^3} \normalordering{\varphi(y)^3}).
    \end{equation}
    We can use Wick's theorem to evaluate this normal ordered product.
    We can then split \(S^{(2)}\) further into four terms based on the number of contractions:
    \begin{equation}
        S^{(2)} = S^{(2)}_0 + S^{(2)}_1 + S^{(2)}_2 + S^{(2)}_{3},
    \end{equation}
    Where \(S^{(2)}_i\) is the term in \(S^{(2)}\) corresponding to terms with \(i\) contractions.
    Note that since we have six factors of \(\varphi\) we have a maximum of three contractions.
    
    \subsubsection{No Contractions}
    The terms with no contractions correspond to the first term in Wick's theorem,
    \begin{equation}
        \normalordering{\varphi(x)^3\varphi(y)^3}.
    \end{equation}
    Since there are no contractions between the field evaluated at the two spacetime points, \(x\) and \(y\), we get disconnected diagrams, such as
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-0-contractions}
        \begin{tikzpicture}[baseline=(vx)]
            \begin{feynman}
                \diagram[horizontal=vx to ox, small]{
                    {i1x, i2x} -- vx,
                    vx -- ox
                };
                \diagram[horizontal=iy to vy, small, xshift=2.5cm, yshift=-1.095cm]{
                    iy -- vy,
                    vy -- {o1y, o2y}
                };
                \node[below right] at (vx) {\(x\)};
                \node[below left] at (vy) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This is a single diagram corresponding to two separate processes occurring at \(x\) and \(y\).
    This diagram corresponds to the term
    \begin{equation}
        \varphi^-(x)\varphi^-(y)\varphi^-(y)\varphi^+(x)\varphi^+(x)\varphi^+(y),
    \end{equation}
    and other terms give similar diagrams.
    
    It can be shown that all disconnected diagrams vanish in \(\varphi^3\) theory.
    Further, any disconnected diagram is simply a combination of connected diagrams, so we don't every need to consider disconnected diagrams.
    In terms of physics disconnected diagrams correspond to multiple unrelated scattering events occurring, and we can simply treat them all separately.
    
    \subsubsection{One Contraction}
    Wick's theorem, generalised for equal times, doesn't have contractions between operators at the same time.
    Hence, we don't need to consider contractions of \(\varphi(x)\) with itself, or \(\varphi(y)\) with itself.
    This means that we have the term\\
    \begin{equation}
        3^2 \wick{\c \varphi(x) \c \varphi(y)} \normalordering{\varphi(x)^2 \varphi(y)^2}.
    \end{equation}
    The factor of \(3^2\) is combinatoric, it comes from there being three ways to select which \(\varphi(x)\) field to contract and three ways to select which \(\varphi(y)\) field to contract, and then since these are all really identical we can combine them into a single term with weighting \(3^2\).
    
    The contraction corresponds to a virtual particle propagating from \(x\) to \(y\).
    The four factors of \(\varphi\) in the normal ordering tell us that we'll have four external particles.
    There are three ways to do this, we can have 1-3, 2-2, or 3-1 interactions, where these are the number of initial particles to final particles.
    
    An example of a 1-3 term is given by the diagram
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-1-contraction-1-3}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[horizontal=i to v1, layered layout, small]{
                    i -- v1 -- v2 -- {o2, o3},
                    v1 -- o1,
                };
                \node[below left] at (v1) {\(x\)};
                \node[below] at (v2) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \varphi^+(x)\varphi^-(x)\varphi^-(y)\varphi^-(y).
    \end{equation}
    This term, in \(\varphi^3\) theory, doesn't conserve momentum since all particles have the same mass, so must give no overall contribution to \(S^{(2)}\).
    
    Similarly any 3-1 terms must vanish.
    
    More interesting are the 2-2 terms.
    There are three such terms.
    The first is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-1-contraction-2-2-s-channel}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[horizontal=v1 to v2, small]{
                    {i1, i2} -- v1 -- v2 -- {o1, o2}
                };
                \node[below] at (v1) {\(x\)};
                \node[below] at (v2) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \varphi^+(x)\varphi^+(x)\varphi^-(y)\varphi^-(y).
    \end{equation}
    We call this the \define{\(\symbf{s}\)-channel}\index{s-channel@\(s\)-channel} diagram.
    
    The second is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-1-contraction-2-2-t-channel}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[horizontal=i1 to o1, small]{
                    i1 -- v1 -- o1,
                    i2 -- v2 -- o2,
                    v1 -- v2,
                    i1 -- [draw=none] i2,
                    o1 -- [draw=none] o2
                };
                \node[below] at (v1) {\(y\)};
                \node[above] at (v2) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \varphi^+(x) \varphi^+(y) \varphi^-(x) \varphi^-(y).
    \end{equation}
    We call this the \define{\(\symbf{t}\)-channel}\index{t-channel@\(t\)-channel} diagram.
    
    The third is
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-1-contraction-2-2-u-channel}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \diagram[horizontal=i1 to o1, small]{
                    i1 -- v1 -- [draw=none] o1,
                    i2 -- v2 -- [draw=none] o2,
                    v1 -- v2,
                    i1 -- [draw=none] i2,
                    o1 -- [draw=none] o2,
                };
                \draw (v2) -- (o1);
                \draw[line width=0.1cm, white] ($(v1)!0.3!(o2)$) -- ($(v1)!0.7!(o2)$);
                \draw (v1) -- (o2);
                \node[below] at (v1) {\(y\)};
                \node[above] at (v2) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This corresponds to the term
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)}\varphi^+(x)\varphi^+(y)\varphi^-(y)\varphi^-(x)
    \end{equation}
    
    If we take any of these three diagrams and swap \(x\) and \(y\) then nothing changes, so this gives us a factor of 2 to include, which nicely cancels the factor of \(1/2!\) in the full expression for \(S^{(2)}\).
    
    \subsubsection{Two Contractions}
    Terms with two contractions correspond to a term proportional to
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \wick{\c \varphi(x) \c \varphi(y)} \normalordering{\varphi(x)\varphi(y)}.
    \end{equation}
    The two contractions mean we have two internally propagating virtual particles, and the two normal ordered terms mean we have two external particles.
    
    There is a single such diagram, up to swapping \(x\) and \(y\):
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-2-contractions}
        \begin{tikzpicture}[baseline=(v1)]
            \begin{feynman}
                \vertex (i);
                \vertex[right=of i] (v1);
                \vertex[right=of v1] (v2);
                \vertex[right=of v2] (o);
                \draw (i) -- (v1);
                \draw (v2) -- (o);
                \draw (v1) to[bend left=90] (v2);
                \draw (v1) to[bend right=90] (v2);
                \node[right] at (v1) {\(x\)};
                \node[left, yshift=-0.05cm] at (v2) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    
    It can be shown that this term actually diverges, giving an infinite result.
    We call this a one \defineindex{loop diagram}, since it has a single loop.
    In contrast the diagrams we have considered so far without loops are \define{tree diagrams}\index{tree diagram}.
    
    \subsubsection{Three Contractions}
    Terms with three contractions correspond to a term proportional to
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} \wick{\c \varphi(x) \c \varphi(y)} \wick{\c \varphi(x) \c \varphi(y)}.
    \end{equation}
    This has no external particles and three internal virtual particles propagating from \(x\) to \(y\).
    It corresponds to the diagram
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-S2-3-contractions}
        \begin{tikzpicture}[baseline=(x)]
            \begin{feynman}
                \vertex (x);
                \vertex[right=of x] (y);
                \draw (x) -- (y);
                \draw (x) to[bend left=90] (y);
                \draw (x) to[bend right=90] (y);
                \node[left] at (x) {\(x\)};
                \node[right] at (y) {\(y\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    This is called a \define{vacuum bubble} since it is formed entirely of virtual particles, and both the initial and final state are vacuums.
    Since there are no external particles there are no physically observable contributions from terms like this.
    It can be shown that bubble diagrams factor out to all levels in all theories, so we don't need to consider them.
    The correspond to some virtual particles coming into existence and then all annihilating.
    
    \section{Feynman Diagrams in Momentum Space}
    \epigraph{As always in QFT you have to get everything right.}{Richard Ball}
    So far with these diagrams we've been working in position space, evaluating the fields at \(x\).
    External states are momentum eigenstates, this suggests that we should instead work in momentum space instead, which means working with \(a(\vv{p})\) and \(a^\hermit(\vv{p})\).
    
    We are interested in computing the matrix element
    \begin{equation}
        S_{fi} = \bra{f} S \ket{i}.
    \end{equation}
    We can express \(\ket{i}\) and \(\ket{f}\) as free particle states created by \(a^\hermit(\vv{p})\) from the vacuum state.
    These states have definite momentum and all of the particles are on-shell.
    
    Consider the one particle state with momentum \(\vv{p}\).
    This is created with a single creation operator acting on the vacuum:
    \begin{equation}
        \ket{\vv{p}} \coloneqq a^\hermit(\vv{p})\ket{0}.
    \end{equation}
    Now act on this with \(\varphi^+(x)\).
    We can expand \(\varphi^+(x)\) in momentum space using \cref{eqn:phi +}:
    \begin{align}
        \textcolor{Red}{\varphi^+(x)}\textcolor{Purple}{\ket{\vv{p}}} &= \textcolor{Red}{\int \invariantmeasure{p'} \, \e^{-ip' \cdot x} a(\vv{p}')} \textcolor{Purple}{a^\hermit(\vv{p})\ket{0}}\\
        &= \int \invariantmeasure{p'} \, \e^{-ip'\cdot x} \commutator{a(\vv{p}')}{a^\hermit(\vv{p}')} \ket{0}\\
        &= \int \invariantmeasure{p'} \, \e^{-ip'\cdot x} \bardelta(\vv{p} - \vv{p}') \ket{0}\\
        &= \e^{-ip\cdot x}\ket{0}.
    \end{align}
    Here we've used
    \begin{equation}
        \commutator{a(\vv{p}')}{a^\hermit(\vv{p})} \ket{0} = a(\vv{p}')a^\hermit(\vv{p})\ket{0} - a^\hermit(\vv{p})a(\vv{p}')\ket{0} = a(\vv{p}')a^\hermit(\vv{p})\ket{0}
    \end{equation}
    with the second term vanishing as we annihilate the vacuum.
    
    Taking the conjugate of this result, and using \((\varphi^+)^\hermit = \varphi^-\) we have
    \begin{equation}
        \bra{\vv{p}} \varphi^-(x) = \bra{0} \e^{ip\cdot x}.
    \end{equation}
    
    Since we're going to be working in momentum space it's useful to have an expression for the contraction in momentum space, we get this through the contour representation of the Feynman propagator:
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi(y)} = i \int \frac{\dl{^4k}}{(2\pi)^4} \frac{\e^{-ik\cdot (x - y)}}{k^2 - m^2 + i\varepsilon} = i\Delta_{\feynman}(x - y),
    \end{equation}
    where the \(k^0\) integral is performed along the real axis, and we set \(\varepsilon\) ot zero after our calculations.
    Note that we've written this here using \(k\), the wavenumber, which is the same as the momentum when \(\hbar = 1\).
    This is mostly to free up the letter \(p\) for the momentum of the particles.
    
    We are now in a position to start computing terms in the Dyson series, which we can view as an expansion in \(g\).
    We won't do all of the ones discussed above, just enough to get the idea.
    
    \subsection{First Order}
    Consider the first order term
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-first-order-full-calculation}
        \begin{tikzpicture}[baseline=(x)]
            \begin{feynman}
                \diagram[small, horizontal=i to x]{
                    i [particle=\(p\)] -- x -- {o1 [particle=\(q\)], o2 [particle=\(q'\)]}
                };
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    Here we've labelled the particles with their momenta.
    The initial state contains just a single particle, so its \(\ket{i} = \ket{\vv{p}}\).
    The final states contain two particles, so the final state is a tensor product of single particle states:
    \begin{equation}
        \ket{f} = \ket{\vv{q},\ket{\vv{q}'}} = \ket{\vv{q}}\ket{\vv{q}'} = \ket{\vv{q}} \otimes \ket{\vv{q}'}.
    \end{equation}
    This works because we assume no interaction between the two final particles, at least not at first order in \(g\).
    
    The matrix element we wish to compute is
    \begin{equation}
        \bra{f} S^{(1)}_{\text{1-2}} \ket{i} = \bra{\vv{q}, \vv{q}'}\left[ \frac{-ig}{3!} \int \dl{^4x} 3\varphi^-(x)^2 \varphi^+(x) \right] \ket{\vv{p}}.
    \end{equation}
    Here \(S^{(1)}_{\text{1-2}}\) is the term in \(S^{(1)}\) corresponding to the term we wish to compute.
    The factor of 3 in the integral comes from the number of ways we can choose one of the factors of \(\varphi\) to be \(\varphi^+\).
    We can show that
    \begin{equation}
        \bra{\vv{q}, \vv{q}'} \varphi^-(x)^2 = 2\e^{iq\cdot x} \e^{iq'\cdot x}\bra{0},
    \end{equation}
    where the factor of two comes from the number of ways we can act on \(\bra{\vv{q}, \vv{q}'}\) with \(\varphi^-(x)^2\).
    We can either act on \(\bra{\vv{q}}\) and then \(\bra{\vv{q}'}\), or the other way round.
    We also have
    \begin{equation}
        \varphi^+(x)\ket{\vv{p}} = \e^{-ip\cdot x}\ket{0}.
    \end{equation}
    
    Putting this into our calculation, and noticing that the factors of \(2\) and \(3\) cancel with the \(3!\) we have
    \begin{align}
        \bra{f} S^{(1)}_{\text{1-2}} \ket{i} &= -ig \int \dl{^4x} \, \e^{iq\cdot x} \e^{iq'\cdot x} \e^{-ip\cdot x}\\
        &= -ig (2\pi)^4\delta^4(q + q' - p).
    \end{align}
    The Dirac delta enforces conservation of momentum, the momentum in, \(p\), must equal the momentum out, \(q + q'\), in order to get a nonzero result.
    
    All external particles are on-shell, and in \(\varphi^3\) theory all particles have the same mass.
    This means that \(p^2 = q^2 = q'^2 = m^2\).
    Now, consider \((p - q)^2\).
    Conservation of momentum tells us that \(p - q = q'\), and so \((p - q)^2 = q'^2 = m^2\).
    We can also expand \((p - q)^2\) to get
    \begin{align}
        m^2 &= (p - q)^2\\
        &= p^2 + q^2 - 2p\cdot q\\
        &= m^2 + m^2 + 2p\cdot q.
    \end{align}
    Hence, we have \(2p\cdot q = m^2\).
    In the rest frame of the incoming particle we have \(p^\mu = (m, \vv{0})\), and we can write \(q^\mu = (E, \vv{q})\) without loss of generality.
    We therefore have
    \begin{equation}
        2 p \cdot q = 2mE = m^2 \implies E = \frac{m}{2}.
    \end{equation}
    This is what we would expect, since the two particles are produced each must get exactly half of the energy available from the first particle, and that energy is just its mass.
    We also know that \(E^2 = m^2 + \vv{q}^2\), and so \(E \ge m\).
    This means that \(E = m/2\) can't satisfy \(p = q + q'\).
    Hence, the contribution from this term vanishes as the condition imposed by the Dirac delta can never be fulfilled.
    
    Note that in other theories with multiple particles with different masses it is possible for this term to be nonzero.
    It's also not true that all vertices like this vanish, the key difference is that if one of the particles is a virtual particle then it needn't satisfy the on-shell condition and so it can have arbitrary momentum, and we can conserve momentum with particles of the same mass.
    
    The key result is that, in \(\varphi^3\) theory,
    \begin{equation}
        \bra{f} S^{(1)}_{\text{1-2}} \ket{i} = 0.
    \end{equation}
    
    \subsection{Second Order}
    For now we restrict ourselves to tree diagrams, we'll come to loops later.
    There are three diagrams to consider then:
    \begin{equation}
        \tikzsetnextfilename{fd-phi-cubed-second-order-full-calculation-s}
        \feynmandiagram[horizontal=v1 to v2, inline=(v1)] {
            {i1 [particle=\(p\)], i2 [particle=\(p'\)]} -- v1 -- [edge label=\(k\)] v2 -- {o1 [particle=\(q\)], o2 [particle=\(q'\)]}
        };
        \quad
        \tikzsetnextfilename{fd-phi-cubed-second-order-full-calculation-t}
        \feynmandiagram[vertical'=v1 to v2, small, baseline=(current bounding box)] {
            i1 [particle=\(p\)] -- v1 -- o1 [particle=\(q\)],
            i2 [particle=\(p'\)] -- v2 -- o2 [particle=\(q'\)],
            v1 -- [edge label=\(k\)] v2,
            i1 -- [draw=none] i2,
            o1 -- [draw=none] o2
        };
        \quad
        \tikzsetnextfilename{fd-phi-cubed-second-order-full-calculation-u}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \begin{feynman}
                \diagram[horizontal=i1 to o1, small]{
                    i1 [particle=\(p'\)] -- v1 -- [draw=none] o1 [particle=\(q'\)],
                    i2 [particle=\(p\)] -- v2 -- [draw=none] o2 [particle=\(q\)],
                    v1 -- [edge label=\(k\)] v2,
                    i1 -- [draw=none] i2,
                    o1 -- [draw=none] o2,
                };
                \draw (v2) -- (o1);
                \draw[line width=0.1cm, white] ($(v1)!0.3!(o2)$) -- ($(v1)!0.7!(o2)$);
                \draw (v1) -- (o2);
                \node[below] at (v1) {\(y\)};
                \node[above] at (v2) {\(x\)};
            \end{feynman}
        \end{tikzpicture}
    \end{equation}
    
    Note that while we assume the same initial and final states the momenta of the virtual particles need not, and in fact aren't, the same, even though we've labelled them all \(k\).
    
    \subsubsection{\texorpdfstring{\(s\)}{s}-Channel}
    Let's start with the \(s\)-channel process.
    We want to compute
    \begin{equation*}
        \bra{f} S^{(2)}_{s} \ket{i} = \bra{\vv{q}, \vv{q}'}\left[ \frac{-g^2}{(3!)^2} \int \dl{^4x} \int \dl{^4y} \, \varphi^-(y)^2 3^2 \wick{\c \varphi(x) \c \varphi(y)} \varphi^+(x)^2 \right] \ket{\vv{p}, \vv{p}'}
    \end{equation*}
    We can compute the action of \((\varphi^{\pm})^2\) on our initial and final states fairly easily:
    \begin{align}
        \varphi^+(x)^2 \ket{\vv{p}, \vv{p}'} &= 2\e^{-ip\cdot x} \e^{-ip'\cdot x}\ket{0},\\
        \bra{\vv{q}, \vv{q}'} \varphi^-(y)^2 &= 2\e^{iq\cdot y} \e^{iq'\cdot y}\bra{0}.
    \end{align}
    Again, the factors of 2 come from there being two orders in which we can act on the paired states.
    
    Noticing that the factor of \(3^2\) in the integrand, as well as the factors of 2 above, cancel with the \(1/(3!)^2\) term, and using \(\braket{0}{0} = 1\), we have
    \begin{equation*}
        \bra{f} S^{(2)}_{s} \ket{i} = -g^2 \int \dl{^4x} \int \dl{^4y} \, \e^{iq\cdot y} \e^{iq'\cdot y} i \int \frac{\dl{^4k}}{(2\pi)^4} \frac{\e^{ik\cdot (x - y)}}{k^2 - m^2 + i\varepsilon} \e^{ip\cdot x} \e^{ip' \cdot x}.
    \end{equation*}
    Performing the \(x\) and \(y\) integrals we get more Dirac deltas:
    \begin{equation*}
        \bra{f} S^{(2)}_{s} \ket{i} = -ig^2 \int \frac{\dl{^4k}}{(2\pi)^4} (2\pi)^4 \delta^4(q + q' - k) (2\pi)^4 \delta^4(k - p - p') \frac{1}{k^2 - m^2 + i\varepsilon}.
    \end{equation*}
    The two Dirac deltas ensure that momentum is conserved at each vertex, and hence overall, since we must have \(q + q' = k\) and \(k = p + p'\) to have a nonzero contribution, and so \(p + p' = q + q'\).
    We can then perform the integral over \(k\) using one the second Dirac delta and we get
    \begin{equation}
        \bra{f} S^{(2)}_{s} \ket{i} = (2\pi)^4\delta^4(q + q' - p - p') \frac{-ig^2}{(p + p')^2 - m^2}.
    \end{equation}
    Note that we can drop the \(i\varepsilon\) term since \(k^2 = (p + p')^2 \ge 4m^2\), using \(p^2 = p'^2 = m^2\), and so the denominator never vanishes.
    It turns out that this is always the case for tree level diagrams, the \(i\varepsilon\) prescription is only needed in loop diagrams.
    If instead we had used the first Dirac delta to perform the integral we would have gotten a factor of \((q + q')^2\) in the denominator instead, but this is equal to \((p + p')^2\) by momentum conservation.
    
    The Dirac delta in this result is again conserving momentum.
    The interesting thing is the rest, which we call the \defineindex{Feynman amplitude} for this process:
    \begin{equation}
        \amplitude_s = \frac{-ig^2}{(p + p')^2 - m^2}.
    \end{equation}
    
    \subsubsection{\texorpdfstring{\(t\)}{t}-Channel}
    Now consider the \(t\)-channel diagram.
    Like the \(s\)-channel term the \(t\)-channel term comes from a term which is sixth order in \(\varphi\).
    In particular, it is \(\normalordering{\varphi(x)^3\varphi(y)^3}\), where we call the vertex shared by \(p\) and \(q\) \(x\), and the vertex shared by \(p'\) and \(q'\) \(y\).
    Since we have four external particles we must contract a pair of fields, and since contractions at the same point give zero we must contract \(\varphi(x)\) and \(\varphi(y)\).
    There are 3 ways to choose which \(\varphi(x)\) and 3 ways to choose which \(\varphi(y)\).
    We then have two factors of \(\varphi(x)\) left and two factors of \(\varphi(y)\).
    Looking back at the diagrams we see that a particle is created and destroyed at each vertex, as well as the internal line corresponding to the contraction.
    This means one of our \(\varphi(x)\) values contributes a \(\varphi^+(x)\) to this term, destroying an incoming particle, and the other contributes a \(\varphi^-(x)\), creating one of the final particles.
    There are two ways to select which \(\varphi(x)\) contributes \(\varphi^+(x)\) and which contributes \(\varphi^-(x)\).
    The exact same is true for \(\varphi(y)\), so on top of the two factors of 3 from choosing the contraction we have two factors of 2.
    This gives an overall combinatorial factor of \(3 \cdot 3 \cdot 2 \cdot 2 = (3!)^2\), cancelling the \((3!)^2\) which appears in the interaction term.
    Hence, we have
    \begin{multline*}
        \bra{f} S^{(2)}_{t} \ket{i}\\
        = \bra{\vv{q}, \vv{q}'} \left[ \frac{-g^2}{(3!)^2} \int \dl{^4x} \int \dl{^4y} \, (3!)^2\wick{\c \varphi(x) \c \varphi(y)} \varphi^-(x)\varphi^-(y)\varphi^+(x)\varphi^+(y) \right] \ket{\vv{p}, \vv{p}'}.
    \end{multline*}
    Now, as before we act on the initial state with the annihilators and the final state with the creation operators, giving
    \begin{align}
        \varphi^+(x)\varphi^+(y) \ket{\vv{p}, \vv{p}'} &= \e^{-ip \cdot x} \e^{-ip' \cdot y} \ket{0},\\
        \bra{\vv{q}, \vv{q}'} \varphi^-(x)\varphi^-(y) &= \e^{iq \cdot x} \e^{iq' \cdot y} \bra{0}.
    \end{align}
    Note that there is no factor of 2 this time, since the particle with momentum \(p\) must be destroyed at \(x\) in this interaction, and hence we cannot act on this particle with \(\varphi^+(y)\), doing so corresponds instead to a \(u\)-channel process.
    
    Putting this together with our results we have
    \begin{equation*}
        \bra{f} S^{(2)}_{t} \ket{i} = -g^2 \int \dl{^4x} \int \dl{^4y} \, \e^{iq\cdot x} \e^{iq' \cdot y} \int \frac{\dl{^4x}}{(2\pi)^4} \frac{\e^{-ik \cdot (y - x)}}{k^2 - m^2 + i\varepsilon} \e^{-ip \cdot x} \e^{-ip' \cdot y}.
    \end{equation*}
    Collecting the exponents into \(x\) and \(y\) exponentials we have
    \begin{equation*}
        \bra{f} S^{(2)}_{t} \ket{i} = -g^2 \int \dl{^4x} \int \dl{^4y} \int \frac{\dl{^4k}}{(2\pi)^4} \e^{i(q + k - p) \cdot x} \e^{i(q' - k - p')} \frac{1}{k^2 - m^2 + i\varepsilon}.
    \end{equation*}
    Performing the \(x\) and \(y\) integrals gives Dirac deltas:
    \begin{equation*}
        \bra{f} S^{(2)}_{t} \ket{i} = -g^2 \int \frac{\dl{^4k}}{(2\pi)^4} (2\pi)^4 \delta^4(q + k - p) (2\pi)^4 \delta^4(q' - k - p') \frac{1}{k^2 - m^2 + i\varepsilon}.
    \end{equation*}
    Performing the \(k\) integral with the first Dirac delta fixes \(k = p - q\), and so we get
    \begin{equation}
        \bra{f} S^{(2)}_{t} \ket{i} = (2\pi)^4\delta^4(q + q - p - p') \frac{-g^2}{(p - q)^2 - m^2}.
    \end{equation}
    As before we have the Dirac delta enforcing energy conservation, and then the amplitude is 
    \begin{equation}
        \amplitude_t = \frac{-ig^2}{(p - q)^2 - m^2}.
    \end{equation}
    
    Similarly we can compute the \(u\)-channel process, and we find
    \begin{equation}
        \bra{f} S^{(2)}_{u} \ket{i} = (2\pi)^4 \delta^4(q + q' - p - p') \frac{-ig^2}{(p - q')^2 - m^2}.
    \end{equation}
    That is, the amplitudes for the \(u\)-channel process is
    \begin{equation}
        \amplitude_u = \frac{-ig^2}{(p - q')^2 - m^2}.
    \end{equation}
    
    We now introduce the \defineindex{Mandelstam invariants}, \(s\), \(t\), and \(u\), defined as
    \begin{alignat}{2}
        s &= (p + p')^2 &&= (q + q')^2,\\
        t &= (p - q)^2 &&= (p' - q')^2,\\
        u &= (p - q')^2 &&= (p' - q)^2.
    \end{alignat}
    These are Lorentz invariants characterising each of the three channels.
    
    Consider the sum of these three invariants:
    \begin{align}
        s + t + u &= (p + p')^2 + (p - q)^2 + (p - q')^2\\
        &= p^2 + p'^2 + 2p\cdot p' + p^2 + q^2 - 2p\cdot q^ + p^2 + q'^2 - 2 p \cdot q'\notag \\
        &= 6m^2 + 2(p\cdot p' - p\cdot q - p\cdot q')
    \end{align}
    having used the on-shell condition, \(p^2 = p'^2 = q^2 = q'^2 = m^2\).
    Now using conservation of momentum we have \(q' = p + p' - q\), and so
    \begin{align}
        s + t + u &= 6m^2 + 2(p \cdot p' - p \cdot q - p \cdot [p + p' - q])\\
        &= 6m^2 + 2(p \cdot p' - p \cdot q - p \cdot p - p \cdot p' + p \cdot q)\\
        &= 6m^2 + 2m^2\\
        &= 4m^2.
    \end{align}
    This shows that, in a theory where all particles have the same mass, the three Mandelstam invariants are not independent.
    
    We are interested in the total matrix element at second order which, ignoring loops, is
    \begin{equation}
        \bra{f} S^{(2)} \ket{i} = \bra{\vv{q}, \vv{q}'} (S^{(2)}_s + S^{(2)}_t + S^{(2)}_u) \ket{\vv{p}, \vv{p}'}.
    \end{equation}
    Combing our results we have
    \begin{equation}
        \bra{f} S^{(2)} \ket{i} = (2\pi)^4 \delta^4(q + q' - p - p') (-ig^2) \left[ \frac{1}{s - m^2} + \frac{1}{t - m^2} + \frac{1}{u - m^2} \right]
    \end{equation}
    and the amplitude is
    \begin{align}
        \amplitude^{(2)} &= \amplitude_s + \amplitude_t + \amplitude_u
        &= -ig^2\left[ \frac{1}{s - m^2} + \frac{1}{t - m^2} + \frac{1}{u - m^2} \right].
    \end{align}
    Notice that the amplitude is invariant under swapping \(p\) and \(p'\) and/or swapping \(q\) and \(q'\), since this simply swaps \(t\) and \(u\) if we make one swap, or does nothing if we make two.
    This implies that our particles are bosons.
    
    Now consider scattering in the centre of mass frame.
    Here the incoming particles enter with equal and opposite momenta, and therefore also with equal energy since they have the same mass and are on shell.
    So, \(p^\mu = (E, \vv{p})\) and \(p'^\mu = (E, -\vv{p})\).
    The energy must split equally between the outgoing particles, so they also have energy \(E\), and the total momentum after the interaction must vanish so they have equal and opposite momenta, \(q^\mu = (E, \vv{q})\) and \(q'^\mu = (E, -\vv{q})\).
    
    Calculating \(s\) we have \(s = (p + p')^2 = 4E^2 \ge 4m^2\), where the inequality comes from \(E^2 = m^2 + \vv{p}^2 \ge m^2\).
    This is what justified us dropping the \(i\varepsilon\) earlier, since if \(s \ge 4m^2\) then \(s - m^2\) will never be zero.
    
    Since all particles have the same mass and energy and are on-shell we must have that \(\vv{q}^2 = \vv{p}^2\), hence
    \begin{equation}
        t = -(\vv{p} - \vv{q})^2 = -2\abs{\vv{p}}^2(1 - \cos \vartheta) \le 0
    \end{equation}
    where \(\cos\vartheta = \vv{p} \cdot \vv{q}/(\abs{\vv{p}\abs{\vv{q}}})\).
    Similarly,
    \begin{equation}
        u = -(\vv{p} + \vv{q})^2 = -2\vv{p}^2(1 + \cos\vartheta) \le 0.
    \end{equation}
    This means that \(t - m^2\) and \(u - m^2\) will always be negative, and in particular are never zero allowing us to drop the \(i\varepsilon\) term.
    This also implies that virtual particles are \emph{always} off-shell, as otherwise we would not be able to conserve momentum in 2-1 and 1-2 vertices.
    
    \section{Feynman Rules}
    The calculations above all follow roughly the same steps, this suggests that we can formulate a way to go from the diagrams to the final result without having to do these steps every time.
    This is what Feynman rules do.
    
    For any \(\varphi^3\) process we have
    \begin{equation}
        \bra{f} S \ket{i} = \delta_{fi} + (2\pi)^4 \delta^4\left( \sum p_f - \sum p_i \right) \sum_{n = 2}^{\infty} \amplitude^{(n)}.
    \end{equation}
    The Kronecker delta accounts for the case where nothing happens, the Dirac delta enforces energy conservation, where the sums are over all final and then all initial states.
    The interesting bit is the amplitude, \(\amplitude^{(n)}\).
    Wick's theorem tells us that the total amplitude, \(\amplitude\), has contributions from all topologically distinct diagrams\footnote{that is, all diagrams which are not related by rearranging where nodes and edges are}, and \(\amplitude^{(n)}\) has contributions from all topologically distinct \(n\) vertex diagrams.
    Further, we have to consider all diagrams with equal weight due to Wick's theorem, bt the factor of \(g^n\) means that we can drop higher order terms, since \(g\) is taken to be small.
    
    From what we've seen so far we can guess most of the Feynman rules, and the others aren't too bad to demonstrate.
    The \defineindex{Feynman rules}\index{Feynman rules!\(\varphi^3\) theory} for \(\varphi^3\) theory are
    \begin{itemize}
        \item Every vertex contributes a factor of \(-ig\) to the amplitude.
        \item Every internal line contributes a factor of \(i\tilde{\Delta}_{\feynman}(k) = i/(k^2 - m^2 + i\varepsilon)\) to the amplitude.
        \item Every external line contributes a factor of \(1\) to the amplitude.
        \item Impose conservation of four-momentum at every vertex.
        \item Every momentum, \(k\), which is not fixed by conservation of four-momentum gives a factor of
        \begin{equation}
            \int \frac{\dl{^4k}}{(2\pi)^4}.
        \end{equation}
        \item A symmetry factor (trivial for simple diagrams).
    \end{itemize}
    Note that \(\tilde{\Delta}_{\feynman}\) is the Fourier transform of \(\Delta_{\feynman}\), which is to say it is the integrand in the contour representation.
    
    \chapter{Getting Measurable Results}
    Experiments can't measure the \(S\) matrix directly.
    Instead we can measure transition rates between states, including decay rates and cross sections.
    In this chapter we'll discuss how to compute these from the \(S\) matrix for comparison with experimental results.
    
    Our starting point will be
    \begin{equation}
        S_{fi} = \delta_{fi} + (2\pi)^4\delta^4\left( \sum p_f - \sum p_i \right) \amplitude.
    \end{equation}
    The \(\delta_{fi}\) term corresponds to the case where nothing happens.
    The Dirac delta, where the sums are over all final and initial particles, enforces conservation of momentum, and \(\amplitude\) is the Feynman amplitude, which is what we compute in QFT.
    
    \section{Transition Rate}
    Consider the probability, \(P_{fi}\), of transitioning from the initial state \(\ket{i}\) to the final state \(\ket{f}\).
    This is simply the square of the \(S\) matrix, which is given by
    \begin{equation}
        P_{fi} = \abs{S_{fi}}^2 = (2\pi)^4\delta^4(0)(2\pi)^4 \sum_f \delta^4\left( \sum p_f - \sum p_i \right) \abs{\amplitude}^2.
    \end{equation}
    assuming \(i \ne f\), that is that something happens.
    We have used here \(\delta(x)^2 = \delta(0)\delta(x)\) as distributions, which follows since
    \begin{equation}
        \int \dl{x} \, f(x) \delta(x)^2 = \delta(0) f(0),
    \end{equation}
    having used the sifting property of one of the Dirac deltas.
    The first sum over \(f\), outside of the Dirac delta, corresponds to a sum, which will turn out to really be an integral, over allowed values of the final momentum.
    
    There's a problem here, in that \(\delta^4(0)\) is infinite.
    Consider
    \begin{equation}
        (2\pi)^4 \delta^4(p) = \int \dl{^4x}  \, \e^{-ip\cdot x}.
    \end{equation}
    This suggests that
    \begin{equation}
        (2\pi)^4\delta^4(0) = \int \dl{^4x} = VT
    \end{equation}
    where \(V\) is the infinite volume of space and \(T\) is the infinite length of time.
    So, \(VT\) is the volume of spacetime.
    To get around the problem we treat the entire system as if it is in a finite box in spacetime, and then proceed to compute quantities per unit spacetime volume.
    
    We therefore look to compute the transition rate per unit volume,
    \begin{equation}
        w_{fi} = \frac{P_{fi}}{VT} = \sum_f (2\pi)^4\delta(\sum p_f - \sum p_i) \abs{\amplitude}^2.
    \end{equation}
    We can proceed using the completeness relation,
    \begin{equation}
        1 = \int \invariantmeasure{p} \, \ket{\vv{p}}\bra{\vv{p}},
    \end{equation}
    which holds if
    \begin{equation}
        \braket{\vv{p}}{\vv{p}'} = \bardelta(\vv{p} - \vv{p}').
    \end{equation}
    Writing out the measure in full,
    \begin{equation}
        1 = \int \frac{\dl{^3\vv{p}}}{(2\pi)^3} \frac{1}{2\omega(\vv{p})} \ket{\vv{p}} \bra{\vv{p}}.
    \end{equation}
    
    The incident flux is
    \begin{equation}
        \braket{\vv{p}}{\vv{p}} = 2\omega(\vv{p}) (2\pi)^3 \delta^3(\vv{0}).
    \end{equation}
    Identifying \((2\pi)^3\delta^3(\vv{0}) = V\) we have
    \begin{equation}
        \frac{\braket{\vv{p}}{\vv{p}'}}{V} = 2\omega(\vv{p}).
    \end{equation}
    This implies we have \(2\omega(\vv{p})\) states per unit volume.
    
    Now consider the decay of a single particle, \(p\), into \(n\) particles, \(p_i\).
    The decay rate in the frame of the particle, that is the transition rate per decaying particle, is then
    \begin{equation}
        \dl{\Gamma} = (2\pi)^4 \delta\left( p - \sum p_f \right) \frac{1}{2\omega(\vv{p})} \prod_f \frac{\dl{^3\vv{p}}}{(2\pi)^32E_f} \abs{\amplitude}^2,
    \end{equation}
    where we write \(E_f = \omega(\vv{p_f})\).
    We can package the measure up into a single thing called the \(n\) particle \defineindex{phase space measure}:
    \begin{equation}
        \phaseSpaceMeasure \coloneqq (2\pi)^4 \delta^4\left( p - \sum p_f \right) \prod_f \frac{\dl{^3p_f}}{(2\pi)^32E_f},
    \end{equation}
    so
    \begin{equation}
        \dl{\Gamma} = \frac{1}{2E(\vv{p})} \abs{\amplitude}^2 \phaseSpaceMeasure,
    \end{equation}
    where \(E(\vv{p}) = \omega(\vv{p})\).
    
    \subsection{One to Two Decay}
    Consider the decay \(p \to p_1 + p_2\).
    When doing an experiment we have detectors covering some solid angle, and so we are interested in not just the decay rate, but the decay rate into some particular solid angle, which is
    \begin{equation}
        \diff{\Gamma}{\Omega}.
    \end{equation}
    
    We have two final particles, so we need to compute \(\phaseSpaceMeasure[2]\), which is given by
    \begin{equation}
        \phaseSpaceMeasure[2] = (2\pi)^4 \delta^4(p - p_1 - p_2) \frac{\dl{^3\vv{p_1}}}{(2\pi)^32\omega(\vv{p_1})} \frac{\dl{^3\vv{p_2}}}{(2\pi)^32\omega(\vv{p_2})}.
    \end{equation}
    We can perform the \(p_2\) integral using one of the Dirac deltas:
    \begin{equation}
        \int \dl{^3\vv{p_2}} \delta^4(p_1 + p_2 - p) = \delta(E_1 + E_2 - E),
    \end{equation}
    where we now fix \(\vv{p_2} = \vv{p} - \vv{p_1}\) and \(E_2 = \abs{\vv{p_2}}^2 + m_2^2\).
    We can then express the momentum space volume element for \(p_1\) in terms of polar variables:
    \begin{equation}
        \dl{^3p_1} = \abs{\vv{p_1}}^2 \dd{p_1} \dd{\Omega}.
    \end{equation}
    Now use
    \begin{equation}
        E_1^2 = \abs{\vv{p_1}}^2 + m_1^2 \implies E_1 \dd{E_1} = p_1\dd{p_1},
    \end{equation}
    since \(\dl{m_1} = 0\).
    Then, the partially integrated phase space is
    \begin{equation}
        \int \phaseSpaceMeasure[2] = \frac{1}{4\pi} \dd{\Omega} \frac{p_1 \dd{E_1}}E_2{} \delta(E_1 + E_2 - E).
    \end{equation}
    We can then write the phase space measure as
    \begin{equation}
        \int \phaseSpaceMeasure[2] = \frac{1}{(4\pi)^2} \frac{\abs{\vv{p}}}{m} \dd{\Omega}.
    \end{equation}
    Thus in the centre of mass frame the decay rate into some solid angle is
    \begin{equation}
        \left( \diff{\Gamma}{\Omega} \right)_{\text{CoM}} = \frac{1}{32\pi^2} \frac{\abs{\vv{p}}}{m^2} \abs{\amplitude}^2.
    \end{equation}
    
    \section{Cross Sections}
    When we consider collisions there is almost no point in considering collisions with three or more particles, since these are so unlikely to occur.
    So, consider a scattering of two particles, \(p_1\) and \(p_2\).
    We can work in the lab frame, where, say, the second particle is stationary, so
    \begin{equation}
        p_1 = (\omega(\vv{p_1}), \vv{p_1}), \qqand p_2 = (m_2, \vv{0}).
    \end{equation}
    We can also write \(E_1 = \omega(\vv{p_1})\).
    
    The incident flux, that is the number of particles crossing unit area per unit time, is given by the volume of particles which would cross through said area in unit time times the density of particles.
    The volume is simply \(\abs{\vv{v}}\), where \(\vv{v}\) is the velocity of the particle, since in one unit time the particles will have moved a distance \(\abs{\vv{v}}\) forward.
    The density of particles is \(2E_1\), so the incident flux is \(2E_1\abs{\vv{v}} = 2\abs{\vv{p_1}}\), where we've used \(\vv{p} = \gamma m\vv{v}\) and \(E = \gamma m\), so \(\abs{\vv{v}} = \abs{\vv{p}}/E\) as expected.
    
    The number of scattering centres in the target per unit volume is the density of particle states, which is \(2E_2 = 2m_2\).
    
    The differential cross section is defined as the transition rate per scattering centre per unit incident flux.
    The idea being that the number of scattering centres and the incident flux are both dependent on the experiment, and the differential cross section is independent of experimental details.
    The differential cross section is then
    \begin{equation}
        \dl{\sigma} = (2\pi)^4 \delta^4\left( \sum p_i - \sum p_f \right) \frac{1}{4\abs{\vv{p_1}}m_2} \left( \prod_f \frac{\dl{^3\vv{p_f}}}{(2\pi)^32E_f} \right) \abs{\amplitude}^2.
    \end{equation}
    
    Now we can use \((p_1 \cdot p_2)^2 = (E_1m_2)^2 = (\vv{p_1} + m_1)^2 m_2^2\) which gives \(\abs{\vv{p_1}}^2m_2^2 = (p_1\cdot p_2)^2 - m_1^2 m_2^2\), which is Lorentz invariant.
    This means that we can write the result in a frame independent way as
    \begin{equation}
        \dl{\sigma} = \frac{\abs{\amplitude}^2}{4\sqrt{(p_1 \cdot p_2)^2 - m_1^2 m_2^2}} \phaseSpaceMeasure.
    \end{equation}
    
    \subsection{Two to Two Scattering}
    Consider the case where we have two incoming and two outgoing particles, \(p_1 + p_2 \to p_3 + p_4\).
    In the centre of mass frame we have
    \begin{equation}
        \int \phaseSpaceMeasure[2] = \frac{1}{(4\pi)^2} \frac{p'}{W}
    \end{equation}
    where \(p' = \abs{\vv{p_3}} = \abs{\vv{p_4}}\), where equality between the magnitude of the momenta of the particles follows by conservation of momentum in the centre of mass frame, meaning \(\vv{p_1} + \vv{p_2} = \vv{0}\), so \(\vv{p_3} + \vv{p_4} = \vv{0}\) also.
    As well, \(W = E_1 + E_2 = E_3 + E_4\), where the second equality is just conservation of energy.
    
    In the centre of mass frame we have
    \begin{equation}
        p_1 = (E_1, \vv{p}), \qqand p_2 = (E_2, -\vv{p}).
    \end{equation}
    Define \([ = \abs{\vv{p}}]\), and so
    \begin{equation}
        \vv{p_1} \cdot \vv{p_2} = E_1E_2 + \vv{p}^2 \implies (p_1 \cdot p_2)^2 - m_1^2 m_2^2 = p^2W^2,
    \end{equation}
    where we've used the on-shell requirement to show this.
    
    Hence, the cross section for scattering per unit solid angle is
    \begin{equation}
        \left( \diff{\sigma}{\Omega} \right)_{\text{CoM}} = \frac{1}{64\pi^2} \frac{1}{W^2} \left( \frac{p'}{p} \right)^2 \abs{\amplitude}^2.
    \end{equation}
    Note that if all particles have the same mass then \(p' = p\) and so often we see this equation without the \(p'/p\) factor.
    This is also a valid approximation in the high energy case where the masses provide a negligible contribution to the energy-momentum relation.
    
    \subsubsection{In \texorpdfstring{\(\varphi^3\)}{Phi Cubed} Theory}
    In \(\varphi^3\) theory the two to two scattering gives us the amplitude
    \begin{equation}
        \amplitude = (-ig^2) \left[ \frac{1}{s - m^2} + \frac{1}{t - m^2} + \frac{1}{u - m^2} \right].
    \end{equation}
    Here \(s = W^2\).
    Hence
    \begin{equation}
        \left( \diff{\sigma}{\Omega} \right)_{\text{CoM}} = \frac{1}{64\pi^2} \frac{g^4}{s} \abs{\frac{1}{s - m^2} + \frac{1}{t - m^2} + \frac{1}{u - m^2}}^2.
    \end{equation}
    
    Now consider high energy fixed angle scattering.
    Then \(\abs{\vv{p}}^2 \gg m^2\), so \(\abs{\vv{p}}^2 \approx E^2\).
    We also have \(s = 4E^2 \approx 4\vv{p}^2\), \(t \approx -2E^2(1 - \cos\vartheta)\) and \(u \approx -2E^2(1 + \cos\vartheta)\).
    This means that the ratios \(s/u\) and \(s/t\) are fixed since \(\vartheta\) is fixed and the factors of \(E^2\) cancel.
    The angular distribution can be written as
    \begin{equation}
        \dl{\Omega} = 2\pi \dd{\cos \vartheta},
    \end{equation}
    and so
    \begin{equation}
        \left( \diff{\sigma}{(\cos \vartheta)} \right)_{\text{CoM}} = \frac{1}{32\pi} \frac{g^4}{s^3}\left( 1 + \frac{s}{t} + \frac{s}{u} \right)^2,
    \end{equation}
    where we get an extra factor of \(2\pi\) from the \(\int_0^{2\pi}\dl{\varphi}\) integral and the term in brackets follows from neglecting the masses and some trig identities.
    
    Note that dimensionally this all works out since the action, \(S\), has dimensions of \([S] = [\text{length}]^4[g\varphi^3]\), which follows from the action being the spacetime integral of \(-g\varphi^3/4!\), and so since action is dimensionless (when \(\hbar = 1\)) and length and energy have inverse units of each other (when \(c = 1\) also) we have \([g\varphi^3] = [\text{energy}]^4\).
    We know that \([(\partial \varphi)^2] = [\text{energy}]^4\), and so we have \([\varphi] = [\text{energy}]\), since \([\partial] = [\text{length}]^{-1} = [\text{energy}]\).
    Hence, we must have that \([g] = 1\) and so this entire quantity has units of \(1/[s^3] = [\text{momentum}]^3\).
    This tells us that the cross section drops off inversely proportionally to the momentum squared, so at high momentums interactions are much less likely, which is what we would expect, high momentum particles move past each other very quickly without much of a chance for interactions.
    
    \part{Complex Scalar Fields}
    \chapter{Charge}
    Classical fields, such as the electromagnetic field, are always real since they are observables.
    Wave functions on the other hand are always complex, since their time evolution, \(\ket{\psi, t} = \e^{iEt}\ket{\psi}\), demands it.
    In quantum field theory fields can be either real or complex, since they are not observable.
    However, the action, Lagrangian, and Hamiltonian must be real, this is needed to ensure that the \(S\) matrix is unitary, which is required for conservation of probability.
    Also, the Hamiltonian corresponds to the energy of the system which is observable, and hence real.
    
    \section{Multiple Real Fields}
    Consider the common definition of complex numbers which starts something like \enquote{a complex number is an ordered pair of real numbers with operations of \dots}.
    Much like this definition a complex field can be thought of as a pair of real fields.
    So, we need to be able to deal with multiple fields.
    Fortunately our work so far generalises very well to multiple real fields, we just have to sum over fields in certain expressions.
    
    Suppose we have \(N\) real scalar fields, \(\varphi_r\), with \(r = 1, \dotsc, N\).
    For simplicity, and because it's all we need for the complex case, we assume that all \(N\) fields correspond to particles of mass \(m\).
    The Lagrangian is then
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi_r(x))(\partial^\mu \varphi_r(x)) - m^2\varphi_r(x)^2,
    \end{equation}
    where we have an implied sum over \(r\).
    We can then define the conjugate fields, and we find
    \begin{equation}
        \pi_r(x) \coloneqq \diffp{\lagrangianDensity}{\dot{\varphi}_r(x)} = \dot{\varphi}_r(x).
    \end{equation}
    Any two distinct fields commute, and for two of the same field we have the usual commutation relations.
    That is, we have the equal time commutation relation
    \begin{equation}
        \commutator{\varphi_r(t, \vv{x})}{\pi_s(t, \vv{x}')} = i\delta_{rs} \delta(\vv{x} - \vv{x}'),
    \end{equation}
    and all other commutators vanish.
    
    The mode expansion is the same as for a single field, but we have independent creation and annihilation operators for each field:
    \begin{equation}
        \varphi_r(x) = \int \frac{\dl{^3\vv[p]}}{(2\pi)^3} \frac{1}{2\pi(\vv{p})} [a_r(\vv{p})\e^{-ip\cdot x} + a_r^\hermit(\vv{p})\e^{ip\cdot x}].
    \end{equation}
    These have the covariant commutation relations
    \begin{equation}
        \commutator{a_r(\vv{p})}{a_s^\hermit(\vv{p}')} = \delta_{rs} 2\omega(\vv{p}) (2\pi)^3\delta(\vv{p} - \vv{p}'),
    \end{equation}
    and all other commutators vanish.
    
    From here we proceed as before, we can define the energy-momentum tensor,
    \begin{equation}
        T^{\mu\nu}(x) \coloneqq \normalordering{(\partial^\mu \varphi_r(x))(\partial^\nu \varphi_r(x)) - \minkowskiMetric^{\mu\nu}\lagrangianDensity(x)},
    \end{equation}
    with an implied sum over \(r\).
    We then get the conserved momentum
    \begin{equation}
        P^\nu = \int \dl{^3x} \, T^{0\nu}(x),
    \end{equation}
    and the conserved current
    \begin{equation}
        j^\mu = \diffp{\lagrangianDensity}{(\partial_\mu \varphi_r)}\delta \varphi_r,
    \end{equation}
    with an implied sum over \(r\).
    
    The interpretation of \(a_r^\hermit(\vv{p})\) and \(a_r(\vv{p})\) as creation and annihilation operators is the same, each creating/annihilating particles of type \(r\).
    This follows since we have
    \begin{equation}
        \commutator{H}{a_r^\hermit(\vv{p})} = Ea_r^\hermit(\vv{p}), \qqand \commutator{H}{a_r(\vv{p})} = -Ea_r(\vv{p}).
    \end{equation}
    So, \(a_r^\hermit(\vv{p})\) creates a particle of type \(r\) and momentum \(\vv{p}\), which we might write as
    \begin{equation}
        a_r^\hermit(\vv{p}) \ket{0} = \ket{\vv{p}, r}.
    \end{equation}
    
    The only time when we \emph{don't} sum over the fields is when defining the number operator, which we want to count particles of type \(r\), not the total number of particles, so we define
    \begin{equation}
        N_r(\vv{p}) \coloneqq a_r^\hermit(\vv{p}) a_r(\vv{p}) \qquad \text{(no sum on \(r\)).}
    \end{equation}
    
    \section{Complex Scalar Field}
    Now, consider a complex scalar field, \(\varphi\).
    We can write this as a sum of two real fields, \(\varphi_1\) and \(\varphi_2\), one multiplied by \(i\):
    \begin{equation}
        \varphi(x) = \frac{1}{\sqrt{2}} [\varphi_1(x) + i\varphi_2(x)].
    \end{equation}
    The factor of \(1/\sqrt{2}\) here is so that the free Lagrangian comes out as the sum of two free Lagrangians for the real fields \(\varphi_1\) and \(\varphi_2\).
    
    Motivated by the need for the Lagrangian to be real we take the usual real scalar field Lagrangian and replace one of every pair of \(\varphi\)s with \(\varphi^\hermit\).
    We then treat \(\varphi\) and \(\varphi^\hermit\) as two independent fields, since from these we have
    \begin{equation}
        \varphi_1 = \frac{1}{\sqrt{2}}(\varphi + \varphi^\hermit), \qand i\varphi_2 = \frac{1}{\sqrt{2}}(\varphi - \varphi^\hermit),
    \end{equation}
    and we assume \(\varphi_1\) and \(\varphi_2\) are independent.
    Doing so we see that if we also drop the factor of \(2\) then we get
    \begin{align}
        \lagrangianDensity &= (\partial_\mu \varphi^\hermit)(\partial^\mu \varphi) - m^2\varphi^\hermit \varphi\\
        &= \frac{1}{2}(\partial_\mu \varphi_i)(\partial^\mu \varphi_i) - \frac{1}{2}m^2\varphi_i^2,
    \end{align}
    with an implied sum over \(i\).
    Note that we assume both fields have particles of the same mass, since otherwise we cannot guarantee that the Lagrangian will be real.
    
    We can define the conjugate field
    \begin{equation}
        \pi \coloneqq \diffp{\lagrangianDensity}{\dot{\varphi}} = \dot{\varphi}^\hermit, \qqand \pi^\hermit = \diffp{\lagrangianDensity}{\dot{\varphi}^\hermit} = \dot{\varphi}.
    \end{equation}
    We then have the equal time commutation relations
    \begin{equation}
        \commutator{\varphi(t, \vv{x})}{\pi(t, \vv{x}')} = \commutator{\varphi^\hermit(t, \vv{x})}{\pi^\hermit(t, \vv{x}')} = i\delta^3(\vv{x} - \vv{x}'),
    \end{equation}
    and all other commutators vanish.
    
    We can define the Hamiltonian as before:
    \begin{align}
        H &= \int \dl{^3\vv{x}} \, (\pi\dot{\varphi} + \pi^\hermit \dot{\varphi}^\hermit - \lagrangianDensity)\\
        &= \int \dl{^3\vv{x}} \, (\pi^\hermit \pi + (\grad \varphi^\hermit) \cdot (\grad \varphi) + m^2 \varphi^\hermit \varphi).
    \end{align}
    This second form makes it clear that this is real.
    
    Computing the Heisenberg equations of motion gives
    \begin{equation}
        (\dalembertian + m^2)\varphi = 0, \qqand (\dalembertian + m^2)\varphi^\hermit = 0.
    \end{equation}
    
    \section{Mode Expansion}
    The mode expansion is similar to the real case, except, in the real case we needed \(a\) and \(a^\hermit\) to give a real field.
    We don't need a real field here, but we do require a second term since this then causes things to cancel when the canonical commutation relations are evaluated outside of the light cone, which enforces causality.
    As a result we still include two terms in the mode expansion, but we no longer require one to be the Hermitian conjugate of the other, so we introduce some other operator, \(b^\hermit\), to replace \(a^\hermit\), with the conjugate for consistency with the real case:
    \begin{equation}
        \varphi(x) = \int \invariantmeasure{p} \, [a(\vv{p}) \e^{-ip\cdot x} + b^\hermit(\vv{p}) \e^{ip \cdot x}].
    \end{equation}
    We then have
    \begin{equation}
        \varphi^\hermit(x) = \int \invariantmeasure{p} \, [b(\vv{p}) \e^{-ip\cdot x} + a^\hermit(\vv{p})\e^{ip\cdot x}].
    \end{equation}
    
    We then have the equal time commutation relations
    \begin{equation}
        \commutator{a(\vv{p})}{a^\hermit(\vv{p})} = \commutator{b(\vv{p})}{b^\hermit(\vv{p})} = \bardelta(\vv{p} - \vv{p}'),
    \end{equation}
    and the two independent mode operators commute:
    \begin{equation}
        \commutator{a(\vv{p})}{b(\vv{p})} = \commutator{a^\hermit(\vv{p})}{b^\hermit(\vv{p})} = 0,
    \end{equation}
    as we as all other commutators vanishing.
    
    We can then show that
    \begin{alignat}{2}
        \commutator{H}{a^\hermit(\vv{p})} &= Ea^\hermit \qquad & \commutator{H}{a(\vv{p})} = -Ea(\vv{p}),\\
        \commutator{H}{b^\hermit(\vv{p})} &= Eb^\hermit \qquad & \commutator{H}{b(\vv{p})} = -Eb(\vv{p}).
    \end{alignat}
    Recall that a real scalar field can be interpreted as an infinite set of harmonic oscillators, one for each momentum, \(\vv{p}\).
    We can then interpret the complex  scalar field as two independent sets of harmonic oscillators, with two for each momentum, \(\vv{p}\).
    This leads to the interpretation of \(a^\hermit\) and \(b^\hermit\) as creating independent particles.
    These particles are bosons, both with respect to exchanging particles made by the same mode operator, so, for example, both created by \(a^\hermit\), and with respect to exchanging particles made by different mode operators, so for example, exchanging a particle created by \(a^\hermit\) with a particle created by \(b^\hermit\).
    
    The energy-momentum operator is
    \begin{equation}
        T^{\mu\nu} = \normalordering{(\partial^\mu\varphi^\hermit)(\partial^\nu \varphi) + (\partial^\nu \varphi^\hermit) (\partial^\mu \varphi) - \minkowskiMetric^{\mu\nu}\lagrangianDensity}.
    \end{equation}
    The conserved momentum is
    \begin{equation*}
        P^\nu = \int \dl{^3\vv{x}} \, T^{0\nu} = \int \invariantmeasure{p} \, p^\nu(a^\hermit(\vv{p})a(\vv{p}) + b^\hermit(\vv{p})b(\vv{p})) = \int \invariantmeasure{p} \, p^\nu (N_a(\vv{p}) + N_b(\vv{p})).
    \end{equation*}
    This means it is the momentum of the combined two sets of particles which is conserved, not the momentum of each set individually.
    
    \section{Charge Conservation}
    There must be something that sets apart particles created by \(a^\hermit\) and particles created by \(b^\hermit\).
    In this section we work out what this is.
    Consider the Lagrangian again,
    \begin{equation}
        \lagrangianDensity = (\partial_\mu\varphi^\hermit)(\partial^\mu\varphi) - m^2\varphi^\hermit \varphi.
    \end{equation}
    Notice that this is invariant under the transformation
    \begin{equation}
        \varphi \mapsto \e^{i\alpha}\varphi
    \end{equation}
    for some constant \(\alpha\).
    Notice that \(\alpha\) cannot depend on \(x\), since then the derivatives would cause problems.
    Under this transformation we have \(\varphi^\hermit \mapsto \e^{-i\alpha}\varphi^\hermit\) as well.
    This is a \(\unitary(1)\) symmetry\footnote{see the \course{Symmetries of Quantum Mechanics} or \course{Symmetries of Particles and Fields} course for details}.
    This is an internal symmetry, acting on the fields, rather than on spacetime.
    
    A symmetry implies a conserved quantity, so lets find out what it is\footnote{If the title of this section hasn't already given it away!}.
    Assume \(\alpha \ll 1\), so we have \(\exp[i\alpha] \approx 1 + i\alpha\), and hence
    \begin{equation}
        \varphi \mapsto \e^{i\alpha}\varphi \approx (1 + i\alpha)\varphi = \varphi + i\alpha \varphi,
    \end{equation}
    and hence
    \begin{equation}
        \delta \varphi = \e^{i\alpha}\varphi - \varphi \approx i\alpha\varphi.
    \end{equation}
    Similarly,
    \begin{equation}
        \delta \varphi^\hermit \approx -i\alpha\varphi.
    \end{equation}
    We then have
    \begin{equation}
        \diffp{\lagrangianDensity}{(\partial_\mu\varphi)} \delta \varphi = i\alpha(\partial^\mu\varphi^\hermit)\varphi, \qqand \diffp{\lagrangianDensity}{(\partial_\mu\varphi^\hermit)}\delta\varphi^\hermit = -i\alpha\varphi^\hermit(\partial^\mu\varphi).
    \end{equation}
    The conserved current is then, normal ordering to avoid later issues\footnote{actually, normal ordering isn't strictly necessary here, but it is in other similar cases so we'll do so here for consistency},
    \begin{align}
        j^\mu &= \normalordering{\diffp{\lagrangianDensity}{(\partial_\mu\varphi)}\delta\varphi + \diffp{\lagrangianDensity}{(\partial_\mu\varphi^\hermit)}\delta\varphi^\hermit}\\
        &= \normalordering{i\alpha(\partial^\mu\varphi^\hermit)\varphi - i\alpha\varphi^\hermit(\partial^\mu\varphi)}.
    \end{align}
    We can rescale this by a factor of \(-\alpha\) to get:
    \begin{equation}
        j^\mu = i\normalordering{\varphi^\hermit (\partial^\mu\varphi) - (\partial^\mu\varphi^\hermit)\varphi}
    \end{equation}
    Notice that we can easily show this is conserved, that is that \(\partial_\mu j^\mu = 0\):
    \begin{align}
        \partial_\mu j^\mu &= i\normalordering{[(\partial_\mu\varphi^\hermit)(\partial^\mu\varphi) + \varphi^\hermit(\dalembertian\varphi) - (\dalembertian\varphi^\hermit)\varphi - (\partial^\mu\varphi^\hermit)(\partial_\mu\varphi)]}\\
        &= i\normalordering{[\varphi^\hermit(\dalembertian\varphi) - (\dalembertian\varphi^\hermit)\varphi]}.
    \end{align}
    The equations of motion for the fields give \(\dalembertian\varphi = -m^2\varphi\) and \(\dalembertian\varphi^\hermit = -m^2\varphi^\hermit\), so
    \begin{equation}
        \partial_\mu j^\mu = i\normalordering{[\varphi^\hermit(-m^2\varphi) - (-m^2\varphi^\hermit)\varphi]} = 0,
    \end{equation}
    where we've used the normal ordering to allow us to swap the order of \(\varphi\) and \(\varphi^\hermit\).
    
    Along with this conserved current we have a conserved charge:
    \begin{equation}
        Q = \int \dl{^3\vv{x}} \, j^0 = i\int \dl{^3\vv{x}} \, \normalordering{(\varphi^\hermit \dot{\varphi} - \dot{\varphi}^\hermit \varphi)}.
    \end{equation}
    We can write \(\varphi\) and \(\varphi^\hermit\) in terms of mode operators, with the time derivatives bringing down a factor of \(\pm i\omega(\vv{p})\), giving
    \begin{align}
        Q &= i \int \dl{^3\vv{x}} \int \invariantmeasure{p} \int \invariantmeasure{p'}\\
        &\quad\quad \Big\{ \normalordering{[a^\hermit(\vv{p}) \e^{ip \cdot x} + b\e^{-ip \cdot x}]i\omega(\vv{p}')[-a(\vv{p}')\e^{-ip'\cdot x} + b^\hermit(\vv{p}')\e^{ip'\cdot x}]} \notag\\
        &\qquad\quad -i\omega(\vv{p})\normalordering{[a^\hermit(\vv{p})\e^{ip\cdot x} - b(\vv{p})\e^{-ip\cdot x}] [a(\vv{p}')\e^{-ip'\cdot x} + b^\hermit(\vv{p}')\e^{ip'\cdot x}]} \Big\}.\notag
    \end{align}
    After expanding this, integrating over \(x\) to get Dirac deltas, and then using these to perform the \(p'\) integral we're left with
    \begin{align}
        Q &= \int \invariantmeasure{p} \normalordering{(a^\hermit(\vv{p})a(\vv{p}) - b(\vv{p}) b^\hermit(\vv{p}))}\\
        &= \int \invariantmeasure{p} (a^\hermit(\vv{p}) a(\vv{p}) - b^\hermit(\vv{p})b(\vv{p})).
    \end{align}
    
    From this we can then show that the following commutation relations hold:
    \begin{alignat}{2}
        \commutator{Q}{a^\hermit(\vv{p})} &= \hphantom{-}a^\hermit \qquad & \commutator{Q}{a(\vv{p})} = -a(\vv{p}),\\
        \commutator{Q}{b^\hermit(\vv{p})} &= -b^\hermit \qquad & \commutator{Q}{b(\vv{p})} = \hphantom{-}b(\vv{p}).
    \end{alignat}
    So finally we have a difference between \(a\) and \(b\).
    This leads to the interpretation that
    \begin{itemize}
        \item \(a^\hermit(\vv{p})\) creates a particle with momentum \(\vv{p}\) and \enquote{charge} \(+1\),
        \item \(b^\hermit(\vv{p})\) creates a particle with momentum \(\vv{p}\) and \enquote{charge} \(-1\),
        \item \(a(\vv{p})\) annihilates a particle with momentum \(\vv{p}\) and \enquote{charge} \(+1\),
        \item \(b(\vv{p})\) annihilates a particle with momentum \(\vv{p}\) and \enquote{charge} \(-1\).
    \end{itemize}
    Here we write \enquote{charge}, since this may not be electrical charge, but it can be.
    
    So, a complex scalar field gives a theory with a pair of charged particles.
    Conservation of \(Q\) implies that particles must always be created and destroyed in pairs.
    This all follows simply from recognising that the Lagrangian must be real and so must depend only on products like \(\varphi^\hermit \varphi\).
    
    There is another interpretation of this, which is that \(a^\hermit\) creates particles, and \(b^\hermit\) creates antiparticles (or vice versa).
    An antiparticle being a particle with the same mass but opposite charge.
    Then
    \begin{itemize}
        \item \(a^\hermit(\vv{p})\) creates a particle with momentum \(\vv{p}\) and \enquote{charge} \(+1\),
        \item \(b^\hermit(\vv{p})\) creates an antiparticle with momentum \(\vv{p}\) and \enquote{charge} \(-1\),
        \item \(a(\vv{p})\) annihilates a particle with momentum \(\vv{p}\) and \enquote{charge} \(+1\),
        \item \(b(\vv{p})\) annihilates an antiparticle with momentum \(\vv{p}\) and \enquote{charge} \(-1\).
    \end{itemize}
    Notice that destroying a particle of charge \(+1\) is the same as creating a particle of charge \(-1\) and vice versa.
    
    For a real field we have \(a = b\) and we can interpret this as the statement that the particles in this theory are their own antiparticles.
    
    \section{Feynman Rules}
    \epigraph{It's useful for you to get used to confusing notation in this course, because it will get a whole lot worse.}{Richard Ball}
    Consider the mode expansions of the fields:
    \begin{align}
        \varphi &= \int \invariantmeasure{p} [a(\vv{p})\e^{-ip\cdot x} + b^\hermit(\vv{p})\e^{ip\cdot x}],\\
        \varphi^\hermit &= \int \invariantmeasure{p} [b(\vv{p})\e^{-ip\cdot x} + a^\hermit(\vv{p})\e^{ip\cdot x}].
    \end{align}
    Using these and the commutation relations for the mode operators we can derive the covariant commutator
    \begin{equation}
        \commutator{\varphi(x)}{\varphi^\hermit(x')} = i\Delta(x - x')
    \end{equation}
    where \(\Delta\) is the same function as for real scalar fields.
    The other commutators, \(\commutator{\varphi(x)}{\varphi(x')}\) and \(\commutator{\varphi^\hermit(x)}{\varphi^\hermit(x)}\), vanish.
    
    A result of this vanishing of of commutators between \(\varphi\)s or between \(\varphi^\hermit\)s is that contractions between two \(\varphi\)s or two \(\varphi^\hermit\)s also vanish, and so the only nonvanishing contractions are
    \begin{equation}
        \wick{\c \varphi(x) \c \varphi^\hermit(x')} = \bra{0} \timeOrdering[\varphi(x) \varphi^\hermit(x')] \ket{0} = i\Delta_{\feynman}(x - x'),
    \end{equation}
    where \(\Delta_{\feynman}\) is the same as for a real scalar field.
    
    As before our state space is given by direct products of single particle states, which are now labelled with the charge as well as the momentum of the particle:
    \begin{equation}
        \ket{\vv{p}, +} = a^\hermit(\vv{p})\ket{0}, \qqand \ket{\vv{p}, -} = b^\hermit(\vv{p})\ket{0}.
    \end{equation}
    Writing \(\varphi = \varphi_+ + \varphi_-\) as before\footnote{We lower the \(+\) and \(-\) labels to make room for \(\hermit\).} we have
    \begin{alignat}{2}
        \varphi_+(x)\ket{\vv{p}, +} &= \e^{-ip\cdot x} \ket{0} \qquad & \varphi_+^\hermit(x)\ket{\vv{p}, -} &= \e^{-ip\cdot x}\ket{0},\\
        \bra{\vv{p}, +}\varphi_-^\hermit(x) &= \e^{ip\cdot x} \bra{0} \qquad & \bra{\vv{p}, -} \varphi_-(x) &= \e^{ip\cdot x}\bra{0}.
    \end{alignat}
    
    So, we conclude that
    \begin{itemize}
        \item \(\varphi_+\) destroys an incoming particle of charge \(+1\),
        \item \(\varphi_+^\hermit\) destroys an incoming particle of charge \(-1\),
        \item \(\varphi_-\) creates an outgoing particle of charge \(-1\),
        \item \(\varphi_-^\hermit\) creates an outgoing particle of charge \(+1\).
    \end{itemize}
    From this we can compute all the Feynman rules for any given interaction.
    
    \chapter{Interactions and Symmetries}
    \section{Types of Interaction}
    We choose to look for interactions which conserve charge.
    While we can easily construct theories where this isn't the case charge conservation is a common phenomena and we wish our theories to exhibit it.
    This means we can no longer have a \(\varphi^3\) interaction, since this would not conserve charge.
    Even if we consider a \(\varphi^3 + (\varphi^\hermit)^3\) interaction, so the Lagrangian is real, we still don't get charge conservation, since the first term transforms as \(\varphi^3 \mapsto \e^{3i\alpha}\varphi^3\), and the second as \((\varphi^{\hermit})^3 \mapsto \e^{-3i\alpha}(\varphi^\hermit)^3\).
    The only way to have a theory conserve charge is if every term in the Lagrangian has as many factors of \(\varphi\) as it does \(\varphi^\hermit\).
    There are a few ways to do this.
    
    \subsection{Four Point Interactions}
    Instead of the three point interaction, \(\varphi^3\), we can consider a four point interaction, \((\varphi^\hermit\varphi)^2\), where interactions involve four particles at once, this would lead to the interaction Lagrangian
    \begin{equation}
        \lagrangianDensity_{\interaction}^{(4)} = -\frac{\lambda}{2}(\varphi^\hermit \varphi)^2.
    \end{equation}
    To keep track of charge in our Feynman diagrams it is conventional to put an arrow on our Feynman diagrams pointing in the direction of charge flow.
    Note that charge flowing backwards in time is the same as negative charge flowing forward in time, that is antiparticles correspond to arrows pointing left and particles to arrows pointing right.
    
    There are three vertices in this theory which are all slightly different, but have the same shape.
    The first is
    \begin{equation}
        \tikzsetnextfilename{fd-charge-4-point-interaction-neutral}
        \rotatebox{45}{\feynmandiagram[inline=(v)]{
                i1 -- [anti fermion] v -- [fermion] i2,
                o1 -- [fermion] v -- [anti fermion] o2
            };}
    \end{equation}
    This corresponds to a particle and antiparticle coming in and a particle and antiparticle going out.
    The net charge in and out is then 0.
    So charge is conserved in this vertex.
    
    The next vertex is
    \begin{equation}
        \tikzsetnextfilename{fd-charge-4-point-interaction-positive}
        \rotatebox{45}{\feynmandiagram[inline=(v)]{
                i1 -- [fermion] v -- [fermion] i2,
                o1 -- [fermion] v -- [fermion] o2
            };}
    \end{equation}
    This corresponds to two particles coming in and two particles going out.
    The net charge in and out is then 2, so charge is conserved in this vertex.
    
    The final vertex is
    \begin{equation}
        \tikzsetnextfilename{fd-charge-4-point-interaction-negative}
        \rotatebox{45}{\feynmandiagram[inline=(v)]{
                i1 -- [anti fermion] v -- [anti fermion] i2,
                o1 -- [anti fermion] v -- [anti fermion] o2
            };}
    \end{equation}
    This corresponds to two antiparticles coming in and two antiparticles going out.
    The net charge in and out is then \(-2\), so charge is conserved in this vertex.
    
    As with the real case the constant factor in this interaction has been chosen such that every vertex gives a factor of \(-i\lambda\).
    
    \subsection{Three Point Interactions}
    If we introduce a new, neutral, real scalar field, \(\Phi\), then we can have a three point interaction of the form
    \begin{equation}
        \lagrangianDensity_{\interaction}^{(3)} = -y\Phi \varphi^\hermit \varphi.
    \end{equation}
    This is known as a \defineindex{Yukawa interaction}.
    
    As with \(\varphi^3\) theory we get three particles in each interaction.
    There are a variety of vertices we can create in this case.
    Denoting the neutral particle with a dashed line one possibility is
    \begin{equation}
        \tikzsetnextfilename{fd-charge-3-point-interaction-annihilation}
        \feynmandiagram[inline=(v), horizontal=v to o]{
            i1 -- [fermion] v,
            i2 -- [anti fermion] v,
            v -- [scalar] o
        };
    \end{equation}
    This corresponds to a particle and antiparticle annihilating into a neutral particle.
    
    Another vertex is
    \begin{equation}
        \tikzsetnextfilename{fd-charge-3-point-interaction-particle-emit}
        \feynmandiagram[inline=(v), horizontal=i to v]{
            i -- [fermion] v,
            v -- [scalar] o1,
            v -- [fermion] o2
        };
    \end{equation}
    This corresponds to a particle emitting a neutral particle.
    Similarly the vertex 
    \begin{equation}
        \tikzsetnextfilename{fd-charge-3-point-interaction-antiparticle-emit}
        \feynmandiagram[inline=(v), horizontal=i to v]{
            i -- [anti fermion] v,
            v -- [scalar] o1,
            v -- [anti fermion] o2
        };
    \end{equation}
    corresponds to an antiparticle emitting a neutral particle.
    Notice that in all of these diagrams the total charge is conserved at each vertex.
    
    As before the constant factors are chosen such that each vertex contributes a factor of \(-iy\) to the amplitude.
    
    A full Lagrangian in a theory with this interaction may take the form
    \begin{equation}
        \lagrangianDensity = (\partial_\mu\varphi^\hermit)(\partial_\mu\varphi) - m^2\varphi^\hermit \varphi + \frac{1}{2}(\partial_\mu \Phi)(\partial^\mu \Phi) - \frac{1}{2}M^2\Phi^2 - y\Phi \varphi^\hermit \varphi,
    \end{equation}
    corresponding to a neutral particle, call it \(s_0\), of mass \(M\) and charged (anti)particles, call them \(s_{\pm}\), of mass \(m\).
    
    In many ways this theory is not that different from the real \(\varphi^3\) theory.
    For example, we still have 2--2 scattering, such as \(s_+s_- \to s_+s_-\), \(s_+s_+ \to s_+s_+\), and \(s_-s_- \to s_-s_-\).
    The exchange particle in these cases is a neutral scalar, \(s_0\).
    Note that in the \(s_+s_- \to s_+s_-\) case since the final two particles are distinct there is no \(u\)-channel diagram.
    Similarly for \(s_{\pm}s_{\pm} \to s_{\pm}s_{\pm}\) there is no \(s\)-channel diagram, since the intermediate particle would have to carry a charge of \(\pm2\).
    
    There is one sort of interaction which we don't have in \(\varphi^3\) theory, and that is decays.
    If \(M > 2m\) then it is possible to have the interaction \(s_0 \to s_+s_-\).
    This would correspond to an \(n = 1\) contribution to the Dyson series, and these terms vanished for \(\varphi^3\) theory.
    This would look like
    \begin{equation}
        \tikzsetnextfilename{fd-neutral-decay-to-charged-particles}
        \feynmandiagram[inline=(v), horizontal=i to v]{
            i -- [scalar] v,
            v -- [fermion] o1,
            v -- [anti fermion] o2
        };
    \end{equation}
    The amplitude for this is \(\amplitude = -iy\).
    We can work out the decay rate starting with
    \begin{equation}
        \left( \diff{\Gamma}{\Omega} \right)_{\text{CoM}} = \frac{1}{32\pi^2} \frac{p}{M^2} \abs{\amplitude}^2,
    \end{equation}
    where \(p\) is the magnitude of the three-momentum of the two charged particles in the centre of mass frame.
    The amplitude squared will give \(\abs{\amplitude}^2 = \abs{-iy}^2 = y^2\).
    % TODO: Kinematics, where does p^2 = M^2 - 4m^2 come from?
%    In this frame the neutral particle has four-momentum \(p^{(0)} = (M, \vv{0})\), and the two charged particles have four-momenta \(p^{(+)} = (E, \vv{p})\) and \(p^{(-)} = (E, -\vv{p})\).
%    Note that they must have the same energy as, apart from charge, the decay is symmetric in the two particles.
%    Hence, we have
%    \begin{equation}
%        M^2 = (p^{(0)})^2 = (p^{(+)} + p^{(-)})^2 = 2m^2 - 2(E^2 + p^2).
%    \end{equation}
    We then have
    \begin{equation}
        \left( \diff{\Gamma}{\Omega} \right)_{\text{CoM}} = \frac{y^2}{64\pi^2} \frac{\sqrt{M^2 - 4m^2}}{M^2}.
    \end{equation}
    So we see that we must have \(M > 2m\) for this to be nonzero and real, assuming \(M, m \ge 0\).
    
    We just said \enquote{assuming \(M, m \ge 0\)}, but what if they aren't?
    In fact, what if they aren't even real?
    We can analytically extend this decay rate as a function of the complex variable \(M\).
    Recall that \(\sqrt{f(z)}\) has a branch cut from \(f(z) = 0\) to infinity.
    So the decay rate has a branch cut from \(M = 2m\) to infinity.
    This is part of a general rule, particles correspond to poles, such as the pole at \(m = \sqrt{s}\) in \(1/(s - m^2)\), and decays correspond to branch cuts, such as the one starting at \(M = 2m\) here.
    
    \section{Symmetries}
    In this section we'll study the effect of parity, time reversal, and charge conjugation on scalar fields.
    We'll start from their action on spacetime, and then derive their action on the fields and mode operators.
    These three symmetries are all \(\integers_2\) symmetries, meaning that, apart from doing nothing, we have a single operation and applying this operation twice is the same as doing nothing.
    The technical word for this is that these symmetries are \defineindex{idempotent}.
    
    \subsection{Parity}
    Parity acts on spacetime by sending \(\vv{x}\) to \(-\vv{x}\).
    That is
    \begin{equation}
        \parity \colon x^\mu = (t, \vv{x}) \mapsto \overbar{x}^\mu = (t, -\vv{x}).
    \end{equation}
    Since the direction of space is also changed the momentum is also changed by sending \(\vv{p}\) to \(-\vv{p}\):
    \begin{equation}
        \parity \colon p^\mu = (E, \vv{p}) \mapsto \overbar{p}^\mu = (E, -\vv{p}).
    \end{equation}
    We will use the notation \(\overbar{x}\) and \(\overbar{p}\) with this meaning throughout this section, including when discussing time reversal and charge conjugation.
    
    The definition of parity above is classical.
    In quantum field theory we have operators and states.
    Parity is promoted to be a unitary operator, \(\parity\), on the states, that is \(\parity^\hermit \parity = 1\).
    Starting with the simplest case, the vacuum state should be invariant under parity, since there's nothing there to change:
    \begin{equation}
        \parity \ket{0} = \ket{0}.
    \end{equation}
    The next simplest state is a single particle state, \(\ket{\vv{p}}\).
    We expect that the momentum of this particle should be reversed under parity:
    \begin{equation}
        \parity \ket{\vv{p}} = \ket{-\vv{p}}.
    \end{equation}
    We can write this momentum state as \(\ket{\vv{p}} = a^\hermit(\vv{p})\ket{0}\), and so we can identify the action of the parity operator on the state with an equivalent action on \(a^\hermit(\vv{p})\).
    Since \(a^\hermit(\vv{p})\) is an operator it transforms with two copies of \(\parity\) as
    \begin{equation}
        \parity a^\hermit(\vv{p}) \parity^{\hermit} = a^\hermit(-\vv{p}),
    \end{equation}
    so
    \begin{equation}
        (\parity a^\hermit(\vv{p}) \parity^\hermit)\ket{0} = a^\hermit(-\vv{p}) \ket{0} = \ket{-\vv{p}}.
    \end{equation}
    Taking the adjoint of this we have
    \begin{equation}
        [\parity a^\hermit(\vv{p}) \parity]^\hermit = (\parity^\hermit)^\hermit a(\vv{p}) \parity^\hermit = \parity a(\vv{p}) \parity^\hermit,
    \end{equation}
    and so
    \begin{equation}
        \parity a(\vv{p}) \parity^\hermit = [a^\hermit(-\vv{p})]^\hermit = a(-\vv{p}).
    \end{equation}
    
    To derive the effect on the field, which is an operator and so transforms as \(\varphi \mapsto \parity \varphi \parity^\hermit\), we can expand \(\varphi\) in the mode operators:
    \begin{align}
        \parity \varphi(x) \parity^\hermit &= \int \invariantmeasure{p} [\parity a(\vv{p}) \parity^\hermit \e^{-ip\cdot x} + \parity a^\hermit(\vv{p}) \parity^\hermit \e^{ip\cdot x}]\\
        &=- \int \invariantmeasure{p} [a(-\vv{p}) \e^{-ip\cdot x} + a^\hermit(-\vv{p})\e^{ip\cdot x}].
    \end{align}
    Note that the parity operator doesn't act on the phase factors, even though they depend on position and momentum, they're just scalars and so aren't affected by operators on the Hilbert space.
    Now take \(\vv{p} \to -\vv{p}\) in the integral, this doesn't change the measure or region of integration and takes \(p \to \overbar{p}\), giving
    \begin{equation}
        \parity \varphi(x) \parity^\hermit = \int \invariantmeasure{p} [a(\vv{p}) \e^{-i\overbar{p}\cdot x} + a^\hermit(\vv{p})\e^{i\overbar{p}\cdot x}].
    \end{equation}
    Now notice that \(\overbar{p} \cdot x = Et + \vv{p} \cdot \vv{x} = p \cdot \overbar{x}\), and so
    \begin{align}
        \parity \varphi(x) \parity^\hermit &= \int \invariantmeasure{p} [a(\vv{p})\e^{-ip\cdot \overbar{x}} + a^\hermit(\vv{p})\e^{ip\cdot \overbar{x}}]\\
        &= \varphi(\overbar{x}).
    \end{align}
    So under parity the field transforms as
    \begin{equation}
        \parity \colon \varphi(x) \mapsto \parity \varphi(x) \parity^\hermit = \varphi(\overbar{x}).
    \end{equation}
    This is what we would expect.
    Note that exactly the same argument can be applied to the conjugate field, and to complex scalar fields, they all transform the same.
    
    One thing that we didn't consider is that quantum states are only defined up to phase.
    This means that its possible to have an equivalent transformation which introduces a phase, so we could have that parity acts on the single particle state as
    \begin{equation}
        \parity \ket{\vv{p}} = \eta_{p} \ket{-\vv{p}}.
    \end{equation}
    Since \(\parity\) is idempotent we have \(\parity \parity \ket{\vv{p}} = \parity \eta_p \ket{-\vv{p}} = \eta_p^2 \ket{\vv{p}}\), and we must have \(\parity \parity \ket{\vv{p}} = \ket{\vv{p}}\), so \(\eta_p^2 = 1\), meaning \(\eta_p = \pm 1\).
    Above we assumed \(\eta_p = 1\), suppose instead that \(\eta_p = -1\), then the mode operators must transform as
    \begin{equation}
        \parity \colon a(\vv{p}) \mapsto \parity a(\vv{p}) \parity^\hermit = -a(-\vv{p}),
    \end{equation}
    and hence the field transforms as
    \begin{equation}
        \parity \colon \varphi(x) \mapsto \parity \varphi(x) \parity^\hermit = -\varphi(\overbar{x}).
    \end{equation}
    In this case we say that \(\varphi\) is a \defineindex{pseudoscalar} field.
    
    \subsection{Time Reversal}
    Time reversal acts on spacetime by sending \(t\) to \(-t\).
    That is
    \begin{equation}
        \timeReversal \colon x^\mu = (t, \vv{x}) \mapsto -\overbar{x}^\mu = (-t, \vv{x}).
    \end{equation}
    Since the direction of time reverses the direction of momentum, proportional to the velocity, \(\dot{x}\), and hence first order in time derivatives, changes direction.
    The energy is unaffected.
    The simplest explanation for why is that the kinetic energy is proportional to \(\dot{x}^2\) and so picks up two negatives upon time reversal, cancelling out for no overall change.
    So, momentum transforms under time reversal as
    \begin{equation}
        \timeReversal \colon p^\mu = (E, \vv{p}) \mapsto \overbar{p}^\mu = (E, -\vv{p}).
    \end{equation}
    
    Again when moving to quantum field theory time reversal is promoted to an operator, \(\timeReversal\) on the states.
    One effect of time reversal is that the initial and final state are swapped.
    This means that amplitudes change:
    \begin{equation}
        \timeReversal \colon \braket{f}{i} \mapsto \braket{i}{f} = \braket{f}{i}^*.
    \end{equation}
    So, time reversal means we have to take the complex conjugate of everything.
    Another way to view this is to consider the time evolution operator, \(\exp[-iEt]\).
    Under time reversal this changes to \(\exp[iEt] = \exp[-iEt]^*\).
    We say that \(\timeReversal\) is an antiunitary operator.
    
    \begin{dfn}{Antiunitary Operator}{}
        Let \(\hilbertSpace\) be a complex Hilbert space with inner product \(\innerproduct{-}{-}\), and let \(U \colon \hilbertSpace \to \hilbertSpace\) be an operator.
        Then \(U\) is an \defineindex{antiunitary operator} if
        \begin{equation}
            \innerproduct{Ux}{Uy} = \innerproduct{x}{y}^*
        \end{equation}
        for all \(x, y \in \hilbertSpace\).
    \end{dfn}
    
    This means that under time reversal the exponential phase factor transforms as
    \begin{equation}
        \timeReversal \colon \e^{ip\cdot x} \mapsto \e^{(-i)\overbar{p} \cdot (-\overbar{x})} = \e^{i\overbar{p}\cdot\overbar{x}} = \e^{ip \cdot x}.
    \end{equation}
    
    Consider the transformation of a single particle state:
    \begin{equation}
        \timeReversal\ket{\vv{p}} = \ket{-\vv{p}}.
    \end{equation}
    We must therefore have
    \begin{equation}
        \timeReversal a^\hermit(\vv{p}) \timeReversal^\hermit = a^\hermit(-\vv{p}), \qqand \timeReversal a(\vv{p}) \timeReversal^\hermit = a(-\vv{p}).
    \end{equation}
    The field then transforms as
    \begin{align}
        \timeReversal \varphi(x) \timeReversal^\hermit &= \int \invariantmeasure{p} [a(-\vv{p}) \e^{ip\cdot x} + a^\hermit(-\vv{p}) \e^{-ip\cdot x}]\\
        &= \int \invariantmeasure{p} [a(\vv{p}) \e^{i\overbar{p} \cdot x} + a(\vv{p})\e^{-i\overbar{p}\cdot x}]\\
        &= \int \invariantmeasure{p} [a(\vv{p}) \e^{-ip\cdot (-\overbar{x})} + a(\vv{p})\e^{ip \cdot (-\overbar{x})}]\\
        &= \varphi(-\overbar{x}).
    \end{align}
    Here we changed \(\vv{p}\) to \(-\vv{p}\) in the second line.
    This means the field transforms as
    \begin{equation}
        \timeReversal \colon \varphi(x) \mapsto \timeReversal \varphi(x) \timeReversal^\hermit = \varphi(-\overbar{x}).
    \end{equation}
    
    Again, we can include a phase factor, \(\eta_t\), which is restricted to be \(\pm 1\).
    Also all of this logic applies to both the conjugate field and to complex scalar fields.
    
    \subsection{Charge Conjugation}
    Classically charge conjugation changes the sign of electric charge.
    So
    \begin{equation}
        \chargeConjugation \colon Q \mapsto -Q.
    \end{equation}
    In quantum field theory this corresponds to exchanging particles and antiparticles.
    Hence, charge conjugation is only interesting for complex scalar fields.
    Charge conjugation acts on the single particle states according to
    \begin{equation}
        \chargeConjugation \ket{\vv{p}, +} = \ket{\vv{p}, -}, \qqand \chargeConjugation \ket{\vv{p}, -} = \ket{\vv{p}, +}.
    \end{equation}
    Exchanging particles and antiparticles is the same as exchanging mode operators \(a \leftrightarrow b\):
    \begin{alignat}{2}
        \chargeConjugation \colon a(\vv{p}) &\mapsto \chargeConjugation a(\vv{p}) \chargeConjugation^\hermit &&= b(\vv{p}),\\
        \chargeConjugation \colon a^\hermit(\vv{p}) &\mapsto \chargeConjugation a^\hermit(\vv{p}) \chargeConjugation^\hermit &&= b^\hermit(\vv{p}),\\
        \chargeConjugation \colon b(\vv{p}) &\mapsto \chargeConjugation b(\vv{p}) \chargeConjugation^\hermit &&= a(\vv{p}),\\
        \chargeConjugation \colon b^\hermit(\vv{p}) &\mapsto \chargeConjugation b^\hermit(\vv{p}) \chargeConjugation^\hermit &&= a^\hermit(\vv{p}).
    \end{alignat}
    The action on the field is then
    \begin{align}
        \chargeConjugation \varphi(x) \chargeConjugation^\hermit = \int \invariantmeasure{p} [b(\vv{p})\e^{-ip\cdot x} + a^\hermit(\vv{p})\e^{ip\cdot x}] = \varphi^\hermit(x),
    \end{align}
    so
    \begin{equation}
        \chargeConjugation\colon \varphi(x) \mapsto \chargeConjugation \varphi(x) \chargeConjugation^\hermit = \varphi^\hermit(x).
    \end{equation}
    Similarly,
    \begin{equation}
        \chargeConjugation\colon \varphi^\hermit(x) \mapsto \chargeConjugation \varphi^\hermit(x) \chargeConjugation^\hermit = \varphi(x).
    \end{equation}
    
    For a complex scalar field we also have the current, \(j^\mu\).
    Acting on this with charge conjugation exchanges \(\varphi^\hermit\) and \(\varphi\), giving
    \begin{align}
        \chargeConjugation j^\mu \chargeConjugation^\hermit &= i \chargeConjugation \normalordering{\varphi^\hermit \partial^\mu \varphi - \varphi \partial^\mu \varphi^\hermit} \chargeConjugation^\hermit\\
        &= i\normalordering{\varphi\partial^\mu \varphi^\hermit - \varphi^\hermit \partial^\mu \varphi}\\
        &= -j^\mu,
    \end{align}
    which is what we would expect.
    Since the total charge is defined as the spatial integral of \(j^0\) we also have
    \begin{equation}
        \chargeConjugation Q \chargeConjugation^\hermit = -Q,
    \end{equation}
    which is the same as the classical action of charge conjugation.
    
    \subsection{Acting on the Lagrangian}
    The free Lagrangian is
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi)(\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2
    \end{equation}
    for a real scalar field, or
    \begin{equation}
        \lagrangianDensity = (\partial_\mu \varphi^\hermit)(\partial^\mu \varphi) - m^2\varphi^\hermit \varphi
    \end{equation}
    for a complex scalar field.
    In either case the the result is invariant under \(\parity\), \(\timeReversal\), and \(\chargeConjugation\).
    
    Now suppose that the interaction Lagrangian is also invariant under \(\parity\), \(\timeReversal\) and \(\chargeConjugation\).
    Then, under \(\parity\) or \(\chargeConjugation\) the amplitude transforms as
    \begin{align}
        \parity \colon \amplitude &\mapsto \amplitude,\\
        \chargeConjugation \colon \amplitude &\mapsto \amplitude.
    \end{align}
    That is, the amplitude is unchanged.
    Under \(\timeReversal\) the amplitude transforms as
    \begin{equation}
        \timeReversal \colon \amplitude \mapsto \amplitude^*.
    \end{equation}
    However, in all three cases \(\abs{\amplitude}^2\) is unchanged and so the physics doesn't change.
    
    Invariance of the interaction term under these symmetries is not a given.
    For example, a \(\varphi^3\) interaction with a pseudoscalar field gains a minus sign under parity.
    However, for the purposes of this course we will only consider theories invariant under \(\parity\), \(\timeReversal\) and \(\chargeConjugation\) separately.
    In \course{Gauge Theories in Particle Physics} we will consider theories which aren't invariant under these symmetries individually, but instead are invariant under the combined \(\chargeConjugation \parity \timeReversal\)
    symmetry.
    
    
    
    
    
    
    
    
    \part{Gaussian Integrals}
    \chapter{Gaussian Integrals}
    This part of the course is full of \define{Gaussian integrals}\index{Gaussian!integral}, so we'll spend some time to get comfortable with them, starting from the simplest one-dimensional case, and then adding in constants, before moving on to the arbitrary dimension case.
    \section{One Dimension}
    \subsection{No Constant Coefficient}
    It is well known that
    \begin{equation}
        Z_{1} \coloneqq \int_{-\infty}^{\infty} \dl{x} \, \exp\left[ -\frac{1}{2}x^2 \right] = \sqrt{2\pi}.
    \end{equation}
    This follows by considering
    \begin{align}
        Z_{1}^2 &= \int_{-\infty}^{\infty} \dl{x} \int \dl{y} \, \exp\left[ -\frac{1}{2}(x^2 + y^2) \right]\\
        &= \int_{0}^{2\pi} \dl{\vartheta} \int_{0}^{\infty} \dl{r} \, r \exp\left[ -\frac{1}{2}r^2 \right]\\
        &= 2\pi \left[ -\exp\left[ -\frac{1}{2}r^2 \right] \right]_{0}^{\infty}\\
        &= 2\pi,
    \end{align}
    and so \(Z_1 = \sqrt{Z_1^2} = 2\pi\).
    
    \subsection{With Coefficient}
    If we include a coefficient, \(a\), with \(\Re(a) > 0\) then we have
    \begin{equation}
        Z_a \coloneqq \int_{-\infty}^{\infty} \dl{x} \, \exp\left[ -\frac{1}{2}ax^2 \right] = \sqrt{\frac{2\pi}{a}},
    \end{equation}
    For \(a \in \reals\) this follows by a change of variables, \(x \mapsto u/\sqrt{a}\), so \(\dl{x} = \dl{u}/\sqrt{a}\), and since \(a > 0\) and so \(\sqrt{a} > 0\) the integration limits are unchanged so we have
    \begin{equation}
        Z_a = \frac{1}{\sqrt{a}} \int_{-\infty}^{\infty} \dl{u} \, \exp\left[ -\frac{1}{2}u^2 \right] = \frac{1}{\sqrt{a}} Z_1 = \sqrt{\frac{2\pi}{a}}.
    \end{equation}
    The same thing holds for \(a \in \complex\) with \(\Re(a) > 0\), but we have to consider some contour integral and its not as straight forward to show.
    
    \subsection{With Linear Term}
    Now consider
    \begin{equation}\label{eqn:gaussian 1d with linear term}
        Z_a(b) \coloneqq \int_{-\infty}^{\infty} \dl{x} \, \exp\left[ -\frac{1}{2}ax^2 + bx \right] = \sqrt{\frac{2\pi}{a}} \exp\left[ -\frac{b^2}{2a} \right].
    \end{equation}
    This is shown by completing the square:
    \begin{equation}
        -\frac{1}{2}ax^2 + bx = -\frac{a}{2}\left( x + \frac{b}{a} \right)^2 + \frac{b^2}{2a}.
    \end{equation}
    Hence,
    \begin{equation}
        Z_a(b) = \int_{-\infty}^{\infty} \dl{x} \, \exp\left[ -\frac{1}{2}a\left( x - \frac{b}{a} \right) \right] \exp\left[ -\frac{b^2}{2a} \right].
    \end{equation}
    Making a change of variables \(x \mapsto u + b/a\), so \(\dl{x} = \dl{u}\) and the integration region is unchanged we get
    \begin{equation*}
        Z_a(b) = \exp\left[ -\frac{b^2}{2a} \right] \int_{-\infty}^{\infty} \dl{u} \, \exp\left[ -\frac{1}{2}au^2 \right] = \exp\left[ -\frac{b^2}{2a} \right] Z_a = \sqrt{\frac{2\pi}{a}} \exp\left[ -\frac{b^2}{2a} \right].
    \end{equation*}
    
    \section{Arbitrary Dimension}
    \subsection{Diagonal Case}
    Let \(x \in \reals^n\), then
    \begin{equation}
        Z_I \coloneqq \int \dl{^nx} \, \exp\left[ -\frac{1}{2}x^\trans x \right] = (2\pi)^{n/2}
    \end{equation}
    where an integral without bounds is understood to be over all of \(\reals^n\).
    This follows since \(x^\trans x = x_ix_i\) and we then can factorise this into a product of \(n\) one-dimensional Gaussian integrals, each giving a factor of \(\sqrt{2\pi}\):
    \begin{equation}
        Z_I = (Z_1)^n = (2\pi)^{n/2}.
    \end{equation}
    
    Now let \(A \in \matrices{n}{\complex}\) be diagonal with \(\Re(A_{ij}) > 0\) for all \(i, j = 1, \dotsc, n\).
    Then we have
    \begin{equation}
        Z_A \coloneqq \int \dl{^nx} \, \exp\left[ -\frac{1}{2}x^\trans A x \right] = (2\pi)^{n/2} \left( \prod_{i=1}^{n} \frac{1}{\sqrt{A_{ij}}} \right).
    \end{equation}
    This follows since \(x^\trans Ax = x_iA_{ij}x_j\), and for \(A\) diagonal we have \(x^\trans Ax = x_i^2A_{ii}\), so \(Z_A\) factorises into a product of one-dimensional Gaussian integrals:
    \begin{equation}
        Z_A = \prod_{i = 1}^{n} Z_{A_{ii}} = (2\pi)^{n/2} \frac{1}{\sqrt{A_{ij}}}.
    \end{equation}
    Now notice that for a diagonal matrix
    \begin{equation}
        \det A = \prod_{i = 1}^{n} A_{ii},
    \end{equation}
    so
    \begin{equation}
        Z_A = (2\pi)^{n/2} (\det A)^{-1/2}.
    \end{equation}
    
    \subsection{Diagonalisable Case}
    Suppose that instead of being diagonal \(A\) is symmetric, and hence diagonalisable.
    That is, there exists some \(D \in \specialOrthogonal(n)\) such that \(A' = D^{-1}AD\) is diagonal.
    Consider the change of variables \(x \mapsto Dx\), and \(x^\trans \mapsto x^\trans D^\trans = x^{\trans}D^{-1}\).
    Since \(\det D = 1\) we have \(\dl{^nx} \mapsto \det(D) \dl{^nx} = \dl{^nx}\).
    Under this transformation we then have
    \begin{align}
        Z_A &= \int \dl{^nx} \, \exp\left[ -\frac{1}{2}x^\trans Ax \right]\\
        &= \int \dl{^nx} \, \exp\left[ -\frac{1}{2}x^\trans D^{-1}ADx \right]\\
        &= (2\pi)^{n/2}(\det(D^{-1}AD))^{-1/2}\\
        &= (2\pi)^{n/2} (\det A)^{-1/2},
    \end{align}
    having used
    \begin{equation*}
        \det(D^{-1}AD) = \det(D^{-1})\det(A)\det(D) = \det(D)^{-1}\det(A)\det(D) = \det A.
    \end{equation*}
    
    This shows that if \(A \in \matrices{n}{\complex}\) is a symmetric matrix with \(\Re(A_{ij}) \ge 0\) for all \(i, j = 1, \dotsc, n\) and the eigenvalues of \(A\), call them \(a_i\), are nonzero, which guarantees that \(\det A \ne 0\), then
    \begin{equation}
        Z_A \coloneq \int \dl{^nx} \, \exp\left[ -\frac{1}{2}x^\trans Ax \right] = (2\pi)^{n/2} (\det A)^{-1/2}.
    \end{equation}
    
    \subsection{With Linear Term}
    Now let \(b \in \reals^n\) and consider
    \begin{equation}
        Z_A(b) \coloneqq \int \dl{^nx} \, \exp\left[ -\frac{1}{2} x^\trans Ax + b^\trans x \right].
    \end{equation}
    Consider the change of variables
    \begin{equation}
        x = u + \Delta b
    \end{equation}
    where \(\Delta = A^{-1}\).
    We then have \(\dl{^nx} = \dl{^nu}\), and there is no change to the region of integration.
    Note that since \(A\) is symmetric so is \(\Delta\).
    We then have
    \begin{align}
        -\frac{1}{2}x^\trans A x &= -\frac{1}{2}(u^\trans + b^\trans \Delta^\trans) A (u + \Delta b)\\
        &= -\frac{1}{2}(u^\trans A u + u^\trans A\Delta b + b^\trans \Delta^\trans A u + b^\trans \Delta^\trans A \Delta b).
    \end{align}
    Now we can use \(\Delta^\trans = \Delta\) and \(u^\trans b = u_ib_i = b^\trans u\) to get
    \begin{align}
        -\frac{1}{2} x^\trans A x &= -\frac{1}{2}(u^\trans A u + u^\trans b + b^\trans u + b^\trans \Delta b)\\
        &= -\frac{1}{2}u^\trans A u - b^\trans u - b \Delta b.
    \end{align}
    Similarly,
    \begin{equation}
        b^\trans x = b^\trans u + b^\trans \Delta b,
    \end{equation}
    and so
    \begin{align}
        -\frac{1}{2}x^\trans A x + b^\trans x &= -\frac{1}{2}u^\trans A u - b^\trans u - \frac{1}{2} b^\trans \Delta b + b^\trans u + b^\trans \Delta b\\
        &=  -\frac{1}{2} u^\trans A u + \frac{1}{2} b^\trans \Delta b.
    \end{align}
    Hence, we have
    \begin{equation}
        \exp\left[ -\frac{1}{2}x^\trans A x + b^\trans x \right] = \exp[-\frac{1}{2} u^\trans A u] \exp\left[ \frac{1}{2} b^\trans \Delta b \right].
    \end{equation}
    Note that these are just exponentials of numbers, so everything commutes and the usual exponential laws apply.
    Finally, we have
    \begin{align}
        Z_A(b) &= \int \dl{^nx} \, \exp\left[ -\frac{1}{2}x^\trans A x + b^\trans x \right]\\
        &= \exp\left[ \frac{1}{2}b^\trans \Delta b \right] \int \dl{^nu} \, \exp\left[ -\frac{1}{2}u^\trans A u \right]\\
        &= Z_A\exp\left[ \frac{1}{2}b^\trans \Delta b \right]\\
        &= (2\pi)^{n/2} (\det A)^{-1/2} \exp\left[ -\frac{1}{2}b^\trans \Delta b \right].
    \end{align}
    
    \section{Generating Function}
    Let \(\mu\) be a measure on \(\reals^n\).
    For our purposes a measure, \(\mu\), is something such that \(\dl{\mu(x)} = \dl{^nx}\, \Omega(x)\) where \(\Omega\) is some integrable function.
    We define the \define{expectation value}\index{expectation value!with respect to a measure} of \(F \colon \reals^n \to \reals\) with respect to \(\mu\) to be
    \begin{equation}
        \expected{F}_\mu \coloneqq \int \dl{\mu(x)} \, F(x) = \int \dl{^nx} \, \Omega(x) F(x).
    \end{equation}
    
    We assume that the measure is normalised such that
    \begin{equation}
        \int \dl{\mu(x)} = 1.
    \end{equation}
    This allows for a probability interpretation of our results.
    
    We then define the \defineindex{generating function}
    \begin{equation}
        Z_\mu(b) \coloneqq \expected{\e^{b^\trans x}}_\mu = \int \dl{\mu(x)} \, \exp[b^\trans x].
    \end{equation}
    We can expand the exponential in a series to get
    \begin{equation}
        \exp[b^\trans x] = \sum_{\ell = 0}^{\infty} \frac{1}{\ell!} (b^\trans x)^n = \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} b_{i_1}\dotsm b_{i_\ell} x_{i_1} \dotsm x_{i_\ell}.
    \end{equation}
    Applying this to the generating function, and realising that we can use the linearity of the integral to pull out all the sums and factors of \(b_i\), we have
    \begin{equation}
        Z_\mu(b) = \sum_{\ell = 0}^{\infty} \frac{1}{\ell!} \sum_{i_1, \dotsc, i_\ell = 1}^{n} b_{i_1} \dotsm b_{i_\ell} \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu
    \end{equation}
    We call
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu = \int \dl{\mu(x)} \, x_{i_1} \dotsm x_{i_\ell}
    \end{equation}
    the \defineindex{correlation function} of \(x\) with respect to the measure \(\mu\).
    In particular it's called the \(\ell\)-point correlation function, since it has \(\ell\) points, \(x_{i_1}\) through \(x_{i_\ell}\).
    
    From this work we can see that the generating function is simply a polynomial in \(b_i\), after computing the integrals in the correlation functions they are just numbers.
    We can then take derivatives to get
    \begin{equation}
        \diffp{}{_k} Z_\mu(b) = \int \dl{\mu(x)} \, \diffp{}{b_k} \exp[b_ix_i] = \int \dl{\mu(x)} \, x_k \exp[b_ix_i].
    \end{equation}
    This allows us to write the correlation function as
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu = \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}} Z_\mu(b) \bigg|_{b = 0},
    \end{equation}
    since \(\exp[b^\trans x]|_{b = 0} = 1\).
    
    We can use this same method to find the expectation value of any function with a Taylor series.
    If
    \begin{equation}\label{eqn:taylor series}
        F(x) = \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} F_{i_1, \dotsc, i_{\ell}} x_{i_1} \dotsm x_{i_\ell}
    \end{equation}
    for some constants \(F_{i_1, \dotsc, i_{\ell}}\), then
    \begin{equation}
        \expected{F}_\mu = \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} F_{i_1, \dotsc, i_{\ell}} \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu.
    \end{equation}
    
    \section{Gaussian Generating Functions}
    Consider the \define{Gaussian measure}\index{Gaussian!measure}
    \begin{equation}
        \dl{\mu_0(x)} \coloneqq \dl{^nx} \, \Omega_0(x) = \dl{^n x} \, \symcal{N}_0 \exp\left[ -\frac{1}{2} x^\trans A x \right]
    \end{equation}
    where \(A \in \matrices{n}{\complex}\) is a symmetric matrix with \(\Re(A_{ij}) \ge 0\) for all \(i, j = 1, \dotsc, n\) and with nonzero eigenvalues, and \(\symcal{N}_0\) is the normalisation factor
    \begin{equation}
        \symcal{N}_0 = (2\pi)^{-n/2} (\det A)^{1/2} = Z_A^{-1}
    \end{equation}
    which ensures that
    \begin{equation}
        \int \dl{\mu_0(x)} = 1.
    \end{equation}
    
    We can then define the \define{Gaussian generating function}\index{Gaussian!generating function},
    \begin{equation}
        Z_0(b) = \frac{Z_A(b)}{Z_A} = \exp\left[ \frac{1}{2} b^\trans \Delta b \right].
    \end{equation}
    Then the \define{Gaussian correlation function}\index{Gaussian!correlation function} is
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_0 \coloneqq \expected{x_{i_1} \dotsm x_{i_\ell}}_{\mu_0} = \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}} \exp\left[ \frac{1}{2}b^\trans \Delta b \right]\bigg|_{b = 0}.
    \end{equation}
    
    If \(F\) is an arbitrary function with a Taylor series as in \cref{eqn:taylor series} then
    \begin{align}
        \expected{F}_0 &= \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_\ell = 1}^{n} F_{i_1, \dotsc, i_{\ell}} \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_{\ell}}} \exp\left[ \frac{1}{2} b^\trans \Delta b \right] \bigg|_{b = 0}\\
        &= F(\diffp{}/{b}) \exp\left[ \frac{1}{2} b^\trans \Delta b \right]\bigg|_{b = 0}
    \end{align}
    where \(F(\diffp{}/{b})\) is a formal operator expression given by \enquote{evaluating} the Taylor series for \(F\) at the partial derivatives:
    \begin{equation}
        F(\diffp{}/{b}) = \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} F_{i_1, \dotsc, i_{\ell}} \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}}.
    \end{equation}
    
    \subsection{One Point Gaussian Correlator}
    Consider the one point Gaussian correlation function,
    \begin{equation}
        \expected{x_k}_0 = \diffp{}{b_k} \expected{\exp[b^\trans x]}_0 \bigg|_{b = 0} = \diffp{}{b_k} \exp\left[ \frac{1}{2}b^\trans \Delta b \right].
    \end{equation}
    We have
    \begin{equation}
        \diffp{}{b_k} \exp\left[ \frac{1}{2}b_i\Delta_{ij}b_j \right] = \frac{1}{2}[\Delta_{kj}b_j + b_i\Delta_{ik}]\exp[b^\trans \Delta b] = \Delta_{kj}b_j \exp[b^\trans \Delta b],
    \end{equation}
    where we've used the symmetry of \(\Delta\) and \(\diffp{b_i}/{b_k} = \delta_{ik}\).
    We then have
    \begin{equation}
        \expected{x_k}_0 = (\Delta_{kj}b_j) \exp\left[ \frac{1}{2}b^\trans \Delta b \right]\bigg|_{b = 0} = 0.
    \end{equation}
    We could have realised that this must vanish since our integrand is an odd function of \(x\) over an even range.
    We can interpret the one point correlation as the mean of a normally distributed value.
    
    \subsection{Two Point Gaussian Correlator}
    Consider the two point Gaussian correlation function,
    \begin{equation}
        \expected{x_kx_l}_{0} = \diffp{}{b_k}\diffp{}{b_l} \expected{\exp[b^\trans x]}_0 \bigg|_{b = 0}.
    \end{equation}
    We've already done the first derivative, so we just have to compute
    \begin{align}
        \expected{x_kx_l}_0 &= \diffp{}{b_l} \left( (\Delta_{kj b_j})\exp\left[ \frac{1}{2}b^\trans \Delta b \right] \right)\bigg|_{b = 0}\\
        &= [\delta_{kkj}\Delta_{kj} + (\Delta_{kj}b_j)(\Delta_{li}b_i)] \exp\left[ \frac{1}{2}b^\trans b \right]\bigg|_{b = 0}\\
        &= \Delta_{kl}.
    \end{align}
    We can interpret the two point correlation as the covariance of two normally distributed values.
    
    \subsection{General Gaussian Correlator}
    Now consider the \(\ell\)-point Gaussian correlation function
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_0 = \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}} \expected{\exp[b^\trans x]}_0 \bigg|_{b = 0}.
    \end{equation}
    In general the derivatives will either act on the exponential term, giving a factor of \(\Delta b\), or they can act on an existing \(\Delta b\) term giving a \(\Delta\) term.
    The only nonzero terms after setting \(b = 0\) are those where each \(\Delta b\) has been acted on by a derivative to remove the \(b\) dependence.
    This requires an even number of derivatives to get a nonzer result.
    In the case when \(\ell\) is even the process for determining the result is as follows:
    \begin{itemize}
        \item Write all pairings \((i_p, i_q)\), which can be formed from the indices \(i_1, \dotsc, i_\ell\).
        Treat \((i_p, i_q)\) and \((i_q, i_p)\) as the same and no index can be in a pair with itself.
        \item Build the set, \(P\), of all sets of pairings, so the elements of \(P\) are sets of \(\ell/2\) pairs such that every index appears in exactly one pair.
        \item The correlator is given by
        \begin{equation}
            \expected{x_{i_1}\dotsm x_{i_\ell}}_0 = \sum_{P} \Delta_{i_p i_q} \dotsm \Delta_{i_ri_s}.
        \end{equation}
    \end{itemize}
    This is \defineindex{Wick's theorem}.
    
    To make this clearer lets consider some examples.
    First, the two point correlator, \(\expected{x_{i_1}x_{i_2}}_0\).
    We have two indices, \(i_1\) and \(i_\ell\), so there is a single pairing, \((i_1, i_2)\), and we have
    \begin{equation}
        \expected{x_{i_1}x_{i_2}}_0 = \Delta_{i_1i_2}.
    \end{equation}
    We can also write this as
    \begin{equation}
        \expected{x_{i_1}x_{i_2}}_0 = \wick{\c x_{i_1} \c x_{i_2}}
    \end{equation}
    where
    \begin{equation}
        \wick{\c x_{i_1} \c x_{i_2}} \coloneqq \Delta_{i_1i_2}.
    \end{equation}
    
    Now consider the four point correlator, \(\expected{x_{i_1}x_{i_2}x_{i_3}x_{i_4}}_0\).
    The set of possible pairings is
    \begin{equation}
        P = \{\{(i_1, i_2), (i_3, i_4)\}, \{(i_1, i_3), (i_2, i_4)\}, \{(i_1, i_4), (i_2, i_3)\}\}.
    \end{equation}
    The correlator is then
    \begin{align}\label{eqn:4 point correlator}
        \expected{x_{i_1}x_{i_2}x_{i_3}x_{i_4}}_0 &= \Delta_{i_1i_2}\Delta_{i_3i_4} + \Delta_{i_1i_3}\Delta_{i_2i_3} + \Delta_{i_1i_4}\Delta_{i_2i_3}\\
        &= \wick{\c x_{i_1} \c x_{i_2}} \wick{\c x_{i_3} \c x_{i_4}} + \wick{\c1 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c2 x_{i_4}} + \wick{\c2 x_{i_1} \c1 x_{i_2} \c1 x_{i_3} \c2 x_{i_4}}.
    \end{align}
    
    Finally, lets do the six point correlation function.
    We'll jump straight to the contractions:
    \begin{align}
        \expected{x_{i_1}x_{i_2}x_{i_3}x_{i_4}x_{i_5}x_{i_6}} &=
        \wick{\c x_{i_1} \c x_{i_2}} \wick{\c x_{i_3} \c x_{i_4}} \wick{\c x_{i_5} \c x_{i_6}}
        + \wick{\c x_{i_1} \c x_{i_2}} \wick{\c1 x_{i_3} \c2 x_{i_4} \c1 x_{i_5} \c2 x_{i_6}}\\
        &+ \wick{\c x_{i_1} \c x_{i_2}} \wick{\c2 x_{i_3} \c1 x_{i_4} \c1 x_{i_5} \c2 x_{i_6}}
        + \wick{\c1 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c2 x_{i_4}} \wick{\c x_{i_5} \c x_{i_6}}\\
        &+ \wick{\c1 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c1 x_{i_6}}
        + \wick{\c1 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c1 x_{i_5} \c2 x_{i_6}}\\
        &+ \wick{\c2 x_{i_1} \c1 x_{i_2} \c1 x_{i_3} \c2 x_{i_4}} \wick{\c x_{i_5} \c x_{i_6}}
        + \wick{\c1 x_{i_1} \c2 x_{i_2} \c3 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c3 x_{i_6}}\\
        &+ \wick{\c1 x_{i_1} \c3 x_{i_2} \c2 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c3 x_{i_6}}
        + \wick{\c2 x_{i_1} \c1 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c1 x_{i_6}}\\
        &+ \wick{\c2 x_{i_1} \c1 x_{i_2} \c3 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c3 x_{i_6}}
        + \wick{\c1 x_{i_1} \c3 x_{i_2} \c2 x_{i_3} \c2 x_{i_4} \c1 x_{i_5} \c3 x_{i_6}}\\
        &+ \wick{\c2 x_{i_1} \c1 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c1 x_{i_5} \c2 x_{i_6}}
        + \wick{\c3 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c2 x_{i_4} \c1 x_{i_5} \c3 x_{i_6}}\\
        &+ \wick{\c3 x_{i_1} \c2 x_{i_2} \c1 x_{i_3} \c1 x_{i_4} \c2 x_{i_5} \c3 x_{i_6}}
    \end{align}
    
    \begin{cde}{}{}
        Here's some Mathematica code to generate all pairs.
        \textcolor{Red}{WARNING:} Horribly inefficient code!
        It generates all permutations of the indices, groups them into pairs, then eliminates repeats, so its \(\order(\ell!)\) in memory, I can't even run it for \(\ell = 12\).
        \begin{lstlisting}[gobble=12, language=mathematica, mathescape]
            Union[Sort /@ 
              Union[Sort /@ Partition[#[[1]], 2] & /@ 
               Table[
                 {#[[i]], #[[i + 1]]}, {i, 1, Length[#], 2}] &@
               Permutations[Range[$\ell$]]
            (* replace $\textcolor{codeCommentColor}{\ell}$ with number of indices (even) *)
        \end{lstlisting}
    \end{cde}
    
    As you can see the number of indices grows quickly.
    Suppose we have \(2m\) indices for some integer \(m\).
    Then there are \(2m - 1\) options for what to pair the first variable with, since it can't be paired with itself.
    Then take the next unpaired variable, there are \(2m - 3\) variables to pair it with, since it can't be paired with itself or either of the already paired variables.
    The next unpaired variable has \(2m - 5\) potential pairings, and so on, until we have four variables left, one of these has three options for pairing, and then we're left with two variables which we have to pair.
    That is, the number of pairings is given by
    \begin{equation}
        \text{number of pairings} = (2m - 1)(2m - 3) \dotsm 3 \dotsm 1 = (2m - 1)!!
    \end{equation}
    where \(!!\) is the double factorial, defined recursively as \(n!! = n(n - 2)!!\) with \(1!! = 0!! = 1\).
    Here are the number of pairings for the first few nonzero correlators:
    \begin{equation*}
        \begin{array}{rrrrrrrrrr}
            2 & 4 & 6  & 8   & 10  & 12    & 14     & 16      & 18       & 20\\
            1 & 3 & 15 & 105 & 945 & \num{10395} & \num{135135} & \num{2027025} & \num{34459425} & \num{654729075}\\
        \end{array}
    \end{equation*}
    So its fair to say that evaluating general correlators with more than a couple of points quickly becomes intractable.

    \chapter{Perturbed Correlators}
    \section{General Theory}
    As with the Gaussian case we can write the expected value of any analytic function, \(F\), with respect to some measure, \(\mu\), as a sum over correlators with respect to \(\mu\):
    \begin{align}
        \expected{F}_\mu &= \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_\ell = 1}^{n} F_{i_1\dotso i_{\ell}} \expected{x_{i_1} \dotsm x_{i_\ell}}_\mu\\
        &= \sum_{\ell = 0}^{\infty} \sum_{i_1, \dotsc, i_{\ell} = 1}^{n} F_{i_1\dotso i_\ell} \diffp{}{b_{i_1}} \dotsm \diffp{}{b_{i_\ell}} Z_\mu(b) \bigg|_{b = 0}\\
        &= F(\diffp{}/{b}) Z_\mu(b)|_{b = 0}.
    \end{align}
    
    We now consider a slightly more general measure, \(\dl{\mu(x)} = \dl{^nx} \,\Omega(x)\) where
    \begin{equation}
        \Omega(x) = \frac{1}{Z_\lambda} \e^{-S_\lambda(x)}
    \end{equation}
    where
    \begin{equation}
        S_\lambda(x) = S_0(x) + \lambda V(x)
    \end{equation}
    with \(S_0(x) = x^\trans Ax/2\), giving the usual Gaussian measure, and \(V\) being some \enquote{potential} acting as a perturbation.
    We will assume \(\lambda\) is small and consider expansions in \(\lambda\).
    The factor \(Z_\lambda\) is given by
    \begin{align}
        Z_\lambda &= \int \dl{^nx}  \, \exp[-S_\lambda(x)]\\
        &= \int \dl{^nx} \, \exp[-S_0(x)]\exp[-\lambda V(x)]\\
        &= Z_0 \expected{\exp[-\lambda V(x)]}_0.
    \end{align}
    We will be interested in computing the value of
    \begin{equation}
        \frac{Z_\lambda}{Z_0} = \expected{\exp[-\lambda V(x)]}_0 = \sum_{k = 0}^{\infty} \frac{(-\lambda)^k}{k!} \expected{V(x)^k}_0.
    \end{equation}
    We can work out the expectation value once we know \(V\) by expanding \(V\).
    
    \section{\texorpdfstring{\(x^4\)}{x to the four} Potential}
    \epigraph{The first instant of a long series of asking \enquote{how do we count?}}{Luigi Del Debbio}
    Consider the example
    \begin{equation}
        V(x) = \frac{1}{4!} \sum_{i = 1}^n x_i^4.
    \end{equation}
    We can quickly write down a series for \(Z_\lambda/Z_0\) in terms of correlators:
    \begin{equation}
        \frac{Z_\lambda}{Z_0} = 1 - \lambda \frac{1}{4!} \sum_i \expected{x_i^4}_0 + \frac{\lambda^2}{2}\frac{1}{(4!)^2} \sum_i \sum_j \expected{x_i^4 x_j^4}_0 + \order(\lambda^3).
    \end{equation}
    Now we just need to compute the correlators.
    
    Starting with \(\expected{x_i^4}_0\).
    This is just a special case of the four point correlator in \cref{eqn:4 point correlator} where we set all indices equal.
    This means that we have
    \begin{align}
        \expected{x_ix_ix_ix_i}_0 &= \Delta_{ii}\Delta_{ii} + \Delta_{ii}\Delta_{ii} + \Delta_{ii}\Delta_{ii}\\
        &= \wick{\c x_i \c x_i} \wick{\c x_i \c x_i} + \wick{\c1 x_i \c2 x_i \c1 x_i \c2 x_i} + \wick{\c2 x_i \c1 x_i \c1 x_i \c2 x_i}\\
        &= 3 \Delta_{ii}^2.
    \end{align}
    Notice that we could have worked this out without needing to do the full four point correlator for arbitrary indices, we just need to find a way to count the number of pairings.
    In this case there are 3 choices for what to pair the first \(x_i\) with, then the two remaining \(x_i\)s must be paired, so there are 3 terms, and all three are the same.
    
    This is good because as we saw for the six point correlator things quickly get out of hand, and the next term involves an 8 point correlator, which would be even worse at \((8 - 1)!! = 105\) terms.
    So, what sort of pairings are there going to be in this correlator?
    First there are going to be pairings akin to two copies of the four point correlator, with all \(x_i\)s paired with \(x_i\)s, and all \(x_j\)s paired with \(x_j\)s.
    There are nine ways to do this, getting a factor of three from pairing up \(x_i\)s and a factor of three from pairing up \(x_j\)s.
    This means we have the term \(9\Delta_{ii}^2\Delta_{jj}^2\).
    
    The next logical term to consider is one which pairs one \(x_i\) with one \(x_j\), however, if we do so we're left with an odd number of \(x_i\)s, so another of these must pair with an \(x_j\).
    So, our next term pairs two \(x_i\)s with two \(x_j\)s, and then the remaining \(x_i\)s are paired with each other and likewise for the remaining \(x_j\)s.
    How many ways are there to do this?
    There are actually (at least) two ways to count this.
    First, we have \(\binom{4}{2}\) ways to choose the two \(x_i\) to pair with \(x_j\)s.
    Then we have four choices of which \(x_j\) to pair with the first and 3 choices of which \(x_j\) to pair with the second, for a total of \(\binom{4}{2} \cdot 4 \cdot 3 = 72\) terms.
    The second way to count starts with choosing two \(x_i\)s, again giving a factor of \(\binom{4}{2}\), and then choosing two \(x_j\)s, for another factor of \(\binom{4}{2}\).
    We can then pair up between these pairs in two ways, pairing the first of each pair, or the first of the \(x_i\)s and the second of the \(x_j\)s, for a factor of 2, giving \(\binom{4}{2}^2 \cdot 2 = 72\).
    Hence, we have the term \(72\Delta_{ij}^2\Delta_{ii}\Delta_{jj}\).
    
    There's no way to pair up exactly three \(x_i\)s with \(x_j\)s, so the only other term is pairing all four \(x_i\)s with an \(x_j\).
    This can be done by holding the \(x_i\) in some fixed order, then considering all permutations of the four \(x_j\)s, and pairing them up in order for each permutation.
    So, we have a factor of \(4! = 24\).
    The final term is then \(24\Delta_{ij}^4\).
    
    At this point there are some checks we can do.
    First, we should, including repeated terms, have 8 indices, and 4 \(\Delta\)s in each term.
    Second, adding up the combinatorial factors should give \(105\), the total number of ways to pair 8 things, and indeed this does work out.
    So, we conclude that
    \begin{equation}
        \expected{x_i^4 x_j^4}_0 = 9\Delta_{ii}^2\Delta_{jj}^2 + 72\Delta_{ij}^2\Delta_{ii}\Delta_{jj} + 24\Delta_{ij}^4.
    \end{equation}
    
    Hence, to second order, we have
    \begin{align}
        \frac{Z_\lambda}{Z_0} &= 1 - \frac{\lambda}{4!} \sum_{i} 3\Delta_{ii}^2 + \frac{\lambda^2}{2 (4!)^2}\sum_{i,j}[9\Delta_{ii}^2\Delta_{jj}^2 + 72\Delta_{ij}^2\Delta_{ii}\Delta_{jj} + 24\Delta_{ij}^4]\\
        &= 1 - \frac{\lambda}{8} \sum_i \Delta_{ii}^2 + \lambda^2\sum_{i,j} \left[ \frac{1}{128}\Delta_{ii}^2\Delta_{jj}^2 + \frac{1}{16}\Delta_{ij}^2\Delta_{ii}\Delta_{jj} + \frac{1}{48}\Delta_{ij}^4 \right]
    \end{align}
    
    \subsection{Diagrammatic Representation}
    There is a diagrammatic representation of terms like this.
    For each propagator, \(\Delta_{ij}\), we draw a line connecting the nodes \(i\) and \(j\):
    \begin{equation}
        \Delta_{ij} = 
        \tikzsetnextfilename{fd-propagator}
        \begin{tikzpicture}[baseline=(i.base)]
            \draw (0, 0) node[left] (i) {\(i\)} -- (1, 0) node[right] {\(j\)};
        \end{tikzpicture}
    \end{equation}
    Note that the position of the nodes doesn't matter, just how they're connected.
    For terms like \(\Delta_{ii}\) with a repeated index we have a sum over \(i\) when these appear.
    As a diagram a repeated index means a closed loop, and so any closed loop implies a sum:
    \begin{equation}
        \sum_i \Delta_{ii} = 
        \tikzsetnextfilename{fd-propgator-repeated-index}
        \begin{tikzpicture}[baseline=(i.base)]
            \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [left] (i) {\(i\)};
        \end{tikzpicture}
    \end{equation}
    Another example might be the term \(\sum_{j}\Delta_{ij}\Delta_{jj}\), which corresponds to the diagram
    \begin{equation}
        \sum_j \Delta_{ij}\Delta_{jj} = 
        \tikzsetnextfilename{fd-propagator-repeated-index-2}
        \begin{tikzpicture}[baseline=(i.base)]
            \draw[looseness=1.5] (0, 0) node[left] (i) {\(i\)} -- (1, 0) to[out=40, in=90] (2, 0) to[out=270, in=-40] (1, 0) node [below] {\(j\)};
        \end{tikzpicture}
    \end{equation}
    Fun Fact™: Diagrams like this one are called tadpole diagrams!
    
    Using this we can write the first order (in \(\lambda\)) \(\Delta_{ii}^2\) term as
    \begin{equation}
        \sum_i \Delta_{ii}^2 = 
        \tikzsetnextfilename{fd-first-order-x4-term}
        \begin{tikzpicture}[baseline={(0, -0.1)}]
            \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(i\)};
            \draw[looseness=1.5, xscale=-1] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0);
        \end{tikzpicture}
    \end{equation}
    Similarly we can write the following
    \begin{equation}
        \expected{x_i^4 x_j^4}_0 = 9\Delta_{ii}^2\Delta_{jj}^2 + 72\Delta_{ij}^2\Delta_{ii}\Delta_{jj} + 24\Delta_{ij}^4.
    \end{equation}
    as
    \begin{equation}
        \sum_{i,j} \expected{x_i^4x_j^4} = 9
        \tikzsetnextfilename{fd-second-order-x4-term-1}
        \begin{tikzpicture}[baseline={(0, -0.6)}]
            \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(i\)};
            \draw[looseness=1.5, xscale=-1] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0);
            \begin{scope}[yshift=-1cm]
                \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(j\)};
                \draw[looseness=1.5, xscale=-1] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0);
            \end{scope}
        \end{tikzpicture}
        + 72 \,
        \tikzsetnextfilename{fd-second-order-x4-term-2}
        \begin{tikzpicture}[baseline={(0, -0.1)}]
            \draw[looseness=1.5] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(i\)};
            \draw[looseness=1.5, xshift=-1cm, xscale=-1] (0, 0) to[out=40, in=90] (1, 0) to[out=270, in=-40] (0, 0) node [below] {\(j\)};
            \draw (0, 0) to[bend right=40] (-1, 0);
            \draw (0, 0) to[bend left=40] (-1, 0);
        \end{tikzpicture}
        + 24 \,
        \tikzsetnextfilename{fd-second-order-x4-term-3}
        \begin{tikzpicture}[baseline=(i.base)]
            \draw (0, 0) node [left] (i) {\(i\)} to[bend right=20] (1, 0) node [right] {\(j\)};
            \draw (0, 0) node [left] (i) {\(i\)} to[bend right=60] (1, 0);
            \draw (0, 0) node [left] (i) {\(i\)} to[bend left=20] (1, 0);
            \draw (0, 0) node [left] (i) {\(i\)} to[bend left=60] (1, 0);
        \end{tikzpicture}
    \end{equation}
    
    \subsection{Computing \texorpdfstring{\(\log(Z_\lambda/Z_0)\)}{log(Zlambda/Z0)}}
    Suppose we want to compute \(\log(Z_\lambda/Z_0)\).
    To do this we notice that \(Z_\lambda/Z_0 = 1 + \order(\lambda)\), and so we can use the Taylor expansion
    \begin{equation}
        \log(1 + \varepsilon) = \sum_{n = 1}^{\infty} \frac{(-1)^{n + 1}}{n}\varepsilon^n \approx \varepsilon - \frac{1}{2}\varepsilon^2 + \frac{1}{3}\varepsilon^3 - \dotsb.
    \end{equation}
    Since we already have \(\varepsilon\) here as a power series in \(\lambda\) we should work this out in powers of \(\lambda\).
    The first order term has only a single contribution, from the first order term in \(\lambda\).
    That is, to first order
    \begin{equation}
        \log\left( \frac{Z_\lambda}{Z_0} \right) = -\frac{\lambda}{8}\sum_i \Delta_{ii}^2 + \order(\lambda^2).
    \end{equation}
    For the second order term there are two contributions, we have the first order term squared, which also picks up a factor of \(-1/2\) from the expansion of the log:
    \begin{equation}
        -\frac{1}{2} \frac{\lambda^2}{8^2} \sum_{i} \Delta_{ii}^2 \sum_j \Delta_{jj}^2 = -\frac{1}{2} \frac{\lambda^2}{128} \sum_{i, j} \Delta_{ii}^2\Delta_{jj}^2.
    \end{equation}
    We also have the first order in \(\varepsilon\) term from the second order in \(\lambda\) term:
    \begin{equation}
        \lambda^2 \sum_{i,j} \left[ \frac{1}{128}\Delta_{ii}^2\Delta_{jj}^2 + \frac{1}{16} \Delta_{ij}^2\Delta_{ii}\Delta_{jj} + \frac{1}{48}\Delta_{ij}^4 \right].
    \end{equation}
    Notice that the first term here cancels with the other \(\lambda^2\) term, to give the second order result
    \begin{equation}
        \log\left( \frac{Z_\lambda}{Z_0} \right) = -\frac{\lambda}{8} \sum_{i} \Delta_{ii}^2 + \lambda^2 \left[ \frac{1}{128}\Delta_{ii}^2\Delta_{jj}^2 + \frac{1}{16}\Delta_{ij}^2 \Delta_{ii} \Delta_{jj} + \frac{1}{48}\Delta_{ij}^4 \right] + \order(\lambda^3).
    \end{equation}
    
    In terms of diagrams, this corresponds to
    \begin{multline}
        \log\left( \frac{Z_\lambda}{Z_0} \right) = -\frac{\lambda}{8} 
        \vcenter{\hbox{\includegraphics{tikz-external/fd-first-order-x4-term}}}\\
        + \frac{\lambda^2}{16} \vcenter{\hbox{\includegraphics{tikz-external/fd-second-order-x4-term-2}}} + \frac{\lambda^2}{48} \vcenter{\hbox{\includegraphics{tikz-external/fd-second-order-x4-term-3}}} + \order(\lambda^3).
    \end{multline}
    Notice that there are no disconnected diagrams, they cancelled out.
    In fact disconnected diagrams will cancel out here to all orders, and this isn't just the case for this potential.
    Taking the log of the generating function yields the generating function of connected contributions.
    We'll prove this later.
    
    \section{Correlators}
    \epigraph{That's it. That's QFT done.}{Luigi Del Debbio}
    Much of the rest of the course will be focused on computing various correlators with respect to some perturbed Gaussian, and of course linking these mathematical expressions to physics.
    We'll often be interested in calculating
    \begin{align}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_\lambda &= \frac{1}{Z_\lambda} \int \dl{^nx} \exp[-S_\lambda(x)] x_{i_1} \dotsm x_{i_\ell}\\
        &= \frac{Z_0}{Z_\lambda} \frac{1}{Z_0} \int \dl{^nx} \exp[-S_\lambda(x)] x_{i_1} \dotsm x_{i_\ell}\\
        &= \frac{Z_0}{Z_\lambda} \frac{1}{Z_0} \int \dl{^nx} \exp[-S_0(x)] \sum_{k = 0}^{\infty} \frac{(-\lambda)^k}{k!} V(x)^k x_{i_1} \dotsm x_{i_\ell}\\
        &= \frac{Z_0}{Z_\lambda} \sum_{k = 0}^{\infty} \frac{(-\lambda)^k}{k!} \expected{V(x)^kx_{i_1} \dotsm x_{i_\ell}}_0.
    \end{align}
    
    The good news is that we've just seen how to compute \(Z_\lambda/Z_0\), so all we need to do is invert this result to get \(Z_0/Z_\lambda\).
    This can be done in a similar way to the logarithm by again noticing that \(Z_\lambda/Z_0 = 1 - \order(\lambda)\) and then using the result
    \begin{equation}
        \frac{1}{1 - \varepsilon} = \sum_{n = 0}^{\infty} \varepsilon^n \approx 1 + \varepsilon + \varepsilon^2 + \dotsb.
    \end{equation}
    
    The results we get will be of the form
    \begin{equation}
        \expected{x_{i_1} \dotsm x_{i_\ell}}_0 = \order(\lambda^0) + \order(\lambda) + \order(\lambda^2) + \dotsb.
    \end{equation}
    The order of \(\lambda^0\) (i.e., constant) term will be the result we get with an unperturbed Gaussian.
    We then have first order corrections, and second order corrections, and so on.
    
    \part{Path Integrals in Quantum Mechanics}
    \chapter{Path Integrals in QM}
    \section{Preliminaries}
    In this chapter we will develop the theory of path integrals in nonrelativistic quantum mechanics.
    This should then rapidly generalise to relativistic quantum field theory, but for now we stick to the nonrelativistic case to avoid unnecessary complications.
    
    Working in quantum mechanics we have a position operator, \(\operator{Q}\), with eigenstates \(\ket{q}\), with eigenvalues \(q\):
    \begin{equation}
        \operator{Q}\ket{q} = q\ket{q}.
    \end{equation}
    We consider a system evolving under the Hamiltonian \(\operator{H}\).
    We will be interested in computing the transition amplitude between two position eigenstates at two different times.
    That is, we want to compute
    \begin{equation}
        \braket{q', t'}{q, t}
    \end{equation}
    where \(t' > t\).
    This amplitude can be computed in terms of the states \(\ket{q}\) and \(\ket{q'}\) and the time evolution operator:
    \begin{equation}
        \braket{q', t'}{q, t} = \bra{q'} \e^{-i\operator{H}(t' - t)}\ket{q}.
    \end{equation}
    This is where path integrals come in.
    
    \section{Constructing the Path Integral}
    \epigraph{We assume this limit exists, and I have no intrest in proving it does.}{Luigi Del Debbio}
    Let \(T = t' - t\), so we're computing
    \begin{equation}
        \braket{q', t'}{q, t} = \bra{q'} \e^{-i\operator{H}T}\ket{q}.
    \end{equation}
    We can split up the time interval between \(t\) and \(t'\) into \(n - 1\) steps of length \(\varepsilon = T/n\).
    Define \(t_k = t + k\varepsilon\), so \(t_0 = t\) and \(t_n = t + n\varepsilon = t + T = t'\).
    We can then take our single factor of \(\exp[-i\operator{H}T]\) and split it into \(n\) factors of \(\exp[-i\operator{H}\varepsilon]\):
    \begin{equation}
        \braket{q', t'}{q, t} = \bra{q'} \underbrace{\e^{-i\operator{H}\varepsilon} \dotsm \e^{-i\operator{H}\varepsilon}}_{n \text{ factors}} \ket{q}.
    \end{equation}
    
    Now take the completeness relation,
    \begin{equation}
        \int \dl{q} \, \ket{q} \bra{q} = \ident.
    \end{equation}
    We can insert this \(n - 1\) times between each of the factors of \(\exp[-i\operator{H}t]\) giving
    \begin{equation}
        \int \left( \prod_{k = 1}^{n - 1} \dl{q_k} \right) \bra{q} \e^{-i\operator{H}\varepsilon} \ket{q_{n-1}} \bra{q_{n-1}} \e^{-i\operator{H}\varepsilon} \ket{q_{n-2}}\bra{q_{n-2}} \dotsm \ket{q_1} \bra{q_1} \e^{-i\operator{H}\varepsilon} \ket{q}.
    \end{equation}
    Now suppose that \(\varepsilon\) is small, or equivalently \(n\) is large.
    We can expand the exponential:
    \begin{equation}
        \e^{-i\operator{H}\varepsilon} = 1 - i\varepsilon\operator{H} + \order(\varepsilon^2).
    \end{equation}
    
    Suppose we have the Hamiltonian
    \begin{equation}
        \operator{H} = \frac{\operator{P}^2}{2} + V(\operator{Q})
    \end{equation}
    where we choose units such that the mass is unity.
    First consider the terms with the potential, here we simply replace the argument of \(V\) with the appropriate position, using \(V(\operator{Q}) \ket{q} = \sum_{\ell = 1}^{\infty} V^{(\ell)}(0) \operator{Q}^\ell \ket{q} = \sum_{\ell = 1}^{\infty} V^{(\ell)}(0)q^\ell = V(q)\ket{q}\), and so
    \begin{align}
        \bra{q_k} V(\operator{Q}) \ket{q_{k - 1}} &= V(q_{k-1}) \braket{q_k}{q_{k-1}}\\
        &= V(q_{k - 1})\delta(q_k - q_{k-1})\\
        &= V\left( \frac{q_k + q_{k-1}}{2} \right)\delta(q_k - q_{k-1})
    \end{align}
    where in the last step we note that for nonzero solutions we have \(q_k = q_{k - 1}\), and in this case \((q_k + q_{k-1})/2 = q_k\).
    This choice is required to make things converge later.
    We can then insert the integral representation of the Dirac delta,
    \begin{equation}
        \delta(q - q') = \int \frac{\dl{p}}{2\pi} \e^{ip(q - q')},
    \end{equation}
    giving
    \begin{equation}
        \bra{q_k} V(\operator{Q}) \ket{q_{k-1}} = \int \frac{\dl{p_k}}{2\pi} V\left( \frac{q_k + q_{k-1}}{2} \right) \e^{ip_k(q_k - q_{k-1})}.
    \end{equation}
    
    For the momentum term we can insert a complete set of states,
    \begin{equation}
        \int \dl{p} \ket{p}\bra{p} = \ident,
    \end{equation}
    giving
    \begin{align}
        \frac{1}{2}\bra{q_k} \operator{P}^2 \ket{q_{k-1}} &= \int \dl{p_k} \, \frac{1}{2} \bra{q_k}\operator{P}^2 \ket{p_k}\braket{p_k}{q_{k-1}}\\
        &= \int \dl{p_k} \, \frac{p_k^2}{2} \braket{q_k}{p_k} \braket{p_k}{q_{k-1}}\\
        &= \int \frac{\dl{p_k}}{2\pi} \, \frac{p_k^2}{2} \e^{ip_k(q_k - q_{k-1})}
    \end{align}
    where in the last step we've recognised the amplitudes as the wave function of the state momentum eigenstate \(\ket{p_k}\) in the position basis,
    \begin{equation}
        \braket{q}{p} = \frac{1}{\sqrt{2\pi}} \e^{ipq}.
    \end{equation}
    
    We can take this and go back to writing \(1 - i\varepsilon\operator{H}\) as \(\exp[-i\operator{H}\varepsilon]\), and the results will agree to first order in \(\varepsilon\), that is
    \begin{multline}
        \bra{q_k} \e^{-i\operator{H}\varepsilon} \ket{q_{k-1}} =\\
        \int \frac{\dl{p_k}}{2\pi} \exp\left\{ -i\varepsilon \left[ \frac{p_k^2}{2} + V\left( \frac{q_k + q_{k-1}}{2} \right) \right] \right\} \exp[ip_k(q_k - q_{k-1})] + \order(\varepsilon^2).
    \end{multline}
    We can recognise the term in the first exponential as the Hamiltonian evaluated at \(\tilde{q}_k = (q_k + q_{k-1})/2\) and \(p_k\), allowing us to rewrite this as
    \begin{equation}
        \bra{q_k} \e^{-i\operator{H}\varepsilon}\ket{q_{k-1}} = \int \frac{\dl{p_k}}{2\pi} \exp[-i\varepsilon H(\tilde{q}_k, p_k)]\exp[ip_k(q_k - q_{k-1})] + \order(\varepsilon^2).
    \end{equation}
    Now combine the two exponentials and factor out \(i\varepsilon\):
    \begin{equation}
        \bra{q_k} \e^{-i\operator{H}\varepsilon}\ket{q_{k-1}} = \int \frac{\dl{p_k}}{2\pi} \exp\left\{ i\varepsilon\left[ p_k\frac{q_k - q_{k-1}}{\varepsilon} - H(\tilde{q}_k, p_k) \right] \right\} + \order(\varepsilon^2).
    \end{equation}
    
    We can now put all of these factors together to get the desired result, although we have to take the limit of \(n \to \infty\), so \(\varepsilon \to 0\), in order to make it exact.
    We end up with a product of the above terms, with the product of exponentials becoming a sum in the exponential.
    We should take the limit in such a way that \(T = \varepsilon n\) is constant.
    The result is
    \begin{gather}
        \braket{q', t'}{q, t} =\\
        \lim_{n\to \infty} \int \left( \prod_{k=1}^{n-1} \dl{q_k} \right) \left( \prod_{j=1}^{n} \frac{\dl{p_j}}{2\pi} \right) \exp\left\{ i\varepsilon \sum_{m=1}^{n} \left[ p_m\frac{q_m - q_{m-1}}{\varepsilon} - iH(\tilde{q}_m, p_m) \right] \right\}.\notag
    \end{gather}
    We assume that this limit exists.
    In the limit the sum becomes an integral and \((q_m - q_{m-1})/\varepsilon\) becomes the velocity, the distinction between \(q_m\) and \(\tilde{q}_m\) is not important in the limit.
    We introduce the following shorthand for this expression:
    \begin{equation}
        \braket{q', t'}{q, t} = \int \DL{q} \DD{p} \exp\left\{ i\int_{t}^{t'} \!\! \dl{\tau} \, \left[ p(\tau)\dot{q}(\tau) - H(q(\tau), p(\tau)) \right] \right\}.
    \end{equation}
    This is the \defineindex{path integral} form of the amplitude.
    Note that we absorb the \(1/(2\pi)\) factor for each momentum integral into the measure.
    
    \subsection{Quadratic Momentum}
    For Hamiltonians, like the one we had above, which are quadratic in the momentum we can partially evaluate this integral.
    Consider the integral
    \begin{equation*}
        \int \frac{\dl{p_k}}{2\pi} \exp\left[ ip_k(q_k - q_{k-1}) - i\varepsilon\frac{p_k^2}{2} \right] = (2\pi i\varepsilon)^{-1/2} \exp\left[ i\varepsilon\left( \frac{q_k - q_{k-1}}{\varepsilon} \right)^2 \right],
    \end{equation*}
    where we've used \cref{eqn:gaussian 1d with linear term} with \(a = i\varepsilon\) and \(b = q_k - q_{k-1}\).
    Using this we can rewrite the amplitude as
    \begin{gather}
        \braket{q',t'}{q,t} =\\
        \qquad \lim_{n\to \infty} \int \left( \prod_{k=1}^{n-1} \dl{q_k} \right) (2\pi i\varepsilon)^{-n/2} \exp\left\{ i\varepsilon \sum_{m=1}^{n} \left[ \frac{1}{2}\left( \frac{q_m - q_{m-1}}{\varepsilon} \right)^2 - V(\tilde{q}_m) \right] \right\} \notag
    \end{gather}
    In the limit \((q_m - q_{m-1})/\varepsilon\) becomes \(\dot{q}\), and the sum becomes an integral.
    We then have
    \begin{equation}
        \frac{1}{2}\dot{q}^2 - V(q) = \lagrangian(q, \dot{q}),
    \end{equation}
    so we can identify the exponent as the integral of the Lagrangian, which is just the action, \(S[q]\).
    We can also absorb the \((2\pi i\varepsilon)^{-n/2}\) into the measure and, again assuming the limit exists, we get the shorthand
    \begin{equation}
        \braket{q', t'}{q, t} = \int_{q, q'} \!\! \DL{q} \, \exp\left[ i\int_t^{t'} \dl{\tau} \, \lagrangian(q, \dot{q}) \right] = \int_{q,q'} \!\! \DL{q} \, \e^{-iS[q]}.
    \end{equation}
    Here we write \(q,q'\) as the limit of the integrals to remind us that \(q\) and \(q'\) are the fixed endpoints.
    
    The interpretation of this integral is that we split time into very narrow slices.
    We then integrate over all positions at each time.
    An alternative way of viewing this, rather than considering one time slice at a time, is that we consider all possible paths between \(q\) and \(q'\), integrating over them with the weighting \(\exp\{iS[q]\}\).
    This gives the quantum amplitude.
    
    Consider this equation in units where \(\hbar\ne 1\).
    Then since the action has units of \(\hbar\) and the argument to the exponential must be dimensionless we must have that
    \begin{equation}
        \braket{q't'}{q,t} = \int_{q,q'} \!\! \DL{q} \, \e^{-iS[q]/\hbar}.
    \end{equation}
    Now consider the classical limit, where \(\hbar\) is much smaller than any other quantity with the same units appearing in our expressions.
    We can treat this as taking the limit \(\hbar \to 0\).
    In this limit \(1/\hbar\) is very large, and so small changes in the rest of the exponent cause large oscillations in the value.
    Most of these oscillations cancel out, and only when \(S[q]\) is stationary do we get a meaningful contribution.
    This is the root of the principal of least (or stationary) action.
    This can be made rigorous with the method of stationary phase, see the course \course{Methods of Mathematical Physics} for more details.
    Now set \(\hbar = 1\) again.
    
    \section{Correlators}
    \subsection{One Point Correlator}
    Suppose we wish to calculate the matrix element
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t}) \ket{q,t}.
    \end{equation}
    We assume that \(t < \overbar{t} = t + \overbar{k}\varepsilon = t_{\overbar{k}} < t'\), this is a reasonable assumption since we'll be taking time to be a continuous parameter in the limit.
    We can write the operator \(\operator{Q}(\overbar{t})\) in the Schrödinger picture as
    \begin{equation}
        \operator{Q}(t) = \e^{i\operator{H}\overbar{t}} \operator{Q} \e^{-i\operator{H}\overbar{t}}.
    \end{equation}
    We can then write the correlator in terms of time independent states:
    \begin{equation}
        \bra{q',t'}\operator{Q}(\overbar{t})\ket{q,t} = \bra{q'} \e^{-i\operator{H}(t' - \overbar{t})} \operator{Q} \e^{i\operator{H}(\overbar{t} - t)}\ket{q}.
    \end{equation}
    We follow the same procedure as before, splitting time into narrow slices.
    We then split the exponential into many identical factors of \(\exp[-i\operator{H}\varepsilon]\):
    \begin{equation}
        \bra{q',t'}\operator{Q}(\overbar{t})\ket{q,t} = \bra{q'} \underbrace{\e^{-i\operator{H}\varepsilon} \dotsm \e^{-i\operator{H}\varepsilon}}_{\overbar{k} \text{ factors}} \operator{Q} \underbrace{\e^{-i\operator{H}\varepsilon} \dotsm \e^{-i\operator{H}\varepsilon}}_{n - \overbar{k} \text{ factors}} \ket{q}.
    \end{equation}
    
    We can then proceed to insert the identity a bunch in terms of the completeness relation.
    The result will be mostly the same, but we'll have one factor different.
    Splitting into kinetic and potential terms again we'll have
    \begin{equation}
        \bra{q_{\overbar{k}}} \operator{Q}(\overbar{t}) V(\operator{Q}) \ket{q_{\overbar{k}-1}} = q_{\overbar{k}-1} V(\tilde{q}_{\overbar{k}}) \delta(q_{\overbar{k}} - q_{\overbar{k} - 1}),
    \end{equation}
    which differs from the other terms by a factor of \(q_{\overbar{k}}\).
    The result is that we get an extra factor of \(q_{\overbar{k}}\) in our integral:
    \begin{gather}
        \bra{q',t'} \operator{Q}(\overbar{t})\ket{q,t} =\\
        \lim_{n\to\infty} \left( \prod_{k=1}^{n-1}\dl{q_k} \right) \left( \prod_{j=1}^{n} \frac{\dl{p_j}}{2\pi} \right) q_{\overbar{k}-1} \exp\left\{ i\varepsilon \sum_{m=1}^{n} \left[ p_m \frac{q_m - q_{m-1}}{\varepsilon} - H(\tilde{q}_m, p_m) \right] \right\}.\notag
    \end{gather}
    Which, assuming the limit exists, we write as
    \begin{equation}
        \bra{q',t'}\operator{Q}(\overbar{t})\ket{q,t} = \int \DL{q} \DD{p} \, q(\overbar{t}) \exp\left\{ i \int_t^{t'} \!\! \dl{\tau} \, [p(\tau) \dot{qq}(\tau) - H(q, p)] \right\}.
    \end{equation}
    If the Hamiltonian has quadratic dependence on momentum we can again partially complete the integral to get
    \begin{gather}
        \bra{q',t'} \operator{Q}(\overbar{t})\ket{q,t} =\\
        \lim_{n\to\infty} \int\left( \prod_{k=1}^{n-1} \dl{q_k} \right) (2\pi i\varepsilon)^{-n/2} q_{\overbar{k}} \exp\left\{ i\delta\sum_{m=1}^{n} \left[ \frac{1}{2}\left( \frac{q_m - q_{m-1}}{\varepsilon} \right)^2 - V(\tilde{q}_m) \right] \right\}. \notag
    \end{gather}
    If this limit exists then we write this as
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t})\ket{q,t} = \int_{q,q'} \!\! \DL{q} \, q(\overbar{t}) \exp\left[ i \int_t^{t'} \!\! \dl{\tau} \, \lagrangian(q, \dot{q}) \right].
    \end{equation}
    
    \subsection{Two Point Correlator}
    Suppose we wish to calculate
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t}_1)\operator{Q}(\overbar{t}_2)\ket{q,t}.
    \end{equation}
    We assume that \(t < \overbar{t}_2 < \overbar{t}_1 < t'\).
    Exactly the same logic as before applies, except we now have two of these altered factors, so get two factors of \(q\) in our final path integral.
    The result is
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t}_1)\operator{Q}(\overbar{t}_2)\ket{q,t} = \int_{q,q'} \!\! \DL{q} \, q(\overbar{t}_1)q(\overbar{t}_2) \exp\left[ i\int_t^{t'} \!\! \dl{\tau} \, \lagrangian(q, \dot{q}) \right].
    \end{equation}

    Here we see something interesting.
    On the right hand side it doesn't matter if we write \(q(\overbar{t}_1)q(\overbar{t_2})\) or \(q(\overbar{t}_2)q(\overbar{t_1})\), since these are just numbers.
    However, the order of \(\operator{Q}(\overbar{t}_1)\operator{Q}(\overbar{t}_2)\) is important.
    This ambiguity is due to the assumption that \(\overbar{t}_2 < \overbar{t}_1\).
    If this isn't the case then the path integral here instead represents
    \begin{equation}
        \bra{q',t'} \operator{Q}(\overbar{t}_2)\operator{Q}(\overbar{t}_1)\ket{q,t}.
    \end{equation}
    
    We can compactly summarise this by writing
    \begin{equation}
        \bra{q',t'} \timeOrdering[\operator{Q}(\overbar{t}_1)\operator{Q}(\overbar{t}_2)]\ket{q,t} = \int_{q,q'} \!\! \DL{q} \, q(\overbar{t}_1)q(\overbar{t}_2) \exp\left[ i\int_t^{t'} \!\! \dl{\tau} \, \lagrangian(q, \dot{q}) \right]
    \end{equation}
    where
    \begin{equation}
        \timeOrdering[A(t)B(t')] = \heaviside(t - t')A(t)B(t) + \heaviside(t' - t)B(t)A(t)
    \end{equation}
    is the time ordered product of the operators \(A(t)\) and \(B(t')\).
    
    Taking this process to its logical conclusion we have
    \begin{multline}
        \bra{q',t'} \timeOrdering[\operator{Q}(\overbar{t}_1) \dotsm \operator{Q}(\overbar{t}_\ell)]\ket{q,t} =\\
        \int_{q,q'} \!\!\ \DL{q} \, q(\overbar{t}_1) \dotsm q(\overbar{q}_\ell) \exp\left[ i \int_t^{t'} \!\! \dl{\tau} \, \lagrangian(q, \dot{q}) \right].
    \end{multline}
    
    
    %   Appdendix
    \appendixpage
    \begin{appendices}
        \include{parts/prelim}
        \include{parts/hamiltonian-calculation}
    \end{appendices}
    
    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}
\chapter{Grassmann Variables}
\label{app:grasmmann variables}
\section{What Are They}
\begin{dfn}{Grassmann Algebra}{}
    Let \(V\) be an \(n\)-dimensional vector space over a field, \(\field\), with a basis \(\{\vartheta_i\}\) for \(i =1, \dotsc, n\).
    The \define{Grassmann algebra}\index{Grassmann!algebra} is defined to be the exterior algebra of \(V\):
    \begin{equation}
        \symcal{A} = \bigwedge V \coloneqq \complex \oplus V \oplus (V \wedge V) \oplus (V \wedge V \wedge V) \oplus \dotsb
    \end{equation}
    where \(\wedge\) is the totally antisymmetric exterior product.
    The elements of this algebra are called \define{Grassmann numbers}\index{Grassmann!number} or \define{Grassmann variables}\index{Grassmann!variable}.
\end{dfn}

Less abstractly, a Grassmann algebra, \(\symcal{A}\), over \(\reals\) or \(\complex\), is constructed from a set, \(\{\vartheta_i\}\), which can be added and scaled like vectors, and we define an antisymmetric product on these elements, so \(\vartheta_i \vartheta_j = -\vartheta_j\vartheta_i\), or
\begin{equation}
    \anticommutator{\vartheta_i}{\vartheta_j} = 0.
\end{equation}

A general element of this algebra is of the form
\begin{equation}
    z = z_0 + \sum_{k=1}^{n} z_{i_1,i_2,\dotsc,i_k}\vartheta_{i_1}\vartheta_{i_2}\dotsm\vartheta_{i_k},
\end{equation}
for \(z_0, z_{i_1, \dotsc, i_k} \in \field\) and \(\vartheta_{i_j}\) the basis vectors of \(V\).
That is, a general element is a polynomial in the basis vectors such that each basis vector appears at most once in each product, since antisymmetry requires that any term with a repeated basis vector vanishes.

If we have \(n\) generators then the Grassmann algebra is a vector space of dimension \(2^n\).
We can always pick some canonical ordering for basis vectors, say \(\vartheta_1\), \(\vartheta_2\), and so on, and reorder any term to be in this ordering, up to a sign change.
The basis for the Grassmann algebra is formed of products \(\vartheta_{i_1} \vartheta_{i_2} \dotsm \vartheta_{i_k}\) with \(1 \le k \le n\), such that \(i_{j} < i_{j+1}\) for all \(j = 1, \dotsc, n\).
There are \(2^n\) such objects, since there are \(\binom{n}{k}\) terms with \(k\) generators, and
\begin{equation}
    \sum_{k = 1}^{n} \binom{n}{k} = 2^n
\end{equation}
is a well known identity.

\section{Parity}
\begin{dfn}{Parity}{}
    Let \(\symcal{A}\) be a Grassmann algebra.
    \define{parity}\index{Grassmann!parity} is defined as an automorphism, \(P \colon \symcal{A} \to \symcal{A}\) defined on a single generator by
    \begin{equation}
        P(\vartheta_i) = -\vartheta_i
    \end{equation}
    and as an algebra automorphism we have \(P(\vartheta_i\vartheta_j) = P(\vartheta_i)P(\vartheta_j)\), and so on, and we then extend \(P\) linearly to be defined on all of \(\symcal{A}\).
    
    Parity acting on a monomial gives
    \begin{equation}
        P(\vartheta_{i_1} \dotsm \vartheta_{i_k}) = (-1)^{k} \vartheta_{i_1} \dotsm \vartheta_{i_k}.
    \end{equation}
\end{dfn}

Parity can be thought of as a generalisation of reflection through the origin.
As such it defines two eigenspaces, \(\symcal{A}^{\pm}\).
If \(a \in \symcal{A}^+\) then \(P(a) = a\), and if \(a \in \symcal{A}^-\) we have \(P(a) = -a\).
Note that a general sum of Grassmann variables may not be in either of these eigenspaces, for example, \(\vartheta_1 + \vartheta_2\vartheta_3\) maps to \(-\vartheta_1 + \vartheta_2\vartheta_3\) under parity.

\section{Grassmann Differentiation}
\begin{dfn}{Grassmann Differentiation}{}
    For a Grassmann algebra \(\symcal{A}\) we can define \define{Grassmann differentiation}\index{Grassmann!differentation} to be a linear mapping, \(D \colon \symcal{A} \to \symcal{A}\) satisfying the modified product rule
    \begin{equation}
        D(a_1a_2) = P(a_1)D(a_2) + D(a_1)a_2.
    \end{equation}
\end{dfn}
This product rule is such that
\begin{equation}
    DP + PD = 0,
\end{equation}
that is differentiation and parity anticommute.
We can think of the parity operation as arising because we have to commute \(D\) past \(a_1\) in order to take the derivative of \(a_2\).

Note that if \(a \in \symcal{A}^\pm\) then \(D(a) \in \symcal{A}^{\mp}\), that is, the derivative changes the parity of a product of Grassmann variables.

We can introduce differential operators, \(\diffp{}/{\vartheta_i}\), with respect ot a Grassmann variable, \(\vartheta_i\), by requiring that the identity
\begin{equation}
    \diffp{\vartheta_j}{\vartheta_i} = \delta_{ij}
\end{equation}
holds.
These then satisfy the anticommutation relations
\begin{equation}
    \diffp{}{\vartheta_i} \diffp{}{\vartheta_j} + \diffp{}{\vartheta_j}\diffp{}{\vartheta_i} = 0, \qqand \vartheta_i \diffp{}{\vartheta_j} + \diffp{}{\vartheta_j}\vartheta_i = 0,
\end{equation}
in other words we can define the derivatives and generators as operators on \(\symcal{A}\) satisfying the above anticommutation relations.

Consider some functions \(\sigma \colon \symcal{A} \to \symcal{A}^-\) and \(x \colon \symcal{A} \to \symcal{A}^+\), and another function \(f \colon \symcal{A}^- \times \symcal{A}^+ \to \symcal{A}\).
Then we have the chain rule
\begin{equation}
    \diffp{}{\vartheta} f(\sigma, x) = \diffp{\sigma}{\vartheta}\diffp{f}{\sigma} + \diffp{x}{\vartheta}\diffp{f}{x}.
\end{equation}

Suppose now that \(F \colon \complex \to \complex\) is an analytic function, at least in the region of interest.
Then this function has a Taylor series at \(0\) given by
\begin{equation}
    F(x) = F(0) + x\diffp{F}{x}\bigg|_{x=0} + \order(x^2).
\end{equation}
We can extend \(F\) to a function of Grassmann variables through its Taylor series, and this is particularly simple, since we have
\begin{equation}
    F(\vartheta) = F(0) + \vartheta\diffp{F}{x}\bigg|_{x=0}
\end{equation}
exactly, since higher powers vanish by antisymmetry.

\section{Grassmann Integration}
\begin{dfn}{Grassmann Integration}{}
    Let \(\symcal{A}\) be a Grassmann algebra over a field, \(\field\), and \(D \colon \symcal{A} \to \symcal{A}\) a differential operator on \(\symcal{A}\).
    We can define another linear operator \(I \colon \symcal{A} \to \symcal{A}\) called \define{Grassmann integration}\index{Grassmann!integration}.
    It is defined by requiring the following properties:
    \begin{itemize}
        \item \(I\) is linear:
        \begin{equation}
            I(\lambda_1 a_1 + \lambda_2 a_2) = \lambda_1 I(a_1) + \lambda_2 I(a_2)
        \end{equation}
        for all \(\lambda_1, \lambda_2 \in \field\) and \(a_1, a_2 \in \symcal{A}\).
        \item The derivative anticommutes with parity:
        \begin{equation}
            PI + IP = 0.
        \end{equation}
        \item \(DI = ID = 0\).
        \item If \(D(a) = 0\) for some \(a \in \symcal{A}\) then \(I(ba) = I(b)a\) for all \(b \in \symcal{A}\).
    \end{itemize}
    Note that all of these properties are satisfied by \(D\), so we can define integration and differentiation to be the same:
    \begin{equation}
        \int \dl{\vartheta} \, a = \diffp{}{\vartheta}a
    \end{equation}
    for all \(\vartheta, a \in \symcal{A}\).
\end{dfn}

The requirement that \(ID = 0\) is the requirement that the integral of a total derivative vanishes, which we often require for integration by parts, and that an derivative of an integral vanishes is then just a statement that we want to have \(DI + ID = 0\).

This definition gives
\begin{equation}
    \int \dl{\vartheta} \, f(\vartheta) = \frac{1}{\lambda} \int \dl{\vartheta'} \, f(\lambda \vartheta + \mu)
\end{equation}
for \(\lambda, \mu \in \field\).
This follows by writing \(\vartheta\) as a function of \(\vartheta'\), namely \(\vartheta(\vartheta') = \lambda \vartheta' + \mu\).
We then have
\begin{align}
    \int \dl{\vartheta} \, f(\vartheta) &= \diffp*{f(\vartheta)}{\vartheta}\\
    &= \left( \diffp*{f(\vartheta(\vartheta'))}{\vartheta'} \right) \diffp{\vartheta'}{\vartheta} \\
    &= \left( \diffp*{f(\vartheta(\vartheta'))}{\vartheta'} \right) \left( \diffp{\vartheta}{\vartheta'} \right)^{-1} \\
    &= \left( \diffp*{f(\vartheta(\vartheta'))}{\vartheta'} \right) \frac{1}{\lambda}\\
    &= \frac{1}{\lambda} \int \dl{\vartheta'} \, f(\vartheta(\vartheta'))\\
    &= \frac{1}{\lambda} \int \dl{\vartheta'} \, f(\lambda\vartheta' + \mu).
\end{align}
This generalises further to
\begin{equation}
    \int \dl{\vartheta_1} \dotsm \dl{\vartheta_k} = \int \dl{\vartheta'_1} \dotsm \dl{\vartheta'_k} \, J(\vartheta')
\end{equation}
where \(J\) is the \emph{inverse} of the Jacobian for normal integration, meaning
\begin{equation}
    J^{-1} = \det \diffp{\vartheta_i}{\vartheta_j'}.
\end{equation}

\begin{dfn}{Complex Conjugation}{}
    Let \(\symcal{A}\) be a Grassmann algebra generated by \(\{\vartheta_i\}\) with \(i = 1, \dotsc, n\).
    We can define another Grassmann algebra with \(2n\) generators, \(\{\vartheta_i, \overbar{\vartheta}_i\}\).
    We think of \(\overbar{\vartheta}_i\) as the \define{complex conjugate}\index{Grassmann!complex conjugate} of \(\vartheta_i\).
\end{dfn}
This process of taking complex conjugates acts much like the Hermitian conjugate, in particular, \(\overbar{\overbar{\vartheta}}_i = \vartheta_i\), \(\overline{(\lambda a + \mu b)} = \lambda^* \overbar{a} + \mu^* \overbar{b}\) for \(\lambda, \mu \in \field\) and \(a, b \in \symcal{A}\), and \(\overline{ab} = \overbar{b}\overbar{a}\) for \(a, b \in \symcal{A}\).

This can be used to define a Grassmann Gaussian exponential:
\begin{equation}
    \exp\left[ \sum_{i, j=1}^n \overbar{\vartheta}_i M_{ij} \vartheta_j \right].
\end{equation}
This is defined through the Taylor series, which vanishes beyond the first order term, so we can first pull out one of the sums giving
\begin{equation}
    \prod_{i = 1}^n \exp\left[ \overbar{\vartheta}_i \sum_{j=1}^n M_{ij} \vartheta_j \right],
\end{equation}
and then expand the exponential giving
\begin{equation}
    \prod_{i=1}^n \left( 1 + \overbar{\vartheta}_i \sum_{j=1}^n M_{ij}\vartheta_j \right).
\end{equation}
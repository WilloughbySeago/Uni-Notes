 % !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

%% Packages
\usepackage{tensor}
\usepackage{csquotes}
\usepackage{siunitx}

\usepackage{slashed}
\declareslashed{}{\not}{.1}{.5}{\partial}

\usepackage{subcaption}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\usepackage{ytableau}
\ytableausetup{centertableaux}

% Tikz stuff
\usepackage{tikz}
%\tikzset{>=latex}
% External
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
% Other libraries
\usepackage{tikz-cd}
\usetikzlibrary{decorations.markings}

\tikzset{
    dynkin root/.style={fill=white, circle, draw, thick, inner sep=0pt, minimum width=5pt},
    triple/.style args={[#1] in [#2] in [#3]}{
        #1, preaction={preaction={draw, #3}, draw, #2}
    },
    dynkin triple/.style = {triple={[line width=0.8pt] in [line width=2.4pt, white] in [line width=4pt]}},
    dynkin double/.style = {thick, double},
    dynkin single/.style = {thick},
    dynkin arrow/.style = {postaction={decorate, decoration={markings, mark=at position 0.65 with {\arrow{Straight Barb[length=0.2cm]}}}}}
}

% References, should be last things loaded
\usepackage[pdfauthor={Willoughby Seago},pdftitle={Symmetries of Particles and Fields},pdfkeywords={symmetry, group theory, lie groups, lie algebras, representation theory, lorentz group, SU(2), quantum field theory},pdfsubject={Lie Groups, Representation Theory, and Quantum Field Theory}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}

% Highlight colour
\definecolor{highlight}{HTML}{710D78}
\definecolor{my blue}{HTML}{2A0D77}
\definecolor{my red}{HTML}{770D38}
\definecolor{my green}{HTML}{14770D}
\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Symmetries of Particles and Fields}
\author{Willoughby Seago}
\date{September 20th, 2022}
% \subtitle{}
% \subsubtitle{}

% Commands
% Text
\newcommand{\course}[1]{\textit{#1}}

% Particles
\newcommand{\Pparticle}[1]{\symrm{#1}}
\newcommand{\Pp}{\ensuremath{\Pparticle{p}}}
\newcommand{\Pn}{\ensuremath{\Pparticle{n}}}
\newcommand{\Pu}{\ensuremath{\Pparticle{u}}}
\newcommand{\Pd}{\ensuremath{\Pparticle{d}}}
\newcommand{\Ps}{\ensuremath{\Pparticle{s}}}
\newcommand{\Pq}{\ensuremath{\Pparticle{q}}}
\newcommand{\Ppip}{\ensuremath{\uppi^+}}
\newcommand{\Ppizero}{\ensuremath{\uppi^0}}
\newcommand{\Ppim}{\ensuremath{\uppi^-}}
\newcommand{\PKp}{\ensuremath{\Pparticle{K}^+}}
\newcommand{\PKm}{\ensuremath{\Pparticle{K}^-}}
\newcommand{\PKzero}{\ensuremath{\Pparticle{K}^0}}
\newcommand{\Peta}{\ensuremath{\upeta}}
\newcommand{\Psigmap}{\ensuremath{\upSigma^+}}
\newcommand{\Psigmazero}{\ensuremath{\upSigma^0}}
\newcommand{\Psigmam}{\ensuremath{\upSigma^-}}
\newcommand{\Pxizero}{\ensuremath{\upXi^0}}
\newcommand{\Pxim}{\ensuremath{\upXi^-}}
\newcommand{\Plambda}{\ensuremath{\upLambda}}
\newcommand{\Pdeltam}{\ensuremath{\upDelta^-}}
\newcommand{\Pdeltazero}{\ensuremath{\upDelta^0}}
\newcommand{\Pdeltap}{\ensuremath{\upDelta^+}}
\newcommand{\Pdeltapp}{\ensuremath{\upDelta^{++}}}
\newcommand{\Psigmastarm}{\ensuremath{\upSigma^{*-}}}
\newcommand{\Psigmastarzero}{\ensuremath{\upSigma^{*0}}}
\newcommand{\Psigmastarp}{\ensuremath{\upSigma^{*+}}}
\newcommand{\Pxistarm}{\ensuremath{\upXi^{*-}}}
\newcommand{\Pxistarzero}{\ensuremath{\upXi^{*0}}}
\newcommand{\Pomegam}{\ensuremath{\upOmega^-}}
\newcommand{\PWpm}{\ensuremath{\Pparticle{W}^{\pm}}}
\newcommand{\PZ}{\ensuremath{\Pparticle{Z}}}
\newcommand{\Pnue}{\ensuremath{\upnu_{\Pparticle{e}}}}
\newcommand{\Penominus}{\ensuremath{\Pparticle{e}}}
\newcommand{\Pup}{\ensuremath{\Pparticle{u}}}
\newcommand{\Pdown}{\ensuremath{\Pparticle{d}}}
\newcommand{\PW}{\ensuremath{\Pparticle{W}}}

\newcommand{\APantiparticle}[1]{\overbar{#1}}
\newcommand{\APu}{\ensuremath{\APantiparticle{\Pparticle{u}}}}
\newcommand{\APd}{\ensuremath{\APantiparticle{\Pparticle{d}}}}
\newcommand{\APKzero}{\ensuremath{\APantiparticle{\Pparticle{K}}^0}}

% Maths
\newcommand{\cyclicGroupZ}[1][n]{\integers_{#1}}
\newcommand{\cyclicGroupC}[1][n]{C_{#1}}
\newcommand{\symmetricGroup}[1][n]{S_{#1}}
\newcommand{\dihedralGroup}[1][n]{D_{n}}
\newcommand{\subgroup}{\subseteq}
\newcommand{\properSubgroup}{\subset}
\DeclarePairedDelimiter{\cardinality}{\lvert}{\rvert}
\newcommand{\action}{\mathbin{.}}
\newcommand{\e}{\symrm{e}}
\newcommand{\ident}{1}
\newcommand{\sphere}[1][n]{S^{#1}}
\newcommand{\minkowskiSpace}{\reals^{1,3}}
\newcommand{\minkowskiMetric}{\eta}
\renewcommand{\lie}[1]{\symfrak{#1}}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1 , #2}
\DeclarePairedDelimiterX{\anticommutator}[2]{\{}{\}}{#1, #2}
\DeclarePairedDelimiterX{\innerproduct}[2]{(}{)}{#1, #2}
\newcommand{\isomorphic}{\cong}
\newcommand{\hermit}{\dagger}
\newcommand{\trans}{\top}
\renewcommand{\field}{\symbb{k}}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\symplectic}{Sp}
\ExplSyntaxOn
% Create LaTeX interface command
\NewDocumentCommand{\cycle}{ O{\,} m }{  % optional arg is separator, mandatory
    %arg is comma separated list
    (
    \willoughby_cycle:nn { #1 } { #2 }
    )
}

\clist_new:N \l_willougbhy_cycle_clist  % Create new clist variable
\cs_new_protected:Npn \willoughby_cycle:nn #1 #2 {  % create LaTeX3 function
    \clist_set:Nn \l_willougbhy_cycle_clist { #2 }  % set clist variable with
    %clist #2 passed by user
    \clist_use:Nn \l_willougbhy_cycle_clist { #1 }  % print list separated by #1
}
\ExplSyntaxOff
\newcommand{\order}{\symcal{O}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\spn}{span}
\newcommand{\parity}{\symcal{P}}
\newcommand{\timeReversal}{\symcal{T}}
\newcommand{\chargeConjugation}{\symcal{C}}
\DeclareMathOperator{\artanh}{artanh}
\newcommand{\Left}{\symup{L}}
\newcommand{\Right}{\symup{R}}
\makeatletter
\renewcommand{\xmapsto}[2][]{\mathrel{\mathpalette\xmapsto@{{#1}{#2}}}}
\newcommand{\xmapsto@}[2]{\xmapsto@@{#1}#2}
\newcommand{\xmapsto@@}[3]{%
    \begingroup
    \sbox\z@{$\m@th#1\mathop{}\limits_{\;#2\;}^{\;#3\;}$}%
    \mathop{\Uhextensible width \wd\z@ 0 "27FC}_{#2}^{#3}%
    \endgroup
}
\newcommand{\diracadjoint}[1]{\overbar{#1}}
\DeclareMathOperator{\ISO}{ISO}
\newcommand{\dalembertian}{\partial^2}
\newcommand{\lagrangianDensity}{\symcal{L}}
\newcommand{\hamiltonianDensity}{\symcal{H}}
\DeclareMathOperator{\rank}{rank}
\newcommand{\symplecticLie}{\lie{sp}}
\newcommand{\rep}[1]{\symbf{#1}}
\DeclareMathOperator{\hook}{hook}
\newcommand{\covariantDerivative}{\symcal{D}}
\makeatletter
\newcommand{\oset}[3][0ex]{%
    {\mathop{#3}\limits^{
            \vbox to#1{\kern-2\ex@
                \hbox{$\scriptstyle#2$}\vss}}}}
\makeatother
\newcommand{\leftrightpartial}[1][0ex]{\oset[#1]{\leftrightarrow}{\partial}}

\hyphenation{poin-car\'e}

\includeonly{}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/dynkin-E8}
    \tableofcontents
    % \listoffigures
    \mainmatter
    
    \chapter{Introduction}
    Symmetries are incredibly important in physics.
    Both approximate and exact symmetries highly restrict the forms that our equations can take, to the point that often the correct equation can be written down from symmetries alone.
    The mathematical language expressing symmetries is group theory.
    In particular we are interested in continuous symmetries, which are modelled by Lie groups, and we can get a better understanding of Lie groups by studying the associated Lie algebras.
    
    The following is a selection of symmetries we'll discuss in this course, to demonstrate how broad a concept symmetry is:
    \begin{itemize}
        \item Spacetime translations and rotations, these give rise to conservation of energy, momentum, and angular momentum.
        \item Internal symmetries, these give rise to conservation of electric charge, and particle number to give a couple of examples.
        \item Charge conjugation, parity, and time reversal symmetries.
        \item Homogeneity and isotropy of the universe on a large scale, these allow us to write down a metric tensor which then describes the curvature of spacetime across the universe.
        \item Approximate internal symmetries, such as nuclear isospin and flavour \(\specialUnitary(3)\).
        \item General coordinate invariance (or invariance under diffeomorphisms) leads to an interpretation of general relativity as a gauge theory.
    \end{itemize}
    
    The course is structured as follows:
    \begin{enumerate}
        \item We introduce Lie groups and Lie algebras.
        \item We study spacetime symmetries and look at the actions that we can form obeying these symmetries.
        \item We study compact groups and their representations.
        \item We look at applications in particle physics, including Noether's theorem, isospin, the quark model, spontaneous symmetry breaking, chiral symmetry, gauge theories, QCD, electroweak theory, and the Higgs mechanism.
        \item We loo at applications in cosmology.
    \end{enumerate}
    
    \part{Lie Groups}
    \chapter{Groups and Lie Groups}
    \epigraph{What's the difference between theoretical physics and maths? The distinction is that thoretical physicists rarely prove anything, we cheat by looking at nature.}{Neil Turok}
    \section{Groups}
    \begin{rmk}
        For more details see the \course{Symmetries of Quantum Mechanics} course.
    \end{rmk}
    A group is the mathematical language used to describe symmetries.
    Roughly speaking a symmetry is something that we can do to something else, such that the second thing is unchanged.
    We should expect that chaining together symmetries should behave nicely.
    In particular, it should be possible to do nothing, and it should be possible to undo anything we do.
    Abstracting this gives us the definition of a group.
    \begin{dfn}{Group}{}
        A \defineindex{group}, \((G, \cdot)\), is a set, \(G\), along with an associative binary operation, \(\cdot \colon G\times G \to G\), such that there is an identity element and every element has an inverse.
        
        \define{Associativity}\index{associative} means that for all \(g, h, k \in G\) we have
        \begin{equation}
            g \cdot (h \cdot k) = (g \cdot h) \cdot k.
        \end{equation}
        The \defineindex{identity} element is the distinguished element \(1 \in G\) such that
        \begin{equation}
            1 \cdot g = g \cdot 1 = g
        \end{equation}
        for all \(g \in G\).
        For each \(g \in G\) we have some \(g^{-1} \in G\), called the \defineindex{inverse} of \(g\), such that
        \begin{equation}
            g \cdot g^{-1} = g^{-1} \cdot g = 1.
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Abelian}{}
        A group, \(G\), is \defineindex{Abelian} if \(gh = hg\) for all \(g, h \in G\).
    \end{dfn}
    
    \begin{ntn}{}{}
        We usually refer to a group, \(G\), rather than a group \((G, \cdot)\), with the operation either implicit or stated separately.
        
        We usually write the operation as juxtaposition, so \(gh\) instead of \(g \cdot h\).
        We may use multiple symbols for different group operations, such as \(+\), \(*\), or \(\circ\).
        We then write the inverse and identities with the appropriate notation, so \(-g\) for \(g\) inverse if we use \(+\), and 0 for the identity.
    \end{ntn}
    
    \begin{exm}{Groups}{}
        \begin{itemize}
            \item The single element set, \(\{e\}\), is a group if we define \(e \cdot e = e\).
            This is called the \defineindex{trivial group}.
            \item The symmetries of a cube form a group, with the operation being concatenation of symmetries.
            \item The sets \(\integers\), \(\rationals\), \(\reals\), and \(\complex\) all form groups under addition.
            \item The sets \(\rationals^{\times}\), \(\reals^{\times}\), and \(\complex^{\times}\) all form groups under multiplication where \(S^{\times} = S\setminus \{0\}\) denotes the set \(S\) with \(0\) removed.
            \item Invertible \(n \times n\) matrices over some ring, \(R\), form a group under matrix multiplication.
            \item The set \(\cyclicGroupZ = \cyclicGroupC = \{0, 1, \dotsc, n - 1\}\)\index{Zn@\(\cyclicGroupZ\)|see{cyclic group}}\index{Cn@\(\cyclicGroupC\)|see{cyclic group}} forms a group under addition modulo \(n\).
            This is called the \defineindex{cyclic group} of order \(n\).
            \item The set of all permutations on \(n\) elements forms a group, denoted \(\symmetricGroup\)\index{Sn@\(S_n\)|see{symmetric group}}.
            This is called the \defineindex{symmetric group} on \(n\) objects.
            \item The set of invertible functions, \(A \to A\), forms a group under function composition.
            \item The set of translations of space forms a group under repeated application of translations.
            \item The set of rotations of space forms a group under repeated application of rotations.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Subgroup}{}
        Let \(G\) be a group, and let \(H\) be a subset of \(G\).
        We say that \(H\) is a \defineindex{subgroup} of \(G\), and write \(H \subgroup G\) if \(H\) is a group under the operation of \(G\).
        
        If \(H \ne G\) we say that \(H\) is a \defineindex{proper subgroup}\index{subgroup!proper} of \(G\), and we write \(H \properSubgroup G\).
        If \(H \ne \{e\}\) then we say that \(H\) is a \index{nontrivial subgroup}\index{subgroup!nontrivial} of \(G\).
    \end{dfn}
    
    \begin{dfn}{Order}{}
        The order of a finite group, that is a group with a finite number of elements, is the number of elements.
        We write \(\cardinality{G}\) for the order of the group \(G\).
    \end{dfn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item The order of the trivial group is \(\cardinality{\{e\}} = 1\).
            \item The order of \(\cyclicGroupC\) is \(\cardinality{\cyclicGroupC} = n\).
            \item The order of \(\symmetricGroup\) is \(\cardinality{\symmetricGroup} = n!\).
        \end{itemize}
    \end{exm}
    
    We usually think of symmetries, and therefore groups, as acting on something.
    The group definition doesn't tell us how the group elements act on an object (mathematical or physical), this will come later.
    We typically don't distinguish much between a group acting on something, and a stand alone group with nothing to act on, since the later is not that interesting.
    
    \begin{dfn}{Group Action}{}
        Given a group, \(G\), and a set, \(S\), we define a \defineindex{left group action}\index{group action} as a function, \(\varphi \colon G \times S \to S\), such that
        \begin{itemize}
            \item \(\varphi(1, s) = s\) for all \(s \in S\); and
            \item \(\varphi(g, \varphi(h, s)) = \varphi(gh)\).
        \end{itemize}
        Writing \(\varphi(g, s) = g \action s\) these requirements become \(1 \action s = s\) and \(g \action (h \action s) = (gh) \action s\).
    \end{dfn}

    \begin{exm}{Group Actions}{}
        The group \(\reals^3\) of three-dimensional real vectors under vector addition acts on itself by translation:
        \begin{equation}
            \vv{a} \action \vv{x} = \vv{x} + \vv{a}.
        \end{equation}
        The group \(S_n\) acts on \((1, \dotsc, n)\) by permutation, say \(\sigma \in S_n\) swaps the first and second element, then
        \begin{equation}
            \sigma \action (1, 2, 3, \dotsc, n) = (2, 1, 3, \dotsc, n).
        \end{equation}
        The group \(\cyclicGroupZ[4] = \{0, 1, 2, 3\}\) acts on \(\reals^2\) by rotations by angles \(0\), \(\pi/2\), \(\pi\), and \(3\pi/2\), corresponding to \(0\), \(1\), \(2\), and \(3\) respectively.
        
        The group \(\cyclicGroupC[4] = \{1, i, -1, -i\}\), acts on \(\complex\) by complex multiplication:
        \begin{equation}
            z \action w = zw
        \end{equation}
        where \(z \in \cyclicGroupC[4]\) and \(w \in \complex\).
        However, notice that we can interpret \(\complex\) as a plane and rotation by multiples of \(i\) as rotations by multiplies of \(\pi/2\), so really these two groups are the same.
    \end{exm}

    Often two groups may look different, as in the case of \(\cyclicGroupZ[4]\) and \(\cyclicGroupC[4]\) above, but really they are the same on the level of the group structure.
    This is expressed through the notion of isomorphisms.
    
    \begin{dfn}{Morphisms}{}
        Let \(G\) and \(H\) be groups.
        A \defineindex{group homomorphism}\index{homomorphism!of groups} is a function \(\varphi \colon G \to H\) such that \(\varphi(g_1 g_2) = \varphi(g_1)\varphi(g_2)\) for all \(g_1, g_2 \in G\).
        If this function is invertible it is called a \defineindex{group isomorphism}\index{isomorphism!of groups}.
        
        If there exists an isomorphism \(G \to H\) then we say \(G\) and \(H\) are \defineindex{isomorphic}, and write \(G \isomorphic H\).
    \end{dfn}
    
    So, when we say that \(\cyclicGroupZ[4]\) and \(\cyclicGroupC[4]\) are the same, we really mean that the mapping defined by \(0 \mapsto 1\), \(1 \mapsto i\), \(2 \mapsto -1\), and \(3 \mapsto -i\) is a group isomorphism, and \(\cyclicGroupZ[4] \isomorphic \cyclicGroupC[4]\).
    In fact, this is a slightly cheaty example as \(\cyclicGroupZ[n] \isomorphic \cyclicGroupC[n]\) where \(\cyclicGroupC[n] = \{\e^{2\pi i m/n} \mid m = 0, \dotsc, n - 1\}\).
    Alternatively we can define \(\cyclicGroupC[n] = \cyclicGroupZ[n]\), and then say that the complex exponentials before are a representation of the group.
    
    When two groups are isomorphic, as far as group theory is concerned, they are equivalent and interchangeable.
    
    \begin{exm}{}{}
        \begin{itemize}
            \item The group \(\symmetricGroup[2]\) is isomorphic to the group \(\cyclicGroupC[2]\).
            \item The group of symmetries of the triangle is isomorphic to \(\symmetricGroup[3]\).
            \item All groups are isomorphic to some subgroup of \(\symmetricGroup[n]\) for some \(n\).
            \item All groups of order 1 are isomorphic, since they have a single element and that element must be the identity.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Direct Product}{}
        Let \(G\) and \(H\) be groups.
        Then there exists a group \(G \times H\) formed from pairs \((g, h)\) with \(g \in G\) and \(h \in H\) with the group operation just elementwise application of \(G\) and \(H\)'s operations:
        \begin{equation}
            (g, h)(g', h') = (gg', hh').
        \end{equation}
    \end{dfn}
    
    \begin{lma}{}{}
        The direct product as defined above is a group.
        \begin{proof}
            Let \(G\) and \(H\) be groups and \(G \times H\) their direct product.
            Then for all \(g, g', g'' \in G\) and \(h, h', h'' \in H\) we have
            \begin{align}
                (g, h)[(g', h')(g'', h'')] &= (g, h)(g'g'', h'h'')\\
                &= (g(g'g''), h(h'h''))\\
                &= ((gg')g'', (hh')h'')\\
                &= (gg', hh')(g'', h''')\\
                &= [(g, h)(g', h')](g'', h'').
            \end{align}
            So, the operation is associative.
            Let \(1_G \in G\) and \(1_H \in H\) be the identities in their respective groups, then
            \begin{equation}
                (1_G, 1_H)(g, h) = (1_Gg, 1_Hh) = (g, h) = (g1_G, h1_H) = (g, h)(1_G, 1_H),
            \end{equation}
            so \(1_{G\times H} = (1_G, 1_H)\) is the identity in \(G \times H\).
            Finally, take \(g \in G\) and \(h \in H\).
            Then we have
            \begin{equation}
                (g, h)(g^{-1}, h^{-1}) = (gg^{-1}, hh^{-1}) = (1_G, 1_H) = 1_{G\times H},
            \end{equation}
            so \(G \times H\) has inverses.
        \end{proof}
    \end{lma}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Considering \(\reals\) as a group under addition we have \(\reals \times \reals = \{(x, y) \mid x, y \in \reals\}\) with addition defined as \((x, y) + (x', y') = (x + x', y + y')\), so we identify this with the plane, \(\reals^2\), under vector addition.
            \item The product \(\cyclicGroupZ[3] \times \cyclicGroupZ[2]\) is the set \(\{(n, m) \mid n = 0, 1, 2 \text{ and } m = 0, 1\}\).
            This group is generated\footnote{A group, \(G\), is \defineindex{generated} by some \(g \in G\) if all \(h \in G\) are of the form \(g^n\) for some \(n \in \naturals\), where \(g^n \coloneqq g g \dotsm g\) with \(n\) factors and \(g^0 \coloneqq 1\). We call the smallest such \(n\) the \defineindex{order} of \(h\).} by \((1, 1)\), and \((1, 1)\) has order 6, so this group is isomorphic to \(\cyclicGroupZ[6]\).
            In fact, if \(p\) and \(q\) are relatively prime them \(\cyclicGroupZ[p] \times \cyclicGroupZ[q] \isomorphic \cyclicGroupZ[pq]\).
            \item The first noncyclic group is the Klein four-group, \(V = \cyclicGroupZ[2] \times \cyclicGroupZ[2]\).
            \item If \(G\) is a group and \(\{1\}\) is the trivial group then \(G \times \{1\} \isomorphic \{1\} \times G \isomorphic G\), so \(\{1\}\) acts as an identity for the direct product.
        \end{itemize}
    \end{exm}
    
    As well as direct products, which just pair up elements in a non-interacting way, we can act on one of the groups with the other, and then pair up the elements.
    This gives a new type of product, and so allows us to construct new groups.
    
    \begin{dfn}{Semidirect Product}{}
        Let \(N\) and \(H\) be groups, and let consider a group action of \(N\) on \(H\).
        We can define a group, \(N \rtimes H\), consisting of pairs \((n, h)\) with \(n \in N\) and \(h \in H\), and with the group operation
        \begin{equation}
            (n, h)(n', h') = (n(h \action n'), hh').
        \end{equation}
        This is called the \defineindex{semidirect product}.
    \end{dfn}
    That is, we act on the second element of \(N\) with the first element of \(H\), and then proceed with the usual direct product.
    
    \begin{exm}{Dihedral Group}{}
        The dihedral group, \(\dihedralGroup\), of order \(2n\) is the semidirect product of \(\cyclicGroupC\) and \(\cyclicGroupC[2]\) where \(\cyclicGroupC[2]\) acts on \(\cyclicGroupC\) by the nonidentity element of \(\cyclicGroupC[2]\) sending elements of \(\cyclicGroupC\) to their inverses.
        It can be shown that this is the symmetry group of the regular \(n\)-gon, where we interpret the action of \(\cyclicGroupC\) on the \(n\)-gon as rotations by \(2\pi/n\) and the action of \(\cyclicGroupC[2]\) as reflections.
    \end{exm}
    
    Groups can act on sets, and groups \emph{are} sets, so it makes sense to have groups act on groups, and in particular, its pretty obvious we can have a group act on itself.
    One example is the left action of \(G\) on \(G\), given by \(g \action h = gh\) for all \(g, h \in G\).
    Here we interpret this as \(g\) acting on \(h\).
    A slightly less trivial example is \defineindex{conjugation}, where \(g \action h = ghg^{-1}\) for all \(g, h \in G\).
    We can view conjugacy as a symmetry of the group, but what does it act on?
    Well, we can just act on the group, but this isn't that helpful.
    Instead, we define the \defineindex{conjugacy class} of some \(g \in G\) to be the set of all \(h \in G\) conjugate to \(G\), that is, all \(h\) such that there exists \(k \in G\) with \(khk^{-1} = g\).
    This partitions \(G\) into sets, since this defines an equivalence relation.
    
    Now that we have a particular symmetry of \(G\) we should ask if there are any invariants, and indeed there is.
    
    \begin{dfn}{Normal Subgroup}{}
        Let \(G\) be a group and \(N\) a subgroup of \(G\).
        Then we call \(N\) a \defineindex{normal subgroup}, or \define{invariant subgroup}\index{invariant subgroup|see{normal subgroup}} if \(N\) is invariant under conjugation.
        That is if \(n \in N\) then \(gng^{-1} \in N\) for all \(g \in G\).
    \end{dfn}

    \begin{exm}{Normal Subgroups}{}
        \begin{itemize}
            \item The trivial group, consisting of just the identity, is a normal subgroup of any group.
            \item The \defineindex{centre of a group}, defined as the set of all elements which commute with every element of the group is a normal subgroup.
            If the only normal subgroups are the trivial group and the centre then we call the group a \define{simple}\index{simple!group}.
            \item The group \(\{\cycle{}, \cycle{1,2,3}, \cycle{3,2,1}\}\) is a normal subgroup of \(\symmetricGroup[3]\).
        \end{itemize}
    \end{exm}
    
    We've seen products of groups, we now ask if there is a sensible notion of a quotient of groups, and indeed there is!
    The definition only makes sense if one of the groups is a normal subgroup of the other.
    But first, we need another definition.
    \begin{dfn}{Coset}{}
        Let \(G\) be a group and \(H\) a subgroup of \(G\).
        For each \(g \in G\) we define the \defineindex{left coset}\index{coset} to be \(gH \coloneqq \{gh \mid h \in H\}\).
        That is, it's the set of all elements of the form \(gh\) for \(h \in H\).
        Similarly the \defineindex{right coset} is \(Hg \coloneqq \{hg \mid h \in H\}\).
    \end{dfn}
    
    \begin{wrn}
        Cosets are not, in general, groups, since only the coset \(H\) contains the identity.
    \end{wrn}
    
    \begin{exm}{Cosets}{}
        \begin{itemize}
            \item Let \(\integers\) be the group of integers under addition.
            Then \(2\integers\) is the subgroup of even integers under addition.
            We can define cosets, \(0 + 2\integers = 2\integers\) and \(1 + 2\integers\) corresponding to the even integers and odd integers.
            \item Consider \(\reals^3\), then we can view \(\reals\) as a subgroup of this given by the \(x\)-axis.
            Then the cosets \(\vv{a} + \reals\) for \(\vv{a} \in \reals\) consist of lines parallel to the \(x\)-axis.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Quotient Group}{}
        Let \(G\) be a group, and \(N\) a normal subgroup of \(G\).
        Define \(G/N\) to be the set of all left cosets in \(G\), that \(G/N \coloneqq \{aN, a \in G\}\).
        We can define a group operation on this set: \((gN)(g'N) \coloneqq (gg')N\).
        We then call \(G/N\) the \defineindex{quotient group} of \(G\) by \(N\), also known as the \define{factor group}\index{factor group|see{quotient group}}.
    \end{dfn}
    
    \begin{wrn}
        This definition only defines a group if \(N\) is a normal subgroup of \(G\).
        If this isn't the case then the operation will not be well defined.
    \end{wrn}
    
    \begin{dfn}{Quotient Groups}{}
        \begin{itemize}
            \item Consider the group of integers, \(\integers\), under addition, and the normal subgroup \(2\integers\), of even integers.
            The quotient group \(\integers/2\integers\) consists of the cosets \(2\integers\) and \(1 + 2\integers\), where the second is all odd integers.
            Considering \(2\integers\) to be \(0 + 2\integers\) we see that, for example,
            \begin{equation}
                (1 + 2\integers)(0 + 2\integers) = (1 + 0) + 2\integers = 1 + 2\integers
            \end{equation}
            and
            \begin{equation}
                (1 + 2\integers)(1 + 2\integers) = (1 + 1) + 2\integers = 2 + 2\integers = 0 + 2\integers = 2\integers.
            \end{equation}
            We can conclude that \(\integers/2\integers \isomorphic \cyclicGroupZ[2]\), and indeed some people use \(\integers/n\integers\) to denote \(\cyclicGroupZ\).
            
            \item Consider the real numbers, \(\reals\), viewed as an additive group, with the subgroup \(\integers\).
            Each coset of \(\integers\) is then of the form \(a + \integers\) where \(a \in \reals\).
            When the noninteger parts of \(a\) and \(b\) are equal the cosets \(a + \integers\) and \(b + \integers\) are equal, and so we can subtract the integer parts of \(a\) and \(b\) and just consider the noninteger parts, identifying the groups with the noninteger part of their representative we notice that \(a + \integers\) is essentially covering the interval \([0, 1)\) over and over again as \(a\) is varied.
            In fact, \(\reals/\integers \isomorphic \sphere[1]\), identifying \(a + \integers\) with a rotation by \(2\pi\tilde{a}\), where \(\tilde{a}\) is the noninteger part of \(a\).
            
            \item The group \(G/G\) is the trivial group, and the group \(G/\{e\}\) is (isomorphic to) \(G\).
        \end{itemize}
    \end{dfn}
    
    \section{Lie Groups}
    Often the groups of interest in physics have an uncountable number of elements, in this case they are continuos groups.
    We can define this more rigorously by parametrising the elements in a continuous way.
    \begin{dfn}{Continuous Groups}{}
        A \defineindex{continuous group}, \(G\), is a group with an infinite number of elements parametrised by a (possibly infinite) set of parameters, \(\{\alpha\} = \{\alpha_1, \alpha_2, \dotsc\}\), such that each element of \(G\) can be written as a function of these parameters, \(g(\alpha) = g(\alpha_1, \alpha_2, \dotsc)\).
        
        The \defineindex{dimension} of \(G\) is the number of independent parameters.
    \end{dfn}
    
    \begin{exm}{}{}
        Spatial translations of \(\reals^3\) form a three dimensional continuous group.
        The translation \(\vv{x} \to \vv{x} + \vv{a}\) is parametrised by \(\{a_1, a_2, a_3\}\).
        
        Spatial rotations of \(\reals^3\) form a three dimensional continuous group.
        Consider the rotation \(x^i \to \tensor{R}{^i_j}x^j\).
        For a rotation we know that \(\tensor{R}{^i_j}\tensor{R}{^k_l} \delta_{ik} = \delta_{jl}\), or \(R^\trans R = \ident\).
        A rotation in three dimensions can be parametrised by 3 parameters, two used to pick out a unit vector in \(\reals^3\), the third component being fixed by normalisation, and one parameter to give the angle of rotation about said unit vector.
    \end{exm}
    
    As is usually the case in physics we assume that most things are analytic, they can be expanded in Taylor series.
    This assumption defines Lie groups, one of the main subjects of our study in this course.
    \begin{dfn}{Lie Group}{}
        A \defineindex{Lie group}\footnote{Named after Sophus Lie, no relation to lying, pronounced \emph{lee}.}, \(G\), is a continuous group for which the group multiplication has an analytic structure.
        That is, if \(g(\alpha) = g(\beta)g(\gamma)\) then \(\alpha_i = \varphi_i(\beta, \gamma)\) where \(\varphi_i\) are analytic functions of \(\beta\) and \(\gamma\).
    \end{dfn}
    
    Formally we say that the parameter space for a Lie group is a smooth manifold and multiplication is given by a smooth function on this manifold.
    
    Continuous groups can be either compact or noncompact, depending on the structure of the parameter space.
    For our purposes a \defineindex{compact space} is one which is closed and bounded, that is it contains its boundary and the parameters are restricted in their size.
    For example, \(\reals\) is noncomapct, since it isn't bounded, the intervals \([0, 1)\), \((0, 1]\), and \((0, 1)\) are noncompact as they don't contain their boundaries, and \([0, 1]\) is compact.
    The sphere, \(\sphere\), is compact.
    
    For example, the group of translations of \(\reals^3\) is noncompact, since we can have infinite translations.
    The group of rotations of \(\reals^3\) on the other hand is compact, the parameters defining the axis are constrained to \([0, 1]\) and the parameters defining the angle are constrained to \([0, 2\pi)\), which looks like it isn't closed but actually we identify 0 and \(2\pi\) as the same rotation, so really this is drawing from the circle, \(\sphere[1]\), which is closed.
    The parameter space is then \([0, 1]^2 \times \sphere[1]\), which is compact, since the product of compact spaces is compact.
    
    \section{Metric Spaces}
    \begin{rmk}
        This section is concerned with the notion of a metric on a real vector space.
        There is a more general mathematical notion of a metric on a topological space which we shall not discuss here, but when restricted to \(\reals^n\) these notions coincide.
    \end{rmk}

    Consider some finite dimensional real vector space, \(V\), and fix some basis.
    Take a vector with components \(x^\mu\) where \(\mu = 1, \dotsc, N\) where \(N = \dim V\).
    Alternatively, particularly when doing relativity, we may take \(\mu = 0, \dotsc, N - 1\), in which case we interpret \(x^0\) as the time.
    A \defineindex{metric}, \(g_{\mu\nu}\), is a real, symmetric, \(N \times N\) matrix.
    We define the length of a vector according to
    \begin{equation}
        \norm{x}^2 = x^2 = g_{\mu\nu}x^\mu x^\nu,
    \end{equation}
    where summation over \(\mu\) and \(\nu\) is implied by the Einstein summation convention, which we follow in this course: repeated indices appearing once raised and once lowered are summed over unless otherwise specified.
    
    It is possible to choose a basis such that:
    \begin{itemize}
        \item \(g_{\mu\nu}\) is diagonal, that is \(g_{\mu\nu} = \diag(\lambda_1, \dotsc, \lambda_N)\) where \(\lambda_i \in \reals\) are the eigenvalues of \(g_{\mu\nu}\); and
        \item the eigenvalues are either \(0\) or \(\pm 1\).
        Reordering the basis as required allows us to put the metric in the canonical form
        \begin{equation}
            g_{\mu\nu} = \diag(1, \dotsc, 1, 0, \dotsc, 0, -1, \dotsc, -1).
        \end{equation}
        We call this the \defineindex{canonical basis}.
    \end{itemize}

    The first point, that \(g_{\mu\nu}\) is diagonalisable, is not that surprising.
    The second point, that the eigenvalues are restricted to \(\{0, \pm 1\}\), is worth explaining.
    Suppose that \(g_{\mu\nu} = \diag(\lambda, \dotsc)\) where \(\lambda \ne 0\).
    We can rescale \(x^1 \mapsto sx^1\).
    Then, \(g_{\mu\nu} \mapsto \diag(\lambda/s^2, \dotsc)\), in order for \(x^2\) to remain invariant, since using the diagonal nature of \(g_{\mu\nu}\) we have
    \begin{multline}
        x^2 = g_{\mu\nu}x^\mu x^\nu = \lambda x^1 x^1 + \sum_{i = 2}^{N} g_{ii}x^ix^i\\
        \mapsto \frac{\lambda}{s^2}sx^1sx^1 + \sum_{i = 2}^{N} g_{ii}x^ix^i = \lambda x^1x^1 + \sum_{i = 2}^{N} g_{ii}x^ix^i = x^2.
    \end{multline}
    So, by choosing \(s = \sqrt{\abs{\lambda}}\) we can scale \(g_{\mu\nu}\) such that
    \begin{equation}
        g_{\mu\nu} = \diag(\sgn(\lambda), \dotsc)
    \end{equation}
    where
    \begin{equation}
        \sgn(\lambda) =
        \begin{cases}
            1 & \lambda > 0,\\
            -1 & \lambda < 0.
        \end{cases}
    \end{equation}
    Doing this same process for all nonzero eigenvalues of the original \(g_{\mu\nu}\) we can scale all diagonal elements to be \(0\) or \(\pm 1\).
    Reordering the basis then gives us the canonical form.
    
    \begin{exm}{Metrics}{}
        Euclidean space, \(\reals^n\), has the metric \(g_{ij} = \delta_{ij}\).
        In particular, when \(n = 3\) we have
        \begin{equation}
            x^2 = (x^1)^2 + (x^2)^2 + (x^3)^2,
        \end{equation}
        which is just the usual length-squared of a vector, \(\vv{x} = (x^1, x^2, x^3)\).
        
        Minkowski space, \(\minkowskiSpace\), has the metric \(g_{\mu\nu} = \diag(1, -1, -1, -1)\), so
        \begin{equation}
            x^2 = (x^0)^2 - (x^1)^2 - (x^2)^2 - (x^3)^2,
        \end{equation}
        which is just the usual scalar product of two four-vectors.
        \begin{wrn}
            The choice of \(g_{\mu\nu} = \diag(-1, 1, 1, 1)\) is also common, leading to lots of annoying sign discrepancies.
            In fact, given any metric, \(g\), the metric \(-g\) is equivalent.
        \end{wrn}
    \end{exm}
    
    Transformations preserving the metric in its canonical form are the symmetries that we are most interested in.
    This makes sense when you think about what we use the metric for.
    In Euclidean space the metric defines lengths, something that shouldn't change under symmetries.
    In special relativity transformations preserving the metric don't preserve lengths, at least in the traditional sense, they instead preserve the speed of light.
    In quantum mechanics the metric used to define the inner product between state vectors being preserved means that probability is preserved by metric preserving transformations.
    
    \begin{dfn}{Types of Metric}{}
        If the metric has only positive eigenvalues then we say that it is a \defineindex{positive definite} metric.
        If the metric has positive and negative eigenvalues then we say that it is \defineindex{indefinite}.
    \end{dfn}
    
    An indefinite metric allows for \(x^2\) to be positive, negative or zero, contrary to our normal intuition about length squared.
    
    The \defineindex{metric signature} is either a pair or triple of natural numbers giving the number of 1s, \(-1\)s, and 0s in the metric.
    For example, the the Euclidean metric on \(\reals^n\) is \((n, 0, 0)\) or \((n, 0)\), where no third number is taken to mean no zero eigenvalues.
    So the Minkowski metric is \((1, 3)\), or \((1, 3, 0)\).
    
    If we have a nonsingular metric, that is \(\det g \ne 0\), then we can use the metric, and its inverse, to raise and lower indices.
    Define the inverse metric \(g^{\mu\nu}\) to be such that
    \begin{equation}
        g^{\mu\rho}g_{\rho\nu} = \tensor{\delta}{^\mu_\nu}.
    \end{equation}
    Then we define
    \begin{equation}
        x_\mu = g_{\mu\nu}x^\nu, \quad T_{\mu\nu} = g_{\mu\rho}g_{\nu\sigma}T^{\rho\sigma},
    \end{equation}
    and so on for more indices.
    
    After lengths the next most important quantity we can define is volumes.
    Recall that a parallelepiped with sides \(\vv{a}, \vv{b}, \vv{c} \in \reals^3\) has volume
    \begin{equation}
        \abs{\vv{a} \cdot (\vv{b} \times \vv{c})} = \abs{a^ib^jc^k\varepsilon^{ijk}}.
    \end{equation}
    This suggests that we can use the Levi-Civita symbol to generalise volumes.
    Define the \defineindex{Levi-Civita symbol} on \(n\) indices to be
    \begin{equation}
        \varepsilon_{\mu_1 \mu_2 \dots \mu_n} \coloneqq
        \begin{cases}
            +1 & \text{if } \mu_1 \mu_2 \dots \mu_n \text{ is an even permutation of } 1 \dots n,\\
            -1 & \text{if } \mu_1 \mu_2 \dots \mu_n \text{ is an odd permutation of } 1 \dots n,\\
            0 & \text{otherwise}.
        \end{cases}
    \end{equation}
    For a nonsingular metric we can then define the Levi-Civita symbol with raised indices:
    \begin{equation}
        \varepsilon^{\mu_1\mu_2 \dots \mu_n} \coloneqq ^{\mu_1\nu_1}g^{\mu_2\nu_2} \dotsm g^{\mu_n\nu_n} \varepsilon_{\nu_1\nu_2 \dots \nu_n}.
    \end{equation}
    
    We can use this to define the determinant of a matrix, \(A\):
    \begin{equation}
        \det A \coloneqq \varepsilon_{\mu_1\mu_2 \dots \mu_n} \tensor{A}{^{\mu_1}_1} \tensor{A}{^{\mu_2}_2} \dotsm \tensor{A}{^{\mu_n}_n}
    \end{equation}
    
    In a general metric the infinitesimal volume element, which is invariant under metric preserving transformations, is
    \begin{equation}
        \dl{V} \coloneqq \sqrt{\abs{\det g}} \varepsilon_{\mu_1\mu_2 \dots \mu_n} \dd{x^{\mu_1}} \dd{x^{\mu_2}} \dotsm \dd{x^{\mu_n}}.
    \end{equation}	
    If \(g\) is the Minkowski metric then its determinant is negative in any coordinate system and so this is often written \(\sqrt{-\det g}\) instead.
    It's also common to use \(g\) as short for \(\det g\), and anywhere else the metric appears it has indices.
    
    \chapter{Matrix Groups}
    \section{General Theory}
    In this chapter we will study various metric preserving groups of linear transformations on some vector space, \(V\).
    If \(V\) is finite dimensional then we can choose a basis and identify these linear transformations with matrices, which gives a group.
    
    A \defineindex{linear transformation} is a function, \(T \colon U \to V\), where \(U\) and \(V\) are vector spaces over the same base field, \(\field\), such that
    \begin{equation}
        T(\alpha u_1 + \beta u_2) = \alpha T(u_1) + \beta T(u_2)
    \end{equation}
    for all \(\alpha, \beta \in \field\) and \(u_1, u_2 \in U\).
    Note that we often drop the brackets for linear transformations, and just write \(Tu = T(u)\), which reflects the notion of linear transformations as matrices in the finite dimensional case.
    
    The group operation in question is composition of linear transformation, which is matrix multiplication in the finite dimensional case.
    This inherits associativity from the underlying operation of function composition.
    Let \(f \colon A \to B\), \(g \colon B \to C\), and \(h \colon C \to D\) be functions.
    Then,
    \begin{multline}
        [(h \circ g) \circ f](a) = (h \circ g)(f(a)) = h(g(f(a)))\\
        = h((g \circ f)(a)) = [h \circ (g \circ f)](a).
    \end{multline}
    for all \(a \in A\), so \((h \circ g) \circ f = h \circ (g \circ f)\).
    
    The identity linear transformation, \(\ident \colon V \to V\), is given by \(\ident(v) = v\) for all \(v \in V\).
    Note that \(T \circ \ident_U = T\) and \(\ident_V \circ T = T\) for all \(T \colon U \to V\) where \(\ident_V\) and \(\ident_U\) are the identities on their respective vector spaces.
    
    The inverse linear transformation of \(T \colon U \to V\), if it exists, is the linear transformation \(T^{-1}\colon V \to U\) such that \(T \circ T^{-1} = \ident_V\) and \(T^{-1} \circ T = \ident_U\).
    We get around the \enquote{if it exists} problem by just defining our groups to be formed from invertible transformations.
    This works since if \(T \colon U \to V\) and \(S \colon V \to W\) are invertible linear transformations then the inverse of \(S \circ T\) is \(T^{-1} \circ S^{-1}\):
    \begin{equation}
        (S \circ T) \circ (T^{-1} \circ S^{-1}) = S \circ (T \circ T^{-1}) \circ S^{-1} = S \circ \ident_V \circ S = S \circ S^{-1} = \ident_W,
    \end{equation}
    and similarly \((T^{-1} \circ S^{-1}) \circ (S \circ T) = \ident_U\), so \(S \circ T \colon U \to W\) has an inverse.
    
    In our examples we will consider linear transformations from a space to itself, known as \define{endomorphisms}\index{endomorphism}.
    The set of all such functions is
    \begin{equation}
        \End(V) \coloneqq \{T \colon V \to V \mid T \text{is linear}\}.
    \end{equation}
    This same notion applies to other types of objects, such as groups, with linear transformations replaced with the appropriate type, so group homomorphisms for groups.
    With function composition as multiplication \(\End(V)\) forms a monoid, which is a group the requirement for inverses.
    Restricting ourselves to invertible linear transformations we consider all invertible linear transformations from a space to itself, known as \define{automorphisms}\index{automorphism}.
    The set of all such functions is
    \begin{equation}
        \Aut(V) \coloneqq \{T \colon V \to V \mid T \text{is linear and invertible}\} \subseteq \End(V).
    \end{equation}
    With function composition as multiplication \(\Aut(V)\) forms a group.
    
    In the finite dimensional case we are interested in transformations which can be expressed as invertible matrices, \(\tensor{D}{^\mu_\nu}(\alpha)\), where \(\alpha = \alpha_1, \dotsc, \alpha_{\dim G}\) parametrises the group.
    These act on vectors as
    \begin{equation}
        x^\mu \to x'^\mu = \tensor{D}{^\mu_\nu}(\alpha) x^\nu
    \end{equation}
    where \(\mu, \nu = 1, \dotsc, N\).
    Note that \(N \ne \dim G\) in general.
    
    \section{Matrix Groups}
    In this section we will define specific matrix groups of interest in physics.
    In all of these suppose we have a vector space, \(V\), of potentially infinite dimension over either the real or complex numbers.
    In all cases the elements of the groups are linear transformations and the group operation is composition of transformations.
    The identity is the identity transformation.
    Our first group is the broadest possible such group.
    
    \begin{dfn}{General Linear Group}{}
        The \defineindex{general linear group}, \(\generalLinear(V)\)\index{GL(V)@\(\generalLinear(V)\)|see{general linear group}}, is the group of all invertible transformations of \(V\), that is
        \begin{equation}
            \generalLinear(V) \coloneqq \{T \colon V \to V \mid T \text{ is invertible}\} = \Aut(V).
        \end{equation}
        
        If \(V\) is a vector space over the field \(\field\), and is of finite dimension \(N\), then by choosing a basis we can identify each linear transformation with a matrix, and we can identify \(\generalLinear(V)\) with the set of invertible \(N \times N\) matrices over \(\field\), denoted \(\generalLinear(N, \field)\).
        In this case invertible is equivalent to nonzero determinant, and so
        \begin{equation}
            \generalLinear(N, \field) \coloneqq \{T \in \matrices{N}{\field} \mid \det T \ne 0\}
        \end{equation}
        where \(\matrices{N}{\field}\) is the set of \(N \times N\) matrices over \(\field\).
    \end{dfn}
    
    There are a variety of notations for the general linear group, and the subsequent subgroups, such as \(\generalLinear_n(\field)\) or \(\generalLinear(n)\), leaving the precise field to context.
    
    All other groups to be defined are subgroups of \(\generalLinear(V)\), and so to show they are groups we need only show that they're closed under the induced operation and contain all inverses.
    This will require various linear algebra facts which we state without proof.
    We also won't worry too much about intricacies with definitions in the infinite dimensional case, we'll just work with matrices.
    
    \begin{dfn}{Special Linear Group}{}
        The \defineindex{special linear group}, \(\specialLinear(V)\)\index{SL(V)@\(\specialLinear(V)\)|see{special linear group}}, for finite dimensional \(V\), is the group of all invertible linear transformations with unit determinant:
        \begin{equation}
            \specialLinear(V) \coloneqq \{T \colon V \to V \mid T \text{ is invertible and } \det T = 1\} \subgroup \generalLinear(V).
        \end{equation}
        Choosing a basis we can identify each linear transformation with an \(N \times N\) matrix over \(\field\) and we get
        \begin{equation}
            \specialLinear(V) \coloneqq \{T \in \matrices{N}{\field} \mid \det T = 1\} \subgroup \generalLinear(N, \field).
        \end{equation}
    \end{dfn}
    
    We know that \(\det(S \circ T) = \det(S) \det(T)\), so if \(\det T = \det S = 1\) then \(\det(S \circ T) = 1\), so \(\specialLinear(V)\) is closed under composition.
    Since \(\det(T^{-1}) = 1/\det(T)\) if \(\det T = 1\) then \(\det(T^{-1}) = 1\), so \(\specialLinear(V)\) contains all inverses.
    Hence, \(\specialLinear(V)\) is a group.
    
    \begin{dfn}{Orthogonal Group}{}
        The \defineindex{orthogonal group}, \(\orthogonal(N)\)\index{O(n)@\(\orthogonal(n)\)|see{orthogonal group}}, is the group preserving the Euclidean metric.
        It consists of all orthogonal \(N \times N\) matrices over \(\reals\):
        \begin{equation}
            \orthogonal(N) \coloneqq \{O \in \matrices{N}{\reals} \mid O^\trans O = OO^\trans = \ident\} \subgroup \generalLinear(N, \reals).
        \end{equation}
    \end{dfn}
    
    Note that \(O^\trans O = \ident\) can be written as \(O^\trans \ident O = \ident\), where the \(\ident\) on the left is understood as the matrix form of the Euclidean metric, so the Euclidean metric is invariant under the action of the Orthogonal group
    \begin{equation}
        \delta_{\mu\nu} \xrightarrow{D \in \orthogonal(N)} \delta_{\alpha\beta} \tensor{D}{^\alpha_\mu}\tensor{D}{^\beta_\nu} = \delta_{\mu\nu}.
    \end{equation}
    This allows us to interpret orthogonal transformations as rotations, since they preserve distances and angles.
    
    Suppose \(O_1, O_2 \in \orthogonal(N)\), then \(O^\trans O = \ident\).
    Then \((O_1O_2)^{\trans}(O_1O_2) = O_2^\trans O_1^\trans O_1 O_2 = O_2^\trans \ident O_2 = \ident\), so \(\orthogonal(N)\) is closed under composition.
    We claim that if \(O \in \orthogonal(N)\) then \(O^{-1} \in \orthogonal(N)\), first note that \(O^{-1} = O^{\trans}\), then we have \((O^{-1})^\trans O^{-1} = (O^{\trans})^{\trans}O^{\trans} = OO^{\trans} = \ident\), so \(O^{-1} \in \orthogonal(N)\).
    Hence \(\orthogonal(N)\) contains all inverses.
    
    \begin{dfn}{Special Orthogonal Group}{}
        The \defineindex{special orthogonal group}, \(\specialOrthogonal(N)\)\index{SO(n)@\(\specialOrthogonal(n)\)|see{special orthogonal group}}, is the subgroup of the orthogonal group given by restricting to \(O \in \orthogonal(N)\) with \(\det O = 1\):
        \begin{equation}
            \specialOrthogonal(N) \coloneqq \{O \in \orthogonal(N) \mid \det O = 1\}.
        \end{equation}
        Note that \(\specialOrthogonal(N) \subgroup \orthogonal(N)\) and \(\specialOrthogonal(N) \subgroup \specialLinear(N, \reals)\).
    \end{dfn}
    
    The special orthogonal group is a group for exactly the same reasons that \(\orthogonal(N)\) and \(\specialLinear(N, \field)\) are.
    
    \begin{dfn}{Unitary Group}{}
        The \defineindex{unitary group}, \(\unitary(N)\)\index{U(n)@\(U(n)\)|see{unitary group}}, is the group preserving the standard inner product on a complex vector space:
        \begin{equation}
            \innerproduct{x}{y} = \sum_i x_i^* y_i.
        \end{equation}
        \begin{wrn}
            We follow the physics convention that an inner product is conjugate linear in its first argument, mathematicians often define it to be conjugate linear in the second instead.
        \end{wrn}
        The unitary group consists of all \(N \times N\) unitary matrices over \(\complex\):
        \begin{equation}
            \unitary(N) \coloneqq \{U \in \matrices{N}{\complex} \mid U^\hermit U = UU^\hermit = \ident\} \subgroup \generalLinear(N, \complex).
        \end{equation}
    \end{dfn}
    
    Note that \(U^\hermit U = \ident\) can be written as \(U^\hermit \ident U = \ident\), with the \(\ident\) on the left understood as the matrix form of the metric on this complex vector space, so the metric is invariant under the action of the unitary group
    \begin{equation}
        \delta_{\mu\nu} \xrightarrow{D \in \unitary(N)} \delta_{\alpha\beta} (\tensor{D}{^\alpha_\mu})^* \tensor{D}{^\beta_\nu} = \delta_{\mu\nu}.
    \end{equation}
    
    The same logic used to show the orthogonal group is a group works for the unitary group if we just replace transposes with Hermitian conjugates.
    
    \begin{dfn}{Special Unitary Group}{}
        The \defineindex{special unitary group}, \(\specialUnitary(N)\)\index{SU(n)@\(\specialUnitary(n)\)|see{special unitary group}}, is the subgroup of the unitary group given by restricting to \(U \in \unitary(N)\) with \(\det U = 1\):
        \begin{equation}
            \specialUnitary(N) \coloneqq \{U \in \unitary(N) \mid \det U = 1\}.
        \end{equation}
        Note that \(\specialUnitary(N) \subgroup \unitary(N)\) and \(\specialUnitary(N) \subgroup \specialLinear(N, \complex)\).
    \end{dfn}
    
    The special unitary group is a group for exactly the same reasons that \(\unitary(N)\) and \(\specialLinear(N, \field)\) are.
    
    \begin{exm}{\(\specialUnitary(2)\)}{exm:SU(2) = three sphere}
        Consider \(\specialUnitary(2)\).
        Start with some \(2\times 2\) matrix over \(\complex\),
        \begin{align}
            U = 
            \begin{pmatrix}
                a & b\\ c & d
            \end{pmatrix}
            .
        \end{align}
        The requirement that \(U^\hermit U = \ident\) tells us that
        \begin{equation}
            U^{-1} = U^{\hermit} \implies 
            \begin{pmatrix}
                d & -b\\
                -c & a
            \end{pmatrix}
            =
            \begin{pmatrix}
                a^* & c^*\\
                b^* & d^*
            \end{pmatrix}
        \end{equation}
        so \(a = d^*\) and \(c^* = -b\).
        Now add in the requirement that \(\det U = 1\) and we have
        \begin{equation}
            ad - bc = 1 \implies aa^* + bb^* = 1.
        \end{equation}
        Now write \(a = \alpha_1 + i \alpha_2\) and \(b = \alpha_3 + i\alpha_4\) for \(\alpha_i \in \reals\).
        Then we have
        \begin{equation}
            \alpha_1^2 + \alpha_2^2 + \alpha_3^2 + \alpha_4^2 = 1.
        \end{equation}
        This defines the three sphere,
        \begin{equation}
            \sphere[3] \coloneqq \{\vv{x} \in \reals^4 \mid x_1^2 + x_2^2 + x_3^2 + x_4^2 = \norm{\vv{x}}^2 = 1\}.
        \end{equation}
        This is a three-dimensional real manifold which is the parameter space for \(\specialUnitary(2)\).
        Note that \(\sphere[3]\) is simply connected, that is any loop can be contracted to a point.
    \end{exm}
    
    \begin{dfn}{Pseudo-Orthogonal Group}{}
        The \defineindex{pseudo-orthogonal group}, \(\orthogonal(n, m)\)\index{O(n, m)@\(\orthogonal(n, m)\)|see{pseudo-orthogonal group}}, is the group preserving the metric with signature \((n, m)\).
    \end{dfn}
    
    \begin{exm}{Lorentz Group}{}
        The \defineindex{proper Lorentz group}, \(\specialOrthogonal(3, 1)\), is the group of Lorentz transformations, \(\Lambda\), preserving the Minkowski metric:
        \begin{equation}
            \Lambda^\trans \minkowskiMetric \Lambda = \minkowskiMetric \iff \tensor{\Lambda}{_\mu^\rho} \minkowskiMetric_{\rho\sigma} \tensor{\Lambda}{^\sigma_\nu} = \minkowskiMetric_{\mu\nu},
        \end{equation}
        and with \(\det \Lambda = 1\).
        
        Note that we can also consider the general \defineindex{Lorentz group} \(\orthogonal(3, 1)\)\index{O(3,1)@\(\orthogonal(3, 1)\)|see{Lorentz group}}, which allows transformations inverting spacetime with \(\det \Lambda \ne 1\), and the \defineindex{proper orthochronous Lorentz group}, \(\specialOrthogonal^+(3, 1)\)\index{SO+(3,1)@\(\specialOrthogonal^+(3, 1)\)|see{proper orthochronous Lorentz group}}, which has \(\tensor{\Lambda}{^0_0} \ge 1\).
    \end{exm}
    
    \begin{dfn}{Pseudo-Unitary Group}{}
        The \defineindex{pseudo-unitary group} is the group preserving an indefinite metric on a complex vector space.
    \end{dfn}
    
    \begin{dfn}{Symplectic Groups}{}
        The \defineindex{symplectic group} \(\symplectic(2N, \field)\)\index{Sp(2n, k)@\(\symplectic(2n, \field)\)|see{symplectic group}}, preserves an antisymmetric metric given by the block diagonal matrix
        \begin{equation}
            g = 
            \begin{pmatrix}
                0 & \ident_N\\
                -\ident_N & 0
            \end{pmatrix}
        \end{equation}
        on a \(2N\)-dimensional vector space over \(\field\).
        
        The \defineindex{compact symplectic group}, \(\symplectic(2N)\)\index{Sp(2n)@\(\symplectic(2n)\)|see{compact symplectic group}}, is \(\symplectic(2N) = \symplectic(2N, \complex) \cap \unitary(2N)\).
        We can think of it as being the result of taking matrices in \(\symplectic(2N), \complex\) and replacing complex numbers with \(2\times 2\) real matrices according to
        \begin{equation}
            x + iy \mapsto 
            \begin{pmatrix}
                x & y\\
                -y & x
            \end{pmatrix}
            .
        \end{equation}
    \end{dfn}
    
    The symplectic group arises in physics when we consider phase space.
    In classical mechanics in three spatial dimensions phase space is the six dimensional space spanned by the three components of position and three components of momentum, so a point in phase space is \((q_1, q_2, q_3, p_1, p_2, p_3)\).
    Symplectic transformations, \(\symplectic(6, \reals)\), are the set of transformations preserving Hamilton's equations:
    \begin{equation}
        \dot{q}_i = \diff{H}{p_i}, \qquad \dot{p}_i = -\diff{H}{q_i}.
    \end{equation}
    
    With some restrictions most Lie groups fit into one of the previously mentioned categories.
    However, there are five Lie groups that don't, these are called the \define{exceptional groups}\index{exceptional group}.
    There is no particularly simple definition of any of them, we just note here that they exist, and are called \(F_4\), \(G_2\), \(E_6\), \(E_7\), and \(E_8\).
    These names come from the classification of semisimple Lie algebras (to be defined later), where we call the Lie algebras of \(\specialLinear(n + 1, \complex)\), \(\specialOrthogonal(2n + 1)\), \(\symplectic(2n)\), and \(\specialOrthogonal(2n)\) by the names \(A_n\), \(B_n\), \(C_n\), and \(D_n\) respectively.
    Here \(n\) is the rank of the Lie algebra (to be defined later).
    
    \section{One Dimensional Groups}
    In this section we will discuss one-dimensional groups, these are groups parametrised by a single value.
    \begin{exm}{}{}
        \begin{itemize}
            \item Translations along \(\reals\) form a one-dimensional Lie group parametrised by the size of the translation, \(a \in \reals\), with the action \(x \mapsto x + a\).
            This group is just \(\reals\).
            This example is noncompact.
            \item Rotations about some fixed axis form a one-dimensional Lie group parametrised by the size of the rotation, \(\vartheta \in [0, 2\pi)\), with the action
            \begin{equation}
                \begin{pmatrix}
                    x\\ y
                \end{pmatrix}
                \mapsto
                \begin{pmatrix}
                    \cos\vartheta & \sin\vartheta\\
                    -\sin\vartheta & \cos\vartheta
                \end{pmatrix}
                \begin{pmatrix}
                    x\\ y
                \end{pmatrix}
                .
            \end{equation}
            This group is \(\specialOrthogonal(2)\).
            This example is compact.
            \item Multiplication by a phase factor forms a one-dimensional Lie group parametrised by the argument of the phase factor, \(\varphi \in [0, 2\pi)\), with the action \(z \mapsto \e^{i\varphi}z\).
            This group is \(\unitary(1)\).
            This example is compact.
        \end{itemize}
        These last two examples are actually isomorphic, using \(\e^{i\varphi} = \cos \varphi + i\sin \varphi\) we can map from \(\unitary(1)\) to \(\specialOrthogonal(2)\) with
        \begin{equation}
            \e^{i\varphi} \mapsto 
            \begin{pmatrix}
                \cos\varphi & \sin\varphi\\
                -\sin\varphi & \cos\varphi
            \end{pmatrix}
            .
        \end{equation}
        So, \(\unitary(1) \isomorphic \specialOrthogonal(2)\).
    \end{exm}
    
    One-dimensional Lie groups are, unsurprisingly, some of the simplest Lie groups.
    In fact, they're so simple that they're not that interesting, a theory we'll develop rigorously through a collection of theorems.
    
    \begin{thm}{}{thm:one dim lie group parameters add}
        All one-dimensional Lie groups can be parametrised so that
        \begin{equation}
            g(a) g(b) = g(a + b)
        \end{equation}
        for all \(a\) and \(b\).
        \begin{proof}
            Consider a one-dimensional Lie group, \(G\), parametrised by some value in the interval \(I\).
            Associativity tells us that
            \begin{equation}
                g(x) [g(y)g(z)] = [g(x)g(y)]g(z)
            \end{equation}
            for all \(x, y, z \in I\).
            We can write \(g(y)g(z)\) and \(g(x)g(y)\) as single group elements \(g(\varphi(y, z))\) and \(g(\varphi(x, y))\) for some analytic function \(\varphi \colon I^2 \to I\), this is just the definition of a Lie group.
            Doing so we have
            \begin{equation}
                g(x) g(\varphi(y, z)) = g(\varphi(x, y))g(z).
            \end{equation}
            Now we can use analyticity again to write \(g(x)g(\varphi(y, z)) = g(x)g(a) = g(\varphi(x, a)) = g(\varphi(x, \varphi(y, z)))\) and \(g(\varphi(x, y))g(z) = g(\varphi(\varphi(x, y), z))\):
            \begin{equation}
                g(\varphi(x, \varphi(y, z))) = g(\varphi(\varphi(x, y), z)).
            \end{equation}
            Hence, we must have
            \begin{equation}
                \varphi(x, \varphi(y, z)) = \varphi(\varphi(x, y), z).
            \end{equation}
            
            Now, take the derivative of this expression with respect to \(z\), the right hand side gives
            \begin{equation}
                \diffp{}{z}\varphi(\varphi(x, y), z),
            \end{equation}
            and the chain rule applied to the left hand side gives
            \begin{equation}
                \diffp{}{z} \varphi(x, \varphi(y, z)) = \diffp{\varphi(x, \varphi(y, z))}{{\varphi(y, z)}} \diffp{\varphi(y, z)}{z}.
            \end{equation}
            
            We are free to shift our parametrisation interval around, so imagine we choose it to contain zero and choose a parametrisation such that \(g(0) = 1\).
            Consider the case where \(z = 0\).
            Then \(g(y)g(z) = g(y)g(0) = g(y)1 = g(y)\), but we also have \(g(y)g(z) = g(\varphi(y, z)) = g(\varphi(y, 0))\), so we must have \(\varphi(y, 0) = y\).
            Thus, evaluating are derivatives at \(z = 0\), we have
            \begin{equation}
                \diffp{\varphi(x, y)}{y} \psi(y) = \psi(\varphi(x, y))
            \end{equation}
            where
            \begin{equation}
                \psi(y) \coloneqq \diffp{\varphi(y, z)}{z} \bigg|_{z = 0}.
            \end{equation}
            This differential equation can be solved by writing it as
            \begin{equation}
                \frac{1}{\psi(\varphi(x, y))} \diffp{\varphi(x, y)}{y} = \frac{1}{\psi(y)}.
            \end{equation}
            We can then integrate with respect to \(y\) and we get
            \begin{equation}
                \rho(\varphi(x, y)) = \rho(y) + c(x)
            \end{equation}
            where \(c\) is an arbitrary function of \(x\), taking the role of our integration constant, but not constant as \(x\) is allowed to vary, and 
            \begin{equation}
                \rho(x) \coloneqq \int_0^x \frac{1}{\psi(t)} \dd{t}
            \end{equation}
            with \(\rho(0) = 0\).
            
            We can determine \(c(x)\) by choosing \(y = 0\), which gives \(\rho(\varphi(x, 0)) = \rho(x) = \rho(0) + c(x) = 0 + c(x)\), so \(c(x) = \rho(x)\).
            Hence,
            \begin{equation}
                \rho(\varphi(x, y)) = \rho(x) + \rho(y).
            \end{equation}
            
            Now we just reparametrise our group from \(g(x)\) to \(\overbar{g}(\rho(x))\), we then have
            \begin{equation}
                \overbar{g}(\rho(x))\overbar{g}(\rho(y)) = g(x)g(y) = g(\varphi(x, y)) = \overbar{g}(\rho(\varphi(x, y))) = \overbar{g}(\rho(x) + \rho(y)).
            \end{equation}
            Hence, our group operation becomes addition in parameter space and we are finished.
        \end{proof}
    \end{thm}
    
    \begin{crl}{}{}
        All one dimensional Lie algebras are Abelian.
        \begin{proof}
            Let \(G\) be a one-dimensional Lie group parametrised such that \(g(a)g(b) = g(a + b)\) for all \(a\) and \(b\).
            Then
            \begin{equation}
                g(a)g(b) = g(a + b) = g(b + a) = g(b)g(a),
            \end{equation}
            and so \(G\) is Abelian.
        \end{proof}
    \end{crl}
    
    The following theorem won't be proved here, but intuitively all it says is that every compact connected Abelian Lie group can be parametrised by phase factors.
    \begin{thm}{}{}
        All compact connected Abelian Lie groups are isomorphic to
        \begin{equation}
            \bigotimes_{i = 1}^{n} \unitary(1) = \underbrace{\unitary(1) \otimes \dotsb \otimes \unitary(1)}
        \end{equation}
        for some \(n \in \naturals\).
    \end{thm}
    Identifying each copy of \(\unitary(1)\) with a circle we see that every compact connected Abelian Lie group is a product of circles, which is a Torus.
    
    This means that in most of the course we'll be interested in non-Abelian Lie groups.
    
    \chapter{Representations}
    \section{What is a Representation}
    Intuitively a representation of a group, \(G\), is a set of \(N \times N\) matrices, \(D\), parametrised by group elements, \(g\), such that matrix multiplication is compatible with the group operation:
    \begin{equation}
        D(g) D(h) = D(gh) \iff \tensor{D}{^\mu_\nu}(g) \tensor{D}{^\nu_\rho}(h) = \tensor{D}{^\mu_\rho}(g h)
    \end{equation}
    for all \(g, h \in G\).
    
    \begin{exm}{\(\symmetricGroup[3]\)}{}
        The symmetric group on 3 objects has a representation, called the \defineindex{permutation representation}, given by the matrices
        \begin{alignat*}{3}
            \cycle{} &\mapsto 
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & 1 & 0\\
                0 & 0 & 1
            \end{pmatrix}
            , \quad & \cycle{1,2} &\mapsto
            \begin{pmatrix}
                0 & 1 & 0\\
                1 & 0 & 0\\
                0 & 0 & 1
            \end{pmatrix}
            , \quad & \cycle{1,3} &\mapsto
            \begin{pmatrix}
                0 & 0 & 1\\
                0 & 1 & 0\\
                1 & 0 & 0
            \end{pmatrix}
            \\
            \cycle{2,3} &\mapsto 
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & 0 & 1\\
                0 & 1 & 0
            \end{pmatrix}
            , \quad & \cycle{1,2,3} &\mapsto
            \begin{pmatrix}
                0 & 1 & 0\\
                0 & 0 & 1\\
                1 & 0 & 0
            \end{pmatrix}
            , \quad & \cycle{3,2,1} &\mapsto
            \begin{pmatrix}
                0 & 0 & 1\\
                1 & 0 & 0\\
                0 & 1 & 0
            \end{pmatrix}
            .
        \end{alignat*}
        To motivate this suppose that the three objects are the basis vectors
        \begin{equation}
            e_1 = 
            \begin{pmatrix}
                1\\0\\0
            \end{pmatrix}
            , \quad e_2 =
            \begin{pmatrix}
                0\\1\\0
            \end{pmatrix}
            , \qand e_3 = 
            \begin{pmatrix}
                0\\0\\1
            \end{pmatrix}
            .
        \end{equation}
        Then this representation acts by permuting the vectors in the obvious way.
        For example, the matrix representing \(\cycle{1,2}\) sends \(e_1\) to \(e_2\), \(e_2\) to \(e_1\), and \(e_3\) to itself.
        
        Another representation of \(\symmetricGroup[3]\) is given by the \defineindex{trivial representation}, which represents all group elements with the \(1\times 1\) zero matrix, \((0)\), sending all vectors to the zero vector.
        
        A third representation of \(\symmetricGroup[3]\) is given by sending each element either to \((1)\) or \((-1)\), depending on the sign of the permutation.
    \end{exm}
    
    There are two ways to formalise the notion of a representation, they are as follows.
    \begin{dfn}{Representation}{}
        Let \(G\) be a group.
        Then a group \define{representation}\index{representation!of a group}, \((D, V)\), is a vector space, \(V\), and a homomorphism, \(D \colon G \to \generalLinear(V)\).
    \end{dfn}
    
    \begin{dfn}{Representation}{}
        Let \(G\) be a group and \(V\) a vector space.
        A group \define{representation}, \((D, V)\), is a vector space, \(V\), and a group action, \(G \times V \to V\) given by \(g \action v = D(g)v\) where \(D(g)\) is some linear transformation.
    \end{dfn}
    
    \begin{ntn}{}{}
        We're generally pretty loose with the exact language as to what objects make up a representation.
        People will refer to \((D, V)\), \(D\), \(V\), and the set of matrices \(\{D(g) \mid g \in G\}\) as the representation interchangeably, with context telling us which we care about.
    \end{ntn}
    
    \begin{dfn}{Representation Dimension}{}
        The dimension of a representation is the dimension of the vector space, \(V\), which is also the number of rows in the matrix.
    \end{dfn}
    
    \begin{wrn}
        The dimension of the representation is, in general, \emph{not} the same as the dimension of a continuous group.
    \end{wrn}
    
    \begin{exm}{\(\symmetricGroup[3]\)}{}
        The permutation representation is of dimension 3, the trivial representation is of dimension 1, as is the sign representation.
    \end{exm}
    
    \section{Properties of Representations}
    \begin{exm}{Faithful Representation}{}
        A representation is faithful if \(\rho\) is injective.
    \end{exm}
    
    \begin{exm}{\(\symmetricGroup[3]\)}{}
        The permutation representation is faithful, the trivial and sign permutations are not.
    \end{exm}
    
    Representations give us another way to think about the matrix groups of the last chapter.
    Instead of defining them as matrices we can define them by symmetries, so, for example, \(\orthogonal(N)\) is the group preserving the Euclidean metric.
    Then the usual interpretation as \(N \times N\) orthogonal matrices is just a representation.
    We call it the \defineindex{defining representation}, since it can be used to define the representation.
    The defining representation must be an isomorphism in order for it to contain all group elements exactly once.
    
    The \defineindex{fundamental representation} is one from which all other all other representations can be built from tensor products.
    
    \begin{dfn}{Equivalence of Representations}{}
        Two representations are \define{equivalent}\index{equivalent representations} if they are related by a similarity transformation.
        That is, if \(G\) is a group with representations \(D\) and \(D'\) then there exists some \(S\), independent of \(g\), such that for all \(G\)
        \begin{equation}
            D'(g) = SD(g)S^{-1}.
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Reducible}{}
        A representation, \((D, V)\), is \define{reducible}\index{reducible representation} if there exists some subspace \(U \subset V\) with \(U \ne V\) and \(U \ne \{0\}\) such that
        \begin{equation}
            D(g) u \in U \text{ for all } u \in U \text{ and } g \in G.
        \end{equation}
        If this is the case we call \(U\) an \defineindex{invariant subspace}.
        A representation is \define{irreducible}\index{irreducible representation} if it is not reducible.
    \end{dfn}
    
    \begin{ntn}{}{}
        People often shorten \enquote{irreducible representation} to \define{irrep}\index{irrep|see{irreducible representation}}.
    \end{ntn}
    
    Notice that if \(D\) is a representation of \(G\) then so is \(D^*\), since
    \begin{equation}
        D(g)^*D(h)^* = (D(g)D(h))^* = D(gh)^*
    \end{equation}
    for all \(g, h \in G\).
    
    \begin{dfn}{}{}
        Let \(D\) be a representation of some group \(G\).
        
        If \(D(g)^* = D(g)\) for all \(g \in G\) then we say that \(D\) is a \defineindex{real representation}.
        
        If \(D\) is not equivalent to \(D^*\) then we say that \(D\) is a \defineindex{complex representation}.
        
        If \(D \ne D^*\) but \(D\) is equivalent to \(D^*\) then we say that \(D\) is a \defineindex{pseudo-real representation}.
    \end{dfn}
    
    Suppose that we have some symmetry described by the group \(G\).
    Then we classify different types of objects based on which representation they transform under.
    If
    \begin{equation}
        s \mapsto s,
    \end{equation}
    that is there is no transformation, then \(s\) transforms under the trivial representation and we call \(s\) a \defineindex{scalar}.
    If
    \begin{equation}
        v^\mu \mapsto \tensor{D}{^\mu_\nu}(g)v^\nu,
    \end{equation}
    where \(D\) is the fundamental representation we say that \(v\) is a \defineindex{vector}.
    If
    \begin{equation}
        T^{\mu_1\mu_2\dots \mu_n} \mapsto \tensor{D}{^{\mu_1}_{\nu_1}}(g) \tensor{D}{^{\mu_2}_{\nu_2}}(g) \dotsm \tensor{D}{^{\mu_n}_{\nu_n}}(g) T^{\nu_1\nu_2 \dots \nu_n}
    \end{equation}
    then we say that \(T\) is a \defineindex{tensor}, note that both scalars and vectors are special cases of tensors.
    
    We can write this last line more succinctly as
    \begin{equation}
        T^\alpha = \tensor{D}{^\alpha_\beta}(g) T^\beta.
    \end{equation}
    We understand this as \(T\) transforming under the tensor product
    \begin{equation}
        \tensor{(D(g) \otimes D(g) \otimes \dotsb \otimes D(g))}{^{\mu_1\mu_2 \dots \mu_n}}_{\nu_1\nu_2\dots \nu_n} = \tensor{D}{^{\mu_1}_{\nu_1}}(g) \tensor{D}{^{\mu_2}_{\nu_2}}(g).
    \end{equation}
    
    \section{Unitary Representations}
    
    \begin{dfn}{Unitary Representation}{}
        Let \(G\) be a group and \((D, V)\) a representation of \(G\).
        We say that \(D\) is a \defineindex{unitary representation} if it preserves the inner product on \(V\), that is if
        \begin{equation}
            \innerproduct{u}{v} = \innerproduct{D(g)u}{D(g)v}
        \end{equation}
        for all \(g, h \in G\).
    \end{dfn}
    
    \begin{thm}{Maschke's Theorem}{thm:maschke}
        Any representation of a finite group is equivalent to a unitary representation.
        \begin{proof}
            Let \(G\) be a finite group, \(V\) a vector space, and \((D, V)\) a representation.
            Let \(\innerproduct{-}{-}\) be the inner product on \(V\).
            Define a new inner product,
            \begin{equation}
                \braket{x}{y} \coloneqq \sum_{g \in G} \innerproduct{D(g)x}{D(g)y}.
            \end{equation}
            Then we claim that \((D, V)\) is a unitary representation with respect to this new inner product and since different inner products are related by similarity transformations \((D, V)\) is equivalent to a unitary representation.
            
            Take some fixed \(h \in G\).
            The rearrangement theorem states that if the elements of the group are listed, \((g_1, \dotsc, g_n)\), then the action \((g_1, \dotsc, g_n) \mapsto (g_1h, \dotsc, g_nh)\) is just a permutation.
            In particular, if \(f \colon G \to \complex\) then
            \begin{equation}
                \sum_{g \in G} f(g) = \sum_{g\in G} f(gh),
            \end{equation}
            since multiplying each argument by \(h\) just permutes the terms.
            
            Using this we can equivalently write the inner product, \(\braket{-}{-}\), as
            \begin{align}
                \braket{x}{y} &= \sum_{g\in G} \innerproduct{D(gh)x}{D(gh)y}\\
                &= \sum_{g\in G} \innerproduct{D(g)D(h)x}{D(g)D(h)y}\\
                &= \sum_{g\in G} \innerproduct{D(g)u}{D(g)v}\\
                &= \braket{u}{v},
            \end{align}
            where \(u = D(h)x\) and \(v = D(h)y\).
            Then we have
            \begin{equation}
                \braket{x}{y} = \braket{u}{v} = \braket{D(h)x}{D(h)y} = \bra{x}D^\hermit(h)D(h)\ket{y},
            \end{equation}
            so for this to hold true for all \(x, y \in V\) we must have \(D^\hermit(h)D(h) = \ident\).
            Hence, \(D\) is unitary with respect to this new inner product.
        \end{proof}
    \end{thm}
    
    A very similar theorem holds for compact Lie groups.
    The proof is almost identical but the sum is replaced with an integral.
    We won't prove it here as it requires some measure theory to make everything work.
    It comes down to the compactness requirement ensuring convergence of the integrals at every step.
    Without this requirement the integrals may not converge, in which case there is no reason for an equivalent unitary representation to exist.
    \begin{thm}{}{}
        Every representation of a compact Lie group is equivalent to a unitary representation.
    \end{thm}
    
    Why do we care about unitary representations?
    Mostly for quantum mechanics.
    If a group action on a Hilbert space of states is to preserve probability then it must be either unitary, so \(\innerproduct{Ux}{Uy} = \innerproduct{x}{y}\), or \defineindex{antiunitary}, that is \(\innerproduct{Ux}{Uy} = \innerproduct{x}{y}^*\).
    An example of an antiunitary transformation is time reversal.
    Consider a system evolving from state \(\ket{\psi}\) to \(\ket{\varphi}\).
    The amplitude for this is \(\braket{\psi}{\varphi}\).
    The time reversed system evolves from state \(\ket{\varphi}\) to state \(\ket{\psi}\).
    The amplitude for this is \(\braket{\varphi}{\psi} = \braket{\psi}{\varphi}^*\).
    
    \part{Lie Algebras}
    \chapter{Lie Algebras}
    There are, broadly, two approaches to Lie algebras.
    One can derive their properties and then abstract, or abstract and then show that there is a connection to Lie groups.
    The first is how a typical physics course goes about it, and fits the historical process of discovering Lie algebras, but I prefer to abstract and then look at applications where possible, so that's what these notes will do.
    
    We'll define Lie algebras as abstract objects, and briefly discuss some properties which follow immediately.
    Then we'll discuss the link to Lie groups, and then go into more detail about the properties of Lie algebras.
    
    \section{What is an Algebra?}
    An algebra is a vector space equipped with a product compatible with the vector space structure.
    Formally, the definition is as follows.
    
    \begin{dfn}{Algebra}{}
        Let \(\field\) be a field and \(A\) a vector space over \(\field\).
        Equip \(A\) with a binary operation \(A \times A \to A\), denoted by juxtaposition here.
        If this product has the following properties then \(A\) is an \defineindex{algebra}:
        \begin{itemize}
            \item Left distributivity: \(x(y + z) = xy + xz\) for all \(x, y, z \in A\);
            \item Right distributivity: \((x + y)z = xz + yz\) for all \(x, y, z \in A\);
            \item Compatibility with scalar multiplication: \((\alpha x)(\beta y) = (\alpha\beta)(xy)\) for all \(\alpha, \beta \in \field\) and \(x, y \in A\).
        \end{itemize}
        We call \(A\) an \defineindex{associative algebra} if the product is associative.
    \end{dfn}
    
    \begin{exm}{Algebra}{}
        \begin{itemize}
            \item The real numbers are a vector space over themselves, and also an algebra over themselves with the product being normal multiplication.
            \item The complex numbers are a vector space over themselves, and also an algebra over themselves with the product being normal multiplication.
            \item The complex numbers are a vector space over \(\reals\), and also an algebra over \(\reals\) with the product being normal complex multiplication when we think of complex numbers as ordered pairs of real numbers.
            \item The vector space \(\reals^3\) when equipped with the cross product forms an algebra over \(\reals\).
            \item The vector space \(\reals^4\) when identified with the quaternions forms an algebra over \(\reals\) with the product being the quaternion product.
            \item Polynomials over \(\reals\), that is \(\reals[x]\), form a vector space over \(\reals\) and when equipped with polynomial multiplication this is an algebra.
            \item The set of \(m \times n\) matrices over \(\reals\) forms an \(mn\)-dimensional vector space.
            This forms an algebra over \(\reals\) when equipped with the normal matrix product.
            \item The set of \(m \times n\) matrices over \(\reals\) forms an \(mn\)-dimensional vector space.
            This forms an algebra over \(\reals\) when equipped with the \defineindex{commutator}, \(\commutator{A}{B} \coloneqq AB - BA\).
        \end{itemize}
    \end{exm}
    
    \section{What is a Lie Algebra}
    A Lie algebra is an algebra where the product is given by a Lie bracket, which is just a particular form of product with a couple of properties which we write as a bracket, so \(\commutator{a}{b}\) instead of \(ab\).
    
    \begin{dfn}{Lie Algebra}{}
        A \defineindex{Lie algebra}, \(\lie{g}\), is a vector space over some field, \(\field\), equipped with a binary operation, \(\commutator{-}{-} \colon \lie{g} \times \lie{g} \to \lie{g}\), satisfying the following:
        \begin{itemize}
            \item \define{Bilinearity}\index{bilinear}: for all \(\alpha, \beta \in \field\) and \(x, y, z \in \lie{g}\) we have
            \begin{equation*}
                \commutator{\alpha x + \beta y}{z} = \alpha\commutator{x}{z} + \beta \commutator{y}{z}, \qand \commutator{z}{\alpha x + \beta y} = \alpha\commutator{z}{x} + \beta \commutator{z}{y}.
            \end{equation*}
            \item \define{Alternativity}\index{alternating}: for all \(x \in \lie{g}\) we have
            \begin{equation}
                \commutator{x}{x} = 0,
            \end{equation}
            where \(0\) is the zero vector.
            \item The \defineindex{Jacobi identity}: for all \(x, y, z \in \lie{g}\) we have
            \begin{equation}
                \commutator{x}{\commutator{y}{z}} + \commutator{y}{\commutator{z}{x}} + \commutator{z}{\commutator{x}{y}} = 0.
            \end{equation}
            Note that this is just \(\commutator{x}{\commutator{y}{z}}\) plus cyclic permutations.
        \end{itemize}
    \end{dfn}
    \begin{ntn}{}{}
        Lie algebras are, unsurprisingly, related to Lie groups.
        If we have a Lie group denoted with capital letters, such as \(G\), \(\generalLinear\), \(\specialOrthogonal\), \(\unitary\), and so on, then we denote the associated Lie algebra by the same letter but lowercase and in the Fraktur script, so \(\lie{g}\), \(\generalLinearLie\), \(\specialOrthogonalLie\), \(\unitaryLie\), and so on.
        For future reference here is the alphabet, in order, in Fraktur:
        \begin{equation}
            \lie{a \, b \, c \, d \, e \, f \, g \, h \, i \, j \, k \, l \, m \, n \, o \, p \, q \, r \, s \, t \, u \, v \, w \, x \, y \, z}.
        \end{equation}
        It is also common to use lowercase, non-Fraktur letters for Lie algebras, such as \(g\), \(\symrm{gl}\), \(\symrm{so}\), \(\symrm{u}\), and so on.
    \end{ntn}
    
    Note that some sources define the Lie bracket to be anticommutative, instead of alternating.
    That is, \(\commutator{x}{y} = -\commutator{y}{x}\) for all \(x, y \in \lie{g}\).
    This is necessarily the case for an alternating Lie bracket:
    \begin{multline}
        0 = \commutator{x + y}{x + y} = \commutator{x}{x + y} + \commutator{y}{x + y}\\
        = \commutator{x}{x} + \commutator{x}{y} + \commutator{y}{x} + \commutator{y}{y} = \commutator{x}{y} + \commutator{y}{x},
    \end{multline}
    and so \(\commutator{x}{y} = -\commutator{y}{x}\).
    However, the reverse implication, that an anticommutative product is alternating, only holds if the characteristic\footnote{the \defineindex{characteristic} of a ring, and hence field, is the number of times you have to add 1 to itself to get 0, or 0 if you never get 0 this way.} of the field is not 2, so our definition is \emph{slightly} more general.
    However, we'll almost entirely work over \(\reals\) and \(\complex\) which have characteristic 0, so it's not an important distinction.
    So, suppose that we work over a field with characteristic not equal to 2, then \(\commutator{x}{x} = -\commutator{x}{x}\) by anticommutativity (in the second bracket we swapped the \(x\)s, you just can't tell).
    Hence, we have
    \begin{equation}
        2 \commutator{x}{x} = \commutator{x}{x} + \commutator{x}{x} = \commutator{x}{x} - \commutator{x}{x} = 0,
    \end{equation}
    and so long as we can divide by 2 we have \(\commutator{x}{x} = 0\).
    It is this dividing by 2 step that fails in a field with characteristic 2.
    
    The Jacobi identity is a bit weird as a requirement.
    Define \(D_x(y) = \commutator{x}{y}\), then we can write the Jacobi identity as
    \begin{equation}
        D_x(\commutator{y}{z}) = \commutator{D_x(y)}{z} + \commutator{y}{D_x(z)},
    \end{equation}
    which looks a lot like the product rule.
    Going back to writing Lie brackets this is
    \begin{equation}
        \commutator{x}{\commutator{y}{z}} = \commutator{\commutator{x}{y}}{z} + \commutator{y}{\commutator{x}{z}}.
    \end{equation}
    Using the anticommutativity of the Lie bracket (in fields of characteristic other than 2, an implicit assumption from now on) and the bilinearity we can rewrite
    \begin{equation}
        \commutator{\commutator{x}{y}}{z} = -\commutator{z}{\commutator{x}{y}}
    \end{equation}
    and
    \begin{equation}
        \commutator{y}{\commutator{x}{z}} = \commutator{y}{-\commutator{z}{x}} = -\commutator{y}{\commutator{z}{x}}.
    \end{equation}
    Hence,
    \begin{equation}
        \commutator{x}{\commutator{y}{z}} = -\commutator{z}{\commutator{x}{y}} - \commutator{y}{\commutator{z}{x}},
    \end{equation}
    which is just the Jacobi identity with some terms moved to the other side.
    
    \begin{exm}{Lie Algebra}{}
        \begin{itemize}
            \item Consider the set of \(m \times n\) matrices over \(\reals\).
            This forms a Lie algebra when equipped with the commutator, \(\commutator{A}{B} = AB - BA\).
            \item Consider \(\reals^3\).
            This forms a Lie algebra when equipped with the cross product, \(\commutator{\vv{v}}{\vv{u}} = \vv{v} \times \vv{u}\).
        \end{itemize}
    \end{exm}
    
    \section{Subalgebras}
    As with groups we can often find Lie algebras hiding inside other Lie algebras, and certain such Lie algebras are special.
    \begin{dfn}{Lie Subalgebra and Ideals}{}
        Let \(\lie{g}\) be a Lie algebra and \(\lie{h}\) a subspace of \(\lie{g}\) (as vector spaces).
        Then \(\lie{h}\) is a \defineindex{Lie subalgebra}, or simply a \defineindex{subalgebra}, of \(\lie{g}\) if \(\commutator{x}{y} \in \lie{h}\) for all \(x, y \in \lie{h}\), where \(\commutator{-}{-}\) is the Lie bracket of \(\lie{g}\).
        
        An \defineindex{ideal}, or an \defineindex{invariant subalgebra}, is a Lie subalgebra \(\lie{i}\) with the stronger condition that \(\commutator{g}{i} \in \lie{i}\) for all \(g \in \lie{g}\).
    \end{dfn}
    
    The definition of an ideal here is analogous to that of a normal subgroup.
    A normal subgroup is invariant under conjugation with any group element and an ideal is invariant under Lie brackets with any algebra element.
    Note that if \(\commutator{g}{i} \in \lie{i}\) then \(\commutator{i}{g} = -\commutator{g}{i} \in \lie{i}\) also as \(\lie{i}\) is a subspace of \(\lie{g}\).
    Similar to the group case a Lie algebra is \define{simple}\index{simple!Lie algebra} if it has no nontrivial ideals, that is the only ideals are itself and the zero dimensional vector space, and it is not Abelian.
    A Lie algebra is \define{Abelian}\index{Abelian!Lie algebra} if the Lie bracket vanishes, that is \(\commutator{x}{y} = 0\) for all \(x, y \in \lie{g}\).
    Note that this is equivalent to saying \(\commutator{x}{y} = \commutator{y}{x}\), the more usual notion of being Abelian, but anticommutativity means \(\commutator{y}{x} = -\commutator{x}{y}\), so we have \(\commutator{x}{y} = -\commutator{x}{y}\), and so \(\commutator{x}{y} = 0\).
    
    \section{Morphisms Between Lie Algebras}
    As often happens in maths after defining an object we should define maps between these objects.
    These maps should preserve the structure of the object, and if such a map is invertible then the structure, at least at the Lie algebra level, of the two objects is the same, and so, when thinking about Lie algebras, we treat the two objects as if they were identical.
    \begin{dfn}{Morphisms}{}
        Let \(\lie{g}\) and \(\lie{h}\) be Lie algebras.
        A \defineindex{Lie algebra homomorphism}\index{homomorphism!of Lie algebras} is a function \(\varphi \colon \lie{g} \to \lie{h}\) preserving the bracket, that is
        \begin{equation}
            \varphi(\commutator{x}{y}) = \commutator{\varphi(x)}{\varphi(y)}
        \end{equation}
        for all \(x, y \in \lie{g}\).
        Here the Lie bracket on the left is that of \(\lie{g}\), and on the right the Lie bracket is that of \(\lie{h}\).
        
        An \defineindex{Lie algebra isomorphism}\index{isomorphism!of Lie algebras} is an invertible Lie algebra homomorphism.
    \end{dfn}
    
    \section{Generators and Structure Constants}
    Lie algebras are vector spaces, and as such have lots of nice things, like bases, that we can use to make calculations easier.
    \begin{dfn}{Generators}{}
        Let \(\lie{g}\) be a Lie algebra.
        A set of elements of \(\lie{g}\) are said to \define{generate}\index{generator} \(\lie{g}\) if the smallest subalgebra containing these elements is \(\lie{g}\) itself.
        That is, if all elements of \(\lie{g}\) can be be generated from linear combinations of the generators, and Lie brackets of the generators, and Lie brackets of Lie brackets of the generators and so on.
        The generators are the basis for \(\lie{g}\) as a vector space.
    \end{dfn}
    
    \begin{ntn}{}{}
        Generators are commonly denoted \(T_a\), where \(a\) indexes the generating set.
    \end{ntn}
    
    Let \(\lie{g}\) be a Lie algebra with generators \(T_a\).
    Since A Lie algebra is a vector space, and any element of it can be expressed as a linear combination of basis vectors, which for a Lie algebra are the generators.
    In particular, the Lie bracket of two elements of the Lie algebra is another element of the Lie algebra, and so can be expressed in this way:
    \begin{equation}
        \commutator{x}{y} = f^a(x, y) T_a
    \end{equation}
    where \(f^a(x, y)\) is the coefficient of \(T_a\) in this expansion, and there is an implicit sum over \(a\).
    Naturally we can consider the case when \(x\) and \(y\) are themselves generators, and this leads to the following definition.
    \begin{dfn}{Structure Constants}{}
        Let \(\lie{g}\) be a Lie algebra with generators \(T_a\).
        Then the \defineindex{structure constant} \(\tensor{c}{^c_{ab}}\) is defined as the coefficient in one of the following:
        \begin{equation}
            \commutator{T_a}{T_b} = \tensor{c}{^c_{ab}}T_c, \qqor \commutator{T_a}{T_b} = \textcolor{highlight}{i}\tensor{c}{^c_{ab}}T_c,
        \end{equation}
        where there is an implicit sum over the index \(c\).
    \end{dfn}
    
    Which of these two statements defines the structure constants depends on convention.
    For Lie algebras formed from real matrices we usually choose the first, and for Lie algebras formed from complex matrices the second.
    If we include the factor of \(i\) here then there are other places we need to include it later, but it can make the generators slightly nicer for physics, such as making them Hermitian when they would otherwise have been anti-Hermitian.
    
    We can use the Jacobi identity to derive an equivalent identity for the structure constants.
    Start with the Jacobi identity applied to three generators:
    \begin{equation}
        0 = \commutator{T_a}{\commutator{T_b}{T_c}} + \commutator{T_b}{\commutator{T_c}{T_a}} + \commutator{T_c}{\commutator{T_a}{T_b}}.
    \end{equation}
    Replacing the inner Lie bracket with structure constants then applying linearity we have
    \begin{align}
        0 &= \commutator{T_a}{\tensor{c}{^d_{bc}}T_d} + \commutator{T_b}{\tensor{c}{^d_{ca}}T_d} + \commutator{T_c}{\tensor{c}{^d_{ab}}T_d}\\
        &= \tensor{c}{^d_{bc}}\commutator{T_a}{T_d} + \tensor{c}{^d_{ca}}\commutator{T_b}{T_d} + \tensor{c}{^d_{ab}}\commutator{T_c}{T_d}.
    \end{align}
    Now replace the Lie brackets with structure constants again:
    \begin{equation}
        0 = \tensor{c}{^d_{bc}}\tensor{c}{^e_{ad}}T_e + \tensor{c}{^d_{ca}}\tensor{c}{^e_{bd}}T_e + \tensor{c}{^d_{ab}}\tensor{c}{^e_{cd}}T_e.
    \end{equation}
    Requiring this to hold for initial choices of generators the coefficients here must vanish, and so
    \begin{equation}
        0 = \tensor{c}{^d_{bc}}\tensor{c}{^e_{ad}} + \tensor{c}{^d_{ca}}\tensor{c}{^e_{bd}} + \tensor{c}{^d_{ab}}\tensor{c}{^e_{cd}}.
    \end{equation}
    
    \begin{lma}{}{}
        Let \(\lie{g}\) be the Lie algebra generated by \(\{T_a\}\) with structure constants \(\tensor{c}{^c_{ab}}\).
        If \(T_a\) are Hermitian then the structure constants are real.
        \begin{proof}
            If \(\{T_a\}\) generates the Lie algebra then so does \(\{-T_a^*\}\).
            Taking the complex conjugate of the defining relation for the structure constants we have
            \begin{equation}
                \commutator{T_a}{T_b}^* = (i\tensor{c}{^c_{ab}}T_a)^* \implies \commutator{T_a^*}{T_b^*} = -i\tensor{c}{^c_{ab}}^*T_c^*.
            \end{equation}
            Linearity tells us that \(\commutator{A}{B} = \commutator{-A}{-B}\), and so
            \begin{equation}
                \commutator{-T_a^*}{T_b^*} = i\tensor{c}{^c_{ab}}^*(-T_c^*).
            \end{equation}
            The structure constants are independent of the choice of generators, so we must have \(\tensor{c}{^c_{ab}} = \tensor{c}{^c_{ab}}^*\), that is \(\tensor{c}{^c_{ab}} \in \reals\).
        \end{proof}
    \end{lma}

    \section{Representations}
    Representations of Lie algebras aren't that different to representations of groups.
    The idea is to find some set of matrices, \(D\), parametrised by the elements of the Lie algebra, say \(x, y \in \lie{g}\), such that the Lie bracket in \(\lie{g}\) corresponds to the commutator of these matrices, that is
    \begin{equation}
        D(\commutator{x}{y}) = D(x)D(y) - D(y)D(x) = \commutator{D(x)}{D(y)},
    \end{equation}
    where the first \(\commutator{-}{-}\) is an abstract Lie bracket, which may or may not take the form of a commutator, and the second \(\commutator{-}{-}\) is the normal commutator.
    The formal definition of a representation of a Lie algebra is as follows.
    
    \begin{dfn}{Representation}{}
        Let \(\lie{g}\) be a Lie algebra.
        Then a \define{Lie algebra representation}\index{representation!of a Lie algebra}, \((D, V)\), is a vector space, \(V\), and a homomorphism of Lie algebras
        \begin{equation}
            D \colon \lie{g} \to \generalLinearLie(V)
        \end{equation}
        where \(\generalLinearLie(V) = \End(V)\) is made into a Lie algebra by equipping it with the commutator as a Lie bracket.
    \end{dfn}
    
    Representations essentially equate to a choice of generators.
    We have already met one of the most important representations of any Lie group, the structure constants themselves.
    
    \begin{dfn}{Adjoint Representation}{}
        The \define{adjoint representation}\index{adjoint representation!of a Lie algebra} of a Lie algebra, \(\lie{g}\), is the representation on \(\lie{g}\) itself, given by choosing the generators to satisfy
        \begin{equation}
            \tensor{(T_a)}{^b_c} = i\tensor{c}{^b_{ac}}.
        \end{equation}
    \end{dfn}
    
    To show that this is indeed a representation we need to show that \(T_a\) defined in this way satisfy \(\commutator{T_a}{T_b} = i\tensor{c}{^c_{ab}}T_c\).
    We can use the antisymmetry of the structure constants to write the Jacobi identity as
    \begin{equation}
        \tensor{(T_a)}{^d_e} \tensor{(T_b)}{^e_c} - \tensor{(T_b)}{^d_e}\tensor{(T_a)}{^e_c} = i\tensor{(T_e)}{^d_c} \tensor{c}{^e_{ab}},
    \end{equation}
    which in matrix notation gives
    \begin{equation}
        \commutator{T_a}{T_b} = i\tensor{c}{^e_{ab}}T_e.
    \end{equation}
    
    \chapter{Lie Algebras and Lie Groups}
    In this chapter we discuss the relationship between a Lie group and the associated Lie algebra.
    Starting with how we can get a Lie algebra from a Lie group by linearisation, then we'll see some examples, then we'll look at how to get back to the Lie group (or close to it) from the Lie algebra.
    
    \section{Linearising}
    Let \(G\) be a Lie group.
    An important idea is that every point in a Lie group is essentially equivalent, since if we study a neighbourhood of \(g \in G\) then we can transform this into a neighbourhood of \(h \in G\) by multiplying on the left by \(hg^{-1}\).
    This suggests that we can learn about a Lie group by studying just a single neighbourhood, and if we have to select a single point in the group it makes sense to choose the identity.
    
    If we want to study the behaviour of something near a fixed point it makes sense to expand about this point, which we're allowed to do for Lie groups by the analyticity part of the definition.
    Doing so up to first order we \defineindex{linearise} the group.
    If the group elements are \(g(\alpha)\), where \(\alpha = \{\alpha^a\}\) is parametrising the group, then expanding about the identity, 1, we get
    \begin{equation}
        g(\alpha) = 1 + i\alpha^a T_a + \order(\alpha^2)
    \end{equation}
    where
    \begin{equation}
        T_a = -i \diffp{g(\alpha)}{\alpha^a}[\alpha = 0]
    \end{equation}
    and \(a\) runs from 1 to \(\dim G\).
    We call \(T_a\) the \define{generators}\index{generator}.
    \begin{wrn}
        The factor of \(i\) here in these definitions is convention.
        It allows us to work with Hermitian matrices rather than anti-Hermitian matrices.
        Not everyone follows this convention, and we won't include it if we're working with real matrices.
    \end{wrn}
    
    \section{Lie Bracket}
    In a group we can measure how Abelian the group is using the \defineindex{group commutator}, defined for \(g, h \in G\) to be the product \(f = ghg^{-1}h^{-1}\).
    Notice that this gives the identity if \(g\) and \(h\) commute.
    If \(G\) is a Lie group then we can expand each term to evaluate this:
    \begin{align}
        g &= 1 + i\alpha^aT_a + \frac{1}{2}(i\alpha^aT_a)^2 + \order(\alpha^3),\\
        h &= 1 + i\beta^aT_a + \frac{1}{2}(i\beta^aT_a)^2 + \order(\beta^3),\\
        f &= 1 + i\gamma^aT_a + \frac{1}{2}(i\gamma^aT_a)^2 + \order(\gamma^3).
    \end{align}
    The inverse of \(g\) can similarly be expanded as
    \begin{equation}
        g^{-1} = 1 - i\alpha^aT_a + \frac{1}{2}(i\alpha^aT_a)^2 + \order(\alpha^3),
    \end{equation}
    this can be checked by expanding \(gg^{-1}\) and showing that we get 1.
    Write \(\alpha\) for \(\alpha^aT_a\) and \(\beta\) for \(\beta^aT_a\), then we have
    \begin{align}
        ghg^{-1}h^{-1} &\approx \left( 1 + i\alpha + \frac{1}{2}(i\alpha)^2 \right)\left( 1 + i\beta + \frac{1}{2}(i\beta)^2 \right)\\
        &\qquad\left( 1 - i\alpha + \frac{1}{2}(i\alpha)^2 \right)\left( 1 - i\beta + \frac{1}{2}(i\beta)^2 \right)\\
        &= 1 + i\alpha + i\beta - i\alpha - i\beta\\
        &- \alpha\beta + \alpha\alpha + \alpha\beta + \beta\alpha + \beta\beta -\alpha\beta\\
        &+ \frac{1}{2}[(i\alpha)^2 + (i\beta)^2 + (i\alpha)^2 + (i\beta)^2] + \order(\alpha^3, \beta^3)\\
        &= -\alpha\beta + \beta\alpha + \order(\alpha^3, \beta^3).
    \end{align}
    That is,
    \begin{equation}
        f = -\alpha^aT_a \beta^bT_b + \beta^bT_b\alpha^aT_a + \order(\alpha^3, \beta^3) = -\commutator{\alpha^aT_a}{\beta^bT_b} + \order(\alpha^3, \beta^3)
    \end{equation}
    where \(\commutator{-}{-}\) is the usual commutator.
    
    From here we define the Lie algebra of the Lie group \(G\) as the Lie algebra, \(\lie{g}\), generated by
    \begin{equation}
        T_a = -i\diffp{g(\alpha)}{\alpha^a}[\alpha = 0]
    \end{equation}
    with the Lie bracket given by the commutator, which is such that
    \begin{equation}
        \commutator{T_a}{T_b} = i\tensor{c}{^c_{ab}}T_c
    \end{equation}
    
    \section{Exponential Map}
    Let \(G\) be a Lie group.
    Consider some one-dimensional Lie subgroup, \(G_1 \subgroup G\).
    We can think of \(G_1\) as a path on the manifold \(G\) parametrised by some parameter \(t\), so
    \begin{equation}
        G_1 = \{g(\alpha(t)) t \in \reals\},
    \end{equation}
    we then write \(g(t)\) for \(g(\alpha(t))\).
    
    Since this is a one-dimensional Lie group by \cref{thm:one dim lie group parameters add} we can choose the parametrisation to be such that \(g(t)g(s) = g(t + s)\) and \(g(0) = 1\).
    Now differentiate this with respect to \(s\) and then set \(s = 0\), we get
    \begin{equation}
        g(t)g'(0) = g'(t).
    \end{equation}
    A solution to this is
    \begin{equation}
        g(t) = \e^{g'(0)t}.
    \end{equation}
    Now, define \(g'(0) = i\gamma^aT_a\), since this is what we get if \(g(s) = 1 + i\gamma^aT_a + \order(\gamma^2)\) and we find that we can write an arbitrary element of \(G_1\) as
    \begin{equation}
        g(t) = \e^{it\gamma^aT_a}.
    \end{equation}
    
    This motivates the following theorem, which we won't prove.
    \begin{thm}{}{}
        Let \(G\) be a compact Lie group with Lie algebra \(\lie{g}\).
        Let \(g \in G\) be continuously connected to the identity.
        Then
        \begin{equation}
            g = \e^{i\gamma^aT_a}
        \end{equation}
        for some \(\gamma^a\) and \(T_a\) the generators of \(\lie{g}\).
    \end{thm}
    In words, if \(G\) is a compact Lie group then every element connected to the identity can be obtained by exponentiating the Lie algebra.
    
    A compact Lie group is a Riemannian manifold.
    The exponential map, \(t \mapsto \exp[it\gamma^aT_a]\), then gives geodesics through the origin on this manifold.
    Every point connected to the identity is then on one of these geodesics.
    Intuitively this makes sense because by definition such a point is connected to the identity and so by simply minimising the path connecting it we get a geodesic.
    
    \subsection{Noncompact Lie Groups}
    The compact requirement is important, for noncompact Lie groups the theorem does not hold, which we demonstrate now by example.
    The group \(\specialLinear(2, \reals)\) is noncompact, it is formed of elements of the form
    \begin{equation}
        \begin{pmatrix}
            a & b\\ c & d
        \end{pmatrix}
    \end{equation}
    with \(ab - cd = 1\).
    The values of \(a\), \(b\), \(c\), and \(d\) are unbounded, and so \(\specialLinear(2, \reals)\) is not compact.
    
    Suppose that all \(g \in \specialLinear(2, \reals)\) can be expressed as \(\exp[l]\) for some \(l \in \specialLinearLie(2, \reals)\).
    Then we can define a one-dimensional path in \(\specialLinear(2, \reals)\) as the image of the map \(t \mapsto \exp[tl]\) with \(t \in [0, 1]\).
    In particular, \(h = \exp[l/2]\) satisfies \(h^2 = g\), so we can talk of \enquote{square roots} of group elements.
    
    We now proceed to demonstrate that such an \(h\) does not exist for all choices of \(g\).
    Let
    \begin{equation}
        g = 
        \begin{pmatrix}
            -4 & 0\\
            0 & -1/4
        \end{pmatrix}
    \end{equation}
    Then suppose that 
    \begin{equation}
        h = 
        \begin{pmatrix}
            a & b\\ c & d
        \end{pmatrix}
        .
    \end{equation}
    We then have
    \begin{equation}
        h^2 = 
        \begin{pmatrix}
            a^2 + bc & b(a + d)\\
            c(a + d) & d^2 + bc
        \end{pmatrix}
        .
    \end{equation}
    Since \(g\) has zero off diagonal elements we have \(b(a + d) = c(a + d) = 0\), meaning either \(b\) and \(c\) are both zero or \(a + d = 0\).
    Suppose that \(b = c = 0\), then looking at the first entry in \(g\) we have \(a^2 + bc = a^2 = -4\), but this can't be the case as \(a\) is real.
    Suppose then that \(a + d = 0\), then \(a = -d\) and so \(a^2 = d^2\).
    The first entry in \(g\) gives us \(a^2 + bc = -4\), but the last gives \(d^2 + bc = a^2 + bc = -1/4\).
    It is not possible for \(a^2 + bc\) to have two separate values so we conclude that there is no way to satisfy the conditions and there is no \(h \in \specialLinear(2, \reals)\) satisfying \(h^2 = g\).
    
    Note that \(\specialLinear(2, \reals)\) is connected, so it's the lack of compactness that is causing the problems here.
    We can show that \(g\) is connected to the identity by considering the piecewise path first defined by
    \begin{equation}
        \begin{pmatrix}
            \cos\vartheta & -\sin\vartheta\\
            \sin\vartheta & \cos\vartheta
        \end{pmatrix}
    \end{equation}
    for \(\vartheta \in [0, \pi]\), which gets us from the identity to
    \begin{equation}
        \begin{pmatrix}
            -1 & 0\\
            0 & -1
        \end{pmatrix}
        ,
    \end{equation}
    and then the path given by
    \begin{equation}
        \begin{pmatrix}
            -\lambda & 0\\ 0 & -\frac{1}{\lambda}
        \end{pmatrix}
        ,
    \end{equation}
    for \(\lambda \in [1, 4]\), which ends us at
    \begin{equation}
        \begin{pmatrix}
            -4 & 0\\
            0 & -\frac{1}{4}
        \end{pmatrix}
        .
    \end{equation}
    That is, the path given by the image of
    \begin{equation}
        t \mapsto
        \begin{cases}
            \begin{pmatrix}
                \cos\vartheta & -\sin\vartheta\\
                \sin\vartheta & \cos\vartheta
            \end{pmatrix}
            & \vartheta = t \in [0, \pi],\\
            \begin{pmatrix}
                -\lambda & 0\\
                0 & -\frac{1}{\lambda}
            \end{pmatrix}
            & \lambda = t + 1 - \pi, t \in [\pi, \pi + 3].
        \end{cases}
    \end{equation}
    This path is continuous (but not differentiable at \(t = \pi\)).
    
    \subsection{Baker--Campbell--Hausdorff}
    For two matrices, \(A\) and \(B\), we have
    \begin{equation*}
        \exp[A]\exp[B] = \exp\left[ A + B + \frac{1}{2}\commutator{A}{B} + \frac{1}{12}(\commutator{A}{\commutator{A}{B}} + \commutator{\commutator{A}{B}}{B}) + \dotsb \right]
    \end{equation*}
    If we know that \(A, B \in \lie{g}\) for some Lie algebra \(\lie{g}\) then we can evaluate the commutators and hence can determine the product of \(\exp[A]\) and \(\exp[B]\) in \(G\).
    
    \section{Matrix Lie Algebras}
    \subsection{General Linear}
    Consider the general linear group, \(\generalLinear(n, \field)\).
    This consists of all invertible \(n \times n\) matrices over \(\field\).
    Now let \(A\) be any \(n \times n\) matrix over \(\field\).
    Then \(\exp[A]\) is an invertible \(n\times n\) matrix over \(\field\), in particular \(\exp[A]^{-1} = \exp[-A]\).
    This shows that the Lie algebra of \(\generalLinear(n, \field)\), denoted \(\generalLinearLie(n, \field)\), is the set of all \(n \times n\) matrices over \(\field\).
    Note that this can be identified with the set of all linear transformations, \(\generalLinearLie(n, \field) \isomorphic \End(\field^n)\).
    Since we have \(n^2\) entries into an \(n \times n\) matrix, and no conditions, we conclude that \(\dim\generalLinearLie(n, \reals) = n^2\).
    Similarly, if \(\field = \complex\) then each entry has two real parameters and so we have \(2n^2\) degrees of freedom so the dimension of \(\generalLinearLie(n, \complex)\) as a real vector space is \(2n^2\).
    
    Now consider the special linear group, \(\specialLinear(n, \field)\).
    We then have the extra condition that \(\det M = 1\) for all \(M \in \specialLinear(n, \field)\).
    To proceed we need the following lemma
    \begin{lma}{}{}
        Let \(A\) be an \(n \times n\) matrix and \(I\) the \(n\)-dimensional identity matrix.
        Then
        \begin{equation}
            \det(I + \varepsilon A) = 1 + \varepsilon\tr A
        \end{equation}
        to first order in \(\varepsilon\).
        \begin{proof}
            Let \(a_i\) be the eigenvalues of \(A\).
            Then the characteristic polynomial of \(A\) is
            \begin{equation}
                \det(t I - A) = (t - a_1)(t - a_2) \dotsm (t - a_n).
            \end{equation}
            Setting \(t = -1\) we get
            \begin{align}
                \det(-I - A) &= (-1)^n \det(I + A)\\
                &= (-1 - a_1)(-1 - a_2) \dotsm (-1 - a_n)\\
                &= (-1)^n(1 + a_1)(1 + a_2) \dotsm (1 + a_n)\\
                &= (-1)^n[1 + a_1 + a_2 + \dotsb + a_n + \order(a_ia_j)]\\
                &= (-1)^n[1 + \tr A + \order(a_ia_j)].
            \end{align}
            Hence \(\det(I + A) = 1 + \tr A + \order(a_ia_j)\)
            Rescaling so that \(A \to \varepsilon A\) we get the result \(\det(I + \varepsilon A) = 1 + \varepsilon\tr A + \order(\varepsilon^2)\).
        \end{proof}
    \end{lma}
    Using this we can see that if \(M \in \specialLinear(n, \field)\) we can expand \(M = I + i\alpha^aT_a + \order(\alpha^2)\) and we get
    \begin{equation}
        1 = \det M \approx \det(I + i\alpha^aT_a) \approx 1 + \alpha^a \tr T_a,
    \end{equation}
    and so the generators of \(\specialLinearLie(n, \field)\) must be traceless, and hence all matrices in \(\specialLinearLie(n, \field)\) are traceless.
    So, \(\specialLinearLie(n, \field)\) consists of all traceless \(n \times n\) matrices.
    We have \(n^2\) entries, but being traceless fixes one, since we can force a matrix to be traceless by setting \(A_{11} = -A_{22} - A_{33} - \dotsb - A_{nn}\), and so \(\dim \specialLinearLie(n, \reals) = n^2 - 1\) and being traceless in both the real and imaginary components fixes two degrees of freedom in the complex case, so \(\dim \specialLinearLie(n, \complex) = 2(n^2 - 1)\) as a real vector space.
    
    Now consider the orthogonal group, \(\orthogonal(n)\).
    Writing some generic \(O \in \orthogonal(n)\) as \(I + \varepsilon A + \order(\varepsilon^2)\) and expanding the defining condition, \(O^\trans O = I\), we get
    \begin{multline}
        I = (I + \varepsilon A + \order(\varepsilon^2)) (I + \varepsilon A^\trans + \order(\varepsilon^2))\\
        = I + \varepsilon A + \varepsilon A^\trans + \order(\varepsilon^2) = I + \varepsilon(A + A^\trans) + \order(\varepsilon^2).
    \end{multline}
    So we must have that \(A = -A^{\trans}\), that is \(A\) is antisymmetric.
    So the Lie algebra, \(\specialOrthogonalLie(n)\) consists of all \(n \times n\) real antisymmetric matrices.
    Being antisymmetric sets all values below the diagonal, and the diagonal must be zero, so we get an upper triangle to fill in of base \(n - 1\), the number of entries in this triangle is \(n(n - 1)/2\).
    
    Consider the special orthogonal group, \(\specialOrthogonal(n)\).
    As with \(\orthogonal(n)\) the matrices in the Lie algebra must be antisymmetric.
    But, this then enforces that these matrices are traceless, since an antisymmetric matrix has zeros down the diagonal, and hence \(\orthogonalLie(n) \isomorphic \specialOrthogonalLie(n)\).
    
    Now consider the unitary group, \(\unitary(n)\).
    Suppose that \(H \in \unitaryLie(n)\), that is \(\exp[i\varepsilon H] \in \unitary(n)\), then \(\exp[i\varepsilon H]\exp[i\varepsilon H]^\hermit = I\) by definition, and we have
    \begin{equation}
        I = \exp[i\varepsilon H]\exp[i\varepsilon H]^\hermit \approx (I + i\varepsilon H)(I - i\varepsilon H^\hermit) = I + i\varepsilon(H - H^\hermit) + \order(\varepsilon^2),
    \end{equation}
    and so we must have \(H = H^\hermit\), so \(H\) is Hermitian.
    Note that if we had not included the factor of \(i\) we would instead have found \(H\) to be anti-Hermitian, which isn't as nice and this is why we include the factor of \(i\).
    So, \(\unitaryLie(n)\) consists of all \(n \times n\) Hermitian matrices.
    Being Hermitian fixes the values below the diagonal, removing \(n(n - 1)/2\) entries, but \(n(n - 1)\) degrees of freedom, since these are complex numbers with two real parameters.
    Being Hermitian also forces the diagonal to be real, removing another \(n\) parameters from the imaginary part of the diagonal.
    This leaves \(n(n - 1)\) real parameters defining the upper triangle and \(n\) defining the diagonal, for a total of \(n(n - 1) + n = n^2\) real parameters, so \(\dim \unitaryLie(n) = n^2\).
    
    Finally, consider \(\specialUnitary(n)\).
    This consists of all Hermitian, traceless, \(n \times n\) matrices.
    Being traceless fixes one real parameter on the diagonal, since the diagonal is real anyway, and so \(\dim \specialUnitaryLie(n) = n^2 - 1\).
    
    \section{Universal Covering Group}
    Every Lie group has a unique Lie algebra which we find by linearising the group.
    Every Lie algebra can be exponentiated to form a Lie group.
    However, this Lie group is not necessarily the same one that we linearised to find the Lie algebra.
    The exponentiated Lie algebra forms a simply connected Lie group, even if the original Lie group wasn't simply connected.
    This means that multiple Lie groups can have the same Lie algebra, but only one of these groups is attained by exponentiating the Lie algebra, this Lie group is called the \defineindex{universal covering group}.
    
    If two Lie groups have the same Lie algebra it is because they are indistinguishable in the neighbourhood of the identity.
    
    \begin{exm}{}{}
        Consider \(\unitary(1)\).
        This has elements \(\e^{i\vartheta}\) for some \(\vartheta \in [0, 2\pi)\).
        Since we identify \(0\) and \(2\pi\) this group is compact, and topologically equivalent to the circle, which is not simply connected.
        The Lie algebra is then just \([0, 2\pi)\).
        
        Consider instead the set of elements of the form \(\e^{\vartheta}\) for \(\vartheta \in \reals\).
        This gives \(\reals_{>0}\), the set of positive real numbers.
        However, the Lie algebra is, up to an unimportant factor of \(i\), the same as for \(\unitary(1)\).
        The ray consisting of the positive real numbers is simply connected, and hence exponentiating this Lie algebra gives this group, rather than \(\unitary(1)\).
    \end{exm}
    
    \begin{exm}{}{exm:SO(n) = O(n)/Z2}
        We've seen that both \(\orthogonal(n)\) and \(\specialOrthogonal(n)\) have the same Lie algebra.
        This is because the requirement of unit determinant is not relevant in the neighbourhood of the identity, where it is satisfied in both groups.
        
        The orthogonal group, \(\orthogonal(n)\), is formed of two disconnected components, one with determinant \(+1\) and one with determinant \(-1\).
        The piece with determinant \(+1\) is just the subgroup \(\specialOrthogonal(n)\).
        The characterisation of the piece determinant \(-1\) depends on the dimension.
        If \(n\) is odd then we can form pairs \((A, +1)\) and \((A, -1)\) for all \(A \in \specialOrthogonal(n)\).
        Then identifying \((A, +1) = A \in \orthogonal(n)\) and \((A, -1) = -A \in \orthogonal(n)\) we get a one-to-one correspondence between these pairs and \(\orthogonal(n)\).
        That means we have
        \begin{equation}
            \orthogonal(n) \isomorphic \specialOrthogonal(n) \times \integers_2 \qquad n \text{ odd}.
        \end{equation}
        
        If instead \(n\) is even then, for example, both \(I\) and \(-I\) are in \(\specialOrthogonal(n)\), and so we cannot do the same thing with pairs.
        The result is that the two components don't commute with each other.
        
        Consider the case where \(n = 2\).
        We can then split \(\orthogonal(2)\) into two parts, the first formed of matrices of the form
        \begin{equation}
            g = 
            \begin{pmatrix}
                \cos \vartheta & -\sin \vartheta\\
                \sin \vartheta & \cos \vartheta
            \end{pmatrix}
            \in \specialOrthogonal(2),
        \end{equation}
        and the second formed of matrices of the form
        \begin{equation}
            \overbar{g} = 
            \begin{pmatrix}
                -\cos \psi & \sin \psi\\
                \sin \psi & \cos \psi
            \end{pmatrix}
            \in \overline{\specialOrthogonal(2)}.
        \end{equation}
        That is, \(\specialOrthogonal(2)\) consists of all matrices in \(\orthogonal(2)\) with determinant 1 and \(\overline{\specialOrthogonal(2)}\) consists of all matrices in \(\orthogonal(2)\) with determinant \(-1\).
        Notice that if \(\overbar{g}, \overbar{h} \in \overline{\specialOrthogonal(2)}\) then we have \(\det(\overbar{g}\overbar{h}) = \det(\overbar{g})\det(\overbar{h}) = (-1)(-1) = 1\), so \(\overbar{g}\overbar{h} \in \specialOrthogonal(2)\).
        
        It can be shown that \(\specialOrthogonal(2)\) is a normal subgroup of \(\orthogonal(2)\), that is \(ghg^{-1} \in \specialOrthogonal(2)\) for all \(g \in \orthogonal(2)\) and \(h \in \specialOrthogonal(2)\).
        In particular, if \(g \in \specialOrthogonal(2)\) and \(\overbar{g} \in \overline{\specialOrthogonal(2)}\) then \(\overbar{g} g \overbar{g}^{-1} \in \specialOrthogonal(2)\).
        
        If we have a normal subgroup then we can write the original group as a semidirect product of the normal subgroup and some other group, in this case
        \begin{equation}
            \orthogonal(2) \isomorphic \specialOrthogonal(2) \rtimes \integers_2,
        \end{equation}
        where the action of \(\integers_2\) on \(\specialOrthogonal(2)\) is
        \begin{equation}
            0 \action g =
            \begin{pmatrix}
                1 & 0\\
                0 & 1
            \end{pmatrix}
            g, \qqand 1 \action g = 
            \begin{pmatrix}
                -1 & 0\\
                0 & 1
            \end{pmatrix}
            g.
        \end{equation}
    \end{exm}
    
    \begin{exm}{}{}
        Perhaps the most important example, at least for physics, of two Lie groups with the same Lie algebras is \(\specialOrthogonal(3)\) and \(\specialUnitary(2)\).
        Start with \(\specialUnitary(2)\), this has a Lie algebra formed from all Hermitian traceless \(2 \times 2\) matrices.
        A basis for these matrices is given by the Pauli matrices, although to we include a factor of \(1/2\) as a matter of convention, giving
        \begin{equation}
            \specialUnitaryLie(2) = \spn_{\reals} \left\{ \frac{1}{2}\sigma_a \right\}.
        \end{equation}
        This Lie algebra then satisfies
        \begin{equation}
            \commutator{\sigma_a/2}{\sigma_b/2} = i\varepsilon_{cab}\sigma_c/2,
        \end{equation}
        that is, the structure constants are \(\tensor{c}{^c_{ab}} = \varepsilon_{cab}\).
        Note that we are free to raise and lower indices here since the Killing form (defined later) is positive definite.
        
        For \(\specialOrthogonal(3)\) the Lie algebra is formed from the matrices \(T_a\) with components \((T_a)_{bc} = i\varepsilon_{bac}\).
        This is simply the adjoint representation of \(\specialOrthogonalLie(2)\) as defined above, so clearly these two Lie algebras are the same.
        
        Consider what happens when we exponentiate them.
        We can consider
        \begin{equation}
            \exp\left[ i\omega \vh{n} \cdot \vv{\sigma}/2 \right]
        \end{equation}
        where our parameters, \(\omega\vh{n}\), are formed from a unit vector, \(\vh{n} \in \sphere[2]\), and some \(\omega \in \reals\).
        Note that a unit vector in \(n\) dimensions requires \(n - 1\) parameters to define.
        Using the identity \(\sigma_i \sigma_j = \delta_{ij}I + i\varepsilon_{ijk}\sigma_k\) we can expand the exponential and collect terms to show that
        \begin{equation}
            \exp[i\omega \vh{n} \cdot \vv{\sigma} /2] = \cos\left( \frac{\omega}{2} \right) I + i\vh{n} \cdot \vv{\sigma} \sin\left( \frac{\omega}{2} \right).
        \end{equation}
        Thus we have to take \(\omega \in [0, 4\pi)\) in order to cover every element of \(\specialUnitary(2)\), at least those which can be reached in this way.
        
        Instead we can consider \(\specialOrthogonal(3)\) and exponentiate
        \begin{equation}
            \exp[i\omega\vh{n}(i\varepsilon_{bac})] = n_bn_c + (\delta_{bc} - n_bn_c)\cos\omega + \varepsilon_{abc} n_a \sin \omega.
        \end{equation}
        This requires a lot of algebra to show, but in the \course{Vectors, Tensors, and Continuum Mechanics} part of the \course{Methods of Theoretical Physics} course we show that this is the general form of a rotation.
        Note then that if \(\omega \in [0, 4\pi)\) as for \(\specialUnitary(2)\) we will hit every element of \(\specialOrthogonal(3)\) twice.
        We say that \(\specialUnitary(2)\) is a double cover of \(\specialOrthogonal(3)\).
        
        As a manifold \(\specialOrthogonal(3)\) corresponds to the three-dimensional ball of radius \(\pi\), where a vector in this ball picks out an axis, and its length picks out the angle of rotation.
        In particular, a vector, \(\vv{x}\), of length \(\pi\) is equivalent as a rotation to the vector \(-\vv{x}\), and so we identify opposite points on the ball.
        This means that a line going from one side of the ball to the other is technically a loop, but is not contractible to a point so \(\specialOrthogonal(3)\) is not simply connected.
        
        On the other hand, we showed in \cref{exm:SU(2) = three sphere} that \(\specialUnitary(2)\) as a manifold is the three sphere, \(\sphere[3]\), which is simply connected.
        We conclude that both the Lie algebras of \(\specialUnitary(2)\) and \(\specialOrthogonal(3)\) both exponentiate to give \(\specialUnitary(2)\).
    \end{exm}
    
    \chapter{Invariant Tensors}
    \section{Structure Constants}
    Recall that for a Lie algebra generated by \(T_a\) the structure constants, \(\tensor{c}{^c_{ab}}\) are defined as
    \begin{equation}
        \commutator{T_a}{T_b} = i\tensor{c}{^c_{ab}}T_c.
    \end{equation}
    These are independent of the representation chosen.
    For example, we saw that the structure constants of \(\specialUnitaryLie(2) \isomorphic \specialOrthogonalLie(3)\) are \(\varepsilon_{ijk}\) whether we think of this Lie algebra as \(2\times 2\) traceless Hermitian matrices (\(\specialUnitaryLie(2)\)) or as antisymmetric traceless real matrices (\(\specialOrthogonalLie(3)\)).
    
    If the generators are Hermitian, so \(T_a^\hermit = T_a\), then the structure constants are real, to show this consider the conjugate of the defining equation:
    \begin{equation}
        \commutator{T_a}{T_b}^\hermit = -i\tensor{c}{^c_{ab}}^*T_c^\hermit,
    \end{equation}
    the conjugate of the commutator is the negative of the commutator of the conjugates:
    \begin{multline}
        \commutator{A}{B}^\hermit = (AB - BA)^\hermit = (AB)^\hermit - (BA)^\hermit\\
        = B^\hermit A^\hermit - A^\hermit B^\hermit = \commutator{B^\hermit}{A^\hermit} = -\commutator{A^\hermit}{B^\hermit}
    \end{multline}
    Hence, we have
    \begin{equation}
        \commutator{T_a}{T_b}^\hermit = -\commutator{T_a^\hermit}{T_b^\hermit} = -\commutator{T_a}{T_b},
    \end{equation}
    and so we have
    \begin{equation}
        \commutator{T_a}{T_b} = i\tensor{c}{^c_{ab}}^* T_c.
    \end{equation}
    Hence we must have \(\tensor{c}{^c_{ab}}^* = \tensor{c}{^c_{ab}}\), and so \(\tensor{c}{^c_{ab}}\) is real.
    This is useful because in a compact Lie group we can always choose finite dimensional representations to be unitary by Maschke's theorem (\cref{thm:maschke}), and the corresponding Lie algebra representation will be Hermitian.
    
    If the generators are purely imaginary, so \(iT_a\) is real, this follows since \(T_a\) being purely imaginary implies \(\commutator{T_a}{T_b}\) is real, and so in the defining equation we have something real, \(\commutator{T_a}{T_b}\) is equal to the structure constants, \(\tensor{c}{^c_{ab}}\), times something real, \(iT_a\), and hence \(\tensor{c}{^c_{ab}}\) must be real.
    This is useful because we can often choose the generators of a noncompact group to be purely imaginary.
    
    The structure constants are antisymmetric in the second two indices, since
    \begin{equation}
        i\tensor{c}{^c_{ab}}T_c = \commutator{T_a}{T_b} = -\commutator{T_b}{T_a} = -i\tensor{c}{^c_{ba}}T_c.
    \end{equation}
    
    If we rescale one of the generators, say \(T_a \mapsto \mu T_a\) for some fixed \(a\), then the structure constants rescale accordingly, for \(b \ne a\)
    \begin{equation}
        \commutator{T_a}{T_b} \mapsto \commutator{\mu T_a}{T_b} = \mu\commutator{T_a}{T_b} = i\mu\tensor{c}{^c_{ab}}T_c.
    \end{equation}
    This can be useful to swap between the physics and maths conventions, where the generators differ by a factor of \(i\).
    
    \subsection{Adjoint Representation}
    The structure constants define the adjoint representation of the Lie algebra.
    There is an associated representation of the Lie group on the Lie algebra.
    This is fully determined by the action of the Lie group on the generators.
    Consider the transformation \(T_a \mapsto gT_ag^{-1}\), where \(g = 1 + i\alpha^aT_a + \dotsb\) and so \(g^{-1} = 1 - i\alpha^aT_a + \dotsb\).
    Expanding this to first order we have
    \begin{align}
        gT_ag^{-1} &\approx (1 + i\alpha^bT_b)T_a(1 - i\alpha^cT_c)\\
        &= T_a + i\alpha^bT_bT_a - i\alpha^cT_aT_c + \order(\alpha^2)\\
        &= T_a + i\alpha^b \commutator{T_b}{T_a} + \order(\alpha^2),
    \end{align}
    where we reindex \(c \to b\) in the last step.
    It turns out that this holds to all orders, and so it can be shown that
    \begin{equation}
        gT_ag^{-1} = T_b \tensor{D}{^b_a}(g)
    \end{equation}
    where \(D(g)\) are some matrices.
    We call \(D\) the \define{adjoint representation}\index{adjoint representation!of a Lie group}.
    
    We need to check that \(D\) really is a representation, that is, we need to check if the following holds for all \(g, h \in G\):
    \begin{equation}
        \tensor{D}{^b_c}(g) \tensor{D}{^c_d}(h) = \tensor{D}{^b_d}(gh).
    \end{equation}
    To do this multiply by \(T_b\) to give
    \begin{align}
        T_b \tensor{D}{^b_c}(g) \tensor{D}{^c_d}(h) &= gT_cg^-1 \tensor{D}{^c_d}(h)\\
        &= gT_c\tensor{D}{^c_d}(h)g^{-1}\\
        &= ghT_ch^{-1}g^{-1}\\
        &= (gh)T_d(gh)^{-1}\\
        &= T_b \tensor{D}{^b_d}(gh).
    \end{align}
    Note that while \(D(h)\) is a matrix \(\tensor{D}{^c_d}(h)\) is a number, so commutes with \(g^{-1}\).
    Since this holds for all \(T_b\) we have
    \begin{equation}
        \tensor{D}{^b_c}(g) \tensor{D}{^c_d}(h) = \tensor{D}{^b_d}(gh),
    \end{equation}
    and so \(D\) is a representation.
    
    For either a real Lie group or a compact Lie group the adjoint representation is real.
    This leads to many nice properties that make it a useful representation.
    
    \section{Invariant Tensors}
    Consider some vector space \(V\), and some vector \(x \in V\) with components \(x^i\).
    If \(G\) is a Lie group with some action on \(V\) defined by matrix multiplication with the representation \(D\) then \(x^i\) transforms under \(g \in G\) as
    \begin{equation}
        x^i \mapsto (g\action x)^i = \tensor{D}{^i_j}(g)x^j.
    \end{equation}
    Similarly, a vector in the dual space, \(\bar{V}\), has components \(x_i\), which transform as
    \begin{equation}
        x_i \mapsto (g\action x)_i = \tensor{D}{_i^j}(g)x_j.
    \end{equation}
    This extends to some tensor, \(T \in V^{\otimes n} \otimes \overbar{V}^{\otimes m}\) with components \(\tensor{T}{^{i_1i_2\dotso i_n}_{j_1j_2\dotso j_m}}\) transforming as
    \begin{multline*}
        \tensor{T}{^{i_1i_2\dotso i_n}_{j_1j_2\dotso j_m}} \mapsto \tensor{(g\action T)}{^{i_1i_2\dotso i_n}_{j_1j_2\dotso j_m}}\\
        = \tensor{D}{^{i_1}_{k_1}}(g)\tensor{D}{^{i_2}_{k_2}}(g) \dotsm \tensor{D}{^{i_n}_{k_n}}(g) \tensor{D}{_{j_1}^{l_1}}(g) \tensor{D}{_{j_2}^{l_2}}(g) \tensor{D}{_{j_m}^{l_m}}(g) \tensor{T}{^{k_1k_2\dotso k_n}_{l_1l_2\dotso l_m}}.
    \end{multline*}
    
    Some special tensors have the property that their components don't change under any transformation in \(G\), these are called invariant tensors, and they can tell us a lot about the group in question.
    
    \begin{dfn}{Invariant Tensor}{}
        An \defineindex{invariant tensor} of rank \(n\), with components \(d_{a_1a_2\dotso a_n}\) is a tensor whose components are invariant under the action of some Lie group in some representation.
        That is, for \(g \in G\) and representation \(D\) we have
        \begin{align}
            d_{a_1a_2\dotso a_n} \mapsto (g \action d)_{a_1a_2\dotso a_n} &= \tensor{D}{^{b_1}_{a_1}}(g)\tensor{D}{^{b_1}_{a_1}}(g) \dotsm \tensor{D}{^{b_1}_{a_1}}(g) d_{b_1b_2\dotso b_n}\notag\\
            &= d_{a_1a_2\dotso a_n}.
        \end{align}
    \end{dfn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Any scalar is an invariant tensor.
            \item Consider the usual representation of \(\orthogonal(n)\) on \(\reals^n\), that is \(\orthogonal(n)\) just acts by matrix multiplication.
            An invariant tensor in this case is \(\delta_{ij}\).
            \item Now restrict things to \(\specialOrthogonal(n)\), we get an additional invariant tensor, \(\varepsilon_{i_1i_2\dotso i_n}\).
            \item Given invariant tensors we can combine them to get other invariant tensors, such as \(\varepsilon_{ijk}\delta_{lm}\) or \(\delta_{ij}\delta_{kl}\delta_{mn}\).
            \item The tensor \(\eta^{\mu\nu}\) is invariant under \(\specialOrthogonal^+(1, 3)\).
            \item For \(\unitary(n)\) acting on \(\complex^n\) in the obvious way \(\delta_{ij}\) is \emph{not} an invariant tensor, but \(\tensor{\delta}{^i_j}\) is, since
            \begin{equation}
                \tensor{\delta}{^i_j} \mapsto \tensor{U}{^i_k}\tensor{\delta}{^k_l}\tensor{U}{_j^l} = \tensor{U}{^i_l}\tensor{U}{_j^l} = \delta^i_j
            \end{equation}
            for some unitary \(U \in \unitary(n)\).
            \item Now restrict things to \(\specialUnitary(n)\), and we get two additional invariant tensors, \(\varepsilon_{i_1i_2\dotso i_n}\) and \(\varepsilon^{i_1i_2\dotso i_n}\).
        \end{itemize}
    \end{exm}
    
    One simple way to create invariant tensors is to take traces of products of generators.
    For example, define some tensor \(d \in V^{\otimes 3}\) with components
    \begin{equation}
        d_{abc} \coloneqq \tr(T_aT_bT_c).
    \end{equation}
    Then this transforms in the adjoint representation as
    \begin{align}
        d_{abc} &\mapsto \tr(gT_ag^{-1}gT_bg^{-1}gT_cg^{-1})\\
        &= \tr(gT_aT_bT_cg^{-1})\\
        &= \tr(g^{-1}gT_aT_bT_c)\\
        &= \tr(T_aT_bT_c)\\
        &= d_{abc}
    \end{align}
    using the cyclic property of the trace.
    
    \section{Killing Form}
    \epigraph{Set 2 equal to 1}{Neil Turok}
    \begin{dfn}{Killing Form}{}
        The \defineindex{Killing form}\footnote{named after Wilhelm Killing, no relation to murder.} is defined as the tensor with components
        \begin{equation}
            g_{ab} \coloneqq \tr_{\symrm{A}}(T_aT_b)
        \end{equation}
        where \(T_a\) are in the adjoint representation, which is what the subscript \(\symrm{A}\) tells us.
    \end{dfn}
    
    The Killing form is symmetric, since
    \begin{equation}
        g_{ab} = \tr_{\symrm{A}}(T_aT_b) = \tr(T_bT_a) = g_{ba}
    \end{equation}
    by the cyclic property of the trace.
    
    If the structure constants are real then the Killing form is a real symmetric matrix, so can be used as a metric on the Lie group.
    We can write the Killing form in terms of the structure constants:
    \begin{equation}
        g_{ab} = -\tensor{c}{^d_{ae}} \tensor{c}{^e_{bd}} = g_{ba}.
    \end{equation}

    For a compact Lie algebra it is possible to diagonalise the Killing form by some orthogonal transformation.
    That is, there is some basis such that \(g_{ab} = \lambda_a\delta_{ab}\) (no sum on \(a\)).
    We can then rescale the basis and reorder it to get a result in the canonical form:
    \begin{equation}
        g = 
        \begin{pmatrix}
            1 & & & & & & & & &\\
            & \ddots & & & & & & &\\
            & & 1 & & & & & &\\
            & & & 0 & & & & & \\
            & & & & \ddots & & & & \\
            & & & & & 0 & & & & \\
            & & & & & & \mathllap{-}1 & & \\
            & & & & & & & \ddots & \\
            & & & & & & & & \mathllap{-}1
        \end{pmatrix}
        .
    \end{equation}

    \begin{exm}{}{}
        In \(\specialUnitaryLie(2)\) the structure constants are \(\varepsilon_{ijk}\), and so in the adjoint representation the generators have components \(\tensor{(T_i)}{^k_l} = i\varepsilon_{kil}\).
        The Killing form is then
        \begin{equation}
            g_{ij} = -\varepsilon_{kil}\varepsilon_{ljk} = 2\delta_{ij},
        \end{equation}
        so the Killing form is already diagonal in this case.
        We can rescale the generators \(T_a \mapsto T_a/\sqrt{2}\), and so \(\tensor{(T_i)}{^k_l} = i\varepsilon_{kil}/\sqrt{2}\) and \(g_{ij} = \delta_{ij}\).
    \end{exm}
    
    The Killing form is an invariant tensor, since it transforms under the adjoint representation as
    \begin{equation}
        g_{ab} = \tr_{\symrm{A}}(T_aT_b) \mapsto \tr_{\symrm{A}}(gT_ag^{-1}gT_bg) = \tr_{\symrm{A}}(T_aT_b) = g_{ab},
    \end{equation}
    having used the cyclic property of the trace.
    
    We can use the Killing form to lower indices, so for example, we can define structure constants with all lower indices
    \begin{equation}
        c_{abc} = g_{ad}\tensor{c}{^d_{bc}}.
    \end{equation}
    The structure constants with lowered indices are invariant tensors, since the normal structure constants and the Killing form are invariant tensors.
    We can write these lowered index structure constants in terms of traces of products of generators as follows:
    \begin{align}
        \tr_{\symrm{A}}(T_a \commutator{T_b}{T_c}) &= \tr_{\symrm{A}}(T_a i\tensor{c}{^d_{bc}}T_d)\\
        &= i\tensor{c}{^d_{bc}} \tr_{\symrm{A}}(T_aT_d)\\
        &= i\tensor{c}{^d_{bc}} g_{ad}\\
        &= ic_{abc}.
    \end{align}
    
    This form form is useful since we have the identity
    \begin{multline}
        \tr(A \commutator{B}{C}) = \tr(ABC - ACB) = \tr(ABC) - \tr(ACB)\\
        = \tr(ABC) - \tr(BAC) = \tr(ABC - BAC) = \tr(\commutator{A}{B}C)
    \end{multline}
    and similarly
    \begin{equation}
        \tr(A\commutator{B}{C}) = \tr(\commutator{C}{A}B).
    \end{equation}
    This means we have
    \begin{equation}
        ic_{abc} = \tr(T_a\commutator{T_b}{T_c}) = \tr(\commutator{T_a}{T_b}T_c) = \tr(T_c\commutator{T_a}{T_b}) = ic_{cab}.
    \end{equation}
    So, we have \(c_{abc} = c_{cab}\), and we know that \(c_{abc}\) is antisymmetric in the last two indices, so \(c_{abc} = -c_{acb}\).
    This means that exchanging the first two indices must also pick up a negative sign to cancel with this one.
    We conclude that \(c_{abc}\) is antisymmetric in all three indices.
    This result holds for compact and noncompact groups.
    
    We can form another similar invariant tensor by replacing the commutator with the \defineindex{anticommutator}, \(\anticommutator{A}{B} = AB + BA\), and then we have
    \begin{equation}
        d_{abc} \coloneqq \tr_{\symrm{A}}(T_a\anticommutator{T_b}{T_c}).
    \end{equation}
    This is symmetric in all three indices.
    
    In general for \(n > 3\) if we take the expression \(\tr(T_{a_1} \dotsm T_{a_n})\) we can always write the antisymmetric part in terms of \(c_{abc}\).
    This means that each time we increment \(n\) we only get one independent extra invariant tensor given by the totally symmetric tensor on \(n\) indices.
    This holds up to a point where we stop getting new tensors at all as we can even decompose the totally symmetric tensors in terms of lower rank symmetric tensors.
    
    \begin{dfn}{Rank}{}
        The \defineindex{rank} of a Lie algebra is the number of invariant tensors under the universal covering group.
    \end{dfn}
    
    \part{Simplicity and Compact Groups}
    \chapter{Simplicity}
    \section{Definitions}
    We repeat some definitions here for ease of reference in this section.
    \begin{dfn}{Lie Subalgebras and Ideals}{}
        Let \(\lie{g}\) be a Lie algebra and \(\lie{h}\) a subspace of \(\lie{g}\) (as vector spaces).
        Then \(\lie{h}\) is a \defineindex{Lie subalgebra}, or simply a \defineindex{subalgebra} of \(\lie{g}\) if \(\commutator{x}{y} \in \lie{h}\) for all \(x, y \in \lie{h}\), where \(\commutator{-}{-}\) is the Lie bracket of \(\lie{g}\).
        
        A \defineindex{proper subalgebra} of \(\lie{g}\) is a subalgebra of \(\lie{g}\) with a smaller set of generators than \(\lie{g}\).
        
        An \defineindex{ideal}, \(\lie{i}\), or \defineindex{invariant subalgebra} of \(\lie{g}\) is a subalgebra of \(\lie{g}\) such that \(\commutator{g}{i} \in \lie{i}\) for all \(g \in \lie{g}\) and \(i \in \lie{i}\).
    \end{dfn}
    Compare these definitions with those of subgroups.
    A subgroup is one which is closed under the group operation, and a subalgebra is one which is closed under the Lie bracket.
    A proper subgroup is one which is not equal to the original group, and a proper subalgebra is one in which the set of generators is not equal to the set of generators of the original Lie algebra.
    An invariant subgroup is one which is closed under conjugation with any group element, and an invariant subalgebra is one which is closed under the Lie bracket with any Lie algebra element.
    
    \begin{dfn}{Simple and Semisimple}{}
        Let \(\lie{g}\) be a Lie group.
        If \(\lie{g}\) has no proper invariant subalgebras it is \defineindex{simple}.
        If \(\lie{g}\) has no proper invariant Abelian subalgebras it is \defineindex{semisimple}.
        Recall that a Lie algebra, \(\lie{h}\), is Abelian if \(\commutator{x}{y} = 0\) for all \(x, y \in \lie{h}\).
    \end{dfn}
    
    \begin{exm}{}{}
        The Lie algebra \(\specialUnitaryLie(2)\) is simple.
        The Lie algebra \(\specialUnitaryLie(2) \times \specialUnitaryLie(2)\) is semisimple, having a subalgebra isomorphic to \(\specialUnitaryLie(2)\), but no Abelian proper subalgebras.
    \end{exm}
    
    \section{Nonsemisimple Case}
    Consider a Lie algebra, \(\lie{l}\), which is \emph{not} semisimple.
    This means that there exists some Lie algebra, \(\lie{s}\), which is an Abelian subalgebra of \(\lie{l}\).
    In this section we'll use the indices \(a, b, c, \dotsc\) to index elements of \(\lie{s}\).
    We'll use the indices \(i, j, k, \dotsc\) to index elements of \(\lie{l}\), and we'll use the indices \(p, q, r, \dotsc\) to index elements of \(\lie{p} = \lie{l} \setminus \lie{s}\), that is the complement of \(\lie{s}\) in \(\lie{l}\), which is itself a vector space.
    Denote by \(S_i\) the generators of \(\lie{s}\), \(P_p\) the generators of \(\lie{p}\), and \(T_a\) the generators of \(\lie{l}\), note that \(T_a\) coincide with \(S_i\) and \(P_p\).
    Indices \(a, b, c, \dotsc\) run from \(1\) to \(\dim \lie{l}\).
    Indices \(i, j, k, \dotsc\) run from \(1\) to \(\dim \lie{s}\).
    Indices \(p, q, r, \dotsc\) run from \(\dim\lie{s} + 1\) to \(\dim\lie{l}\).
    
    We then have two facts:
    \begin{enumerate}
        \item \(\commutator{S_i}{S_j} = i\tensor{c}{^a_{ij}}T_a = 0\) since \(\lie{s}\) is Abelian.
        Hence \(\tensor{c}{^a_{ij}} = 0\) since \(T_a\) are linearly independent so can only sum to zero if the coefficients vanish.
        \item \(\commutator{S_i}{T_a} = i\tensor{c}{^b_{ia}}T_b \in \lie{s}\) since \(\lie{s}\) is an invariant subalgebra.
        This implies that \(\tensor{c}{^p_{ia}} = 0\) as any element of \(\lie{s}\) can be expressed as a linear combination of \(S_i\) with no \(P_p\) terms.
    \end{enumerate}
    
    Now, consider the elements of the Killing form with one index restricted to \(\lie{s}\).
    By definition we have
    \begin{equation}
        g_{ia} = \tr_{\symrm{A}}(T_iT_a) = -\tensor{c}{^c_{id}} \tensor{c}{^d_{ac}}.
    \end{equation}
    Using the second fact we see that \(\tensor{c}{^c_{id}}\) vanishes unless \(c\) is between 1 and \(\dim\lie{s}\), and so we can set \(c = j\), giving
    \begin{equation}
        g_{ia} = -\tensor{c}{^j_{id}}\tensor{c}{^d_{aj}}.
    \end{equation}
    Using the second fact again we see that \(\tensor{c}{^d_{aj}}\) vanishes unless \(d\) is between 1 and \(\dim\lie{s}\), and so we can set \(d = k\), giving
    \begin{equation}
        g_{ia} = -\tensor{c}{^j_{ik}}\tensor{c}{^k_{aj}} = 0.
    \end{equation}
    Here we've used the first fact which tells us that \(\tensor{c}{^j_{ik}} = 0\) identically.
    Symmetry means that \(g_{ai} = 0\) also, and so we see that the metric takes the form
    \begin{equation}
        \begin{pmatrix}
            0 & 0\\
            0 & g_{pq}
        \end{pmatrix}
    \end{equation}
    where \(g_{pq}\) is a \((\dim\lie{l} - \dim\lie{s}) \times (\dim\lie{l} - \dim\lie{s})\) matrix and the zeros are all block matrices of zeros.
    That is, the metric is zero apart from on the subspace \(\lie{p}\).
    The metric has \(\dim\lie{s}\) zero eigenvalues.
    
    \subsection{Semisimple Case}
    Consider a semisimple Lie algebra, \(\lie{g}\).
    Then the Killing form has no zero eigenvalues, and so is invertible.
    We define the inverse to be the tensor \(g^{ab}\) such that
    \begin{equation}
        g^{ab}g_{bc} = \tensor{\delta}{^a_c}.
    \end{equation}
    For a semisimple Lie algebra the killing form is a pseudo-Riemannian metric, this means we can choose a basis in which \(g_{ab}\) and \(g^{ab}\) are diagonal and have only 1 and \(-1\) on the diagonal.
    
    \section{Casimir Operators}
    \begin{dfn}{Casimir Operator}{}
        A \defineindex{Casimir operator} is anything\footnote{the technical definition being any element of the centre of the universal enveloping algebra, which is basically the associative algebra with the same representations as the Lie algebra, so in the case of matrix Lie algebras its simply the set of matrices with matrix multiplication as the associative product in the algebra, this is what allows us to work with the universal enveloping algebra informally. The formal definition of the universal enveloping algebra is to construct the tensor algebra, \(T(\lie{g}) \coloneqq \field \oplus \lie{g} \oplus (\lie{g} \otimes \lie{g}) \oplus (\lie{g} \otimes \lie{g} \otimes \lie{g}) \oplus \dotsb\), then define recursively the bracket on \(T^n(\lie{g}) \coloneqq \lie{g}^{\otimes n}\) by \(\commutator{x\otimes y}{z} \coloneqq x \otimes \commutator{y}{z} + \commutator{x}{y}\otimes z\) and extend this linearly, note that the Lie brackets on the right correspond to the Lie brackets on \(T^k(\lie{g})\) and \(T^{n - k}(\lie{g})\) for some \(k < n\). The universal enveloping algebra is then \(U(\lie{g}) = T(\lie{g})/{\sim}\) where \(\sim\) is the equivalence relation \(x \otimes y - y \otimes x = \commutator{x}{y}\)} which commutes with all elements of the Lie algebra.
    \end{dfn}
    
    \begin{dfn}{Quadratic Casimir}{}
        The \defineindex{quadratic Casimir} of a semisimple Lie algebra with Killing form \(g_{ab}\) is defined as
        \begin{equation}
            C^{(2)} \coloneqq g^{ab}T_aT_b.
        \end{equation}
        Note that this definition is independent of the representation we choose.
    \end{dfn}
    
    For a nonsemisimple Lie algebra we can define a quadratic Casimir in a similar way but we have to choose a different bilinear form, since \(g^{ab}\) doesn't exist.
    
    Having claimed that \(C^{(2)}\) is a Casimir operator, and so commutes with all elements of the Lie algebra we should prove it.
    It is sufficient to show that \(C^{(2)}\) commutes with all generators of the Lie algebra.
    
    \begin{lma}{}{}
        Let \(\lie{g}\) be a semisimple Lie algebra with Killing form \(g_{ab}\).
        Then the quadratic Casimir, \(C^{(2)} = g^{ab}T_aT_b\), commutes with all generators of \(\lie{g}\).
        
        \begin{proof}
            We'll prove this two ways, first using the Lie group structure, then using the Lie algebra structure.
            Let \(T_a\) be the generators of \(\lie{g}\).
            Consider the universal covering group, \(G\), of \(\lie{g}\), that is we get \(G\) by exponentiating \(\lie{g}\).
            Take some \(g \in G\).
            Then consider \(gC^{(2)}g^{-1}\).
            If \(C^{(2)}\) commutes with all the group elements then it must commute with the generators, since we can expand the group elements about the identity as a series of generators, so if \(gC^{(2)}g = C^{(2)}\) then \(g\) must have commuted with \(C^{(2)}\) and we will have proven the statement.
            Inserting the definition of \(C^{(2)}\) we have
            \begin{equation}
                gC^{(2)}g^{-1} = g^{ab} gT_aT_bg^{-1}.
            \end{equation}
            Here we've pulled the inverse Killing form out since it is just a number.
            We can insert an identity in the form \(g^{-1}g\) to get
            \begin{equation}
                gC^{(2)}g^{-1} = g^{ab}gT_ag^{-1}gT_bg^{-1}.
            \end{equation}
            Now we can recognise \(gT_ag = T_c \tensor{D}{^c_a}(g)\) as the adjoint representation acting on \(T_a\), and so
            \begin{equation}
                gC^{(2)}g^{-1} = g^{ab} T_c \tensor{D}{^c_a}(g) T_d\tensor{D}{^d_b}(g).
            \end{equation}
            Now using the fact that \(g^{ab}\) is an invariant tensor we have
            \begin{equation}
                g^{cd} \mapsto g^{ab} \tensor{D}{^c_a}(g)\tensor{D}{^d_b}(g) = g^{cd},
            \end{equation}
            and so
            \begin{equation}
                gC^{(2)}g^{-1} = g^{cd}T_cT_d = C^{(2)}.
            \end{equation}
            This completes the first proof.\\[1.5ex]
            We have the identity
            \begin{equation}
                \commutator{AB}{C} = A\commutator{B}{C} + \commutator{A}{C}B.
            \end{equation}
            We then have
            \begin{align}
                \commutator{C^{(2)}}{T_c} &= \commutator{g^{ab}T_aT_b}{T_c}\\
                &= g^{ab}\commutator{T_aT_b}{T_c}\\
                &= g^{ab}T_a\commutator{T_b}{T_c} + g^{ab}\commutator{T_a}{T_c}T_b\\
                &= g^{ab}T_ai\tensor{c}{^d_{bc}}T_d + g^{ab}i\tensor{c}{^d_{ac}}T_dT_b\\
                &= i\tensor{c}{^d_{bc}}T^bT_d + i\tensor{c}{^d_{ac}}T_dT^a\\
                &= i\tensor{c}{_{dbc}}T^bT^d + i\tensor{c}{_{dac}}T^dT^a \qquad b \to a\text{ in first term}\\
                &= i\tensor{c}{_{dac}}T^aT^d + i\tensor{c}{_{dac}}T^dT^a\\
                &= i\tensor{c}{_{dac}}\anticommutator{T^a}{T^d}\\
                &= 0
            \end{align}
            where in the last step we notice that \(\tensor{c}{_{dac}}\) is antisymmetric in \(a\) and \(d\), and \(\anticommutator{T^a}{T^d}\) is symmetric in \(a\) and \(d\), so their product vanishes.
            This completes the second proof.
        \end{proof}
    \end{lma}
    
    Consider the trace of two generators in some representation, \(R\):
    \begin{equation}
        \tr_{R}(T_aT_b) \eqqcolon \overbar{g}_{ab}.
    \end{equation}
    Obviously choosing \(R\) to be the adjoint representation recovers the definition of the Killing form.
    This is an invariant tensor, meaning that
    \begin{equation}
        D^\trans \overbar{g} D = \overbar{g}
    \end{equation}
    for all \(D\) in the adjoint representation.
    Now suppose that we are working in a semisimple Lie algebra.
    Then the inverse of the Killing form exists.
    Consider the product \(g^{ab}\overbar{g}_{bc}\), or in terms of matrices \(g^{-1}\overbar{g}\).
    Since both \(g\) and \(\overbar{g}\) are invariant tensors we have \(g = D^\trans g D\) and \(\overbar{g} = D^\trans \overbar{g} D\) for representation \(D\) evaluated at some arbitrary group element.
    Inverting the first of these we have \(g^{-1} = D^{-1}g^{-1}(D^\trans)^{-1}\).
    Putting these together we have
    \begin{equation}
        g^{-1} \overbar{g} = D^{-1} g^{-1} (D^\trans)^{-1} D^{\trans} g D = D^{-1}g^{-1}\overbar{g}D.
    \end{equation}
    Multiplying on the left by \(D\) we get
    \begin{equation}
        D g^{-1} \overbar{g} D = g^{-1}\overbar{g},
    \end{equation}
    so \(g^{-1}\overbar{g}\) commutes with all \(D\) in the adjoint representation.
    
    \begin{lma}{Schur's Lemma}{}
        Let \(G\) be a Lie group with finite dimensional irreducible representations \(D, D' \colon G \to \generalLinear(V)\).
        Let \(T \colon V \to V'\) be a linear map satisfying \(T \circ D(g) = D'(g) \circ T\) for all \(g \in G\).
        Then \(T = \lambda \ident\) for some \(\lambda \in \complex\).
    \end{lma}
    
    Schur's lemma basically tells us that any matrix which commutes with all the matrices in an irreducible representation of a group is simply a multiple of the identity.
    The adjoint representation of a semisimple Lie group is an irreducible representation, and so we must have that \(g^{-1}\overbar{g} = \lambda \ident\) for some \(\lambda \in \complex\).
    This means that, up to a scale constant, \(\overbar{g}\) is the inverse to \(g^{-1}\), so up to a scale constant \(\overbar{g}\) is the Killing form, i.e., \(\overbar{g}_{ab} = \lambda g_{ab}\).
    
    This means that the trace of two generators is given by the Killing form times some, representation dependent, number, \(C(R)\):
    \begin{equation}
        \tr_R(T_aT_b) = C(R)g_{ab}.
    \end{equation}
    Now consider the quadratic Casimir \(C^{(2)} = g^{ab}T_aT_b\), which we can now write as
    \begin{equation}
        C^{(2)} = C_R^{(2)}\ident
    \end{equation}
    by Schur's lemma, where \(C_R^{(2)}\) is another representation dependent number.
    
    Now consider \(g^{ab}\tr_R(T_aT_b) = C(R)g^{ab}g_{ab}\).
    The left hand side is 
    \begin{equation}
        g^{ab}\tensor{(T_a)}{^c_d}\tensor{(T_b)}{^d_c} = C_R^{(2)}\dim R.
    \end{equation}
    The right hand side is
    \begin{equation}
        g^{ab}g_{ab}C(R) = \tensor{\delta}{^a_a}C(R) = C(R) \dim G.
    \end{equation}
    This means we can work out the eigenvalue of the quadratic Casimir operator by evaluating
    \begin{equation}
        C_R^{(2)} = C(R)\frac{\dim G}{\dim R}
    \end{equation}
    for some representation \(R\).
    
    \begin{exm}{}{}
        Consider \(\specialUnitary(2)\).
        The adjoint representation is \(3 \times 3\) matrices \(\tensor{(T_a)}{^b_c} = \varepsilon_{bac}\).
        Some basic identities give us \(\tr_{\symrm{A}}(T_aT_b) = 2\delta_{ab}\).
        The Casimir operator in this case is \(\vv{J}^2\), and has eigenvalue \(j(j + 1)\), this should be familiar from quantum mechanics, see \course{Principles of Quantum Mechanics}, and \course{Symmetries of Quantum Mechanics} has the group theory details.
        So we have the eigenvalue \(2\), meaning \(j = 1\).
        
        There is also a two dimensional representation given by \(T_a = \sigma_a/2\).
        Then \(\tr_2(T_aT_b) = \tr(\sigma_a\sigma_b)/4 = \delta_{ab}/2\).
        We then have \(C_R^{(2)} = (1/2)(3/2) = 3/4\), which is \(j(j + 1)\) with \(j = 1/2\).
    \end{exm}
    
    \part{Spacetime Symmetries}
    \chapter{Lorentz Group}
    \section{Relativity Recap}
    \begin{rmk}
        We assume the reader is familiar with the basics of relativity.
        Should this not be the case see notes from any and all of \course{Relativity Nuclear and Particle Physics} (basics), \course{Classical Electrodynamics}, \course{Quantum Theory}, or \course{Quantum Field Theory}.
    \end{rmk}
    
    An event in spacetime is given by a four vector with components \(x^\mu = (x^0, \vv{x})\), where \(x^0 = ct\).
    Four vectors are defined by their transformation rule:
    \begin{equation}
        x^\mu \mapsto x'^\mu = \tensor{\Lambda}{^\mu_\nu}x^\nu \iff x \mapsto x' = \Lambda x,
    \end{equation}
    where \(\Lambda\) is a Lorentz transformation, which we'll define shortly.
    
    There is a metric tensor, \(\minkowskiMetric_{\mu\nu}\), which is real, symmetric, and invertible (\(\det\minkowskiMetric \ne 1\)).
    The inverse of this metric is \(\minkowskiMetric^{\mu\nu}\), and is such that \(\minkowskiMetric^{\mu\nu}\minkowskiMetric_{\nu\rho} = \tensor{\delta}{^\mu_\rho}\).
    We can use this to raise and lower indices, \(\minkowskiMetric_{\mu\nu}x^\mu = x_\nu\), and \(\minkowskiMetric^{\mu\nu}x_\nu = x^\mu\).
    We can define an inner product on four-vectors:
    \begin{equation}
        x \cdot y \coloneqq x^\mu y^\nu \minkowskiMetric_{\mu\nu} = x^\mu y_\mu = x^\trans \minkowskiMetric y.
    \end{equation}
    We can choose an orthonormal, with respect to this inner product, metric such that
    \begin{equation}
        \minkowskiMetric_{\mu\nu} = \minkowskiMetric^{\mu\nu} = \diag(1, -1, -1, -1).
    \end{equation}
    Then \(x_\mu = (x^0, -\vv{x})\).
    
    Formally the space of all position four-vectors forms a Lorentzian manifold, meaning the metric signature is \((1, D - 1)\) in \(D\) dimensions, here the first number is the number of positive eigenvalues of the metric and the second the number of negative eigenvalues.
    This Lorentzian manifold is what we call \defineindex{Minkowski space}, \(\minkowskiSpace\)\index{R13@\(\minkowskiMetric\)|see{Minkowski space}}.
    
    \section{The Lorentz Group}
    \begin{dfn}{Lorentz Transformation}{}
        A \defineindex{Lorentz transformation}, \(\Lambda\), is a transformation of Minkowski space, \(\minkowskiSpace\), preserving the metric.
        That is, \(\Lambda\) is a \(4 \times 4\) real matrix such that
        \begin{equation}
            \Lambda^\trans \minkowskiMetric \Lambda = \minkowskiMetric.
        \end{equation}
        
        The \defineindex{Lorentz group}, \(\orthogonal(1, 3)\)\index{O(1,3)@\(\orthogonal(1, 3)\)|see{Lorentz group}}, is the group of all Lorentz transformations.
    \end{dfn}
    
    A consequence of this definition is that Lorentz transformations preserve the inner product, if \(x \mapsto \Lambda x\) and \(y \mapsto \Lambda y\) then
    \begin{equation}
        x \cdot y \mapsto (\Lambda x)^\trans \minkowskiMetric (\Lambda y) = x^\trans (\Lambda^\trans \minkowskiMetric \Lambda) y = x^\trans \minkowskiMetric y = x \cdot y
    \end{equation}
    
    We have the determinant identities
    \begin{equation}
        \det(AB) = \det(A)\det(B), \qqand \det A^\trans = \det A.
    \end{equation}
    Using these we have
    \begin{equation}
        \det \minkowskiMetric = \det(\Lambda^\trans \minkowskiMetric \Lambda) = \det(\Lambda^\trans)\det(\minkowskiMetric)\det(\Lambda) = \det(\Lambda)^2\det(\minkowskiMetric)
    \end{equation}
    and so
    \begin{equation}
        \det(\Lambda)^2 = 1 \implies \det(\Lambda) = \pm 1.
    \end{equation}
    We call Lorentz transformations with \(\det \Lambda = 1\) \define{proper Lorentz transformations}\index{proper Lorentz transformation}.
    They are analogous to proper rotations.
    
    \begin{dfn}{Proper Lorentz Group}{}
        The \defineindex{proper Lorentz group}, \(\specialOrthogonal(1, 3)\), is the subgroup of \(\orthogonal(1, 3)\) formed from all Lorentz transformations with unit determinant.
    \end{dfn}
    
    Consider the defining property of Lorentz transformations, written out in terms of components:
    \begin{equation}
        \minkowskiMetric_{\mu\nu} \tensor{\Lambda}{^\mu_\rho} \tensor{\Lambda}{^\nu_\sigma} = \minkowskiMetric_{\rho\sigma}.
    \end{equation}
    Setting \(\rho = \sigma = 0\) gives
    \begin{equation}
        \minkowskiMetric_{\mu\nu} \tensor{\Lambda}{^\mu_0} \tensor{\Lambda}{^\nu_0} = 1.
    \end{equation}
    The term on the left is zero unless \(\mu = \nu\), and so we get
    \begin{equation}
        (\tensor{\Lambda}{^0_0}) - \sum_i (\tensor{\Lambda}{^i_0})^2 = 1.
    \end{equation}
    Hence, we have \((\tensor{\Lambda}{^0_0})^2 \ge 1\), since \(\Lambda\) is a real matrix, so \(\sum_i(\tensor{\Lambda}{^i_0})^2 \ge 0\).
    Hence we either have \(\tensor{\Lambda}{^0_0} \ge 1\) or \(\tensor{\Lambda}{^0_0} \le -1\).
    We call Lorentz transformations with \(\tensor{\Lambda}{^0_0} \ge 1\) \defineindex{orthochronous}, since they preserve the direction of time.
    
    \begin{dfn}{Proper Orthochronous Lorentz Group}{}
        The \defineindex{proper orthochronous Lorentz group}, \(\specialOrthogonal^+(1, 3)\), is the subgroup of \(\specialOrthogonal(1, 3)\) formed from all Lorentz transformations, \(\Lambda\), with \(\tensor{\Lambda}{^0_0} \ge 1\).
    \end{dfn}
    
    It is common refer to the proper orthochronous Lorentz group as simply \emph{the Lorentz group}, since it is the group of symmetries that we require our theories to be invariant under.
    
    The Lorentz group \(\orthogonal(1, 3)\), has four connected components:
    \begin{itemize}
        \item \(\det \Lambda = +1\) and \(\tensor{\Lambda}{^0_0} \ge +1\);
        \item \(\det \Lambda = -1\) and \(\tensor{\Lambda}{^0_0} \ge +1\);
        \item \(\det \Lambda = -1\) and \(\tensor{\Lambda}{^0_0} \le -1\);
        \item \(\det \Lambda = +1\) and \(\tensor{\Lambda}{^0_0} \le -1\).
    \end{itemize}
    The first of these, which is \(\specialOrthogonal^+(1, 3)\), contains the identity.
    The second contains the parity operator, \(\parity \coloneqq \diag(1, -1, -1, -1)\).
    The third contains the time reversal operator, \(\timeReversal \coloneqq \diag(-1, 1, 1, 1)\).
    The fourth contains the product \(\parity \timeReversal = \timeReversal \parity = \diag(-1, -1, -1, -1)\).
    We can use these to move between the different components as follows:
    \begin{equation}
        \tikzexternaldisable
        \begin{tikzcd}[sep=2.5cm, arrows={<->}]
            ++ \arrow[r, "\displaystyle\timeReversal"] \arrow[d, "\displaystyle\parity"'] \arrow[dr, "\displaystyle\parity\timeReversal"'] & -- \arrow[d, "\displaystyle\parity"]\\
            -+ \arrow[r, "\displaystyle\timeReversal"] & +-
        \end{tikzcd}
        \tikzexternalenable
    \end{equation}
    where \(\pm\pm\) corresponds to the sign of \(\det \Lambda\) and then the sign of \(\tensor{\Lambda}{^0_0}\), and since \(\parity\) and \(\timeReversal\) are self inverses we can move either way along the arrow using them.
    
    This is analogous to how \(\orthogonal(n)\) has two disconnected components, one with \(\det R = +1\) and one with \(\det R = -1\), and we can move between them using \(-\ident_n\).
    As in this case elements of the disconnected parts of the Lorentz groups do not generally commute with each other.
    
    The proper orthochronous Lorentz group is formed from rotations and boosts.
    A general rotation is of the form
    \begin{equation}
        \Lambda_R = 
        \begin{pmatrix}
            1 & 0\\
            0 & R
        \end{pmatrix}
        =
        \left(
            \begin{array}{cccc}
                1 & 0 & 0 & 0 \\ \cline{2-4}
                0 & \multicolumn{3}{|c|}{} \\
                0 & \multicolumn{3}{|c|}{R} \\
                0 & \multicolumn{3}{|c|}{} \\ \cline{2-4}
            \end{array}
        \right).
    \end{equation}
    The property that \(\Lambda^\trans \Lambda = 1\) implies \(R^\trans R = 1\), and similarly \(\det \Lambda = 1\) implies \(\det R = 1\), so \(R \in \specialOrthogonal(3)\), which is how we interpret this as a rotation.
    It simply leaves time, the first coordinate, unchanged.
    There are three parameters needed to specify a rotation.
    For example, we can specify a unit vector with two (the third component being fixed by the normalisation condition) and then the size of the rotation is the third, or we can use three Euler angles to specify a rotation.
    If we go with the first specification then the parameters specifying two coordinates of a unit vector are taken from \([-1, 1]\), and the angle from \([0, 2\pi)\), identifying \(0\) and \(2\pi\).
    Similarly in the third case the Euler angles are all taken from closed intervals, exactly which interval depending on you convention for defining Euler angles.
    Either way we see that rotations form a compact subgroup of the Lorentz group, and this subgroup is (isomorphic to) \(\specialOrthogonal(3)\).
    
    A general boost is of the form
    \begin{equation*}
        \Lambda_{\vartheta, \vh{n}} = 
        \begin{pmatrix}
            \cosh(\vartheta) & \sinh(\vartheta) \vh{n}^\trans\\
            \sinh(\vartheta) \vh{n}  & \ident_3 + \cosh(\vartheta) \vh{n} \vh{n}^\trans
        \end{pmatrix}
        =
        \begin{pmatrix}
            c & n_1 s & n_2 s & n_3 s \\
            s n_1 & 1 + c n_1^2 & cn_1n_2 & cn_1n_3 \\
            s n_2 & n_2n_1 & 1 + n_2^2 & n_2n_3 \\
            s n_3 & n_3n_1 & n_3n_2 & 1 + n_3^2
        \end{pmatrix}
    \end{equation*}
    where \(c = \cosh \vartheta\) and \(s = \sinh \vartheta\).
    Here \(\vh{n}\) specifies the direction of the boost, so \(\vh{n} = \vh{x}\) for the standard Lorentz boost, and \(\vartheta\) is a parameter called the \defineindex{rapidity}, defined as \(\artanh(v/c)\).
    While the components defining \(\vh{n}\) are bounded between \([-1, 1]\) the rapidity is unbounded, since \(\artanh\) maps the interval \([0, 1]\) to \([0, \infty]\).
    This means that the boosts form a noncompact subgroup of the Lorentz group.
    
    A general Lorentz transformation can then be written as a combination of a rotation and a boost, \(\Lambda_R\Lambda_B\).
    There are then 6 parameters needed to specify this Lorentz transformation, the simplest being 3 specifying \(\Lambda_R\) and 3 specifying \(\Lambda_B\), as discussed above.
    The Lorentz group is noncompact, since it has a noncompact subgroup (the boosts).
    
    \chapter{Lorentz Algebra}
    \section{Deriving the Algebra}
    The Lorentz group is a Lie group, and the component connected to the identity is the proper orthochronous Lorentz group, \(\specialOrthogonal^+(1, 3)\).
    The Lie algebra of the full Lorentz group and this subgroup coincide, as does the Lie algebra of the in-between-subgroup \(\specialOrthogonal(1, 3)\), which is analogous to \(\orthogonal(n)\) and \(\specialOrthogonal(n)\) having the same Lie algebras.
    
    We can derive the Lorentz algebra, \(\specialOrthogonalLie^+(1, 3)\), in the same way as we did for the other matrix groups.
    We consider some Lorentz transformation, \(\Lambda \in \specialOrthogonal^+(1, 3)\), close to the identity and expand.
    It's easier to do this working with the components, rather than the full matrices, so in our expansion the identity matrix becomes \(\tensor{\delta}{^\mu_\nu}\) and we have
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = \tensor{\delta}{^\mu_\nu} + \tensor{\omega}{^\mu_\nu} + \order(\omega^2)
    \end{equation}
    where \(\tensor{\omega}{^\mu_\nu}\) are the components of some matrix and \(\abs{\tensor{\omega}{^\mu_\nu}} \ll 1\).
    
    We now use the defining property of a Lorentz transformation,
    \begin{equation}
        \minkowskiMetric_{\mu\nu} = \minkowskiMetric_{\sigma\rho} \tensor{\Lambda}{^\sigma_\mu} \tensor{\Lambda}{^\rho_\nu},
    \end{equation}
    and expanding this we get
    \begin{align}
        \minkowskiMetric_{\mu\nu} &= \minkowskiMetric_{\sigma\rho} \tensor{\Lambda}{^\sigma_\mu} \tensor{\Lambda}{^\rho_\nu}\\
        &= \minkowskiMetric_{\sigma\rho}(\tensor{\delta}{^\sigma_\mu} + \tensor{\omega}{^\sigma_\mu} + \order(\omega^2)) (\tensor{\delta}{^\rho_\nu} + \tensor{\omega}{^\rho_\nu} + \order(\omega^2))\\
        &= \minkowskiMetric_{\sigma\rho}(\tensor{\delta}{^\sigma_\mu}\tensor{\delta}{^\rho_\nu} + \tensor{\delta}{^\sigma_\mu} \tensor{\omega}{^\rho_\nu} + \tensor{\omega}{^\sigma_\mu}\tensor{\delta}{^\rho_\nu} + \order(\omega^2))\\
        &= \minkowskiMetric_{\mu\nu} + \minkowskiMetric_{\mu\rho}\tensor{\omega}{^\rho_\nu} + \minkowskiMetric_{\sigma\nu}\tensor{\omega}{^\sigma_\mu} + \order(\omega^2)\\
        &= \minkowskiMetric_{\mu\nu} + \omega_{\mu\nu} + \omega_{\nu\mu} +\order(\omega^2).
    \end{align}
    So, we see that \(\omega\) must be antisymmetric, \(\omega_{\mu\nu} = -\omega_{\nu\mu}\), so that the two linear terms in \(\omega\) cancel.
    
    \section{Exponentiating the Lorentz Algebra}
    We start with the assumption that a general infinitesimal Lorentz transformation, \(\Lambda\), can be written as
    \begin{equation}
        \tensor{\Lambda}{^\gamma_\delta} = \tensor{\delta}{^\gamma_\delta} + i\frac{\omega^{\alpha\beta}}{2}\tensor{(M_{\alpha\beta})}{^\gamma_\delta}
    \end{equation}
    where \(M_{\alpha\beta}\) is a generator of the Lie algebra of Lorentz transformations and is labelled by \(\alpha\) and \(\beta\), and above we look at the component in row \(\gamma\) and column \(\delta\).
    Think of \(M_{\alpha\beta}\) as being \(T_a\), but we use two labels, \(\alpha\) and \(\beta\), instead of just one, \(a\), because this will help us interpret the generators later.
    Here \(\omega_{\alpha\beta}\) is just some parameter, like \(\alpha^a\) in a generic Lie algebra.
    In terms of matrices, rather than components, we can write this as
    \begin{equation}
        \Lambda = \ident + i\frac{\omega^{\alpha\beta}}{2}M_{\alpha\beta}.
    \end{equation}
    We previously wrote an infinitesimal Lorentz transformation as
    \begin{equation}
        \tensor{\Lambda}{^\gamma_\delta} = \tensor{\delta}{^\gamma_\delta} + \tensor{\omega}{^\gamma_\delta},
    \end{equation}
    so we have
    \begin{equation}
        \tensor{\omega}{^\gamma_\delta} = i\frac{\omega^{\alpha\beta}}{2}\tensor{(M_{\alpha\beta})}{^\gamma_\delta}.
    \end{equation}
    We can solve this to show that in the defining representation
    \begin{equation}
        \tensor{(M_{\alpha\beta})}{^\gamma_\delta} = -i(\tensor{\delta}{^\gamma_\alpha}\minkowskiMetric_{\beta\delta} - \tensor{\delta}{^\gamma_\beta}\minkowskiMetric_{\alpha\delta}).
    \end{equation}
    Notice that this is antisymmetric in \(\alpha\) and \(\beta\), which makes sense since \(\omega^{\alpha\beta}\) is also antisymmetric in \(\alpha\) and \(\beta\) so no symmetric part of \(M_{\alpha\beta}\) can contribute.
    This means there are only \(6\) independent generators, \(M_{\alpha\beta}\).
    This also explains the choice of the factor of 2, we don't want to double count.
    
    Consider the case where \(\alpha\) and \(\beta\) are taken as spatial indices \(i\) and \(j\), and we consider the spatial components of \(M_{ij}\).
    From the definition we find that
    \begin{align}
        \tensor{(M_{ij})}{^k_l} &= -i(\tensor{\delta}{^k_i} \minkowskiMetric_{jl} - \tensor{\delta}{^k_j} \minkowskiMetric_{il})\\
        &= -i(-\delta_{ki}\delta_{jl} + \delta_{kj}\delta_{il})\\
        &= i(\delta_{ki}\delta_{jl} - \delta_{kj}\delta_{il}).
    \end{align}
    notice that this is antisymmetric under exchange of \(k\) and \(l\), and so accounting for the factor of \(i\) we see that \(M_{ij}\) are Hermitian.
    This is what we would expect for the generators of a rotation.
    
    Similarly we can consider the case when \(\alpha = 0\), and \(\beta\) is spatial:
    \begin{align}
        \tensor{(M_{0i})}{^\gamma_\delta} &= -i(\tensor{\delta}{^\gamma_0} \minkowskiMetric_{i\delta} - \tensor{\delta}{^\gamma_i} \minkowskiMetric_{0\delta})\\
        &= -i(-\tensor{\delta}{^\gamma_0} \delta_{i\delta} - \tensor{\delta}{^\gamma_i}\delta_{0\delta})\\
        &= i(\tensor{\delta}{^\gamma_0} \delta_{i\delta} + \tensor{\delta}{^\gamma_i}\delta_{0\delta}).
    \end{align}
    Notice that this is symmetric under exchange of \(\gamma\) and \(\delta\), so accounting for the factor of \(i\) we find that \(M_{0i}\) are anti-Hermitian.
    This is what we would expect for the generators of boosts.
    
    From these results we see that
    \begin{equation}
        \Lambda = \exp\left[ i\frac{\omega^{ij}}{2} M_{ij} \right]
    \end{equation}
    is unitary.
    We also see that the space of real parameters \(\omega^{ij}\) is compact, since the group element, \(\Lambda\), is a periodic function of \(\omega^{ij}\).
    On the other hand
    \begin{equation}
        \Lambda = \exp\left[ i\frac{\omega^{0i}}{2}M_{0i} \right]
    \end{equation}
    is not unitary and the space of parameters is not compact.
    
    This seems like a problem.
    We want to be able to do relativistic quantum mechanics.
    However, quantum mechanics requires unitary transformations, and we've seen here that the Lorentz group is not compact, and therefore isn't guaranteed to have any nontrivial finite dimensional faithful\footnote{A \defineindex{faithful representation} is one in which the homomorphism \(D \colon G \to \generalLinear(V)\) is injective.} unitary representations, and in fact doesn't.
    We'll come back to this later.
    
    \section{The Lie Bracket of the Lorentz Algebra}
    Now that we've seen what elements of the Lorentz algebra are we should work out what the Lie bracket is.
    We could just work in the defining representation and compute commutators of \(M_{\alpha\beta}\), but there's a less brute-force method.
    Consider some infinitesimal Lorentz transformation \(\Lambda\) and also some other infinitesimal Lorentz transformation which we can write as \(\ident + \omega\).
    Then consider the following, where we make use of the fact that a representation is a homomorphism:
    \begin{align}
        D(\Lambda)D(1 + \omega) D(\Lambda)^{-1} &= D(\Lambda)D(1 + \omega)D(\Lambda^{-1})\\
        &= D(\Lambda(1 + \omega)\Lambda^{-1})\\
        &= D(1 + \Lambda\omega\Lambda^{-1})\\
        &= D(1 + \Lambda\omega\Lambda^{\trans}).
    \end{align}
    In the last step we used the fact that for an infinitesimal Lorentz transformation \(\Lambda^{-1} = \Lambda^\trans\), or in terms of components \((\Lambda^{-1})^{\mu\nu} = (\Lambda^{\trans})^{\nu\mu}\).
    This follows from the defining relation, \(\Lambda^\trans \minkowskiMetric \Lambda = \minkowskiMetric\), we can take the inverse of to get
    \begin{equation}
        \Lambda^{-1}\minkowskiMetric^{-1}(\Lambda^{\trans})^{-1} = \Lambda^{-1}\minkowskiMetric(\Lambda^\trans)^{-1} = \eta,
    \end{equation}
    which follows since the inverse of a Lorentz transformation is also a Lorentz transformation.
    Multiplying by \(\Lambda^\trans\) on the right we get
    \begin{equation}
        \Lambda^{-1}\minkowskiMetric = \minkowskiMetric \Lambda^\trans.
    \end{equation}
    In terms of components this is
    \begin{equation}
        \tensor{(\Lambda^{-1})}{^\nu_\rho} \minkowskiMetric^{\rho\mu} = \minkowskiMetric^{\nu\rho} \tensor{(\Lambda^\trans)}{^\mu_\rho} \implies (\Lambda^{-1})^{\mu\nu} = (\Lambda^{\trans})^{\mu\nu}.
    \end{equation}
    
    By definition we have
    \begin{equation}
        D(1 + \omega) = 1 + i\frac{\omega^{\alpha\beta}}{2}M_{\alpha\beta} + \order(\omega^2).
    \end{equation}
    Hence,
    \begin{align}
        D(\Lambda)D(1 + \omega)D(\Lambda)^{-1} &= D(\Lambda)\left[ 1 + i \frac{\omega^{\alpha\beta}}{2}M_{\alpha\beta} \right]D(\Lambda)^{-1}\\
        &= 1 + i\frac{\omega^{\alpha\beta}}{2}D(\Lambda)M_{\alpha\beta}D(\Lambda)^{-1}.
    \end{align}
    Hence, we have
    \begin{equation}
        D(1 + \Lambda\omega\Lambda^\trans) = 1 + \frac{i}{2}\underbrace{\tensor{\Lambda}{^\alpha_\mu}\omega^{\mu\nu}\tensor{\Lambda}{^\beta_\nu}}_{(\Lambda\omega\Lambda^\trans)^{\alpha\beta}}M_{\alpha\beta}.
    \end{equation}
    This must be true for any choice of our parameters, so, comparing our two results and renaming some indices we have
    \begin{equation}\label{eqn:deriving commutator in Lorentz algebra}
        D(\Lambda)M_{\alpha\beta}D(\Lambda)^{-1} = \tensor{\Lambda}{^\mu_\alpha}\tensor{\Lambda}{^\nu_\beta}M_{\mu\nu}.
    \end{equation}
    That is, \(M_{\alpha\beta}\) transforms as a rank 2 tensor.
    
    Now consider some Lorentz transformation which we can write as
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = \tensor{\delta}{^\mu_\nu} + \tensor{\varepsilon}{^\mu_\nu} + \order(\varepsilon^2).
    \end{equation}
    We then have
    \begin{equation}
        D(\Lambda) = 1 + i\frac{\varepsilon^{\mu\nu}}{2}M_{\mu\nu}, \qqand D(\Lambda)^{-1} = 1 - i\frac{\varepsilon^{\mu\nu}}{2}M_{\mu\nu}.
    \end{equation}
    Substituting this into the left hand side of \cref{eqn:deriving commutator in Lorentz algebra} we get
    \begin{align}
        D(\lambda)M_{\alpha\beta}D(\Lambda) &= \left[ 1 + i\frac{\varepsilon^{\mu\nu}}{2}M_{\mu\nu} \right] M_{\alpha\beta} \left[ 1 - i\frac{\varepsilon^{\mu\nu}}{2}M_{\mu\nu} \right]\\
        &= M_{\alpha\beta} + \frac{i}{2}\varepsilon^{\mu\nu}M_{\mu\nu}M_{\alpha\beta} - \frac{i}{2}\varepsilon^{\mu\nu}M_{\alpha\beta}M_{\mu\nu} + \order(\varepsilon^2)\\
        &= M_{\alpha\beta} + \frac{i}{2}\varepsilon^{\mu\nu}\commutator{M_{\mu\nu}}{M_{\alpha\beta}} + \order(\varepsilon^2).
    \end{align}
    Substituting \(\tensor{\Lambda}{^\mu_\nu} = \tensor{\delta}{^\mu_\nu} + \tensor{\varepsilon}{^\mu_\nu}\) into the right hand side we get
    \begin{align}
        \tensor{\Lambda}{^\mu_\alpha}\tensor{\Lambda}{^\nu_\beta}M_{\mu\nu} &= (\tensor{\delta}{^\mu_\alpha} + \tensor{\varepsilon}{^\mu_\nu}) (\tensor{\delta}{^\nu_\beta} + \tensor{\varepsilon}{^\nu_\beta})M_{\mu\nu}\\
        &= M_{\alpha\beta} + \tensor{\delta}{^\mu_\alpha}\tensor{\varepsilon}{^\nu_\beta}M_{\mu\nu} + \tensor{\delta}{^\nu_\beta}\tensor{\varepsilon}{^\mu_\nu}M_{\mu\nu} + \order(\varepsilon^2)\\
        &= M_{\alpha\beta} + \tensor{\varepsilon}{^\nu_\beta}M_{\alpha\nu} + \tensor{\varepsilon}{^\mu_\nu}M_{\mu\beta} + \order(\varepsilon^2).
    \end{align}
    Now we can use the antisymmetry of \(\varepsilon^{\mu\nu}\) to write
    \begin{equation}
        \tensor{\varepsilon}{^\nu_\beta} = \varepsilon^{\nu\gamma}\minkowskiMetric_{\gamma\beta} = -\varepsilon^{\gamma\nu}\minkowskiMetric_{\gamma\beta} = -\tensor{\varepsilon}{_\gamma^\nu}
    \end{equation}
    and so
    \begin{equation}
        \tensor{\Lambda}{^\mu_\alpha}\tensor{\Lambda}{^\nu_\beta}M_{\mu\nu} = M_{\alpha\beta} + \tensor{\varepsilon}{^\mu_\nu} M_{\mu\beta} - \tensor{\varepsilon}{_\beta^\nu}M_{\alpha\nu}.
    \end{equation}
    We can then identify
    \begin{equation}
        \frac{i}{2}\varepsilon^{\mu\nu}\commutator{M_{\mu\nu}}{M_{\alpha\beta}} = \tensor{\varepsilon}{^\mu_\nu} M_{\mu\beta} - \tensor{\varepsilon}{_\beta^\nu}M_{\alpha\nu}.
    \end{equation}
    The right hand side can be rewritten as
    \begin{equation}
        \frac{1}{2}\varepsilon^{\mu\nu}(M_{\mu\beta} \minkowskiMetric_{\nu\alpha} - M_{\nu\beta}\minkowskiMetric_{\mu\alpha} - M_{\alpha\nu}\minkowskiMetric_{\mu\beta} + M_{\alpha\mu}\minkowskiMetric_{\nu\beta})
    \end{equation}
    which can be shown by expanding this and using the antisymmetry of \(\varepsilon^{\mu\nu}\).
    Note that this result is antisymmetric in \(\mu\) and \(\nu\), as well as \(\alpha\) and \(\beta\).
    This must hold for all \(\varepsilon^{\mu\nu}\) and so
    \begin{equation}
        \commutator{M_{\alpha\beta}}{M_{\mu\nu}} = i(M_{\alpha_\mu} \minkowskiMetric_{\beta\nu} - M_{\alpha\nu} \minkowskiMetric_{\beta\mu} - M_{\beta\mu} \minkowskiMetric_{\alpha\nu} + M_{\beta\nu} \minkowskiMetric_{\alpha\mu}).
    \end{equation}
    Note that we can simply remember the first term here and then the other terms follow by antisymmetrising \(\mu\) and \(\nu\), as well as \(\alpha\) and \(\beta\).
    This Lie bracket defines the \(\specialOrthogonalLie(1, 3)\) Lie algebra of the Lorentz group, \(\specialOrthogonal^+(1, 3)\).
    
    \section{Rotations and Boosts}
    We can write the generators of the Lorentz algebra in a more familiar way by first considering 
    \begin{equation}
        J_i = -\frac{1}{2}\varepsilon_{ijk}M_{jk}.
    \end{equation}
    The commutation relations for \(J_i\) can be computed using those for \(M_{jk}\):
    \begin{align}
        \commutator{J_i}{J_j} &= \frac{1}{4} \varepsilon_{imn}\varepsilon_{jpq} \commutator{M_{mn}}{M_{pq}}\\
        &= \frac{i}{4}\varepsilon_{imn}\varepsilon_{jpq} (M_{mp}\minkowskiMetric_{nq} - M_{mq}\minkowskiMetric_{np} - M_{np}\minkowskiMetric_{mq} + M_{nq}\minkowskiMetric_{mp})\\
        &= -\frac{i}{4}\varepsilon_{imn}\varepsilon_{jpq} (M_{mp}\delta_{nq} - M_{mq}\delta_{np} - M_{np}\delta_{mq} + M_{nq}\delta_{mp})\\
        &= -\frac{1}{4}(\varepsilon_{imq}\varepsilon_{jpq}M_{mp} - \varepsilon_{imp}\varepsilon_{jpq}M_{mq} - \varepsilon_{iqn}\varepsilon_{jpq}M_{np} + \varepsilon_{ipn}\varepsilon_{jpq}M_{nq}) \notag\\
        &= -\frac{1}{4}((\delta_{ij}\delta_{mp} - \delta_{ip}\delta_{mj})M_{mp} + (\delta_{ij}\delta_{mq} - \delta_{mj}\delta_{iq})M_{mq}\\
        &\quad+ (\delta_{ij}\delta_{np} - \delta_{ip}\delta_{nj})M_{np} + (\delta_{ij}\delta_{nq} - \delta_{iq}\delta_{nj})M_{nq})\\
        &= -i(\delta_{ij}\delta_{mp} - \delta_{ip}\delta_{mj})M_{mp}\\
        &= -i(\delta_{ij}M_{mm} - M_{ji})\\
        &= -iM_{ij}\\
        &= i\varepsilon_{ijk}J_k
    \end{align}
    where in the last step we use \(M_{ij} = -\varepsilon_{ijk}J_{k}\), which follows from considering
    \begin{multline}
        -\frac{1}{2}\varepsilon_{ijk}J_k = -\frac{1}{2}\varepsilon_{ijk}\varepsilon_{klm}M_{lm}\\
        = -\frac{1}{2}(\delta_{il}\delta_{jm} - \delta_{im}\delta_{lj})M_{lm} = -\frac{1}{2}M_{ij} + \frac{1}{2}M_{ji} = M_{ij}.
    \end{multline}
    
    So, we find that
    \begin{equation}
        \commutator{J_i}{J_j} = i\varepsilon_{ijk}J_k,
    \end{equation}
    which is exactly the Lie algebra of \(\specialUnitaryLie(2) \isomorphic \specialOrthogonalLie(3)\).
    So, we can interpret \(J_i\) as the generators of rotations.
    
    If we similarly let
    \begin{equation}
        K_i = M_{0i}
    \end{equation}
    then we can interpret these as the generators of boosts.
    Similar calculations give the result that
    \begin{equation}
        \commutator{K_i}{K_j} = \commutator{M_{0i}}{M_{0j}} = iM_{ij} = -i\varepsilon_{ijk}J_k.
    \end{equation}
    We can also compute the commutator of a rotation and boost:
    \begin{multline}
        \commutator{J_i}{K_j} = -\frac{1}{2}\varepsilon_{imn} \commutator{M_{mn}}{M_{0j}}\\
        = -\frac{1}{2}\varepsilon_{imn}i(M_{m0}\minkowskiMetric_{nj} - M_{n0}\minkowskiMetric_{mj}) = i\varepsilon_{ijk}M_{0k} = i\varepsilon_{ijk}K_k.
    \end{multline}
    
    \section{Casimirs}
    We can construct Casimir operators for \(\specialOrthogonalLie(1, 3)\) with products of the generators and the invariant tensors, \(\delta_{ij}\) and \(\varepsilon_{ijk}\).
    There are two linearly independent Casimir operators, meaning \(\specialOrthogonalLie(1, 3)\) has rank 2.
    One possible basis for the Casimir operators is\footnote{Compare this to the products \(F^{\mu\nu}F_{\mu\nu}\) and \(F^{\mu\nu}F^*_{\mu\nu}\) in \course{Classical Electrodynamics}.}
    \begin{equation}
        \frac{1}{2} M_{\mu\nu}M^{\mu\nu} = \vv{J}^2 - \vv{K}^2, \qqand \frac{1}{2}\varepsilon_{\mu\nu\rho\sigma} M^{\mu\nu}M^{\rho\sigma} = -\vv{J} \cdot \vv{K}.
    \end{equation}
    
    
    \chapter{Relation to \texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)}}
    \section{Decoupling the Algebra}
    We've seen that \(\specialOrthogonalLie(1, 3)\) is generated by \(\{J_i, K_i\}\).
    These are, in a sense, coupled, in that Lie brackets of \(K_i\)s give \(J_i\)s.
    We can decouple them by defining two new operators
    \begin{align}
        N_i &\coloneqq \frac{1}{2}(J_i + i K_i),\\
        N_i^\hermit &= \frac{1}{2}(J_i - i K_i).
    \end{align}
    Note that, since \(K_i\) is not Hermitian, \(N_i^\hermit\) is \emph{not} the Hermitian conjugate of \(N_i\).
    These are linearly independent combinations of \(J_i\) and \(K_i\) so also span the Lie algebra \(\specialOrthogonalLie(1, 3)\).
    
    These decoupled generators have the commutation relations
    \begin{equation}
        \commutator{N_i}{N_j} = i\varepsilon_{ijk}N_k, \qquad \commutator{N_i^\hermit}{N_j^\hermit} = i\varepsilon_{ijk}N_k^\hermit, \qqand \commutator{N_i}{N_j^\hermit} = 0.
    \end{equation}
    We can identify the subalgebras generated by \(N_i\) and \(N_i^\hermit\) separately as two copies of \(\specialUnitaryLie(2)\) which don't interact.
    This means that \(\specialOrthogonalLie(1, 3)\) is a direct sum of \(\specialUnitaryLie(2)\) algebras:
    \begin{equation}
        \specialOrthogonalLie(1, 3) \isomorphic \specialUnitaryLie(2) \oplus \specialUnitaryLie(2).
    \end{equation}
    
    While these two copies of \(\specialUnitaryLie(2)\) don't interact they aren't completely independent, since they're related by parity transformations given by taking the Hermitian conjugate, we have \(J_i \mapsto J_i\), \(K_i \mapsto -K_i\), and so \(N_i \mapsto N_i^\hermit\).
    
    \section{\texorpdfstring{\(\specialLinearLie(2, \complex) \isomorphic \specialOrthogonalLie(1, 3)\)}{sl(2, C) isomorphic to so(1, 3)}}
    Consider the Lie algebra \(\specialLinearLie(2, \complex)\).
    This consists of complex \(2 \times 2\) traceless matrices.
    One basis generating this algebra is \(\{\tfrac{1}{2}\sigma_i, -\tfrac{i}{2}\sigma_i\}\), where \(\sigma_i\) are the Pauli matrices.
    Note that we need both \(\sigma_i\) and \(i\sigma_i\) since \(\specialLinearLie(2, \complex)\) is a real algebra, meaning that when we form a linear combination the coefficients are real, so \(i\sigma_i\) gives us different linear combinations.
    
    After some playing around we find that \(\sigma_i/2\) have the same commutation relations as \(J_i\) and \(-i\sigma_i/2\) have the same commutation relations as \(K_i\).
    This means that
    \begin{equation}
        \specialOrthogonalLie(1, 3) \isomorphic \specialLinearLie(2, \complex).
    \end{equation}
    \begin{wrn}
        Strictly, this isn't quite true. Instead \(\specialOrthogonalLie_{\complex}(1, 3) \isomorphic \specialLinearLie_{\complex}(2, \complex)\) where \(V_{\complex} = {V \otimes \complex} = V \oplus iV\) is the complexification of \(V\) given by extending the scalars from \(\reals\) to \(\complex\) and extending scalar multiplication linearly. However, we have embeddings \(\specialOrthogonalLie(1, 3) \hookrightarrow \specialOrthogonalLie_{\complex}(1, 3)\) and \(\specialLinearLie(2, \complex) \hookrightarrow \specialLinearLie_{\complex}(2, \complex)\), and this all works out fine if we only consider finite dimensional representations.
    \end{wrn}
    
    This extends to an isomorphism of groups:
    \begin{equation}
        \specialOrthogonal(1, 3) \isomorphic \specialLinear(2, \complex) / \cyclicGroupZ[2].
    \end{equation}
    This is analogous to \(\specialOrthogonal(n) \isomorphic \orthogonal(n)/\cyclicGroupZ[2]\) in \cref{exm:SO(n) = O(n)/Z2}.
    
    We can construct an explicit map \(\specialOrthogonal(1, 3) \to \specialLinear(2, \complex)\) by considering the four-vectors \(\sigma_\mu = (1, \vv{\sigma})\) and \(\overbar{\sigma}_\mu = (1, -\vv{\sigma})\), then defining
    \begin{equation}
        X = x^\mu \sigma_\mu = 
        \begin{pmatrix}
            x^0 + x^3 & x^1 - ix^2\\
            x^1 + ix^2 & x^0 - x^3
        \end{pmatrix}
        .
    \end{equation}
    Consider the determinant of this matrix:
    \begin{align}
        \det X &= (x^0 + x^3)(x^0 - x^3) - (x^1 - ix^2)(x^1 + ix^2)\\
        &= (x^0)^2 - (x^1)^2 - (x^2)^2 - (x^3)^2\\
        &= x \cdot x.
    \end{align}
    So, \(\det X\) is invariant under Lorentz transformations of \(x\).
    
    We now study the transformation of \(X\) under a Lorentz transformation of \(x\).
    As a \(2 \times 2\) complex matrix \(X\) will transform under a subgroup of \(\generalLinear(2, \complex)\), and transforms as
    \begin{equation}
        X \mapsto A(\Lambda) X A^\hermit(\Lambda)
    \end{equation}
    where \(A(\Lambda)\) is a \(2 \times 2\) complex matrix depending on the Lorentz transformation, \(\Lambda\), applied to \(x\).
    Since \(\det X\) doesn't change we have
    \begin{equation}
        \det(AXA^\hermit) = \det(A)\det(X)\det(A^\hermit) = \det(X).
    \end{equation}
    We can see that this is certainly satisfied if \(A \in \specialLinear(2, \complex)\), and so \(\det A = 1\) and \(\det A^\hermit = (\det A^\trans)^* = (\det A)^* = 1^*\).
    So, we define a map \(A \colon \specialOrthogonal(1, 3) \to \specialLinear(2, \complex)\).
    We can also \enquote{invert} this map by defining
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = \frac{1}{2} \tr(\overbar{\sigma}^\mu A \sigma_\nu A^\hermit).
    \end{equation}
    This has two solutions for \(A\):
    \begin{equation}
        A = \pm \frac{\sigma_\mu \tensor{\Lambda}{^\mu_\nu}\overbar{\sigma}^\nu}{\sqrt{\tensor{\Lambda}{^\mu_\mu}}}.
    \end{equation}
    It is this ambiguous \(\pm\) which is modded out by the \(\cyclicGroupZ[2]\) factor.
    
    \section{Representation of \texorpdfstring{\(\specialUnitaryLie(2)\)}{su(2)}}
    \begin{rmk}
        See \course{Principles of Quantum Mechanics} and \course{Symmetries of Quantum Mechanics} for details.
    \end{rmk}
    The generators of \(\specialUnitaryLie(2)\) are \(T_i\) with \(i = 1, 2, 3\), which satisfy the commutation relation
    \begin{equation}
        \commutator{T_i}{T_j} = i\varepsilon_{ijk}.
    \end{equation}
    We define the ladder operators \(T_{\pm} \coloneqq T_1 \pm i T_2\), as well as the Casimir \(T^2 = T_1^2 + T_2^2 + T_3^2\).
    These are such that
    \begin{equation}
        \commutator{T_3}{T_{\pm}} = \pm T_{\pm}, \qquad \commutator{T_+}{T_-} = 2T_3, \qqand \commutator{T^2}{T_3} = 0.
    \end{equation}
    
    We label the representations of \(\specialUnitaryLie(2)\) by \(j\), which for now is just some number.
    The \(j\) representation has a basis formed of states \(\{\ket{j, m}\}\) where \(j\) labels the representation and \(m\) labels the basis element according to the eigenvalues of \(T_3\):
    \begin{equation}
        T_3 \ket{j, m} = m\ket{j, m}.
    \end{equation}
    we call \(m\) the weight of the state.
    Consider
    \begin{align}
        T_3T_{\pm} \ket{j, m} &= (\commutator{T_3}{T_{\pm}} + T_{\pm}T_3)\ket{j, m}\\
        &= (\pm T_{\pm} + T_{\pm} T_3)\ket{j, m}\\
        &= T_{\pm}(\pm 1 + T_3)\ket{j, m}\\
        &= (m \pm 1) T_{\pm}\ket{j, m}.
    \end{align}
    This tells us that \(T_{\pm}\ket{j, m}\) has eigenvalue \(m \pm 1\) for \(T_3\), so \(T_{\pm}\ket{j, m} \propto \ket{j, m \pm 1}\).
    
    It is possible to write \(T^2\) as \(T^2 = T_- T_+ + T_3^2 + T_3\), which can be shown by expanding \(T_{\pm}\) in terms of \(T_1\) and \(T_2\).
    Take \(m = j\) as the highest allowed value, so that \(T_+\ket{j, j} = 0\).
    This is allowed since we haven't otherwise specified what \(j\) is, so we can simply choose to label each representation by its highest weight.
    Then we have
    \begin{equation}
        T^2\ket{j, j} = (T_-T_+ + T_3^2 + T_3) \ket{j, j} = j(j  + 1)\ket{j, j}.
    \end{equation}
    Considering finite dimensional representations there must also be a lowest weight, call it \(\overbar{m}\), which is such that \(T_- \ket{j, \overbar{m}} = 0\).
    We can also write \(T^2\) as \(T^2 = T_+ T_- + T_3^2 - T_3\), and so we find
    \begin{equation}
        T^2\ket{j, \overbar{m}} = (T_+T_- + T_3^2 - T_3) \ket{j, \overbar{m}} = \overbar{m}(\overbar{m} - 1) \ket{j, \overbar{m}}.
    \end{equation}
    
    Since \(T^2\) is a Casimir operator, and so commutes with all generators of the representation, it must be a multiple of the identity.
    Hence the eigenvalues are all equal and so we have
    \begin{equation}
        j(j + 1) = \overbar{m}(\overbar{m} - 1) \implies \overbar{m} = -j.
    \end{equation}
    Note that we also get the solution \(\overbar{m} = j + 1\), but we neglect this since by definition \(j\) is the highest weight.
    
    So, the \(j\) representation of \(\specialUnitaryLie(2)\) has \(2j + 1\) states, \(\{\ket{j, m}\}\), with \(m \in \{-j, -j + 1, \dotsc, j - 1, j\}\).
    It can be shown that \(j\) takes on integer of half integer values.
    The fundamental representation of \(\specialUnitaryLie(2)\) corresponds to the two dimensional representation with \(j = 1/2\).
    
    \section{Representation of \texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)}}
    the representation theory of \(\specialLinearLie(2, \complex)\) follows almost immediately from
    \begin{equation}
        \specialLinearLie(2, \complex) \isomorphic \specialUnitaryLie(2) \oplus \specialUnitaryLie(2).
    \end{equation}
    Each representation of \(\specialLinearLie(2, \complex)\) is labelled by two numbers, \(\specialUnitaryLie(2)\) representations are labelled by \(j\), so a generic representation of \(\specialLinearLie(2, \complex)\) is labelled \((m, n)\).
    For each copy of \(\specialUnitaryLie(2)\) we get a ladder operator:
    \begin{equation}
        N_{\pm} = N_1 \pm iN_2, \qqand N_{\pm}^\hermit = N_1^\hermit \pm iN_2^\hermit.
    \end{equation}
    We also get two Casimir operators,
    \begin{equation}
        N^2 = N_1^2 + N_2^2 + N_3^2, \qqand {N^\hermit}^2 = {N_1^\hermit}^2 + {N_2^\hermit}^2 + {N_3^\hermit}^2,
    \end{equation}
    These have eigenvalues \(m(m + 1)\) and \(n(n + 1)\) respectively.
    The basis of the \((m, n)\) representation is then given by \(\{\ket{m, m_3} \oplus \ket{n, n_3}\}\), where \(m_3\) and \(n_3\) are the eigenvalues of \(N_3\) and \(N_3^\hermit\) respectively, and \(\ket{m, m_3}\) are basis states in the \(m\) representation of \(\specialUnitaryLie(2)\) and \(\ket{n, n_3}\) are basis states in the \(n\) representation of \(\specialUnitaryLie(2)\).
    
    We classify objects based on which representation of \(\specialLinearLie(2, \complex)\) they transform under.
    We'll see more details later, but for now we just list some classes:
    \begin{itemize}
        \item \define{Scalars}\index{scalar}: Transform under the \((0, 0)\) representation.
        These correspond to spin 0 particles.
        \item \define{Weyl spinors}\index{Weyl spinor}, also known as two-component spinors: Transform under the \((\slashfrac{1}{2}, 0)\) or \((0, \slashfrac{1}{2})\) representations.
        These correspond to spin \(1/2\) particles.
        \item \defineindex{Dirac spinors}\index{Dirac spinor}, also known as four-component spinors: Transform under the \((\slashfrac{1}{2}, 0) \oplus (0, \slashfrac{1}{2})\) representations.
        These correspond to spin \(1/2\) particles.
        \item \define{Vectors}\index{vector}: Transform under the \((\slashfrac{1}{2}, \slashfrac{1}{2})\) representation.
        These correspond to spin one particles.
        \item Antisymmetric rank 2 tensors, such as \(F_{\mu\nu}\): Transform under \((1, 0)\) or \((0, 1)\).
        The adjoint representation of \(\specialLinearLie(2, \complex)\) corresponds to \((1, 0) \oplus (0, 1)\).
        \item Symmetric traceless rank 2 tensors: Transform under the \((1, 1)\) representation.
    \end{itemize}
    
    Suppose that \(D_j \colon \specialUnitaryLie(2) \to \generalLinearLie(V_j)\) is a representations of \(\specialUnitaryLie(2)\) labelled by \(j\), so \(\dim V = 2j + 1\).
    Then we can take two representations, \(D_j\) and \(D_{j'}\), and we can construct a representation of \(\specialLinearLie(2, \complex)\), which we label \((j, j')\), and has the representation space \(V_j \otimes V_{j'}\), and so \(\dim(j, j') = (2j + 1)(2j' + 1)\).
    
    \chapter{Spinors}
    \section{\texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)} Spinors}
    Consider the \((\slashfrac{1}{2}, 0)\) and \((0, \slashfrac{1}{2})\) representations of \(\specialLinearLie(2, \complex)\).
    We call objects transforming under these representations \define{Weyl spinors}, or just spinors for short.
    Since \((2 \cdot \slashfrac{1}{2} + 1)(2 \cdot 0 + 1) = 2\) we have that \(\dim(\slash{1}{2}, 0) = \dim(0, \slashfrac{1}{2}) = 2\).
    So, we define two two-component objects,
    \begin{equation}
        \psi_{\Left} = 
        \begin{pmatrix}
            \psi_{1\Left}\\ \psi_{2\Left}
        \end{pmatrix}
        \in (\slashfrac{1}{2}, 0)
        , \qqand \psi_{\Right} = 
        \begin{pmatrix}
            \psi_{1\Right}\\ \psi_{2\Right}
        \end{pmatrix}
        \in (0, \slashfrac{1}{2}).
    \end{equation}
    Here \(\Left\) and \(\Right\) are just labels, standing for left and right, they don't have any physical significance (for now).
    
    If we want to talk of components of spinors, although this is not as common as working with components of vectors, we do so using indices near the start of the Latin alphabet, such as \(a, b, c, \dotsc\).
    The convention is that we drop the \(\Left\) and \(\Right\) labels and then use undotted/dotted indices respectively, so a left handed spinor may have components \(\psi_a\), whereas a right handed spinor would have components \(\psi_{\dot{a}}\).
    
    Consider an \(\specialLinear(2, \complex)\) transformation.
    This may be represented by \(\Lambda_{\Left}\) on \((\slashfrac{1}{2}, 0)\) and \(\Lambda_{\Right}\) on \((0, \slashfrac{1}{2})\).
    These will be \(2 \times 2\) complex unit determinant matrices.
    Under these representations we expect the left and right handed spinors to transform as
    \begin{align}
        \psi_{\Left} \xmapsto{\Lambda_{\Left}} \psi'_{\Left} = \Lambda_{\Left} \psi_{\Left},\\
        \psi_{\Right} \xmapsto{\Lambda_{\Right}} \psi'_{\Right} = \Lambda_{\Right} \psi_{\Right}.
    \end{align}
    
    We expect that \(\Lambda_{\Left}\) and \(\Lambda_{\Right}\) are given by exponentiating the generators of the Lie algebra, \(\{\sigma_i/2, -i\sigma_i/2\}\).
    First consider rotations, given by \(\specialUnitary(2) \subseteq \specialLinear(2, \complex)\).
    The generators are \(J_i = \sigma_i\), and both the left and right handed spinors transform under
    \begin{equation}
        \Lambda_{\Left, \Right} = \exp\left[ \frac{i}{2} \vv{\sigma} \cdot \vv{\omega} \right]
    \end{equation}
    where \(\vv{\sigma} = (\sigma_1, \sigma_2, \sigma_3)\) and \(\vv{\omega}\) has components \(\omega_i  = \varepsilon_{ijk} \omega^{jk}\), where \(\omega^{jk}\) are the parameters of the Lorentz group parametrising the equivalent \(\specialOrthogonal^+(1, 3)\) transformation.
    Notice that this transformation is unitary since \(\omega_i \in \reals\) and \(\sigma_i\) are Hermitian.
    
    If we include boosts then the left and right spinors transform slightly differently:
    \begin{align}
        \Lambda_{\Left} &= \exp\left[ \frac{i}{2} \vv{\sigma} \cdot (\vv{\omega} - i\vv{\nu}) \right],\\
        \Lambda_{\Right} &= \exp\left[ \frac{i}{2} \vv{\sigma} \cdot (\vv{\omega} + i\vv{\nu}) \right]
    \end{align}
    where \(\vv{\nu}\) has components \(\nu^i = \omega^{0i}\).
    These are \emph{not} unitary, since \(K_i\) is not Hermitian.
    
    The two representations, \((\slashfrac{1}{2}, 0)\) and \((0, \slashfrac{1}{2})\), are related by parity, since under a parity transformation positions are inverted, \(x^i \mapsto -x^i\), so velocities are inverted, \(v^i \mapsto -v^i\), and hence momentum is inverted, \(p^i \mapsto -p^i\).
    Hence \(\vv{x} \times \vv{p} \mapsto \vv{x} \times \vv{p}\), so angular momentum doesn't change.
    This corresponds to the action of parity on \(J_i\) being \(J_i \mapsto J_i\).
    On the other hand boosts are negated, since the direction of the boost is inverted, and so \(K_i \mapsto -K_i\), or equivalently \(\nu_i \mapsto -\nu_i\).
    Hence, under parity
    \begin{equation}
        \psi_{\Left} \leftrightarrow \psi_{\Right}, \qqand \Lambda_{\Left} \leftrightarrow \Lambda_{\Right}.
    \end{equation}
    
    \subsection{Properties of Spinors}
    Notice that
    \begin{equation}
        \Lambda_{\Left}^{-1} = \Lambda_{\Right}^\hermit.
    \end{equation}
    We also have the identities \(\sigma_2\sigma_i\sigma_2 = -\sigma_i^*\) and \(\sigma_2^2 = \ident\), and so
    \begin{align}
        \sigma_2\Lambda_{\Left}\sigma_2 &= \exp\left[ \frac{i}{2}\sigma_2 \sigma_i \sigma_2 (\omega_i + i\nu_i) \right]\\
        &= \exp\left[ -\frac{i}{2}\sigma_i^* (\omega_i + i\nu_i) \right]\\
        &= \exp\left[ \frac{i}{2}\sigma_i(\omega_i - i\nu_i) \right]^*\\
        &= \Lambda_{\Right}^*.
    \end{align}
    Hence,
    \begin{equation}
        \sigma_2 \Lambda_{\Left}^\hermit \sigma_2 = \Lambda_{\Right}^\trans.
    \end{equation}
    
    We also have \(\sigma_i^* = \sigma_i^\trans\), and so \(\sigma_2\sigma_i\sigma_2 = -\sigma_i^\trans\), giving
    \begin{align}
        \sigma_2\Lambda_{\Left}^\trans \sigma_2 &= \sigma_2\exp\left[ \frac{i}{2}\sigma_i^\trans (\omega_i + i\nu_i) \right]\sigma_2\\
        &= \exp\left[ \frac{i}{2}\sigma_2 \sigma_i^\trans \sigma_2(\omega_i + i \nu_i) \right] \\
        &= \exp\left[ -\frac{i}{2}\sigma_i (\omega_i + i\nu_i) \right]\\
        &= \Lambda_{\Left}^{-1}
    \end{align}
    since \(\sigma_i^2 = \ident\).
    Hence, we have
    \begin{equation}
        \Lambda_{\Left}^\trans \sigma_2 \Lambda_{\Left} = \sigma_2,
    \end{equation}
    which follows since if we multiply on the left by \(\sigma_2\) the left hand side gives \(\sigma_2\Lambda_{\Left}^\trans \sigma_2 \Lambda_{\Left} = \Lambda_{\Left}^{-1}\Lambda_{\Left} = \ident\), and the right hand side gives \(\sigma_2^2 = \ident\).
    Similarly, we have
    \begin{equation}
        \Lambda_{\Right}^\trans \sigma_2 \Lambda_{\Right} = \sigma_2.
    \end{equation}
    
    Now consider some left handed spinor, \(\psi_{\Left} \in (\slashfrac{1}{2}, 0)\), transforming under \(\specialLinear(2, \complex)\) according to \(\Lambda_{\Left}\).
    Taking the complex conjugate of the transformation definition we get
    \begin{equation}
        \psi_{\Left}'^* = \Lambda_{\Left}^* \psi_{\Left}^*.
    \end{equation}
    Multiplying on the left by \(\sigma_2\) we have
    \begin{equation}
        \sigma_2 \psi_{\Left}^* = \sigma_2\Lambda_{\Left}^* \psi_{\Left}^* = \Lambda_{\Right}\sigma_2\psi_{\Left}^*.
    \end{equation}
    Hence, we see that \(\sigma_2\psi_{\Left}^*\) transforms as a right handed spinor, and so \emph{is} a right handed spinor, \(\sigma_2\psi_{\Left}^* \in (0, \slashfrac{1}{2})\).
    
    The charge conjugation operation is defined by
    \begin{align}
        \psi_{\Left} \xmapsto{\chargeConjugation} \sigma_2 \psi_{\Left}^*,\\
        \psi_{\Right} \xmapsto{\chargeConjugation} \sigma_2 \psi_{\Right}^*.
    \end{align}
    So, left and right handed spinors are related by charge conjugation.
    
    \section{Direct Product Representations}
    Given two left handed spinors, \(\psi_{\Left}, \chi_{\Left} \in (\slashfrac{1}{2}, 0)\) we should be able to form a scalar, since we have
    \begin{equation}
        (\slashfrac{1}{2}, 0) \otimes (\slashfrac{1}{2}, 0) \isomorphic (0, 0) \oplus (1, 0).
    \end{equation}
    This is analagous to combing two spin \(1/2\) particles to get either a spin 0 system or a spin 1 system, and simultaneously combining two independent spin 0 particles to get a spin 0 system.
    
    A naive first attempt at constructing a scalar may be to consider \(\chi_{\Left}^\trans \psi_{\Left}\), after all this works to combine two vectors and form a scalar.
    However, we see that this doesn't work since under an \(\specialLinear(2, \complex)\) transformation the product above transforms as
    \begin{equation}
        \chi_{\Left}^{\hermit} \psi_{\Left} \mapsto \chi_{\Left}^\trans \Lambda_{\Left}^\trans \Lambda_{\Left} \psi_{\Left}.
    \end{equation}
    Since \(\Lambda_{\Left}^\trans \Lambda_{\Left} \ne \ident\) this doesn't give a scalar.
    
    Instead, consider the product \(\chi_{\Left}^\trans \sigma_2 \psi_{\Left}\).
    This transforms as
    \begin{equation}
        \chi_{\Left}^\trans \sigma_2 \psi_{\Left} \mapsto \chi_{\Left}^\trans \Lambda_{\Left}^{\trans} \sigma_2 \Lambda_{\Left} \psi_{\Left} = \chi_{\Left}^\trans \sigma_2 \psi_{\Left}.
    \end{equation}
    Here we've used one of the identities we derived in the last section, \(\Lambda_{\Left}^{\trans} \sigma_2 \Lambda_{\Left} = \sigma_2\).
    Since \(\chi_{\Left}^\trans \sigma_2 \psi_{\Left}\) transforms trivially it is a scalar, \(\chi_{\Left}^\trans \sigma_2 \psi_{\Left} \in (0, 0)\).
    
    Now choose \(\chi_{\Left} = -\sigma_2\psi_{\Right}^*\), that is \(\psi_{\Right} = \sigma_2\chi_{\Left}\).
    Then \(\psi_{\Right}^\hermit\psi_{\Left}\) is a scalar.
    Similarly, \(\psi_{\Left}^\hermit \psi_{\Right}\) is a scalar.
    
    The fact that we have two different ways of forming a scalar is due to the fact that we have two different copies of \(\specialUnitaryLie(2)\).
    
    Going the other way we also get \((1, 0)\) in the decomposition of \((\slashfrac{1}{2}, 0) \otimes (\slashfrac{1}{2}, 0)\), suggesting we should be able to build a four-vector from two left handed spinors.
    This corresponds to the important fact that \(2 \times 2 = 4\), so we can make four-component objects by taking tensor products of two-component objects.
    
    We start by considering \(\psi_{\Left}^\hermit \psi_{\Left}\).
    Under a Lorentz transformation this transforms as
    \begin{equation}
        \psi_{\Left}^\hermit \psi_{\Left} \mapsto \psi_{\Left}^\hermit \Lambda_{\Left}^\hermit \Lambda_{\Left} \psi_{\Left}.
    \end{equation}
    Now, suppose that \(\Lambda_{\Left}\) is infinitesimal.
    Then we can expand it to first order in the parameters giving
    \begin{equation}
        \Lambda_{\Left} = \exp\left[ \frac{i}{2} \vv{\sigma} \cdot (\vv{\omega} - i\vv{\nu}) \right] \approx 1 + \frac{i}{2}\vv{\sigma} \cdot (\vv{\omega} - i \vv{\nu}) + \order(\omega^2, \nu^2, \omega\nu).
    \end{equation}
    Similarly,
    \begin{equation}
        \Lambda_{\Left}^\hermit = \exp\left[ -\frac{i}{2}\vv{\sigma} \cdot (\vv{\omega} + i \vv{\nu}) \right] \approx 1 - \frac{i}{2} \vv{\sigma} \cdot (\vv{\omega} + i \vv{\nu}) + \order(\omega^2, \nu^2, \omega\nu).
    \end{equation}
    We can then expand the transformed spinor product to first order:
    \begin{align}
        \psi_{\Left}^\hermit\Lambda_{\Left}^\hermit \Lambda_{\Left}\psi_{\Left} &= \psi_{\Left}^\hermit \left[ 1 - \frac{i}{2} \sigma_i(\omega_i + i\nu_i) \right] \left[ 1 + \frac{i}{2} \sigma_j(\omega_j - i\nu_j) \right] \psi_{\Left}\\
        &= \psi_{\Left}^\hermit \left[ 1 - \frac{i}{2} \sigma_i (\omega_i + i\nu_i) + \frac{i}{2} \sigma_j (\omega_j - i\nu_j) \right] \psi_{\Left}\\
        &= \psi_{\Left}^\hermit \left[ 1 - \frac{1}{2}\left( i\omega_i\sigma_i - i\omega_i\sigma_j + \nu_i\sigma_i + \nu_i\sigma_i \right) \right] \psi_{\Left}\\
        &= \psi_{\Left}^\hermit [ 1 + \nu_i\sigma_i ] \psi_{\Left}\\
        &= \psi_{\Left}^\hermit \psi_{\Left} + \nu_i \psi_{\Left}^\hermit \sigma_i \psi_{\Left}\\
        &= \psi_{\Left}^\hermit \psi_{\Left} - \omega_{0i}\psi_{\Left}^\hermit\sigma_i\psi_{\Left}\\
        &= \psi_{\Left}^\hermit - \omega_{0j}\psi_{\Left}^\hermit\sigma_j\psi_{\Left} + \omega_{00}\psi_{\Left}^\hermit\psi_{\Left}
    \end{align}
    where we've used \(\nu_i = \omega_{i0} = -\omega_{0i}\) and \(\omega_{00} = 0\).
    
    Now, this isn't how a four-vector transforms, so we try the same thing as before, inserting a Pauli matrix in between the two terms.
    Our current result doesn't have any free indices, which we'll need to make a four vector, so we put a free index on the Pauli matrix:
    \begin{align}
        \psi_{\Left}^\hermit \sigma_i \psi_{\Left} &\mapsto \psi_{\Left}^\hermit\Lambda_{\Left}^\hermit \sigma_i \Lambda_{\Left}\psi_{\Left}\\
        &= \psi_{\Left}^\hermit \left[ 1 - \frac{i}{2} \sigma_j(\omega_j + i\nu_j) \right] \sigma_i \left[ 1 + \frac{i}{2} \sigma_k(\omega_k - i\nu_k) \right] \psi_{\Left}\\
        &= \psi_{\Left}^\hermit \left[ \sigma_i - \frac{i}{2}\sigma_j\sigma_i\omega_j + \frac{i}{2}\sigma_i\sigma_k\omega_k + \frac{1}{2}\sigma_j\sigma_i\nu_j + \frac{1}{2}\sigma_i\sigma_k\nu_k \right]\psi_{\Left}\\
        &= \psi_{\Left}^\hermit \left[ \sigma_i - \frac{i}{2}\commutator{\sigma_i}{\sigma_j}\omega_j + \frac{1}{2}\anticommutator{\sigma_i}{\sigma_j} \nu_j \right] \psi_{\Left}.
    \end{align}
    Now we use the identities
    \begin{equation}
        \commutator{\sigma_i}{\sigma_j} = i\varepsilon_{ijk}, \qqand \anticommutator{\sigma_i}{\sigma_j} = 2\delta_{ij}
    \end{equation}
    as well as inverting the relationship \(\omega_i = -\varepsilon_{ijk}\omega_{jk}/2\) to give \(\omega_{ij} = -\varepsilon_{ijk}\omega_k\), and so
    \begin{equation}
        \frac{i}{2}\commutator{\sigma_i}{\sigma_j}\omega_j = \frac{i}{2}i\varepsilon_{ijk}\sigma_k \omega_j = -\frac{1}{2}\varepsilon_{kij}\sigma_k \omega_j = \omega_{ki} \sigma_k.
    \end{equation}
    Hence,
    \begin{align}
        \psi_{\Left}^\hermit \sigma_i \psi_{\Left} &\mapsto \psi_{\Left}^\hermit \left[ \sigma_i - \omega_{ki} \sigma_k + \delta_{ij}\nu_j \right] \psi_{\Left}\\
        &= \delta_{ij}\psi_{\Left}^\hermit \sigma_j\psi_{\Left} - \omega_{ij} \psi_{\Left}^\hermit \sigma_j \psi_{\Left} + \omega_{i0}\psi_{\Left}^\hermit \psi_{\Left}.
    \end{align}
    Define \(\sigma_\mu \coloneqq (1, \sigma_i)\).
    Then we can combine these two results into
    \begin{equation}
        \psi_{\Left}^\hermit \sigma_\mu \psi_{\Left} \mapsto (\tensor{\delta}{_\mu^\nu} + \tensor{\omega}{_\mu^\nu})\psi_{\Left}^\hermit \sigma_\nu \psi_{\Left},
    \end{equation}
    which is exactly how a four-vector transforms under an infinitesimal Lorentz transformation, so we conclude that \(\psi_{\Left}^\hermit \sigma_\mu \psi_{\Left}\) \emph{is} a four-vector.
    
    If we similarly define \(\overbar{\sigma}_\mu = (1, -\sigma_i)\) then we find that \(\psi_{\Right}^\hermit \overbar{\sigma}_\mu \psi_{\Right}\) is also a four-vector.
    
    \section{Dirac Spinors}
    \begin{rmk}
        See the \course{Quantum Field Theory} course for \emph{a lot} more on spinors and the gamma matrices.
        See \course{Particle Physics} for a brief, less technical introduction.
    \end{rmk}
    We've seen that the two-component spinors, \(\psi_{\Left} \in (\slashfrac{1}{2}, 0)\) and \(\psi_{\Right} \in (0, \slashfrac{1}{2})\), transform into each other under parity.
    Therefore if we want parity invariance then we need to find a way to include left and right handed spinors equally.
    The solution is to define a \defineindex{Dirac spinor},
    \begin{equation}
        \psi \coloneqq \psi_{\Left} \oplus \psi_{\Right} = 
        \begin{pmatrix}
            \psi_{\Left} \\ \psi_{\Right}
        \end{pmatrix}
        .
    \end{equation}
    This transforms under the reducible \((\slashfrac{1}{2}, 0) \oplus (0, \slashfrac{1}{2})\) representation of \(\specialUnitary(2, \complex)\), according to
    \begin{equation}
        \psi \mapsto 
        \begin{pmatrix}
            \Lambda_{\Left} & 0\\
            0 & \Lambda_{\Right}
        \end{pmatrix}
        = \Lambda \psi.
    \end{equation}
    We can see that \(\psi\) has four components, two from each of \(\psi_{\Left}\) and \(\psi_{\Right}\).
    This is due to the important fact that \(2 + 2 = 4\).
    This collision between \(2 \times 2 = 4\) and \(2 + 2 = 4\) is why spinors and four-vectors have the same number of components, despite being quite different objects.
    
    Under parity a Dirac spinor transforms as
    \begin{equation}
        \psi = 
        \begin{pmatrix}
            \psi_{\Left}\\ \psi_{\Right}
        \end{pmatrix}
        \mapsto 
        \begin{pmatrix}
            \psi_{\Right}\\ \psi_{\Left}
        \end{pmatrix}
        =
        \begin{pmatrix}
            0 & \ident\\
            \ident & 0
        \end{pmatrix}
        \psi = \gamma^0 \psi
    \end{equation}
    where we identify the \(4 \times 4\) matrix \(\gamma0^5\) as representing the parity operator on Dirac spinors.
    
    We can project \(\psi\) onto the left and right handed spinor subspaces using the projection operators
    \begin{equation}
        P_{\Left} = \frac{1}{2}(1 + \gamma^5), \qqand P_{\Right} = \frac{1}{2}(1 - \gamma^5),
    \end{equation}
    where \(\gamma^5\) is the \(4 \times 4\) matrix
    \begin{equation}
        \gamma^5 \coloneqq 
        \begin{pmatrix}
            \ident & 0\\
            0 & -\ident
        \end{pmatrix}
        .
    \end{equation}
    These are idempotent projectors, meaning \(P_{\Left, \Right}^2 = P_{\Left, \Right}\), and they are orthogonal, so \(P_{\Left} P_{\Right} = P_{\Right} P_{\Left} = 0\).
    \begin{wrn}
        There are varying conventions for the exact definition of these projectors, usually differing by a sign.
    \end{wrn}
    
    We can use Dirac spinors to construct two scalars under \(\specialLinear(2, \complex)\).
    First, we have
    \begin{equation}
        \psi_{\Left}^\hermit \psi_{\Right} + \psi_{\Right}^\hermit \psi_{\Left} = \psi^\hermit \gamma^0 \psi = \diracadjoint{\psi}\psi
    \end{equation}
    where \(\diracadjoint{\psi} \coloneqq \psi\gamma^0\) is the \defineindex{Dirac adjoint}.
    This is a symmetric combination of the two scalars we can form from left and right handed two-component spinors.
    Under parity it transforms into itself, we say it is even under parity, or a proper scalar.
    
    We can also take an antisymmetric combination of the two scalars:
    \begin{equation}
        \psi_{\Left}^\hermit\psi_{\Right} - \psi_{\Right}^\hermit\psi_{\Left} = -\psi\hermit \gamma^0\gamma^5\psi = -\diracadjoint{\psi}\gamma^5\psi.
    \end{equation}
    This transforms into its negative under parity.
    We say it is odd under parity or a pseudoscalar.
    
    Similarly, we can define two four-vectors:
    \begin{align}
        \psi_{\Left}^\hermit \sigma_{\mu} \psi_{\Left} + \psi_{\Right}^\hermit \overbar{\sigma}_\mu \psi_{\Right} = \diracadjoint{\psi}\gamma_\mu \psi,\\
        \psi_{\Left}^\hermit \sigma_{\mu} \psi_{\Left} - \psi_{\Right}^\hermit \overbar{\sigma}_\mu \psi_{\Right} = \diracadjoint{\psi}\gamma_\mu\gamma^5\psi.
    \end{align}
    The first of these is odd under parity, which is what we expect for vectors, and the second is even under parity, and we call it a \defineindex{pseudovector}, or an \define{axial vector}\index{axial vector!see{pseudovector}}.
    Here we've define the \defineindex{Dirac matrices}, or \define{gamma matrices}\index{gamma matrices!see{Dirac matrices}} in the Weyl representation
    \begin{equation}
        \gamma_\mu = 
        \begin{pmatrix}
            0 & \overbar{\sigma}_\mu\\
            \sigma_\mu & 0
        \end{pmatrix}
        , \qquad \gamma_0 = 
        \begin{pmatrix}
            0 & \ident\\
            \ident & 0
        \end{pmatrix}
        , \qqand \gamma_i = 
        \begin{pmatrix}
            0 & -\sigma_i\\
            \sigma_i & 0
        \end{pmatrix}
        .
    \end{equation}
    These satisfy the \defineindex{Clifford algebra}
    \begin{equation}
        \anticommutator{\gamma_\mu}{\gamma_\nu} = 2\minkowskiMetric_{\mu\nu}.
    \end{equation}
    The fifth gamma matrix, \(\gamma^5\), is related to \(\gamma^\mu\) by
    \begin{equation}
        \gamma^5 = i\gamma_0\gamma_1\gamma_2\gamma_3, \qqand\anticommutator{\gamma^5}{\gamma_\mu} = 0.
    \end{equation}
    We also have the identity
    \begin{equation}
        \gamma_\mu^\hermit = \gamma_0\gamma_\mu\gamma_0.
    \end{equation}

    \chapter{Poincar\'e Group}
    \section{Definition}
    \begin{dfn}{Poincar\'e Group}{}
        The Poincar\'e group is defined as the group of all Lorentz transformations and translations of spacetime.
    \end{dfn}
    
    The Poincar\'e group is also called the inhomogeneous Lorentz group.
    Like the Lorentz group strictly we have nonorthochronous and nonproper Lorentz transformations, but we don't usually consider these and simply say \enquote{Poincar\'e group} for the group of proper orthochronous Lorentz transformations and translations.
    
    The Poincar\'e group is the group of all isometries of spacetime, so it is sometimes denoted \(\ISO(1, 3)\).
    
    A general Poincar\'e transformation is given by a pair \((\Lambda, a)\), with \(\Lambda \in \specialOrthogonal^+(1, 3)\) and \(a \in \minkowskiSpace\).
    It acts on some \(x \in \minkowskiSpace\) via
    \begin{equation}
        x^\mu \mapsto x'^\mu = \tensor{\Lambda}{^\mu_\nu} x^\nu + a^\mu.
    \end{equation}
    
    Now consider two Poincar\'e transformations, \((\Lambda, a)\) and \((\tilde{\Lambda}, \tilde{a})\).
    Act on \(x \in \minkowskiSpace\) with these according to
    \begin{equation}
        x^\mu \xmapsto{(\Lambda, a)} x'^\mu \xmapsto{(\tilde{\Lambda}, \tilde{a})} x''^\mu,
    \end{equation}
    so,
    \begin{align}
        x''^\mu &= \tensor{\tilde{\Lambda}}{^\mu_\nu}x'^\nu + \tilde{a}^\mu\\
        &= \tensor{\tilde{\Lambda}}{^\mu_\nu} (\tensor{\Lambda}{^\nu_\rho} x^\rho + a^\nu) + \tilde{a}^\mu\\
        &= \tensor{\tilde{\Lambda}}{^\mu_\nu} \tensor{\Lambda}{^\nu_\rho} x^\rho + \tensor{\tilde{\Lambda}}{^\mu_\nu}a^\nu + \tilde{a}^\mu.
    \end{align}
    We can recognise this as a single Poincar\'e transformation \((\tilde{\Lambda}\Lambda, \tilde{\Lambda}a + \tilde{a})\).
    We can identify this way of combining two transformations as a semidirect product.
    In particular, we have \(\ISO(1, 3) = \minkowskiSpace \rtimes \orthogonal(1, 3)\), or more often we consider \(\minkowskiSpace \rtimes \specialOrthogonal^+(1, 3)\).
    
    We can then represent the product of some generic Poincar\'e transformations, \((\Lambda, a), (\tilde{\Lambda}, \tilde{a}) \in \ISO(1, 3)\) as
    \begin{equation}
        (\tilde{\Lambda}, \tilde{a})(\Lambda, a) = (\tilde{\Lambda}\Lambda, \tilde{\Lambda} a + \tilde{a}).
    \end{equation}
    Note that \((\ident, a)\) is a pure translation and \((\Lambda, 0)\) is a pure Lorentz transformation.
    This allows us to factorise a Poincar\'e transformation as
    \begin{equation}
        (\Lambda, a) = (\Lambda, 0)(\ident, a) = (\ident, a)(\Lambda, 0).
    \end{equation}
    Note that \((\ident, 0)\) is the identity for \(\ISO(1, 3)\).
    
    \section{Poincar\'e Algebra}
    The generators of Lorentz transformations generate pure Lorentz transformations:
    \begin{equation}
        (\Lambda, 0) = \exp\left[ \frac{i}{2}\omega^{\mu\nu}M_{\mu\nu} \right] \approx 1 + \frac{i}{2}\omega^{\mu\nu}M_{\mu\nu}.
    \end{equation}
    The generators of translations are \(P_\mu\):
    \begin{equation}
        (\ident, a) = \exp[ia_\mu P^\mu] \approx 1 + ia_\mu P^\mu.
    \end{equation}
    As the notation suggests we will soon identify \(P^\mu\) with the momentum.
    That momentum generates translations should be of no surprise from quantum mechanics and Noether's theorem.
    Although note that we don't give an explicit form for \(P^\mu\), whereas in quantum mechanics we would identify it with \(i\partial^\mu\) and proceed from here.
    This is simply a choice of representation, and one we choose not to make here.
    
    To work out the commutation relations we consider restricted cases.
    First, we already know the commutation relations for \(M_{\mu\nu}\):
    \begin{equation}
        \commutator{M_{\alpha\beta}}{M_{\mu\nu}} = i(M_{\alpha_\mu} \minkowskiMetric_{\beta\nu} - M_{\alpha\nu} \minkowskiMetric_{\beta\mu} - M_{\beta\mu} \minkowskiMetric_{\alpha\nu} + M_{\beta\nu} \minkowskiMetric_{\alpha\mu}).
    \end{equation}

    Next, set \(\Lambda = \tilde{\Lambda} = \ident\), and we have
    \begin{equation}
        (\ident, \tilde{a})(\ident, a) = (\ident, a + \tilde{a}) = (\ident, \tilde{a} + a) = (\ident, a)(\ident, \tilde{a}).
    \end{equation}
    So, unsurprisingly, translations commute.
    This means that we must have
    \begin{equation}
        \commutator{P_\mu}{P_\nu} = 0.
    \end{equation}
    This should also be familiar from quantum mechanics.
    
    Now set \(\tilde{a} = 0\) and take \(a\) to be infinitesimal and \(\tilde{\Lambda} = \Lambda^{-1}\).
    Then we have
    \begin{equation}
        (\Lambda^{-1}, 0)(\Lambda, a) = (\Lambda^{-1}\Lambda, \Lambda^{-1}a + 0) = (\ident, \Lambda^{-1}a).
    \end{equation}
    Expanding this the right hand side is
    \begin{equation}
        (\ident, \Lambda^{-1}a) = \exp[i(\Lambda^{-1}a)_\mu P^\mu] = \exp[i\tensor{\Lambda}{_\nu^\mu}a_\mu P^\nu] \approx 1 + i\tensor{\Lambda}{_\nu^\mu} a_\mu P^\nu.
    \end{equation}
    We can write \((\Lambda^{-1}, 0)\) as \((\Lambda, 0)^{-1}\), and then we find that on the left hand side we have
    \begin{align}
        (\Lambda^{-1}, 0)(\Lambda, a) &= (\Lambda, 0)^{-1}(\ident, a)(\Lambda, 0)\\
        &\approx (\Lambda, 0)^{-1}(1 + ia_\mu P^\mu)(\Lambda, 0)\\
        &= (\Lambda, 0)^{-1}(\Lambda, 0) + ia_\mu (\Lambda, 0)^{-1}P^\mu(\Lambda, 0).
    \end{align}
    We can then read off from these two relations that
    \begin{equation}
        (\Lambda, 0)^{-1}P^\mu(\Lambda, 0) = \tensor{\Lambda}{^\mu_\nu}P^\nu.
    \end{equation}
    That is, under a Lorentz transformation, \(\Lambda \in \specialOrthogonal^+(1, 3)\), corresponding to a Poincar\'e transformation, \(S \in \ISO(1, 3)\), the \(P^\mu\) operator transforms as \(S^{-1}P^\mu S = \tensor{\Lambda}{^\mu_\nu}P^\nu\), and so the operator \(P^\mu\) is a four-vector.
    
    Now suppose that \(\Lambda\) is infinitesimal.
    Then the left hand side is
    \begin{equation}
        (\Lambda^{-1}, 0)(\ident, a)(\Lambda, 0) = \exp\left[ -\frac{i}{2}\omega^{\mu\nu}M_{\mu\nu} \right]\exp[ia_\rho P^\rho] \exp\left[ \frac{i}{2}\omega^{\alpha\beta}M_{\alpha\beta} \right].
    \end{equation}
    Expanding this we have
    \begin{equation}
        \left( 1 - \frac{i}{2} \omega^{\mu\nu}M_{\mu\nu} \right)(1 + ia_\rho P^\rho)\left( 1 + \frac{i}{2} \omega^{\alpha\beta} M_{\alpha\beta} \right).
    \end{equation}
    Keeping terms to second order we have
    \begin{multline}
        1 - \frac{i}{2}\omega^{\mu\nu}M_{\mu\nu} + ia_\rho P^\rho + \frac{i}{2}\omega^{\alpha\beta}M_{\alpha\beta} + \frac{1}{2}\omega^{\mu\nu}M_{\mu\nu} a_\rho P^\rho\\
        - a_\rho P^\rho \frac{1}{2}\omega^{\alpha\beta}M_{\alpha\beta} - \frac{1}{4} \omega^{\mu\nu}M_{\mu\nu}\omega^{\alpha\beta}M_{\alpha\beta}.
    \end{multline}
    Comparing this to the expanded right hand side and we can consider just the terms with a single factor of \(P\), since the generators are linearly independent.
    So, we have
    \begin{equation}
        i\tensor{\Lambda}{_\nu^\mu} a_\mu P^\nu = ia_\rho P^\rho + \frac{1}{2} a^\rho\omega^{\mu\nu} M_{\mu\nu} P^\rho - \frac{1}{2} a^\rho \omega^{\mu\nu} P^\rho M_{\mu\nu}
    \end{equation}
    where we've relabelled \(\alpha, \beta \to \mu, \nu\) in the last term.
    In order for \(P^\mu\) to transform as a four-vector, which we know it does, the second and third terms must vanish, but the second and third terms are just
    \begin{equation}
        \frac{1}{2}a^\rho \omega^{\mu\nu} \commutator{M_{\mu\nu}}{P_\rho}.
    \end{equation}
    One can check that this vanishes if
    \begin{equation}
        \commutator{M_{\mu\nu}}{P_\rho} = -i(P_\mu \minkowskiMetric_{\nu\rho} - P_\nu \minkowskiMetric_{\mu\rho}).
    \end{equation}
    The factor of \(-i\) is so that if we expand
    \begin{equation}
        (1, \Lambda^{-1}a) \approx 1 + i\tensor{\Lambda}{_\nu^\mu}a_\mu P^\nu - \frac{1}{2}(\tensor{\Lambda}{_\nu^\mu}a_\mu P^\nu)^2
    \end{equation}
    and compare to second order these two expressions agree.
    
    To summarise,
    \begin{align}
        \commutator{P_\mu}{P_\nu} &= 0,\\
        \commutator{M_{\alpha\beta}}{M_{\mu\nu}} &= i(M_{\alpha_\mu} \minkowskiMetric_{\beta\nu} - M_{\alpha\nu} \minkowskiMetric_{\beta\mu} - M_{\beta\mu} \minkowskiMetric_{\alpha\nu} + M_{\beta\nu} \minkowskiMetric_{\alpha\mu}),\\
        \commutator{M_{\mu\nu}}{P_\rho} &= -i(P_\mu \minkowskiMetric_{\nu\rho} - P_{\nu}\minkowskiMetric_{\mu\rho}).
    \end{align}
    This last part holds for any operator valued four-vector, \(V_\mu\):
    \begin{equation}
        \commutator{M_{\mu\nu}}{V_\rho} = -i(V_\mu \minkowskiMetric_{\nu\rho} - V_{\nu}\minkowskiMetric_{\mu\rho}).
    \end{equation}
    This is just an infinitesimal rewriting of the transformation law for four-vectors.
    
    We can write these relations in terms of \(J_i\) and \(K_i\) as well as \(P_\mu = (H, P_i)\), where, as the notation suggests, we'll identify \(H\) with the energy.
    Then
    \begin{align}
        \commutator{J_i}{P_i} &= i\varepsilon_{ijk}P_k,\\
        \commutator{J_i}{H} &= 0,\\
        \commutator{K_i}{P_j} &= iH\delta_{ij},\\
        \commutator{K_i}{H} &= -iP_i.
    \end{align}
    This form makes it slightly easier to interpret these relations.
    Identifying \(J_i\) with (infinitesimal) rotations, \(K_i\) with (infinitesimal) boosts, \(P_i\) with the three momentum, and \(H\) with the energy the first relation tells us that an infinitesimal rotation rotates the momentum infinitesimally.
    The second tells us that rotations don't change the energy of the system.
    The third tells us that a boost increases the momentum in the direction of the boost.
    The fourth tells us that a boost in the direction of the momentum decreases the energy slightly, this makes sense since, for example, increasing the speed of a frame to match that of a particle reduces that particle's kinetic energy to zero.
    
    These commutation relations define the \defineindex{Poincar\'e algebra}.
    This is a 10 dimensional algebra, with 6 dimensions corresponding to Lorentz transformations and four to translations.
    Notice that the Lorentz group is a proper subgroup of the Poincar\'e group, and so the Poincar\'e group is not compact, since the Lorentz group isn't.
    The subalgebra generated by just the \(P^\mu\) is an Abelian subalgebra, \(\commutator{P^\mu}{P^\nu} = 0\), and it is also invariant, since \(\commutator{M_{\mu\nu}}{P_\rho}\) is a linear combination of \(P_\rho\).
    This means that the Poincar\'e algebra is not semisimple, and hence is also not simple.
    
    \section{Casimir Operators}
    The Poincar\'e algebra has two Casimir operators.
    The first is quite easy to guess, its \(P^2 = P_\mu P^\mu\).
    One can easily check that
    \begin{equation}
        \commutator{P^2}{P_\mu} = \commutator{P^2}{M_{\mu\nu}} = 0
    \end{equation}
    using the identity
    \begin{equation}
        \commutator{AB}{C} = A\commutator{B}{C} + \commutator{A}{C}B.
    \end{equation}
    So we have
    \begin{equation}
        \commutator{P^2}{P_\mu} = P^\nu\commutator{P_\nu}{P_\mu} + P_\nu\commutator{P^\nu}{P_\mu} = 0.
    \end{equation}
    Here we use the ability to raise and lower pairs of indices without changing anything to lower the \(\nu\) inside the commutator while raising the \(\nu\) outside so we get \(\commutator{P_\nu}{P_\mu} = 0\).
    Hence,
    \begin{align}
        \commutator{P^2}{M_{\mu\nu}} &= P^\mu\commutator{P_\mu}{M_{\nu\rho}} + \commutator{P_\mu}{M_{\nu\rho}}P^\mu\\
        &= -P^\mu\commutator{M_{\nu\rho}}{P_\mu} - \commutator{M_{\nu\rho}}{P_\mu}P^\mu\\
        &= -P^\mu\commutator{M_{\nu\rho}}{P_\mu} + \commutator{M_{\rho\nu}}{P_\mu}P^\mu\\
        &= iP^\mu(P_\nu \minkowskiMetric_{\rho\mu} - P_\rho \minkowskiMetric_{\nu\mu}) - i(P_\nu \minkowskiMetric_{\rho\mu} - P_\rho \minkowskiMetric_{\nu\mu})P^\mu\\
        &= 0
    \end{align}
    where we've used the antisymmetry of \(M_{\mu\nu}\).
    
    The Lorentz algebra Casimirs, \(M^{\mu\nu}M_{\mu\nu}\) and \(\varepsilon_{\mu\nu\rho\sigma}M^{\mu\nu}M^{\rho\sigma}\), are not Casimirs of the Poincar\'e algebra, since the don't commute with \(P_\mu\).
    In order to form a Casimir we need to combine both \(M_{\mu\nu}\) and \(P_\mu\).
    Eventually one might come across the combination given by defining
    \begin{equation}
        W_\mu \coloneqq \frac{1}{2}\varepsilon_{\mu\nu\rho\sigma}M^{\nu\rho}P^\sigma,
    \end{equation}
    which we call the \defineindex{Pauli--Lubanski vector}.
    This is not itself a Casimir, since these must be scalars as by definition they commute with Lorentz transformations.
    However, we can form a scalar,
    \begin{equation}
        W^2 = W^\mu W_\mu,
    \end{equation}
    and this \emph{is} a Casimir.
    
    First, we should show that \(W^2\) is linearly independent to \(P^2\), then we'll show that it commtues with all generators.
    Consider
    \begin{equation}
        W_\mu P^\mu = \frac{1}{2}\varepsilon_{\mu\nu\rho\sigma} M^{\nu\rho} P^\sigma P^\mu = 0.
    \end{equation}
    This vanishes since \(\varepsilon_{\mu\nu\rho\sigma}\) is antisymmetric in \(\sigma\) and \(\mu\), whereas \(P^\sigma P^\mu\) is symmetric in \(\sigma\) and \(\mu\).
    We also have
    \begin{align}
        \commutator{W^\mu}{P^\alpha} &= \frac{1}{2}\varepsilon_{\mu\nu\rho\sigma}\commutator{M^{\nu\rho}P^\sigma}{P^\alpha}\\
        &= \frac{1}{2}\varepsilon_{\mu\nu\rho\sigma}\{M^{\nu\rho}\commutator{P^\sigma}{P^\alpha} + \commutator{M^{\mu\nu}}{P^\alpha}P^\sigma\}\\
        &= \frac{1}{2}\varepsilon_{\mu\nu\rho\sigma}\{0 - i(P_\nu \minkowskiMetric_{\rho\alpha} - P_\rho \minkowskiMetric_{\nu\alpha})P^\sigma\}
        = 0.
    \end{align}
    This vanishes since both terms in the curly brackets are symmetric in at least one pair of indices, and \(\varepsilon_{\mu\nu\rho\sigma}\) is antisymmetric in any pair of indices.
    
    Since \(W_\mu\) is a four-vector, which we know because it's formed as a product of tensors, \(\varepsilon_{\mu\nu\rho\sigma}\), \(M^{\mu\nu}\), and \(P^\sigma\), and it has a single index, we know that
    \begin{equation}
        \commutator{M_{\mu\nu}}{W_\rho} = -i(W_\mu \minkowskiMetric_{\nu\rho} - W_\nu \minkowskiMetric_{\mu\rho}).
    \end{equation}
    We then have
    \begin{equation}
        \commutator{W^2}{P_\mu} = W^\nu\commutator{W_\nu}{P_\mu} + \commutator{W_\nu}{P_\mu}W^\nu = 0
    \end{equation}
    and the proof that \(W^2\) commutes with \(M_{\mu\nu}\) is identical to the proof for \(P^2\) commuting with \(M_{\mu\nu}\), so
    \begin{equation}
        \commutator{W^2}{M_{\mu\nu}} = 0.
    \end{equation}
    Hence, \(W^2\) is a Casimir operator independent to \(P^2\).
    
    \section{Classifying Irreducible Representations}
    As with \(\specialUnitary(2)\), where we label irreducible representations by \(j\), with \(J^2 \ket{j, m} = j(j + 1)\ket{j, m}\), we label irreducible representations of the Poincar\'e group by the eigenvalues of the Casimir operators \(P^2\) and \(W^2\).
    Denote the eigenvalues of \(P_\mu\) by \(p_\mu = (p_0, -\vv{p})\), and the eigenvalue of \(P^2\) by \(p^2\).
    
    First, note that \(p_\mu\) are continuous.
    This follows since we can rescale \(P_\mu \mapsto \alpha P_\mu\) for any real \(\alpha \ne 0\), and none of the commutation relations defining the Poincar\'e algebra are changed:
    \begin{gather}
        \commutator{P_\mu}{P_\nu} = 0 \mapsto \commutator{\alpha P_\mu}{\alpha P_\nu} = \alpha^2\commutator{P_\mu}{P_\nu} = 0,\\
        \commutator{M_{\mu\nu}}{P_\rho} = -i(P_\mu \minkowskiMetric_{\nu\rho} - P_{\nu}\minkowskiMetric_{\mu\rho}) \mapsto \commutator{M_{\mu\nu}}{\alpha P_\rho} = -i(\alpha P_\mu \minkowskiMetric_{\nu\rho} - \alpha P_{\nu}\minkowskiMetric_{\mu\rho}) \notag\\
        \implies \alpha \commutator{M_{\mu\nu}}{P_\rho} = -i\alpha(P_\mu \minkowskiMetric_{\nu\rho} - P_{\nu}\minkowskiMetric_{\mu\rho}).
    \end{gather}
    The other commutation relation doesn't involve \(P_\mu\).
    This means that if \(P_\mu\) is a valid generator with eigenvalue \(p_\mu\) then so is \(\alpha P_\mu\) and it has eigenvalue \(\alpha p_\mu\).
    Since \(\alpha\) can take any nonzero real value we conclude that the space of possible eigenvalues is \(\reals\setminus\{0\}\), and hence the eigenvalues are continuous.
    This of course implies that \(P^2\) has continuous eigenvalues.
    
    We now consider all possible ways of labelling irreducible representations of the Poincar\'e group by the values of \(p^2\) and \(p_\mu\), as well as the value of \(W^2\).
    Since we are free to rescale the eigenvalues by a positive scale factor, essentially by changing the units we use, the only thing that matters is the sign of the eigenvalues.
    
    \subsection{\texorpdfstring{\(p^2 = 0\) and \(p_\mu = 0\)}{p squared and p mu Zero}}
    In this case \(P_\mu = 0\) and so \(W_\mu = 0\), and hence \(W^2 = 0\).
    This corresponds to the trivial representation, which acts as the identity.
    In terms of physics this representation corresponds to the vacuum, or ground state.
    Essentially the Lorentz transformation of the vacuum is the vacuum, since there is nothing there that could possibly change.
    
    \subsection{\texorpdfstring{\(p^2 > 0\)}{p squared Positive}}
    If \(p^2 > 0\) we can choose some \(m \in \reals\) such that \(p^2 = m^2\).
    We also have \(p^2 = m^2 = p_0^2 - \abs{\vv{p}}^2\), which we can rearrange to \(p_0^2 = m^2 + \abs{\vv{p}}^2\).
    We can then recognise this as \(E^2 = m^2 + \abs{\vv{p}}^2\), the relativistic energy-momentum relation.
    
    We can then consider two subcases, either \(p_0 \ge m\) or \(p_0 \le -m\).
    It's not possible to have \(-m \le p_0 \le m\) and have \(p_0^2 + m^2 + \abs{\vv{p}}^2\) at the same time since \(\abs{\vv{p}}^2\) is nonnegative.
    
    We can move to a frame where \(p_\mu = (m, \vv{0})\), this is the rest frame.
    Then we have
    \begin{equation}
        W_\mu = \frac{1}{2} \varepsilon_{\mu\nu\rho\sigma} M^{\mu\nu} P^\sigma = \frac{1}{2} \varepsilon_{\mu\nu\rho\sigma} M^{\mu\nu} m \delta^{0\sigma} = \frac{1}{2} \varepsilon_{\mu\nu\rho0} M^{\mu\nu}m.
    \end{equation}
    This means that \(W_0 = 0\), since
    \begin{equation}
        W_0 = \frac{1}{2}\varepsilon_{0\nu\rho0} M^{\nu\rho}m = 0.
    \end{equation}
    We also have
    \begin{equation}
        W_i = \frac{1}{2}\varepsilon_{i\nu\rho 0} M^{\nu\rho}m = \frac{1}{2}\varepsilon_{ijk} M^{jk} = -mJ_i.
    \end{equation}
    Here we've noticed that by fixing the final index of \(\varepsilon_{\mu\nu\rho\sigma}\) to be \(\sigma = 0\) we must have that, for a nonvanishing contribution, \(\mu\), \(\nu\), and \(\rho\) are nonzer, so we can replace them with spatial indices \(i\), \(j\), and \(k\), and drop the extra zero index.
    We then recognise the definition of \(J_i\) in terms of \(M^{jk}\).
    Since \(J_i\) satisfies the \(\specialUnitaryLie(2)\) algebra we know that \(W^2 = W_0W^0 - W_iW^i = -m^2\vv{J}^2\) has eigenvalues \(-m^2 s(s + 1)\) where \(s = 0, \pm 1/2, \pm 1, \pm 3/2, \dotsc\).
    
    So, representations with \(p^2 > 0\) can be labelled by a continuous label, their momentum, \(p_i\), and two discrete labels, their spin, \(s\), and \(s_3 = -s, -s + 1, \dotsc, s - 1, s\).
    
    We then label the representation with the value of \(p^2 = m^2\) and \(w^2 = -m^2s(s + 1)\).
    States within the representation are then labelled by the three-momentum \(\vv{p}\), spin \(s\), and spin along some fixed axis, \(s_3\).
    This is pure group theory, interpreted as physics.
    In a sense particles \emph{are} representations of the Poincar\'e group, and there's not much else to it, the states that the particle can be in are then states within that representation.
    
    \subsection{\texorpdfstring{\(p^2 = 0\) and \(p_\mu \ne 0\)}{p squared Zero and p mu Nonzero}}
    We can similarly say that \(p^2 = m^2\), and so \(m^2 = 0\), and \(m = 0\).
    This means that there is no rest frame.
    Instead we consider a frame where \(p_\mu = (p_0, 0, 0, p_0)\), which satisfies \(p^2 = p_0^2 - \vv{p}^2 = p_0^2 - p_0^2 = 0\).
    We interpret particles corresponding to representations like this as massless.
    
    It seems that only representations where we also have \(W^2 = 0\) are physically relevant.
    This follows since states where this isn't the case allow continuous values of spin, and we don't observe this in nature.
    So we only consider representations where \(W^2 = 0\).
    
    We always have \(W_\mu P^\mu = 0\), and hence \(W_\mu p^\mu = 0\), so
    \begin{equation}
        W_\mu p^\mu = W_0 p_0  - W_3p_0 = 0 \implies W_3 = W_0.
    \end{equation}
    We then have
    \begin{equation}
        W^2 = W_0^2 - W_1^2 - W_2^2 - W_3^2 = -W_1^2 - W_2^2 = 0 \implies -W_1^2 = W_2^2
    \end{equation}
    and since \(W_1\) and \(W_2\) are real we must have \(W_1 = W_2 = 0\).
    This means that in this case we have
    \begin{equation}
        W_\mu = \lambda P_\mu
    \end{equation}
    where \(\lambda\) is some invariant we call the \defineindex{helicity}.
    
    Let \(\eta^\mu = (1, \vv{0})\).
    Then we have
    \begin{equation}
        W_\mu \eta^\mu = \lambda P_\mu \eta^\mu \implies \lambda = \frac{W_\mu \eta^\mu}{P_\mu \eta^\mu} = \frac{\frac{1}{2}\varepsilon_{\mu\nu\rho\sigma}M^{\mu\nu}P^\sigma \eta^\mu}{P_\mu \eta^\mu} \frac{\frac{1}{2}\varepsilon_{ijk}M^{ij}p^k}{p^0} = \frac{\vv{J} \cdot \vv{p}}{\abs{\vv{p}}}
    \end{equation}
    where we've used \(p_0 = p_3 = \abs{\vv{p}}\).
    This is the normal definition of helicity, as the component of spin in the direction of the momentum.
    
    It can be shown that \(\lambda = \pm s\) where \(s\) is the spin of the representation in the sense of the previous case.
    This means that massless representations (particles) are labelled by a binary left/right handed, or \(\lambda = \pm s\), label.
    This corresponds to the fact that there are only two (physical) polarisations of the photon.
    States within the representation are then labelled by \(p_i\).
    
    \subsection{\texorpdfstring{\(p^2 < 0\)}{p squared Negative}}
    This representation corresponds to tachyons, i.e., particles travelling faster than light.
    They are acausal and unphysical, so we won't consider them further.
    
    \chapter{Fields}
    \section{Why Fields?}
    The motivation for introducing fields into our theories is from Casimir operators.
    In the semisimple or compact case the Casimirs have discrete spectra, and so states correspond to finite dimensional vectors (using vectors here in the mathematical sense to mean \enquote{elements of a vector space}, which in this case could include scalars, spinors, four-vectors, or tensors).
    In the nonsemisimple case some Casimir operators will have continuous spectra, and so the states are instead given by elements of an infinite dimensional vector space, these are functions of spacetime, or fields as we call them in physics.
    
    In the case of a Poincar\'e transformation with \(p^2 = m^2 \ne 0\) we had states \(\ket{p_i, m, s, s_z}\) where \(p_i\) are continuous.
    Now we generalise this by introducing fields, \(\varphi_s(x)\) where \(s\) labels the discrete dependencies of the field, which is usually, but not always, the spin, and \(x\) is a point in spacetime.
    We then have states
    \begin{equation}
        \ket{x, s} = \varphi_s(x) \ket{0}
    \end{equation}
    where \(\ket{0}\) is the vacuum state, defined by transforming trivially in the \((0, 0)\) representation of the Poincar\'e group.
    We then construct a general state as a superposition of \(\ket{x, s}\).
    
    \section{Transformation of Fields}
    Under a Poincar\'e transformation a state \(\ket{\psi}\) transforms as
    \begin{equation}
        \ket{\psi} \mapsto U^{-1}(a, \Lambda) \ket{\psi}
    \end{equation}
    where \(U\) is a representation of the Poincar\'e group and \(x\) transforms according to \(x \mapsto \Lambda x + a\).
    We use \(U^{-1}\) since this is a passive transformation, the state isn't changing, but the coordinate system is.
    
    Now consider how \(\ket{x, s} = \varphi_s(x)\ket{0}\) transforms under this representation:
    \begin{equation}
        \varphi_s(x) \mapsto U^{-1}(a, \Lambda) \varphi_s(x)\ket{0},
    \end{equation}
    but we know that \(\ket{0}\) is also a state, so transforms as
    \begin{equation}
        \ket{0} \mapsto U^{-1}(a, \Lambda)\ket{0}.
    \end{equation}
    In order for both of these transformation laws to hold it must be that \(\varphi_s(x)\) transforms as
    \begin{equation}
        \varphi_s(x) \mapsto U^{-1}(a, \Lambda) \varphi_s(x) U(a, \Lambda).
    \end{equation}
    
    Suppose that \(\varphi\) is a scalar field depending only on position, then it must transform as
    \begin{equation}
        \varphi_s(x) \mapsto U^{-1}(a, \Lambda) \varphi(x) U(a, \Lambda) = \varphi(\Lambda x + a),
    \end{equation}
    since the value of the scalar field cannot change under a Poincar\'e transformation of the coordinates.
    
    We can write \(U(a, \Lambda)\) as
    \begin{equation}
        U(a, \Lambda) = U(a, \ident) U(0, \Lambda),
    \end{equation}
    and under an infinitesimal transformation we know that
    \begin{equation}
        U(a, \ident) \approx 1 + ia_\mu P^\mu, \qqand U(0, \Lambda) \approx 1 + \frac{i}{2}\omega^{\mu\nu}M_{\mu\nu}.
    \end{equation}
    Expanding \(U^{-1}\varphi U\) to first order we have
    \begin{align}
        U^{-1}\varphi U &= U^{-1}(a, \Lambda)\varphi(x)U(a, \Lambda)\\
        &= U^{-1}(0, \Lambda)U^{-1}(a, \ident) \varphi(x) U(a, \ident) U(0, \Lambda)\\
        &= \left( 1 - \frac{i}{2}\omega^{\mu\nu}M_{\mu\nu} \right)(1 - ia_\rho P^\rho) \varphi(x) (1 + ia_\sigma P^\sigma)\left( 1 + \frac{i}{2}\omega^{\alpha\beta}M_{\alpha\beta} \right)\notag\\
        &= \varphi(x) - \frac{i}{2}\omega^{\mu\nu}M_{\mu\nu}\varphi(x) + \frac{i}{2}\varphi(x)\omega^{\alpha\beta}M_{\alpha\beta}\\
        &\qquad- ia_\rho P^\rho \varphi(x) + a_\sigma\varphi(x) P^\sigma\\
        &= \varphi(x) - \frac{i}{2}\omega^{\mu\nu} \commutator{M_{\mu\nu}}{\varphi(x)} - ia_\rho \commutator{P^\rho}{\varphi(x)}.
    \end{align}
    
    If \(x \mapsto x' = \Lambda x + a\) then we have
    \begin{equation}
        \diffp{}{x_\mu} \mapsto \diffp{}{x_\mu'} = \diffp{x_\nu}{x'_\mu} \diffp{}{x_\nu} = \tensor{\Lambda}{^\mu_\nu} \diffp{}{x_\nu}
    \end{equation}
    where we've identified \(\diffp{x_\nu}/{x'_\mu} = \tensor{\Lambda}{^\mu_\nu}\) by differentiating the transformation law for \(x\).
    What this means is that if \(\varphi\) is a scalar then \(\partial^\mu \varphi\) is a vector, \(\partial^\mu \partial^\nu \varphi\) is a rank 2 tensor, and \(\partial_\mu \partial^\mu \varphi \eqqcolon \dalembertian \varphi\) is a scalar again.
    
    Consider the translation \(x \mapsto x + a\), we have
    \begin{equation}
        \varphi(x) \mapsto \varphi(x + a) = \varphi(x) + a_\mu \partial^\mu \varphi + \order(a^2),
    \end{equation}
    and similarly under a Lorentz transformation, \(x \mapsto \Lambda x\), we have
    \begin{equation}
        \varphi(x) \mapsto \varphi(\Lambda x) = \varphi(x) + \tensor{\omega}{_\mu^\nu}x_\nu \partial^\mu \varphi + \order(\omega^2).
    \end{equation}
    We can then identify the change in \(\varphi\) under a Poincar\'e transformation as
    \begin{equation}
        \delta \varphi = -a_\mu \commutator{P^\mu}{\varphi} - \frac{i}{2}\omega_{\mu\nu} \commutator{M^{\mu\nu}}{\varphi} = a_\mu \partial^\mu \varphi + \tensor{\omega}{_\mu^\nu}x_\nu \partial^\mu \varphi.
    \end{equation}
    Since this holds for all infinitesimal Poincar\'e transformations we have
    \begin{align}
        \commutator{P^\mu}{\varphi} &= i\partial^\mu \varphi,\\ \commutator{M^{\mu\nu}}{\varphi} &= i(x^\nu \partial^\mu - x^\mu \partial^\nu) \varphi = (x^\nu P^\mu - x^\mu P^\nu)\varphi.
    \end{align}
    Here we antisymmetries the coefficient of \(\tensor{\omega}{_\mu^\nu}\) because we know that \(M^{\mu\nu}\) is antisymmetric.
    Note that the last equality only holds when we are acting on the vacuum state, which has \(P^\mu \ket{0} = 0\).
    
    Writing \(P^\mu = (H, \vv{P})\) we have
    \begin{align}
        \commutator{H}{\varphi} &= i\diffp{\varphi}{t},\\
        \commutator{\vv{P}}{\varphi} &= -i\grad \varphi,\\
        \commutator{\vv{J}}{\varphi} &= -i\vv{x} \times \grad\varphi.
    \end{align}
    The first of these is pretty much Heisenberg's equation of motion, the second should be familiar from quantum mechanics where \(-i\grad\) is the momentum operator, and after making this identification we can identify the third as the angular momentum operator.
    Using \(P^\mu\ket{0} = 0\) we have
    \begin{equation}
        H\ket{\vv{x}, t} = i\diffp{}{t} \ket{x, t},
    \end{equation}
    which is Schrdinger's equation.
    
    Now consider the Fourier transform of our field,
    \begin{equation}
        \tilde{\varphi}(p) = \int \dl{^4x} \, \e^{-ip\cdot x} \varphi(x).
    \end{equation}
    Then we have \(\commutator{P_\mu}{\tilde{\varphi}(p)} = p_\mu\varphi(p)\), essentially moving to the momentum basis.
    
    Suppose now that \(\varphi\) is \emph{not} a scalar field.
    Then we'll have an extra factor of 
    \begin{equation}
        D(\Lambda) = \exp\left[ \frac{i}{2}\omega^{\mu\nu}S_{\mu\nu} \right]
    \end{equation}
    in the transformation law where \(S_{\mu\nu}\) is the generator of the Lorentz group in whatever representation \(\varphi\) transforms under.
    That is,
    \begin{equation}
        U^{-1}(a, \Lambda) \varphi(x) U(a, \Lambda) = D(\Lambda)\varphi(\Lambda x + a).
    \end{equation}
    We then have
    \begin{equation}
        \commutator{P_\mu}{\varphi} = i\partial_\mu \varphi
    \end{equation}
    as before, since the \(a_\mu\) dependence is unchanged.
    The commutation relation for \(M_{\mu\nu}\) becomes
    \begin{equation}
        \commutator{M_{\mu\nu}}{\varphi} = i(x_\nu \partial_\mu - x_\mu \partial_\nu)\varphi + S_{\mu\nu}.
    \end{equation}
    Defining \(L_{\mu\nu} \coloneqq x_\nu \partial_\mu - x_\mu \partial_\nu\) we can identify \(L_{\mu\nu} + S_{\mu\nu}\) as the total angular momentum, with \(L_{\mu\nu}\) being the orbital angular momentum and \(S_{\mu\nu}\) the spin.
    
    For a scalar \(D(\Lambda) = 1\), and so \(S_{\mu\nu} = 0\), meaning scalars have no spin.
    For a Weyl spinor \(\omega^{\mu\nu}S_{\mu\nu} = i\omega^{\mu\nu}\sigma_{\mu\nu}/2\), where \(\sigma{\mu\nu} = \commutator{\sigma_\mu}{\overbar{\sigma}_\nu}\).
    For a four-vector \(\tensor{(\omega^{\mu\nu}S_{\mu\nu})}{^\alpha_\beta} = \tensor{\omega}{^\alpha_\beta}\).
    
    The Pauli--Lubanski vector, \(W^\mu = \varepsilon_{\mu\nu\rho\sigma} M^{\nu\rho}P^\sigma/2\), becomes just \(W^\mu = \varepsilon_{\mu\nu\rho\sigma}S^{\nu\rho}P^\sigma/2\), since \(L^{\nu\rho}\) is symmetric.
    This means that \(W^{\mu}\), and hence the Casimir \(W^2\), only depends on the spin angular momentum, not on the orbital angular momentum.
    
    \part{Symmetry in Action}
    \chapter{Action Principles}
    \section{Action}
    \begin{rmk}
        For action principles in classical mechanics see \course{Lagrangian Dynamics}, for action principles in classical field theory see \course{Classical Electrodynamics}, for action principles in quantum mechanics see \course{Quantum Theory}, and for action principles in quantum field theories see \course{Quantum Field Theory}.
    \end{rmk}
    It seems like all of physics can be described using action principles.
    If we have some degrees of freedom, be they coordinates or fields, then the action is a functional of these degrees of freedom, such that the action is extremised for the classical path of motion.
    Things get a little more complicated when we consider quantum mechanics, then we instead consider a sum (or integral) over the amplitudes for different paths, each weighted by a factor of \(\exp[i S]\), where \(S\) is the action.
    This reduces to the classical path through the method of stationary phase\footnote{See \course{Methods of Mathematical Physics}.}
    
    We will consider a field theory with some field, \(\Phi\), which evolves according to an action principle.
    We make no \textit{a priori} assumptions about \(\Phi\), apart from being differentiable, \(\Phi\) could be a scalar, vector, spinor, tensor, and so on.
    We also allow for the theories with multiple fields by assuming a sum over all fields.
    
    We make the following assumptions of our action, chosen to give reasonable physics:
    \begin{itemize}
        \item \textit{Locality}: We assume that the action is of the form
        \begin{equation}
            S = \int_{V} \dl{^4x}\, \lagrangianDensity
        \end{equation}
        where \(\dl{^4x} = \dl{t} \dd{x} \dd{y} \dd{z}\) is the Lorentz invariant integration measure, \(V\) is some region of integration, and \(\lagrangianDensity\) is the \defineindex{Lagrangian density}, although we follow the very common abuse of termination calling this the \defineindex{Lagrangian}, which is more properly given by
        \begin{equation}
            L = \int \dl{^3x} \, \lagrangianDensity \implies S = \int \dl{t} \, L.
        \end{equation}
        We assume that \(\lagrangianDensity\) is a function of \(\Phi\) the field, and also derivatives of \(\Phi\), all evaluated at just a single point, \(x\).
        This precludes action at a distance.
        This requirement ensures causality in quantum theory.
        
        \item \textit{Unitarity}: We assume the action, and by extension the Lagrangian, is real, this guarantees probability conservation in quantum theories.
        
        \item \textit{Causality}: We require that \(S\) leads to classical equations of motion which obey causality.
        In practice it can be shown that this requires that \(\lagrangianDensity\) depends only on the field and first or second order derivatives.
        
        \item \textit{Symmetry}: We assume that the action is Poincar\'e invariant.
        Since \(\dl{^4x}\) is Poincar\'e invariant this means that \(\lagrangianDensity\) must be a Lorentz invariant function of the fields.
        Translation invariance is not required, since by integrating over a region we can recover translation invariance from a non-translation invariant object.
        
        We also impose further symmetries depending on the system under consideration.
        These may include reflection symmetry, rotational symmetry, gauge symmetries, such as \(\unitary(1)\), and so on.
        
        \item \textit{Renormalisability}: Terms in the action with dimension \([\text{energy}]^{-n}\) for some \(n > 0\) generally cause the quantum theory to lose all predictive power at energies on the order of one energy unit.
        We either require that such terms don't appear or are negligibly small.
        This gives another reason for neglecting higher order derivatives.
    \end{itemize}
    
    \section{Classical Equations of Motion}
    In a theory described by an action principle the classical equations of motion are equivalent to the requirement that the action is extremised, typically we minimise the action.
    This can be compactly stated as
    \begin{equation}
        \delta S = 0,
    \end{equation}
    which is Hamilton's principle.
    Here \(\delta S = S[\Phi + \delta \Phi] - S[\Phi]\) where \(\delta \Phi\) is some small variation in the field \(\Phi\).
    
    It is possible to go from the definition of \(S\) and calculate \(\delta S\) each time and set it to zero to obtain the classical equations of motion.
    However, this will result in us repeating the same steps over and over.
    It is often useful to make an assumption about the form of \(\lagrangianDensity\) and then derive equations which hold for all \(\lagrangianDensity\) of this form.
    
    Suppose that \(\lagrangianDensity\) is a function of the field, \(\Phi\), and its derivative, \(\partial_\mu \Phi\).
    Then we have
    \begin{align}
        \delta S &= \delta \int \dl{^4x} \,v\lagrangianDensity(\Phi, \partial_\mu \Phi)\\
        &= \int \dl{^4x} \, \lagrangianDensity(\Phi + \delta\Phi, \partial_\mu(\Phi + \delta \Phi)) - \int \dl{^4x} \, \lagrangianDensity(\Phi, \partial_\mu \Phi)\\
        &= \int \dl{^4x} \, [\lagrangianDensity(\Phi + \delta\Phi, \partial_\mu(\Phi + \delta\Phi)) - \lagrangianDensity(\Phi, \partial_\mu \Phi)]\\
        &= \int \dl{^4x} \, \delta\lagrangianDensity.
    \end{align}
    We can work out the variation in \(\lagrangianDensity\) by expanding to first order:
    \begin{equation}
        \lagrangianDensity(\Phi + \delta \Phi, \partial_\mu (\Phi + \delta \Phi)) = \lagrangianDensity(\Phi, \partial_\mu \Phi) + \diffp{\lagrangianDensity}{\Phi} \delta \Phi + \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \delta(\partial_\mu \Phi).
    \end{equation}
    So, the variation in \(\lagrangianDensity\) is
    \begin{equation}
        \delta\lagrangianDensity = \diffp{\lagrangianDensity}{\Phi} \delta \Phi + \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \delta(\partial_\mu \Phi).
    \end{equation}
    Now consider the \(\delta(\partial_\mu \Phi)\) term, we have \begin{equation}
        \partial_\mu(\delta \Phi) = \partial_\mu(\Phi + \delta \Phi - \Phi) = \partial_\mu (\Phi + \delta \Phi) - \partial_\mu(\Phi) = \delta(\partial_\mu \Phi).
    \end{equation}
    So, \(\delta(\partial_\mu \Phi) = \partial_\mu(\delta \Phi)\).
    Hence, the variation in \(\lagrangianDensity\) is
    \begin{equation}
        \delta\lagrangianDensity = \diffp{\lagrangianDensity}{\Phi} \delta \Phi + \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \partial_\mu(\delta\Phi).
    \end{equation}
    
    The variation in \(S\) is then
    \begin{align}
        \delta S &= \int_V \dl{^4x} \, \left( \diffp{\lagrangianDensity}{\Phi} \delta \Phi + \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \partial_\mu(\delta \Phi) \right)\\
        &= \int_V \dl{^4x} \, \left( \diffp{\lagrangianDensity}{\Phi} \delta \Phi + \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \delta \Phi \right) - \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \right) \delta \Phi \right)\\
        &= \int_V \dl{^4x} \left( \diffp{\lagrangianDensity}{\Phi} - \partial_\mu \left( \diffp{\lagrangianDensity}{\Phi} \right) \right) \delta\Phi - \int_V \dl{^4x} \, \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \delta \Phi \right)\\
        &= \int_V \dl{^4x} \left( \diffp{\lagrangianDensity}{\Phi} - \partial_\mu \left( \diffp{\lagrangianDensity}{\Phi} \right) \right) \delta\Phi - \int_{\partial V} \dl{\Sigma} \, n_\mu \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \delta\Phi.
    \end{align}
    Here we've used the chain rule to rewrite the second term in the first line.
    We then use the divergence theorem in the last line to turn the integral into an integral over the boundary, \(\partial V\).
    Here \(\Sigma\) is a surface element, and \(n_\mu\) is normal to the surface.
    
    If we require that \(\delta S = 0\) then for any variation, \(\delta\Phi\), chosen such that \(\delta\Phi\) vanishes on \(\partial V\) we must have that the integrand in the first integral vanishes, that is
    \begin{equation}
        \diffp{\lagrangianDensity}{\Phi} = \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu\lagrangianDensity)} \right).
    \end{equation}
    These are the \defineindex{Euler--Lagrange equations} for a Lagrangian depending only on the field \(\Phi\) and its derivative, \(\partial_\mu \Phi\).
    
    \section{Examples}
    In this section we'll give some examples of Lagrangians and the classical equations of motion we derive from them.
    For details about the solutions and where the Lagrangians come from see \course{Quantum Field Theory}.
    
    \subsection{Scalar Field}
    The Lagrangian for a free scalar field, \(\varphi\), is
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi)(\partial^\mu \varphi) - \frac{1}{2}m^2\varphi^2
    \end{equation}
    where \(m\) is a constant with units of mass.
    We have
    \begin{equation}
        \diffp{\lagrangianDensity}{\varphi} = -m^2\varphi
    \end{equation}
    and
    \begin{align}
        \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} &= \diffp{}{(\partial_\mu \varphi)} \frac{1}{2} \minkowskiMetric^{\nu\rho}(\partial_\nu \varphi)(\partial_\rho \varphi)\\
        &= \frac{1}{2}\minkowskiMetric^{\nu\rho} \left[ \diffp{(\partial_\nu \varphi)}{(\partial_\mu \varphi)} (\partial_\rho \varphi) + (\partial_\nu \varphi) \diffp{(\partial_\rho \varphi)}{(\partial_\mu \varphi)} \right]\\
        &= \frac{1}{2}\minkowskiMetric^{\nu\rho} [\tensor{\delta}{^\mu_\nu} (\partial_\rho \varphi) + (\partial_\nu \varphi) \tensor{\delta}{^\mu_\rho}]\\
        &= \frac{1}{2}[\minkowskiMetric^{\mu\rho} (\partial_\rho \varphi) + (\partial_\nu \varphi) \minkowskiMetric^{\nu\mu}]\\
        &= \frac{1}{2}[(\partial^\mu\varphi) + (\partial^\mu\varphi)]\\
        &= \partial^\mu\varphi
    \end{align}
    Hence,
    \begin{equation}
        \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) = \partial_\mu \partial^\mu \varphi = \dalembertian\varphi.
    \end{equation}
    For most calculations like this we can just proceed as if \((\partial_\mu \varphi)(\partial^\mu \varphi)\) was \((\partial_\mu\varphi)^2\), and then work out whether we need upper or lower indices to match what we start with.
    
    
    The classical equations of motion are thus
    \begin{equation}
        -m^2\varphi = \dalembertian\varphi \implies (\dalembertian + m^2)\varphi = 0.
    \end{equation}
    This is the \defineindex{Klein--Gordon equation}.
    
    \subsection{Fermion Field}
    For fermions the field, \(\psi\), is a spinor.
    We treat \(\psi\) and its conjugate, \(\psi^\hermit\), as independent variables.
    The Lagrangian for a free spin \(1/2\) fermion field is then
    \begin{equation}
        \lagrangianDensity = \diracadjoint{\psi} (i\slashed{\partial} - m)\psi
    \end{equation}
    where \(\diracadjoint{\psi} = \psi^\hermit\gamma^0\) and \(\slashed{\partial} = \gamma^\mu \partial_\mu\).
    We then have
    \begin{equation}
        \diffp{\lagrangianDensity}{\psi^\hermit} = \gamma^0(i\slashed{\partial} - m)\psi, \qqand \diffp{\lagrangianDensity}{(\partial_\mu \psi^\hermit)} = 0.
    \end{equation}
    Since \((\gamma^0)^2 = \ident\) we can multiply by \(\gamma^0\) and we get the classical equations of motion
    \begin{equation}
        (i\slashed{\partial} - m)\psi = 0.
    \end{equation}
    This is the \defineindex{Dirac equation}.
    
    We can also derive an equation of motion by varying \(\psi\).
    This can be done using the Euler--Lagrange equations, but here we'll do it by varying the action,
    \begin{equation}
        S = \int \dl{^4x} \, \diracadjoint{\psi}(i\slashed{\partial} - m)\psi.
    \end{equation}
    We then have
    \begin{equation}
        \delta \lagrangianDensity = \diracadjoint{\psi}(i\slashed{\partial} - m)(\psi + \delta\psi) - \diracadjoint{\psi}(i\slashed{\partial} - m)\psi = -\diracadjoint{\psi}(i\slashed{\partial} - m)\delta\psi
    \end{equation}
    and so
    \begin{equation}
        \delta S = \int \dl{^4x} \, \diracadjoint{\psi}(i\slashed{\partial} - m)\delta\psi.
    \end{equation}
    We have \(\diracadjoint{\psi}\slashed{\partial} \delta \psi\), we can rewrite this as \(\slashed{\partial}(\diracadjoint{\psi}\delta\psi) - (\slashed{\partial}\diracadjoint{\psi})\delta\psi\).
    The first term gives a surface term which doesn't effect the action if we choose \(\delta\psi = 0\) on the boundary of the integration region, and so we have
    \begin{equation}
        \delta S = \int \dl{^4x} \, (-i\slashed{\partial}\diracadjoint{\psi} - m\diracadjoint{\psi})\delta\psi.
    \end{equation}
    Demanding this vanishes for all variations \(\delta \psi\) vanishing on the boundary we have the equation of motion
    \begin{equation}
        -i\slashed{\partial}\diracadjoint{\psi} - m\diracadjoint{\psi} = 0.
    \end{equation}
    This isn't actually a new equation, it's just the Hermitian conjugate of the Dirac equation:
    \begin{align}
        [-i\slashed{\partial}\diracadjoint{\psi} - m\diracadjoint{\psi}]^\hermit &= [-i\gamma^\mu \partial_\mu \psi^\hermit \gamma^0 - m\psi^\hermit \gamma^0]^\hermit\\
        &= i\partial_\mu \gamma^0 \psi \gamma^\mu - m \gamma^0 \psi
    \end{align}
    which is \(\gamma^0\) times the left hand side of the Dirac equation.
    
    \subsection{Gauge Fields}
    The Lagrangian for a free gauge field, \(A^\mu\), is
    \begin{equation}
        \lagrangianDensity = -\frac{1}{4} F^{\mu\nu}F_{\mu\nu}, \qqwhere F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu.
    \end{equation}
    This gives the Maxwell equations with no sources, \(\partial_\mu F^{\mu\nu} = 0\), or generalises to give the equations of motion for a Yang--Mills theory.
    
    \section{Surface Terms}
    The classical equations of motion arise by setting the coefficient of the bulk variation to zero.
    This is just a fancy way to say we demand that the variation in the action vanishes, apart from a possible surface term.
    We call this the variation in the action the bulk variation, as opposed to the local variation of a field, since the action is defined as an integral over spacetime.
    
    In terms of the Lagrangian this means that the transformation
    \begin{equation}
        \lagrangianDensity \mapsto \lagrangianDensity + \partial_\mu K^\mu
    \end{equation}
    doesn't change the bulk variation, and hence doesn't change the equations of motion.
    Such a change of \(\lagrangianDensity\) is called a \defineindex{canonical transformation}.
    
    It is possible that such a transformation does result in a change of boundary conditions on \(\delta \Phi\), such as changing from \(\delta\Phi = 0\) on the boundary to \(\partial_t \delta\Phi = 0\) on the boundary.
    
    \section{Noether's Theorem}
    Consider a variation \(\Phi \mapsto \Phi + \delta \Phi\) where
    \begin{equation}
        \delta \Phi = \alpha \diffp{\Phi}{\alpha} \bigg|_{\alpha = 0}.
    \end{equation}
    Note the similarity to how we define the generators of a Lie algebra from the Lie group.
    This variation could come from a translation, \(x \mapsto x + a\), in which case we have
    \begin{equation}
        \Phi(\alpha) = \Phi(0) + \diffp{\Phi}{\alpha} \bigg|_{\alpha = 0} \alpha + \dotsb.
    \end{equation}
    Here \(\alpha\) is the parameter of some continuous symmetry, and \(\alpha\) is constant in spacetime.
    We call this a \defineindex{global symmetry}, where we do he same thing to \(\Phi\) at every point in spacetime.
    
    If the integration measure, \(\dl{^4x}\), is invariant for a global symmetry, which it is for Lorentz boosts and translations, then the symmetry will be a symmetry of the equations of motion, provided that \(\lagrangianDensity\) transforms as
    \begin{equation}
        \lagrangianDensity \mapsto \lagrangianDensity + \alpha\partial_\mu K^\mu
    \end{equation}
    to first order in \(\alpha\) for some differentiable \(K^\mu\), which may simply be zero.
    
    Then the variation in \(\delta\lagrangianDensity\) under this symmetry is
    \begin{align}
        \delta\lagrangianDensity &= \diffp{\lagrangianDensity}{\Phi} \delta\Phi + \diffp{\lagrangianDensity}{(\partial_\mu \Phi)}\partial_\mu(\delta\Phi)\\
        &= \diffp{\lagrangianDensity}{\Phi} \alpha \diffp{\Phi}{\alpha} + \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \partial_\mu\left( \alpha\diffp{\Phi}{\alpha} \right)\\
        &= \alpha \diffp{\lagrangianDensity}{\Phi} \diffp{\Phi}{\alpha} + \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \alpha \diffp{\Phi}{\alpha} \right) - \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \right) \alpha\diffp{\Phi}{\alpha}\\
        &= \alpha\diffp{\lagrangianDensity}{\Phi} \diffp{\Phi}{\alpha} + \alpha \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \diffp{\Phi}{\alpha} \right) - \diffp{\lagrangianDensity}{\Phi} \alpha \diffp{\Phi}{\alpha}\\
        &= \alpha\partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \diffp{\Phi}{\alpha} \right)
    \end{align}
    where all derivatives with respect to \(\alpha\) are evaluated at \(\alpha = 0\).
    We then used the Euler--Lagrange equations in the penultimate line to replace the \(\partial_\mu(\diffp{\lagrangianDensity}/{(\partial_\mu \Phi)})\) term with \(\diffp{\lagrangianDensity}/{\Phi}\).
    If the Lagrangian does vary as \(\lagrangianDensity \mapsto \lagrangianDensity + \alpha\partial_\mu K^\mu\) then we have \(\delta\lagrangianDensity = \alpha\partial_\mu K^\mu\), and so we must have
    \begin{equation}
        \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu\Phi)} \diffp{\Phi}{\alpha} - K^\mu \right) = 0.
    \end{equation}
    Defining
    \begin{equation}
        J^\mu \coloneqq \diffp{\lagrangianDensity}{(\partial_\mu\Phi)} \diffp{\Phi}{\alpha} - K^\mu
    \end{equation}
    we see that we have
    \begin{equation}
        \partial_\mu J^\mu = 0,
    \end{equation}
    and so we can interpret \(J^\mu\) as a conserved current.
    We call \(J^\mu\) the \defineindex{Noether current} associated with the symmetry generated by \(\alpha\).
    
    Something similar can also be done for a \defineindex{local symmetry}, where \(\alpha\) is a function of \(x\).
    
    We can define a conserved quantity, \(Q\), which we call a charge in general, according to
    \begin{equation}
        Q \coloneqq \int \dl{^3x} \, J^0(x).
    \end{equation}
    This is conserved since we have \(\partial_t J^0 = -\div\vv{J}\), since \(\partial_{\mu}J^\mu = 0\), and so
    \begin{align}
        \diff{}{t}Q &= \diff{}{t} \int_V \dl{^3x} \, J^0(x)\\
        &= \int_V \dl{^3x} \, \diffp{}{t}J^0(x)\\
        &= -\int_V \dl{^3x} \, \div\vv{J}(x)\\
        &= - \int_{\partial V} \dl{\vv{S}} \cdot \vv{J}
    \end{align}
    so if \(\vv{J}\) vanishes on the boundary, or if \(V\) has no boundary, such as if we have periodic boundary conditions, or a region like a sphere, then \(Q\) is conserved.
    
    \begin{exm}{}{}
        Suppose \(\lagrangianDensity\) doesn't depend explicitly on \(x\).
        Then the theory is translation invariant, that is the transformation \(x \mapsto x + a\) doesn't change anything.
        We then have
        \begin{equation}
            \Phi(x) \mapsto \Phi(x + a) = \Phi(x) + a^\mu \partial_\mu \Phi(x) + \order(\alpha^2).
        \end{equation}
        We also have
        \begin{equation}
            \lagrangianDensity \mapsto \lagrangianDensity + a^\mu \partial_\mu \lagrangianDensity = \lagrangianDensity + a^\nu \partial_\mu (\tensor{\delta}{_\mu^\nu}\lagrangianDensity)
        \end{equation}
        and we can then take \(\tensor{K}{_\nu^\mu} = \tensor{\delta}{_\mu^\nu} \lagrangianDensity\).
        We use the Kronecker delta to separate the parameters, \(\alpha^\nu\), of which we actually have four, from the variation, \(\tensor{K}{_\nu^\mu}\), which we also have four of, one for each \(\alpha^\nu\), hence the second index.
        
        We will then have four conserved currents, one for each \(\alpha^\nu\), which we package up into a single object, \(\tensor{T}{_\nu^\mu}\), given by
        \begin{equation}
            \tensor{T}{_\nu^\mu} = \diffp{\lagrangianDensity}{(\partial_\mu \Phi)} \partial_\nu \Phi - \tensor{\delta}{_\nu^\mu} \lagrangianDensity.
        \end{equation}
        This is conserved, by which we mean
        \begin{equation}
            \partial_\mu \tensor{T}{_\nu^\mu} = 0.
        \end{equation}
        We call \(\tensor{T}{_\nu^\mu}\) the \defineindex{energy-momentum tensor}, and for translation invariance we call the conserved quantities
        \begin{equation}
            P^\mu = \int \dl{^3x} \tensor{T}{_0^\mu}
        \end{equation}
        the energy, \(P^0\), and momentum, \(\vv{P}\).
        Of course, they exactly coincide with what we would normally call the energy and momentum, although they generally don't coincide with the \emph{canonical} momentum, \(p = \diffp{\lagrangianDensity}/{(\partial_0 \Phi)}\).
    \end{exm}
    
    \subsection{Symmetry of the Energy-Momentum Tensor}
    In general the energy-momentum tensor, as defined here, is not symmetric.
    In particular for fields with spin the first term fails to be symmetric.
    This is a problem if we want to do general relativity, since the Einstein field equations\footnote{See \course{General Relativity} for details.},
    \begin{equation}
        G^{\mu\nu} = 8\pi G T^{\mu\nu},
    \end{equation}
    relate the symmetric Einstein tensor, \(G^{\mu\nu}\), to the energy-momentum tensor, \(T^{\mu\nu}\).
    So, in order to couple our fields to gravity we need the energy-momentum tensor to be symmetric.
    
    The solution is to symmetrise the energy-momentum tensor by adding a term given by a divergence, which is automatically conserved.
    Define the \defineindex{Belinfante energy-momentum tensor}
    \begin{equation}
        \Theta_{\mu\nu} = T_{\mu\nu} + \partial^\alpha L_{\alpha\mu\nu}
    \end{equation}
    where \(L_{\alpha\mu\nu}\) is a three index tensor antisymmetric under exchange of \(\alpha\) and \(\mu\) or \(\alpha\) and \(\nu\).
    Then we have
    \begin{equation}
        \partial^\mu \partial^\alpha L_{\alpha\mu\nu} = 0
    \end{equation}
    as this is the product of a symmetric (\(\partial^\mu \partial^\alpha\)) and antisymmetric (\(L_{\alpha\mu\nu}\)) tensors.
    Thus, the divergence of the Belinfante energy-momentum tensor is
    \begin{equation}
        \partial \Theta_{\mu\nu} = \partial^\mu T_{\mu\nu} + \partial^\mu \partial^\alpha L_{\alpha\mu\nu} = 0
    \end{equation}
    with the first term vanishing by the conservation of \(T_{\mu\nu}\).
    We can simply choose \(L_{\alpha\mu\nu}\) to cancel the antisymmetric part of \(T_{\mu\nu}\).
    
    Note that in GR we can construct the energy-momentum tensor directly by varying the matter action, given by the Lagrangian
    \begin{equation}
        \lagrangianDensity = \frac{1}{2\kappa} R \sqrt{-g} + \lagrangianDensity_{\symrm{M}} \sqrt{-g}
    \end{equation}
    where \(\kappa = 8\pi G/c^4\), \(R\) is the Ricci scalar, \(g = \det g_{\mu\nu}\) is the determinant of the metric (which is negative in our sign convention), and \(\lagrangianDensity_{\symrm{M}}\) is the Lagrangian for any matter fields in the theory.
    Varying the corresponding action with respect to the metric gives the Einstein field equations, and the energy-momentum tensor
    \begin{equation}
        \Theta^{\mu\nu} = \frac{2}{\sqrt{-g}} \diffd{S}{g_{\mu\nu}},
    \end{equation}
    which automatically inherits symmetry from the symmetric metric tensor, \(g_{\mu\nu}\).
    
    \chapter{Field Theory}
    \section{Real Scalar Fields}
    A generic action for a real scalar field, \(\varphi\), can be written as
    \begin{equation}
        S = \int \dl{^4x} \left[ \frac{1}{2} (\partial_\mu \varphi)(\partial^\mu) - V(\varphi) \right]
    \end{equation}
    where the first term is the kinetic term and the second is a potential.
    In units where \(\hbar = 1\) the action is dimensionless, \([S] = 1\).
    The measure has dimensions \([\dl{^4x}] = [\text{length}]^4 = [\text{mass}]^{-4}\).
    This means the Lagrangian must have units of \([\lagrangianDensity] = [\text{mass}]^4\).
    Since the dimensions of the derivatives are \([\partial_\mu] = [\text{length}]^{-1} = [\text{mass}]\) we must have
    \begin{equation}
        1 = [\dl{^4 x} \, (\partial_\mu \varphi) (\partial^\mu \varphi)] = [\text{mass}]^{-4} [\text{mass}]^2 [\varphi]^2 \implies [\varphi] = [\text{mass}].
    \end{equation}
    
    Using this dimensional analysis we can construct a more general Lagrangian, subject to the dimensions working out and the result being a Lorentz invariant scalar:
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi)(\partial^\mu \varphi) - V(\varphi) +  \frac{(\dalembertian \varphi)^2}{\Lambda^2} +  \frac{[(\partial_\mu \varphi) (\partial^\mu \varphi)]^2}{\Lambda_2^4} + \dotsb
    \end{equation}
    where \(\Lambda_i\) are arbitrary scales with dimensions \([\Lambda_i] = [\text{mass}] = [\text{length}]^{-1}\).
    
    At low energies, \(E\), we expect that \(\abs{\partial_\mu \varphi}\) scales with \(E^2\), from dimensional analysis, meaning that higher derivative terms are suppressed by powers of \(E/\Lambda_i\).
    This justifies neglecting these extra terms to get the equations of motion for \(\varphi\), which we do using the Euler--Lagrange equations:
    \begin{equation}
        \partial_\mu\left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) = \diffp{\lagrangianDensity}{\varphi}.
    \end{equation}
    For the Lagrangian
    \begin{equation}
        \lagrangianDensity = \frac{1}{2}(\partial_\mu \varphi)(\partial^\mu \varphi) - V(\varphi)
    \end{equation}
    we have
    \begin{equation}
        \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} = \partial^\mu \varphi \implies \partial_\mu \left( \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \right) = \dalembertian \varphi
    \end{equation}
    and
    \begin{equation}
        \diffp{\lagrangianDensity}{\varphi} = -\diffp{V}{\varphi}.
    \end{equation}
    Hence, the equations of motion are
    \begin{equation}
        \dalembertian \varphi = - \diffp{V}{\varphi}.
    \end{equation}
    
    For a renormalisable theory it we require that couplings do not have negative mass dimensions.
    If we consider a polynomial potential, \(V\), then this restricts us to powers of \(\varphi\) up to fourth order in four dimensions:
    \begin{equation}
        V(\varphi) = \frac{1}{2}m^2\varphi^2 + \frac{g}{3!}\varphi^3 + \frac{\lambda}{4!}\varphi^4,
    \end{equation}
    where the \(1/n!\) coefficients are conventional to make the final results nicer.
    
    The first term is called the \defineindex{mass term} and the last two terms represent interactions.
    
    Note that the dimensions of the couplings, \(g\) and \(\lambda\), are \([g] = [\text{mass}]\) and \([\lambda] = 1\), so in four-dimensions \(\lambda\) is dimensionless, meaning \enquote{\(\varphi^4\) theory}, with this potential and \(g = 0\), is a popular toy model since we can do perturbation theory if \(\lambda\) is small.
    If we wanted a \(\varphi^6\) term the coefficient would need to have dimensions of \([\text{mass}]^{-2}\), so the theory would not be renormalisable.
    
    If \(g = \lambda = 0\) then the equations of motion are simply
    \begin{equation}
        (\dalembertian + m^2)\varphi = 0,
    \end{equation}
    which is the Klein--Gordon equation.
    The solutions are plane waves, \(\varphi(x) = \exp[-ip \cdot x]\), with \(p^\mu = (E, \vv{p})\) the four-momentum.
    Substituting this into the equation of motion we get
    \begin{equation}
        (p^2 + m^2)\e^{-ip \cdot x} = 0 \implies p^2 = m^2 \implies E^2 = \abs{\vv{p}}^2 + m^2,
    \end{equation}
    which is the relativistic energy-momentum relation.
    
    The energy-momentum tensor for a general renormalisable potential is
    \begin{align}
        T_{\mu\nu} &= (\partial_\mu \varphi)(\partial_\nu \varphi) - \minkowskiMetric_{\mu\nu}\lagrangianDensity\\
        &= (\partial_\mu \varphi)(\psi_\nu \varphi) - \minkowskiMetric_{\mu\nu} \left[ \frac{1}{2} (\partial_\rho \varphi)(\partial^\rho \varphi) - V \right].
    \end{align}
    We can explicitly show that this is conserved:
    \begin{align}
        \partial_\mu T^{\mu\nu} &= (\dalembertian \varphi)(\partial^\nu \varphi) + (\partial_\mu \varphi) (\partial^\mu \partial^\nu \varphi) - \frac{1}{2} \partial^\nu ((\partial^\mu \varphi)(\partial_\mu \varphi)) + \partial^\nu V\\
        &= \left[  \dalembertian \varphi + \diffp{V}{\varphi}  \right] \partial^\nu \varphi\\
        &= 0
    \end{align}
    where step we used the chain rule to write
    \begin{equation}
        \partial^\nu V = \diffp{V}{\varphi} \partial^\nu \varphi
    \end{equation}
    and then recognised the result as vanishing by the equations of motion.
    
    The energy density is
    \begin{equation}
        \hamiltonianDensity = T_{00} = \frac{1}{2}\dot{\varphi}^2 + \frac{1}{2}(\grad\varphi)^2 + V(\varphi),
    \end{equation}
    which is nonnegative so long as \(V(\varphi) \ge 0\).
    The momentum density is
    \begin{equation}
        T_{0i} = P_i = \dot{\varphi} \partial_i \varphi.
    \end{equation}

    \subsection{Spontaneous Symmetry Breaking}
    Consider the potential
    \begin{equation}
        V(\varphi) = \frac{1}{2}m^2 \varphi^2 + \frac{\lambda}{4!}\varphi^4.
    \end{equation}
    This has a \(\cyclicGroupZ[2]\) symmetry, sending \(\varphi \mapsto -\varphi\) doesn't change anything.
    
    \begin{figure}
        \subcaptionbox{Quadratic Potential}[0.45\textwidth]{
            \tikzsetnextfilename{quadratic-potential}
            \begin{tikzpicture}
                \draw[very thick, ->] (-3, 0) -- (3, 0) node [right] {\(\varphi\)};
                \draw[very thick, ->] (0, 0) -- (0, 3) node [above] {\(V\)};
                \clip (-3, 0) rectangle (3, 3);
                \draw[ultra thick, highlight, domain=-3:3] plot (\x, \x*\x/3);
            \end{tikzpicture}
        }
        \subcaptionbox{Quartic Potential}[0.45\textwidth]{
            \tikzsetnextfilename{quartic-potential}
            \begin{tikzpicture}
                \draw[very thick, ->] (-3, 0) -- (3, 0) node [right] {\(\varphi\)};
                \draw[very thick, ->] (0, 0) -- (0, 3) node [above] {\(V\)};
                \clip (-3, 0) rectangle (3, 3);
                \draw[ultra thick, highlight, domain=-3:3, samples=1000] plot (\x, {0.6*\x*\x*\x*\x - 1.6*\x*\x + 1.067});
            \end{tikzpicture}
        }
        \caption[Potentials and their symmetries]{The potentials \(V(\varphi) = m^2\varphi^2/2\) and \(V(\varphi) = m^2\varphi^2/2 + \lambda \varphi^4/4!\) with \(m^2 < 0\) and \(\lambda > 0\). Notice that the quadratic potential has one minimum and the quartic two, related by \(\varphi \mapsto -\varphi\).}
        \label{fig:potentials}
    \end{figure}
    
    \Cref{fig:potentials} shows \(V(\varphi)\) with \(\lambda = 0\) and with \(\lambda > 0\).
    In the first case we have a single minimum at \(\varphi = 0\), in the second we have two minima at \(\pm\varphi^*\).
    
    This demonstrates an important idea called \defineindex{spontaneous symmetry breaking}, where the \(\cyclicGroupZ[2]\) symmetry is spontaneously broken by introducing two non-equivalent minima when we take \(\lambda \ne 0\).
    
    \section{Complex Scalar Field}
    If \(\varphi\) is instead a complex scalar field then we can write it as
    \begin{equation}
        \varphi = \frac{1}{\sqrt{2}} (\varphi_1 + i\varphi_2)
    \end{equation}
    where \(\varphi_i\) are real scalar fields.
    We can then treat these as two independent real scalar fields with the Lagrangian
    \begin{equation}
        \lagrangianDensity = \frac{1}{2} (\partial_\mu \varphi_1) (\partial^\mu \varphi_2) - V((\varphi_1^2 + \varphi_2^2)/2),
    \end{equation}
    where we assume that the potential depends only on the magnitude of \(\varphi\) for simplicity.
    We can rewrite this Lagrangian as
    \begin{equation}
        \lagrangianDensity = (\partial_\mu \varphi^*)(\partial^\mu \varphi) - V(\varphi^*\varphi),
    \end{equation}
    which looks very similar to the Lagrangian for a real field.
    Note that we treat \(\varphi\) and \(\varphi^*\) as independent fields here.
    
    Notice that, regardless of the form \(V(\varphi^*\varphi)\), we have a global \(\unitary(1)\) symmetry, sending \(\varphi \mapsto \e^{i\vartheta}\varphi\) for \(\vartheta \in [0, 2\pi)\) doesn't change anything, since in doing so we send \(\varphi^* \mapsto (\e^{i\vartheta}\varphi)^* = \e^{-i\vartheta}\varphi^*\).
    
    If \(V(\varphi^*\varphi) = m^2 \varphi^*\varphi + \lambda(\varphi^*\varphi)^2/2\) then the field equations are
    \begin{equation}
        (\dalembertian + m^2)\varphi = -\lambda \varphi(\varphi^* \varphi), \qqand (\dalembertian + m^2) \varphi^* = -\lambda \varphi^*(\varphi\varphi^*).
    \end{equation}
    This is the famous \enquote{sombrero potential}, plotted in \cref{fig:sombrero potential}.
    
    \begin{figure}
        \tikzsetnextfilename{sombrero-potential}
        \begin{tikzpicture}
            \begin{axis}[
                hide axis,
                samples=50,
                domain=0:360,
                y domain=0:1.25
                ]
                \addplot3 [surf, shader=flat, draw=highlight, fill=highlight!20, z buffer=sort] ({sin(x)*y}, {cos(x)*y}, {(y^2-1)^2});
            \end{axis}
        \end{tikzpicture}
        \caption{The \enquote{sombrero potential}, \(V(\varphi^*\varphi) = m^2\varphi^*\varphi + \lambda(\varphi^*\varphi)^2/2\) for \(m^2 < 0\) and \(\lambda > 0\).}
        \label{fig:sombrero potential}
    \end{figure}
    
    The \(\unitary(1)\) symmetry corresponds to rotating around the origin.
    This symmetry is spontaneously broken by a quartic term with \(\lambda \ne 0\), since we now have a continuum of ground states lying in a circle.
    
    The Noether current for a complex field is simply the sum of the Noether currents for the two independent parts, either \(\varphi_1\) and \(\varphi_2\) or \(\varphi\) and \(\varphi^*\).
    The symmetry under which this current is conserved is the \(\unitary(1)\) symmetry, and so \(\varphi \mapsto \e^{i\vartheta}\varphi \approx (1 + i\vartheta)\varphi\), so \(\delta\varphi = i\vartheta \varphi\), and similarly \(\delta\varphi^* = -i\vartheta\varphi\).
    Thus, the Noether current is, up to an unimportant overall constant factor,
    \begin{align}
        J^\mu &= \diffp{\lagrangianDensity}{(\partial_\mu \varphi)} \delta\varphi + \diffp{\lagrangianDensity}{(\partial_\mu\varphi^*)}\delta\varphi^*\\
        &\propto i\varphi\partial^\mu \varphi^* - i\varphi^*\partial^\mu \varphi,
    \end{align}
    where we scale out the factor of \(\vartheta\).
    Notice that \(J^\mu\) is real, since \((i\varphi\partial^\mu \varphi^*)^* = -i\varphi^*\partial^\mu\varphi\).
    
    We can directly connect this conserved current to the conserved current of electromagnetism\footnote{See \course{Quantum Field Theory}.}.
    The complex scalar field then represents a electrically charged scalar particle.
    This has many interesting applications, such as the existence of antiparticles, and of course all of electromagnetism, if you like that sort of thing.
    
    \section{Spinor Fields}
    Consider a spinor, \(\psi(x)\).
    The action in this case is
    \begin{equation}
        S = \int \dl{^4x} \, \diracadjoint{\psi}(i\slashed{\partial} - m)\psi.
    \end{equation}
    We must have \([m\diracadjoint{\psi}\psi] = [\text{mass}]^4\), so \([\diracadjoint{\psi}] = [\psi] = [\text{mass}]^{3/2}\).
    
    The equation of motion for the spinor field is the Dirac equation:
    \begin{equation}
        (i\slashed{\partial} - m)\psi = 0.
    \end{equation}
    
    Classically spinors are defined to be anticommuting variables, known as Grassmann variables, meaning \(\psi(x)\psi(y) = -\psi(y)\psi(x)\).
    An important question is how we take the complex conjugate of such variables, there are two options.
    We could treat it as a normal complex conjugate, not changing the order, or we could treat it as more like a Hermitian conjugate, where the order of the terms flips.
    Both choices are valid, but distinct.
    We follow the more common option of having the complex conjugate swap the order of the terms, so \([\psi(x)\psi(y)]^* = \psi(y)^* \psi(x)^*\).
    We will also sometimes write \(\dagger\) in the place of \(*\) for this reason.
    
    The Dirac spinor representation of the Lorentz algebra, \((\slashfrac{1}{2}, 0) \oplus (0, \slashfrac{1}{2})\), splits into two irreducible representations, left handed, \((\slashfrac{1}{2}, 0)\), and right handed, \((0, \slashfrac{1}{2})\).
    This is done using the fifth gamma matrix, \(\gamma^5 = i\gamma^0\gamma^1\gamma^2\gamma^3\), which anticommutes with \(\gamma^\mu\), and hence with the generators of the spinor representations of the Lorentz algebra.
    If \(\gamma^5\) has eigenvalue \(+1\) we call the spinor left handed and if it has eigenvalue \(-1\) we call it right handed.
    A four component spinor, \(\psi \in (\slashfrac{1}{2}, 0) \oplus (0, \slashfrac{1}{2})\) can then be written as
    \begin{equation}
        \psi = 
        \begin{pmatrix}
            \psi_{\Left}\\ \psi_{\Right}
        \end{pmatrix}
    \end{equation}
    where \(\psi_{\Left} \in (\slashfrac{1}{2}, 0)\) and \(\psi_{\Right} \in (0, \slashfrac{1}{2})\).
    From now on we work only with Dirac spinors, with the two component Weyl spinors becoming four component Dirac spinors with two zero components:
    \begin{equation}
        \psi_{\Left} \to 
        \begin{pmatrix}
            \psi_{\Left}\\ 0
        \end{pmatrix}
        , \qqand \psi_{\Right} \to 
        \begin{pmatrix}
            0\\ \psi_{\Right}
        \end{pmatrix}
        .
    \end{equation}
    
    We can write an action for a left or right-handed fermion:
    \begin{equation}
        S_{\Left} = \int \dl{^4x} \, \diracadjoint{\psi}_{\Left} i\slashed{\partial} \psi_{\Left}, \qqand S_{\Right} = \int \dl{^4x} \, \diracadjoint{\psi}_{\Right} i\slashed{\partial} \psi_{\Right}
    \end{equation}
    For a single left handed fermion we cannot have a mass term since we can't form a (nonzero) scalar from \(\diracadjoint{\psi}_{\Left}\) and \(\psi_{\Left}\):
    \begin{align}
        \diracadjoint{\psi}_{\Left}\psi_{\Left} &= \psi_{\Left}^* \gamma^0 \frac{1}{2}(1 + \gamma^5) \psi\\
        &= \psi_{\Left}^*\frac{1}{2}(1 - \gamma^5)\gamma^0\\
        &= 0
    \end{align}
    since
    \begin{equation}
        \frac{1}{2}(1 - \gamma^5)\psi_{\Left} = P_{\Right}\psi_{\Left} = 0 \implies \psi_{\Left}^* \frac{1}{2}(1 - \gamma^5) = 0.
    \end{equation}
    
    This is why fermions in the standard model are so light.
    To first approximation they should be massless.
    While \(\diracadjoint{\psi}_{\Left}\psi_{\Right} \ne 0\), which could allow for a mass term, this term is not gauge invariant in the standard model, so we can't include it in the Lagrangian.
    The solution is to introduce a new scalar field, \(H\), such that we get a gauge invariant interaction \(\diracadjoint{\psi}_{\Left} \psi_{\Right} H\), which we can interpret as a mass term.
    This \(H\) is the \defineindex{Higgs field}.
    
    \subsection{Conserved Currents}
    The action
    \begin{equation}
        S = \int \dl{^4x} \, \diracadjoint{\psi} i\slashed{\partial}\psi
    \end{equation}
    has a \(\unitary(1)\) symmetry, \(\psi \mapsto \e^{i\alpha}\psi\).
    The conserved current is
    \begin{equation}
        J^\mu = \diffp{\lagrangianDensity}{(\partial_\mu \psi)}\delta\psi + \diffp{\lagrangianDensity}{(\partial_\mu \psi^*)}\delta\psi^*.
    \end{equation}
    Expanding the exponentials gives \(\delta\psi = i\alpha\psi\) and \(\delta\psi^* = -i\alpha\psi\).
    The derivatives are
    \begin{equation}
        \diffp{\lagrangianDensity}{(\partial_\mu \psi)} = \diracadjoint{\psi}\gamma^\mu, \qqand \diffp{\lagrangianDensity}{(\partial_\mu \psi^*)} = 0,
    \end{equation}
    so the conserved current is, up to an overall factor
    \begin{equation}
        J^\mu = \diracadjoint{\psi} \gamma^\mu \psi = \diracadjoint{\psi}_{\Left} \gamma^\mu \psi_{\Left} + \diracadjoint{\psi}_{\Right}\gamma^\mu \psi_{\Right}.
    \end{equation}
    This symmetry becomes the \(\unitary(1)\) of electromagnetism giving us charged fermions.
    
    There is another \(\unitary(1)\) symmetry, given by \(\psi \mapsto \e^{i\alpha \gamma^5}\psi\) and so \(\psi^* \mapsto (\e^{i\alpha \gamma^5}\psi)^* = \psi^* \e^{-i\alpha\gamma^5}\), this is a symmetry since
    \begin{align}
        \diracadjoint{\psi} i \slashed{\partial} \psi &\mapsto \psi^* \e^{-i\alpha \gamma^5} \gamma^0 i\slashed{\partial} \e^{i\alpha\gamma^5} \psi\\
        &= -\psi^* \gamma^0 \e^{-i\alpha \gamma^5} i\slashed{\partial} \e^{i\alpha\gamma^5} \psi\\
        &= \diracadjoint{\psi} i\slashed{\partial} \e^{-i\alpha\gamma^5} \e^{i\alpha\gamma^5}\psi\\
        &= \diracadjoint{\psi} i\slashed{\partial} \psi.
    \end{align}
    This symmetry has a conserved axial vector current
    \begin{equation}
        j_5^\mu = \diracadjoint{\psi} \gamma^5 \gamma^\mu \psi = \diracadjoint{\psi}_{\Left}\gamma^\mu \psi_{\Left} - \diracadjoint{\psi}_{\Right} \gamma^\mu \psi_{\Right}.
    \end{equation}
    
    Notice that including a mass term, \(m\diracadjoint{\psi}\psi\) preserves the original \(\unitary(1)\) symmetry, however the second \(\unitary(1)\) symmetry is nor preserved in this case since
    \begin{equation}
        \diracadjoint{\psi} \psi \mapsto \psi^* \e^{-i\alpha\gamma^5} \gamma^0 \e^{i\alpha\gamma^5} \psi = -\psi^* \gamma^0 \e^{-i\alpha\gamma^5} \e^{i\alpha\gamma^5} \psi = -\diracadjoint{\psi}\psi
    \end{equation}
    This means that for massive fermions \(j_5\) is not a conserved current, and instead it can be shown that
    \begin{equation}
        \partial_\mu j_5^\mu = -2im\diracadjoint{\psi} \gamma^5 \psi \ne 0.
    \end{equation}
    We say that the axial symmetry is \define{softly broken}\index{softly broken symmetry} since in the high energy regime, where \(E \gg m\), the symmetry breaking is a very small effect.
    
    There is no renormalisable self-interaction between spinors in 4 dimensions.
    It is possible to have a renormalisable interaction with two scalar fields, \(\sigma\) and \(\pi\), with the interaction term
    \begin{equation}
        g \sigma \diracadjoint{\psi}\diracadjoint{\psi} + ig' \pi \diracadjoint{\psi} \gamma^5 \psi
    \end{equation}
    for some dimensionless constants \(g\) and \(g'\).
    This interaction can be used to define an effective Lagrangian for an effective field theory corresponding to interactions within protons and neutrons.
    If \(g = g'\) then chiral symmetry is preserved so long as the fields transform as
    \begin{equation}
        \delta\psi = i\beta\gamma^5\psi, \qquad \delta \sigma = 2\beta \pi, \qqand \delta \pi = -2\beta \sigma
    \end{equation}
    for some infinitesimal \(\beta\).
    The second two transformation laws can be interpreted as the chiral symmetry acting as \(\specialOrthogonal(2)\) on
    \begin{equation}
        \begin{pmatrix}
            \sigma\\ \pi
        \end{pmatrix}
    \end{equation}
    rotating \(\sigma\) into \(\pi\) and vice versa while preserving \(\sigma^2 + \pi^2\).
    This is a continuous symmetry of this action, and has interesting consequences in nuclear physics.
    It arises from the symmetries in QCD when quark masses are small.
    
    \part{Compact Lie Groups}
    \chapter{Compact Lie Groups}
    \section{Cartan Decomposition}
    Let \(\lie{g}\) be a Lie algebra which isn't simple, let \(\lie{s} \subset \lie{g}\) be a proper invariant subalgebra of \(\lie{g}\).
    Using the metric \(g_{ab}\) we can define a subspace, \(\lie{p}\), orthogonal to \(\lie{s}\).
    Using indices \(i\), and \(j\) for elements of \(\lie{s}\) and indices \(p\) and \(q\) for elements of \(\lie{p}\) we have, by definition
    \begin{equation}
        g_{ip} = \tr(A_iA_p) = 0
    \end{equation}
    for all \(A_i \in \lie{s}\) and \(A_p \in \lie{p}\) in the adjoint representation.
    We also have \(\tr(A_i \commutator{A_j}{A_p}) = \tr(\commutator{A_i}{A_j} A_p) = 0\) since as an invariant subalgebra we have \(\commutator{A_i}{A_j} \in \lie{s}\) for all \(A_i, A_j \in \lie{s}\).
    This means that \(\commutator{A_j}{A_p} \in \lie{p}\), for all \(A_j \in \lie{s}\) and \(A_p \in \lie{p}\).
    That is, \(\commutator{\lie{s}}{\lie{p}} \subseteq \lie{p}\).
    Since \(\lie{s}\) is an invariant subalgebra by definition \(\commutator{\lie{s}}{\lie{p}} \subseteq \lie{s}\).
    By construction \(\lie{s} \cap \lie{p} = \{0\}\), and so \(\commutator{\lie{s}}{\lie{p}} = \{0\}\), that is \(\commutator{A_i}{A_p} = 0\) for all \(A_i \in \lie{s}\) and \(A_p \in \lie{p}\).
    So all elements of \(\lie{s}\) commute with all elements of \(\lie{p}\).
    
    We say that \(\lie{g}\) decomposes into a \defineindex{direct sum}: \(\lie{g} = \lie{s} \oplus \lie{p}\).
    We can show that \(\lie{p}\) is also a subalgebra of \(\lie{g}\) (so far we have only assumed it is a subspace, we now show it's closed under the Lie bracket).
    We know that \(\tr(\commutator{A_p}{A_q}A_i) = \tr(A_p\commutator{A_q}{A_i}) = 0\) since \(\commutator{A_q}{A_i} = 0\) and so we have \(\commutator{\lie{p}}{\lie{p}} = \lie{p}\).
    We can now prove the following theorem.
    
    \begin{thm}{}{thm:semisimple lie algebra is direct sum of simple lie algebras}
        Let \(\lie{g}\) be a semisimple Lie algebra.
        Then \(\lie{g}\) is a direct sum of mutually commuting\footnote{mutually commuting is just what it means to be combined through a direct sum} simple Lie algebras \(\lie{g}_i\):
        \begin{equation}
            \lie{g} = \lie{g}_1 \oplus \lie{g}_2 \oplus \dotsb \oplus \lie{g}_n = \bigoplus_{i=1}^n \lie{g}_i.
        \end{equation}
        \begin{proof}
            If \(\lie{g}\) is simple then it is trivially a sum of simple Lie algebras, so suppose \(\lie{g}\) is not simple.
            Then by definition it has a proper invariant subalgebra, \(\lie{s}\), and the construction above allows us to construct an orthogonal subalgebra, \(\lie{p}\), such that \(\lie{g} = \lie{s} \oplus \lie{p}\).
            Both \(\lie{s}\) and \(\lie{p}\) must be semisimple, since any subalgebra of \(\lie{s}\) or \(\lie{p}\) is also a subalgebra of \(\lie{g}\), which is assumed to be semisimple.
            If either \(\lie{s}\) or \(\lie{p}\) is not simple simply repeat the procedure, decomposing it into a sum of two invariant subalgebras.
            Since \(\lie{g}\) is finite dimensional this procedure must eventually terminate since the dimension of a direct sum is the sum of the dimension of the terms in the sum.
        \end{proof}
    \end{thm}
    
    We've seen direct sums already, but this is a more formal treatment.
    This is called the internal definition of the direct sum, we start with some Lie algebra, \(\lie{g}\), and work entirely inside of \(\lie{g}\).
    The external definition on the other hand starts with \(\lie{s}\) and \(\lie{p}\) and details a construction for combining elements of \(\lie{s}\) and \(\lie{p}\) to form a new Lie algebra, which we call \(\lie{g}\).
    
    Such a decomposition of a Lie algebra induces a decomposition of a semisimple group, if \(G\) is a Lie group with semisimple Lie algebra \(\lie{g} = \lie{g}_1 \oplus \dotsb \oplus \lie{g}_n\) then
    \begin{equation}
        G \isomorphic (G_1 \otimes G_2 \otimes \dotsb \otimes G_n) / Z(G)
    \end{equation}
    where \(G_i\) are Lie groups with Lie algebras \(\lie{g}_i\) and
    \begin{equation}
        Z(G) \coloneqq \{h \mid hg = gh \forall g \in G\}
    \end{equation}
    is the \defineindex{centre} of \(G\), defined as the set of all elements commuting with all other elements of \(G\).
    Note that \(Z(G)\) must be discrete, otherwise it could be used to define an Abelian subalgebra of \(\lie{g}\), but \(\lie{g}\) is semismiple.
    
    We can think of the exponential map \(\exp \colon \lie{g} \to G\) as turning addition into multiplication, just as it does for numbers, but we have to account for the elements which commute and therefore get double counted.
    
    \begin{exm}{}{}
        On the Lie algebra level
        \begin{equation}
            \specialOrthogonalLie(4) \isomorphic \specialUnitaryLie(2) \oplus \specialUnitaryLie(2),
        \end{equation}
        and on the Lie group level
        \begin{equation}
            \specialOrthogonal(4) \isomorphic (\specialUnitary(2) \otimes \specialUnitary(2)) / \cyclicGroupZ[2].
        \end{equation}
    \end{exm}
    
    \section{Compact Groups}
    Let \(G\) be a compact Lie group.
    Then by Maschke's theorem (\cref{thm:maschke}) any representation is equivalent to a unitary representation, so we may choose to work solely with unitary representations.
    In terms of Lie algebras this means we can assume that the generators are all Hermitian.
    
    Consider the generators in the adjoint representation, which we assume are Hermitian, \(A_a = A_a^\hermit\).
    Then \(\tensor{(A_a)}{^b_c} = \tensor{(A_a^*)}{^c_b}\), and so we have
    \begin{equation}
        \tensor{(A_a)}{^b_c} = i\tensor{c}{^b_{ac}} = -i\tensor{c}{^c_{ab}} = (i\tensor{c}{^c_{ab}})^* = \tensor{(A_a^*)}{^c_b}.
    \end{equation}
    We then have
    \begin{equation}
        g_{ab} = \tr(A_a A_b) = \tensor{(A_a)}{^d_c} \tensor{(A_b)}{^c_d} = (-i\tensor{c}{^c_{ad}}) i\tensor{c}{^c_{bd}} = \tensor{c}{^c_{ad}} \tensor{c}{^c_{bd}}.
    \end{equation}
    
    \begin{thm}{}{}
        If \(G\) is a compact Lie group then its Lie algebra, \(\lie{g}\), can be decomposed as
        \begin{equation}
            \lie{g} = \lie{g}_0 \oplus \lie{g}_1 \oplus \dotsb \lie{g}_n
        \end{equation}
        where \(\lie{g}_0\) is Abelian, and hence \(\lie{g}_0 \isomorphic \unitaryLie(1) \oplus \dotsb \oplus \unitaryLie(1)\), \(\lie{g}_i\) for \(i \ne 0\) are simple.
        \begin{proof}
            If \(\lie{g}\) is not semisimple then it has a proper Abelian invariant subalgebra, \(\lie{s} \subset \lie{g}\), and \(g_{ia} = \tr(A_i A_a) = 0\) for all \(A_i \in \lie{s}\) and \(A_a \in \lie{l}\), as we've seen previously.
            
            We've shown that for Hermitian \(A_a\) we have \(g_{ia} = \tensor{c}{^b_{ic}} \tensor{c}{^b_{ac}}\), and so \(g_{ii} = (\tensor{c}{^b_{ic}}^2) = 0\) (no sum on \(i\)).
            Hence, \(\tensor{c}{^b_{ic}} = 0\) for all \(i\), \(b\) and \(c\), meaning \(\commutator{\lie{s}}{\lie{g}} = \{0\}\).
            It then follows that \(\lie{g} = \lie{g}_0 \oplus \lie{g}'\) where \(\lie{g}_0 = \lie{s}\) is Abelian and \(\lie{g}'\) is semisimple.
            Then by \cref{thm:semisimple lie algebra is direct sum of simple lie algebras} we can decompose \(\lie{g}'\) into a direct sum of simple Lie algebras completing the proof.
        \end{proof}
    \end{thm}
    
    This means that we can restrict ourselves to studying only semisimple, or even simple, compact groups, since we can always decompose a compact Lie group into a product of semisimple or simple Lie groups.
    Note that this decomposition fails in the general case as it relies on the Hermiticity of the generators, which is not guaranteed.
    
    \begin{thm}{}{}
        If \(G\) is a compact Lie group then the Killing form, \(g_{ab}\), is positive semidefinite (has nonnegative eigenvalues).
        If \(\lie{g}\), the Lie algebra of \(G\), is semisimple we can choose a basis in which \(g_{ab} = \delta_{ab}\) and we can then ignore the distinction between upper and lower indices.
        \begin{proof}
            Take some real vector, \(x^a\), then
            \begin{equation}
                g_{ab}x^a x^b = \tensor{c}{^c_{ad}} \tensor{c}{^c_{bd}} x^a x^b = (\tensor{c}{^c_{ad}}x^a)^2 \ge 0.
            \end{equation}
            Hence all eigenvalues of \(g_{ab}\) are nonnegative, so by reordering and rescaling the generators we can write \(g_{ab}\) as \(g_{ab} = \diag(1, \dotsc, 1, 0, \dotsc, 0)\).
            
            If \(\lie{g}\) is semisimple then \(\det g_{ab} \ne 0\) and so \(g_{ab} x^a x^b \ge 0\) for all real nonzero vectors, \(x^a\), hence all eigenvalues are strictly positive.
            We can rescale the generators such that \(g_{ab} = \diag(1, \dotsc, 1)\).
        \end{proof}
    \end{thm}
    
    \chapter{Cartan Subalgebra}
    \section{The Cartan Subalgebra}
    \begin{dfn}{Cartan Subalgebra}{}
        Let \(\lie{g}\) be a semisimple Lie algebra.
        The \defineindex{Cartan subalgebra}, \(\lie{c} \subseteq \lie{g}\), is the subalgebra generated by the maximal set of commuting generators.
    \end{dfn}
    
    \begin{rmk}
        The definition of a Cartan subalgebra does not uniquely define \emph{a} Cartan subalgebra, instead we may have multiple Cartan subalgebras, however for a finite dimensional Lie algebra over an algebraically closed field of characteristic zero (such as \(\complex\)) all Cartan subalgebras are isomorphic.
    \end{rmk}
    
    Denote by \(H_i\) the generators of the Cartan subalgebra, \(\lie{c}\).
    These are also generators of \(\lie{g}\).
    Denote by \(E_m\) the generators of \(\lie{g}\) which are not in \(\lie{c}\) such that \(\{H_i, E_m\}\) generates \(\lie{g}\).
    If \(r = \rank \lie{g}\) is the rank of \(\lie{g}\) and \(d = \dim \lie{g}\) is the dimension of \(\lie{g}\) then there are \(d \) linearly indepenent commuting generators, so \(\dim\lie{c} = r\), and there are \(d - r\) generators \(E_m\).
    
    Since \(H_i, E_m \in \lie{g}\) we can take their commutator, getting another element of \(\lie{g}\), which we can expand as usual in terms of the structure constants:
    \begin{equation}
        \commutator{H_i}{E_m} = ic_{kim}T_k
    \end{equation}
    where we use the fact that \(\lie{g}\) is semisimple to lower all indices.
    We can then write this as
    \begin{equation}
        \commutator{H_i}{E_m} = ic_{jim}H_j + ic_{nim}E_n
    \end{equation}
    where \(j\) runs from 1 to \(r\) indexing the generators of \(\lie{c}\), and \(n\) runs from \(r + 1\) to \(d\) indexing the generators of \(\lie{g}\) not in \(\lie{c}\).
    We then have
    \begin{equation}
        c_{jim} = c_{mji} \propto \commutator{H_j}{H_i} = 0
    \end{equation}
    and so we have
    \begin{equation}
        \commutator{H_i}{E_m} = ic_{jim}E_j.
    \end{equation}
    
    We've proved that \(c_{nim}\) is totally antisymmetric.
    Then \((A_i)_{nm} = ic_{nim}\) defines a Hermitian matrix, \(A_i\).
    These Hermitian matrices are just the generators \(H_i\) in the adjoint representation, and so all commute by the definition of the Cartan subalgebra\footnote{depending on what values we allow \(n\) and \(m\) to take these are either the \((d-r)\)-dimensional representation of \(\lie{c}\) (\(1 \le n, m \le d - r\)) or the \(d\)-dimensional representation of \(\lie{g}\) (\(1 \le n, m \le d\)).}.
    This means that we can simultaneously diagonalise all of these matrices.
    Let \(\lambda_i^\alpha \eqqcolon \alpha_i\) be the eigenvalues of \(A_i\).
    The associated eigenvector is \(E_\alpha\), which is one of these generators of \(\lie{g}\) not in the generators of \(\lie{c}\).
    That is, \(\alpha_i\) is the \(\alpha\)th eigenvalue of the \(i\)th mutually commuting generator.
    We can then interpret \(\alpha\) as vectors in \(\reals^r\).
    We call them the \define{root vectors}\index{root vector} of the Lie algebra, or simply the roots.
    
    In this diagonalised basis
    \begin{equation}
        \commutator{H_i}{E_\alpha} = \alpha_i E_\alpha.
    \end{equation}
    Note that \(\abs{\alpha} \ne 0\) since \(E_\alpha\) doesn't commute with \(H_i\), else it would be in the Cartan subalgebra.
    We call \(E_\alpha\) the step operator for the root \(\alpha\).
    
    The \(E_\alpha\) are not Hermitian.
    Taking the Hermitian conjugate of the above equation we have
    \begin{equation}
        \commutator{H_i}{E_\alpha}^\hermit = -\commutator{H_i}{E_\alpha^\hermit} = \alpha_iE_\alpha^\hermit.
    \end{equation}
    Hence, \(E_\alpha^\hermit = E_{-\alpha}\).
    This implies that \(d - r\) is even, since the generators of \(\lie{g}\) not in \(\lie{c}\) must pair up into \(E_{\pm \alpha}\), and \(\alpha \ne 0\).
    
    For example, consider \(\specialUnitaryLie(2)\).
    Here the Cartan subalgebra can be taken as the span of \(T_3 = H\), and \(T_{\pm} = E_{\pm \alpha}\) are the remaining generators, and we know that \((T_+)^\hermit = T_-\).
    
    Consider the following commutator, but note that \(E_\alpha E_\beta\) is not (necessarily) an element of the Lie algebra, since it involves a matrix product of generators rather than a Lie bracket:
    \begin{align}
        \commutator{H_i}{E_\alpha E_\beta} &= \commutator{H_i}{E_\alpha}E_\beta + E_\alpha \commutator{H_i}{E_\beta}\\
        &= \alpha_i E_\alpha E_\beta + \beta_i E_\alpha E_\beta\\
        &= (\alpha_i + \beta_i) E_\alpha E_\beta.
    \end{align}
    Take the trace of this equation using the fact that the trace of any commutator vanishes we have
    \begin{equation}
        (\alpha_i + \beta_i)\tr(E_\alpha E_\beta) = 0,
    \end{equation}
    and so either \(\alpha + \beta = 0\) or \(\tr(E_\alpha E_\beta) = 0\).
    If \(\alpha + \beta = 0\) then
    \begin{equation*}
        \tr(E_\alpha E_\beta) = \tr(E_\alpha E_{-\alpha}) = \tr(E_\alpha E_\alpha^\hermit) = (E_\alpha)_{ij} (E_\alpha^\hermit)_{ji} = (E_\alpha)_{ij}(E_\alpha^*)_{ij} = \sum_{i,j} \abs{(E_\alpha)_{ij}}^2 > 0
    \end{equation*}
    so the trace is nonvanishing.
    We can rescale the generators such that \(\tr(E_\alpha E_{-\alpha}) = 1\) for all roots \(\alpha\).
    We can likewise choose to have \(\tr(H_iH_j) = \delta_{ij}\).
    Finally, \(\tr(H_i E_\alpha) = 0\) since \(\tr(H_i\commutator{H_j}{E_\alpha}) = \alpha_j\tr(H_iE_\alpha)\) and \(\tr(H_i\commutator{H_j}{E_\alpha}) = \tr(\commutator{H_i}{H_j}E_\alpha) = 0\).
    
    So if \(\alpha + \beta \ne 0\) then \(\commutator{E_\alpha}{E_\beta}\) is proportional to \(E_{\alpha + \beta}\), and if \(\alpha + \beta = 0\) then \(\commutator{E_\alpha}{E_\beta} = x_i H_i\) for some \(x\) since we must have \(\commutator{E_\alpha}{E_\beta}\) commute with all \(H_i\) and so \(\commutator{E_\alpha}{E_\beta}\) is a linear combination of \(H_i\).
    
    From this we define the \defineindex{Cartan--Weyl basis} for a semisimple Lie algebra, \(\lie{g}\), as the basis \(\{H_i, E_\alpha\}\) such that
    \begin{align}
        \commutator{H_i}{H_j} = 0, \qquad \commutator{H_i}{E_{\pm\alpha}} = \pm \alpha_i E_{\pm \alpha}, \qquad \commutator{E_\alpha}{E_{-\alpha}} = \alpha_i H_i,
    \end{align}
    and
    \begin{equation}
        \commutator{E_\alpha}{E_\beta} = 
        \begin{cases}
            N_{\alpha\beta} E_{\alpha + \beta} & \text{if } \alpha + \beta \text{ is a root},\\
            0 & \text{otherwise},
        \end{cases}
    \end{equation}
    where \(N_{\alpha\beta}\) is some constant, and we choose to normalise these generators such that
    \begin{equation}
        \tr(H_i H_j) = \delta_{ij}, \quad \tr(E_\alpha E_\beta) = \delta_{\alpha + \beta, 0}, \qand \tr(H_i E_\alpha) = 0.
    \end{equation}
    
    \section{Chevalley Basis}
    Define the coroots
    \begin{equation}
        \alpha_i^{\vee} \coloneqq \frac{2\alpha_i}{\alpha^2}
    \end{equation}
    where \(\alpha^2 = \alpha \cdot \alpha\) is the usual Euclidean inner product of \(\alpha\) with itself.
    Then the \defineindex{Chevalley basis} is defined as
    \begin{align}
        H_\alpha^{\symrm{C}} &= \alpha_i^{\vee} H_i^{\symrm{CW}} = \frac{2\alpha_i H_i^{\symrm{CW}}}{\alpha^2},\\
        E_\alpha^{\symrm{C}} &= \sqrt{\frac{2}{\alpha^2}} E_\alpha^{\symrm{CW}}
    \end{align}
    where \(\symrm{C}\) stands for the Chevalley basis and \(\symrm{CW}\) the Cartan--Weyl basis.
    
    From this definition it follows that
    \begin{equation}
        \tr(E_\alpha^{\symrm{C}} E_{-\alpha}^{\symrm{C}}) = \frac{2}{\alpha^2}, \quad \commutator{H_\alpha^{\symrm{C}}}{E_{\pm\alpha}^{\symrm{C}}} = \pm 2 E_{\pm \alpha}^{\symrm{C}}, \qand \commutator{E_{\alpha}^{\symrm{C}}}{E_{-\alpha}^{\symrm{C}}} = H_\alpha^{\symrm{C}}.
    \end{equation}
    
    For example, consider \(\specialUnitaryLie(2)\), in the Chevalley basis \(H^{\symrm{C}} = 2T_3\) and \(E_{\pm}^{\symrm{C}} = T_{\pm}\) since we have \(\commutator{2T_3}{T_{\pm}} = \pm 2 T_{\pm}\) and \(\commutator{T_{+}}{T_{-}} = 2T_3\).
    
    Recall that \(2T_3\) has integer eigenvalues (since \(T_3\) has half integer eigenvalues), and so we expect that \(H_\alpha^{\symrm{C}}\) has integer eigenvalues also, and we call these the \define{weights}\index{weight}.
    This holds in any irreducible representation.
    
    In any representation we have \(H_i^{\symrm{CW}} \ket{\lambda} = \lambda_i \ket{\lambda}\) and hence
    \begin{equation}
        H_\alpha^{\symrm{C}} \ket{\lambda} = \frac{2 \alpha \cdot \lambda}{\alpha^2} \ket{\lambda}.
    \end{equation}
    This implies that
    \begin{equation}
        \frac{2\alpha \cdot \lambda}{\alpha^2} \in \integers
    \end{equation}
    for all roots \(\alpha\) and weights \(\lambda\) in any representation.
    
    \section{Angles Between Roots}
    \begin{rmk}
        We now drop the \(\symrm{C}\) superscript and we work exclusively in the Chevalley basis.
    \end{rmk}
    
    Consider
    \begin{equation}
        \commutator{H_\alpha}{E_\beta} = \frac{2\alpha \cdot \beta}{\alpha^2} E_\beta
    \end{equation}
    comparing this to the \(\specialUnitaryLie(2)\) relations we can think of \(E_\beta\) as a step operator for \(H_\alpha\).
    Since \(H_\alpha\) has integer eigenvalues we know that \(2\alpha \cdot \beta/\alpha^2 \in \integers\).
    Similarly \(2\alpha \cdot \beta/\beta^2 \in \integers\) for all roots \(\alpha\) and \(\beta\).
    We then have
    \begin{equation}
        \frac{2\alpha \cdot \beta}{\alpha^2} \frac{2\alpha \cdot \beta}{\beta^2} = \frac{4(\alpha \cdot \beta)^2}{\alpha^2 \beta^2}
    \end{equation}
    but the Cauchy--Schwartz inequality tells us that
    \begin{equation}
        (\alpha \cdot \beta)^2 \le \alpha^2 \beta^2,
    \end{equation}
    and so we have
    \begin{equation}
        \frac{2\alpha \cdot \beta}{\alpha^2} \frac{2\alpha \cdot \beta}{\beta^2} = \frac{4(\alpha \cdot \beta)^2}{\alpha^2 \beta^2} \le 4.
    \end{equation}
    This quantity is also clearly positive, and each fraction on the left is an integer.
    This limits the values that these fractions can take to
    \begin{equation}
        \frac{\alpha \cdot \beta}{\alpha^2 \beta^2} \in \left\{ 0, \pm \frac{1}{2}, \pm 1, \pm \frac{3}{2}, \pm 2 \right\}.
    \end{equation}
    We also know that if \(\vartheta\) is the angle between \(\alpha\) and \(\beta\) then the possible values of \(\cos\vartheta\) are
    \begin{equation}
        \cos\vartheta = \frac{\alpha \cdot \beta}{\sqrt{\alpha^2 \beta^2}} \in \left\{ 0, \pm\frac{1}{2}, \pm\frac{1}{\sqrt{2}}, \pm\frac{\sqrt{3}}{2}, \pm 1 \right\},
    \end{equation}
    and hence the possible values of \(\vartheta\) are
    \begin{equation}
        \vartheta \in \left\{ 0, \pm \frac{\pi}{2}, \pm \frac{\pi}{3}, \pm\frac{2\pi}{3}, \pm\frac{\pi}{4}, \pm\frac{3\pi}{4}, \pm\frac{\pi}{6}, \pm\frac{5\pi}{6}, \pi \right\}.
    \end{equation}
    Clearly the \(\cos\vartheta = \pm 1\), \(\vartheta = 0, \pi\) case corresponds to \(\alpha = \pm \beta\).
    
    Using just these facts we can quite easily construct all root systems, and hence all Cartan subalgebras, of low rank semisimple Lie algebras.
    
    \section{Root Systems}
    \subsection{Rank 1}
    If \(\lie{g}\) is a rank 1 Lie algebra then it's Cartan subalgebra has just a single generator, \(H\), and there are two step operators, \(E_\alpha\) and \(E_{-\alpha}\), with two roots, \(\pm \alpha\).
    This is a one dimensional system.
    Graphically we can represent it as a root diagram:
    \begin{equation}
        \tikzsetnextfilename{root-system-A1}
        \begin{tikzpicture}[
            baseline = (alpha.base),
            >={Latex[width=1.5mm]}
            ]
            \draw[->] (0, 0) -- (1, 0) node (alpha) [right] {\(\alpha\)};
            \draw[->] (0, 0) -- (-1, 0) node [left] {\(-\alpha\)};
            \fill (0, 0) circle [radius = 0.05];
        \end{tikzpicture}
    \end{equation}
    This is the only rank 1 semisimple Lie algebra, so it corresponds to \(\specialUnitaryLie(2) \isomorphic \specialOrthogonalLie(3) \isomorphic \symplecticLie(2)\).
    In the notation we will shortly introduce this is known as \(A_1 \isomorphic B_1 \isomorphic C_1\).
    
    If we imagine a point somewhere on this line then applying \(E_\alpha\) moves a step to the right, adding \(\alpha\), and applying \(E_{-\alpha}\) moves a step to the left, adding \(-\alpha\).
    
    \subsection{Rank 2}
    If \(\lie{g}\) is a rank 2 Lie algebra then there are four possible Cartan subalgebras, each with two generators, \(H_1\) and \(H_2\).
    The first has orthogonal roots,
    \begin{equation}
        \tikzsetnextfilename{root-system-A1xA1}
        \begin{tikzpicture}[
            baseline = (alpha.base),
            >={Latex[width=1.5mm]}
            ]
            \draw[->] (0, 0) -- (1, 0) node (alpha) [right] {\(\alpha\)};
            \draw[->] (0, 0) -- (-1, 0) node [left] {\(-\alpha\)};
            \draw[->] (0, 0) -- (0, 1) node [above] {\(\beta\)};
            \draw[->] (0, 0) -- (0, -1) node [below] {\(\mathllap{-}\beta\)};
            \fill (0, 0) circle [radius = 0.05];
        \end{tikzpicture}
    \end{equation}
    To move right by one step in this diagram apply \(E_\alpha\), and to move up apply \(E_\beta\).
    This corresponds to the Lie algebras \(\specialUnitaryLie(2) \oplus \specialUnitaryLie(2) \isomorphic \specialOrthogonalLie(4)\), or in the new notation we will shortly define, \(A_1 \oplus A_1 \isomorphic D_2\).
    
    The second case has roots with an angle of \(\pi/3\):
    \begin{equation}
        \tikzsetnextfilename{root-systsem-A2}
        \begin{tikzpicture}[
            baseline = (alpha.base),
            >={Latex[width=1.5mm]}
            ]
            \draw[highlight!50] (1, 0) -- (60:1) -- (120:1) -- (180:1) -- (240:1) -- (300:1) -- cycle;
            \draw[->] (0, 0) -- (1, 0) node (alpha) [right] {\(\alpha\)};
            \draw[->] (0, 0) -- (-1, 0) node [left] {\(-\alpha\)};
            \draw[->] (0, 0) -- (60:1) node [above, xshift=0.4cm] {\(\alpha + \beta\)};
            \draw[->] (0, 0) -- (-60:1) node [below, xshift=0.1cm] {\(\mathllap{-}\beta\)};
            \draw[->] (0, 0) -- (120:1) node [above] {\(\beta\)};
            \draw[->] (0, 0) -- (-120:1) node [below, xshift=-0.6cm] {\(-\alpha - \beta\)};
            \fill (0, 0) circle [radius = 0.05];
        \end{tikzpicture}
    \end{equation}
    This corresponds to the Lie algebra \(\specialUnitaryLie(3)\), or in the new notation \(\specialUnitaryLie(3)\).
    
    The third case has roots with an angle of \(3\pi/4\), and now not all of the roots are the same length:
    \begin{equation}
        \tikzsetnextfilename{root-system-B2}
        \begin{tikzpicture}[
            baseline = (alpha.base),
            >={Latex[width=1.5mm]}
            ]
            \draw[highlight!50] (1, 1) -- (1, -1) -- (-1, -1) -- (-1, 1) -- cycle;
            \draw[->] (0, 0) -- (1, 0) node (alpha) [right] {\(\alpha\)};
            \draw[->] (0, 0) -- (-1, 0) node [left] {\(-\alpha\)};
            \draw[->] (0, 0) -- (0, 1) node [above] {\(\alpha + \beta\)};
            \draw[->] (0, 0) -- (0, -1) node [below] {\(\mathllap{-}\alpha - \beta\)};
            \draw[->] (0, 0) -- (1, 1) node [above right, shift={(-0.1, -0.1)}] {\(2\alpha + \beta\)};
            \draw[->] (0, 0) -- (1, -1) node [below right, shift={(-0.1, 0.1)}] {\(\mathllap{-}\beta\)};
            \draw[->] (0, 0) -- (-1, 1) node [above left, shift={(0.1, -0.1)}] {\(\beta\)};
            \draw[->] (0, 0) -- (-1, -1) node [below left, , shift={(0.1, 0.1)}] {\(-2\alpha - \beta\)};
            \fill (0, 0) circle [radius = 0.05];
        \end{tikzpicture}
    \end{equation}
    This corresponds to the Lie algebra \(\specialOrthogonalLie(5) \isomorphic \symplecticLie(4)\).
    
    \begin{dfn}{Symplectic}{}
        The noncompact \defineindex{symplectic group}, \(\symplectic(2N, \complex)\), is defined as the subgroup of \(\generalLinear(2N, \complex)\) preserving \(x_i\tilde{y}_i - \tilde{x}_iy_i\) for \(x_1, \dotsc, x_N, \tilde{x}_1, \dotsc, \tilde{x}_N), (y_1, \dotsc, y_N, \tilde{y}_1, \dotsc, \tilde{y}_N) \in \complex^{2N}\).
        
        The \defineindex{compact symplectic group}, \(\symplectic(2N)\) is defined as \(\symplectic(2N, \complex) \cap \specialUnitary(2N)\).
        
        The Lie algebra of \(\symplectic(2N)\) is \(\symplecticLie(2N)\).
    \end{dfn}
    In the new notation this root system is \(B_2 \isomorphic C_2\), although if we're thinking of \(C_2\) then instead we would draw the root system slightly rotated and swap the names of the roots, but it's the same (up to isomorphism), we're just preempting the start to two distinct patterns:
    \begin{equation}
        \tikzsetnextfilename{root-system-C2}
        \begin{tikzpicture}[
            baseline = (alpha.base),
            >={Latex[width=1.5mm]}
            ]
            \draw[highlight!50] (2, 0) -- (0, 2) -- (-2, 0) -- (0, -2) -- cycle;
            \draw[->] (0, 0) -- (2, 0) node (alpha) [right] {\(\alpha\)};
            \draw[->] (0, 0) -- (-2, 0) node [left] {\(-\alpha\)};
            \draw[->] (0, 0) -- (0, 2) node [above] {\(\alpha + \beta\)};
            \draw[->] (0, 0) -- (0, -2) node [below] {\(\mathllap{-}\alpha - \beta\)};
            \draw[->] (0, 0) -- (1, 1) node [above right, shift={(-0.1, -0.1)}] {\(2\alpha + \beta\)};
            \draw[->] (0, 0) -- (1, -1) node [below right, shift={(-0.1, 0.1)}] {\(\mathllap{-}\beta\)};
            \draw[->] (0, 0) -- (-1, 1) node [above left, shift={(0.1, -0.1)}] {\(\beta\)};
            \draw[->] (0, 0) -- (-1, -1) node [below left, , shift={(0.1, 0.1)}] {\(-2\alpha - \beta\)};
            \fill (0, 0) circle [radius = 0.05];
        \end{tikzpicture}
    \end{equation}
    In both of these the longer root vector is \(\sqrt{2}\) times longer than the shorter one.
    
    The fourth and final case has angle \(5\pi/6\) and the longer root vector is \(\sqrt{3}\) times the shorter one:
    \begin{equation}
        \tikzsetnextfilename{root-system-G2}
        \begin{tikzpicture}[
            baseline = (alpha.base),
            >={Latex[width=1.5mm]}
            ]
            \draw[->] (0, 0) -- (0:1) node (alpha) [right] {\(\alpha\)};
            \draw[->] (0, 0) -- (60:1);
            \draw[->] (0, 0) -- (120:1);
            \draw[->] (0, 0) -- (180:1);
            \draw[->] (0, 0) -- (240:1);
            \draw[->] (0, 0) -- (300:1);
            \draw[->] (0, 0) -- (36:{sqrt(3)});
            \draw[->] (0, 0) -- (-36:{sqrt(3)});
            \draw[->] (0, 0) -- (144:{sqrt(3)}) node [above left, shift={(0.1, -0.15)}] {\(\beta\)};
            \draw[->] (0, 0) -- (-144:{sqrt(3)});
            \draw[->] (0, 0) -- (90:{sqrt(3)});
            \draw[->] (0, 0) -- (-90:{sqrt(3)});
            \fill (0, 0) circle [radius = 0.05];
        \end{tikzpicture}
    \end{equation}
    Unlike the other root systems we've seen this doesn't correspond to a Lie algebra we've already met, in fact the Lie algebra corresponding to this root system wasn't discovered until the root system was, and so it inherits it's name from the root system.
    The root system is called \(G_2\), and the Lie algebra \(\lie{g}_2\).
    The corresponding Lie group is \(G_2\) also.
    This is one of the exceptional Lie groups/algebras, it doesn't fit any of the infinite classes in the classification of semisimple Lie algebras which we will give shortly.
    
    \subsection{Higher Rank}
    Root systems corresponding to arbitrary rank Lie algebras have been classified by Coxeter who studied regular lattices in arbitrary dimensions.
    These lattices can be generated by taking linear combinations of the root vectors with integer coefficients.
    Part of this classification uses something called Coxeter diagrams, which we won't discuss here as their applications to Lie theory are limited, but a subset of these diagrams with a slightly different notation, called Dynkin diagrams, are the topic of the next section.
    
    \section{Dynkin Diagrams}
    These root systems are pretty hard to visualise, just look at \(G_2\), and we're still in two dimensions.
    In three dimensions (rank 3) they just get worse, and in more dimensions it quickly becomes pretty hopeless trying to picture them.
    Fortunately these root systems carry much more information than we really need, after all most of the vectors appearing in them are just sums of other vectors appearing in them.
    We use this to pare down the amount of information we have to give to fully specify a root system.
    
    A \defineindex{positive root} is one which makes an angle between \(0\) (inclusive) and \(\pi\) (exclusive) with the positive \(x\)-axis.
    Considering positive roots removes half the roots, which are just the negatives of the positive roots.
    A \defineindex{simple positive root} is a positive root which cannot be written as a linear combination of positive roots.
    In the examples above the simple positive roots are \(\alpha\) and \(\beta\).
    Note that the choice of which roots are positive simple roots is not unique, but the number of simple positive roots and how they relate to each other is.
    
    Root systems are completely specified by their simple positive roots, and more importantly only up to scaling, so we just need to come up with a notation to define the angle and relative length of the positive simple roots and from this we can specify all compact semisimple Lie algebras as it can be shown that there is a one to one correspondence between root systems and compact semisimple Lie algebras through their Cartan subalgebras.
    This notation is called \defineindex{Dynkin diagrams}.
    
    A Dynkin diagram consists of one node for every simple positive root.
    There are \(r\) simple positive roots, which is the number of dimensions of the root lattice, and the rank of the Lie algebra.
    We connect two nodes, \(n\) and \(m\), corresponding to the simple positive roots \(\alpha_n\) and \(\alpha_m\), by
    \begin{equation}
        \max\left\{ \abs*{\frac{2\alpha_n \cdot \alpha_m}{\alpha_n^2}}, \abs*{\frac{2\alpha_n \cdot \alpha_m}{\alpha_m^2}} \right\}
    \end{equation}
    lines, which is to say by 0, 1, 2, or 3 lines.
    We define the \defineindex{Cartan matrix}, \(K_{\alpha\beta}\), for simple positive roots \(\alpha\) and \(\beta\), to have elements
    \begin{equation}
        K_{\alpha\beta} = \frac{2\alpha \cdot \beta}{\alpha^2}.
    \end{equation}
    
    
    In the case where the two roots are not the same length, we add an arrow pointing from the longest to the shortest root, alternatively this can be read as \(>\) symbol telling us which root has the greater length.
    
    For example, the rank one root system from before, \(A_1\), has a single root, so its Dynkin diagram consists of a single node:
    \begin{equation}
        \tikzsetnextfilename{dynkin-A1}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[dynkin root] at (0, 0) {};
        \end{tikzpicture}
    \end{equation}
    The first example of a rank two root system was \(A_1 \oplus A_1\), and both roots are the same length and orthogonal, so their inner product is zero and we don't draw any connecting lines:
    \begin{equation}
        \tikzsetnextfilename{dynkin-A1xA1}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
        \end{tikzpicture}
    \end{equation}
    Notice that the direct sum is just given by drawing the two diagrams next to each other, this is a general feature of Dynkin diagrams, for this reason we'll restrict ourselves to connected Dynkin diagrams, since these correspond to Lie algebras which aren't just the sum of other semisimple Lie algebras.
    
    The second example of a rank two system was \(A_2\), with two roots of equal length at an angle of \(\pi/3\), and thus \(2 \alpha \cdot \beta/\alpha^2 = 2\alpha \cdot \beta/\beta^2 = 2\cos(\pi/3) = 1\), so we draw \(A_2\) as
    \begin{equation}
        \tikzsetnextfilename{dynkin-A2}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (1, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
        \end{tikzpicture}
    \end{equation}
    
    Here we've begun to build up our first infinite family of root systems/semisimple Lie algebras, \(A_n\), where  \(A_n\) has \(n\) nodes connected in a line, so is formed of \(n\) roots all of the same length, a general \(A_n\) looks like
    \begin{equation}
        \tikzsetnextfilename{dynkin-An}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (3, 0);
            \draw[dashed] (3, 0) -- (5, 0);
            \draw[dynkin single] (5, 0) -- (6, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
            \node[dynkin root] at (5, 0) {};
            \node[dynkin root] at (6, 0) {};
            \node[anchor=base] at (0, -0.4) {1};
            \node[anchor=base] at (1, -0.4) {2};
            \node[anchor=base] at (2, -0.4) {3};
            \node[anchor=base] at (3, -0.4) {4};
            \node[anchor=base] at (5, -0.4) {\(n - 1\)};
            \node[anchor=base] at (6, -0.4) {\(n\)};
        \end{tikzpicture}
    \end{equation}
    It can be shown that \(A_n\) corresponds to \(\specialUnitaryLie(n + 1)\).
    
    Now consider the third example of a rank 2 root system, \(B_2 \isomorphic C_2\).
    This consists of two roots of lengths \(1\) and \(\sqrt{2}\), choosing to normalise the shorter root, which we'll take as \(\alpha\), at an angle of \(3\pi/4\).
    Hence we connect the two nodes with \(\abs{2\alpha \cdot \beta / \alpha^2} = \abs{2\cos(3\pi/4)} = 2\) lines.
    Arbitrarily choosing one node to correspond to the longer root and denoting this choice by an arrow pointing to the shorter root:
    \begin{equation}
        \tikzsetnextfilename{dynkin-B2}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin double, dynkin arrow] (0, 0) -- (1, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
        \end{tikzpicture}
    \end{equation}
    This is \(B_2\).
    Recall that we also drew the root system rotated and relabelled the roots, this corresponds instead to the diagram
    \begin{equation}
        \tikzsetnextfilename{dynkin-C2}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin double, dynkin arrow] (1, 0) -- (0, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
        \end{tikzpicture}
    \end{equation}
    This is \(C_2\).
    
    Clearly since only connectivity and arrow direction matters, not the order in which we write the nodes, these two diagrams are the same (isomorphic).
    The difference in names becomes clear when we look at the infinite families to which these diagrams belong.
    The Dynkin diagrams \(B_n\) have \(n\) nodes connected in a line with the last two connected by a double line and an arrow pointing at the last node:
    \begin{equation}
        \tikzsetnextfilename{dynkin-Bn}
        \begin{tikzpicture}[baseline=(current bounding box), font=\small]
            \draw[dynkin single] (0, 0) -- (3, 0);
            \draw[dynkin double, dynkin arrow] (6, 0) -- (7, 0);
            \draw[dashed] (3, 0) -- (5, 0);
            \draw[dynkin single] (5, 0) -- (6, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
            \node[dynkin root] at (5, 0) {};
            \node[dynkin root] at (6, 0) {};
            \node[dynkin root] at (7, 0) {};
            \node[anchor=base] at (0, -0.4) {1};
            \node[anchor=base] at (1, -0.4) {2};
            \node[anchor=base] at (2, -0.4) {3};
            \node[anchor=base] at (3, -0.4) {4};
            \node[anchor=base] at (5, -0.4) {\(n - 2\)};
            \node[anchor=base] at (6, -0.4) {\(n - 1\)};
            \node[anchor=base] at (7, -0.4) {\(n\)};
        \end{tikzpicture}
    \end{equation}
    This corresponds to a root system with \(n - 1\) roots of the same length and one shorter root.
    On the other hand \(C_n\) has \(n\) nodes connected in a line with the last two connected by a double line and an arrow pointing at the penultimate node:
    \begin{equation}
        \tikzsetnextfilename{dynkin-Cn}
        \begin{tikzpicture}[baseline=(current bounding box), font=\small]
            \draw[dynkin single] (0, 0) -- (3, 0);
            \draw[dynkin double, dynkin arrow] (7, 0) -- (6, 0);
            \draw[dashed] (3, 0) -- (5, 0);
            \draw[dynkin single] (5, 0) -- (6, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
            \node[dynkin root] at (5, 0) {};
            \node[dynkin root] at (6, 0) {};
            \node[dynkin root] at (7, 0) {};
            \node[anchor=base] at (0, -0.4) {1};
            \node[anchor=base] at (1, -0.4) {2};
            \node[anchor=base] at (2, -0.4) {3};
            \node[anchor=base] at (3, -0.4) {4};
            \node[anchor=base] at (5, -0.4) {\(n - 2\)};
            \node[anchor=base] at (6, -0.4) {\(n - 1\)};
            \node[anchor=base] at (7, -0.4) {\(n\)};
        \end{tikzpicture}
    \end{equation}
    This corresponds to a root system with \(n - 1\) roots of the same length and one longer root.
    
    From this we see that it is just happenstance that \(B_2 \isomorphic C_2\), there just aren't enough nodes/roots to tell if we have lots of long roots and one short or lots of short roots and one long, basically the fact that \(B_2 \isomorphic C_2\) is nothing more than the fact that \(2 - 1 = 1\), so we have one root of each length.
    
    In general one can show that \(B_n\) corresponds to \(\specialOrthogonalLie(2n + 1)\), so rotations in odd dimensions, and \(C_n\) to \(\symplecticLie(2n)\).
    That we split rotations into and odd case and, later, an even case reflects that rotations in odd and even dimensions are quite different in nature.
    
    Going back to the first rank 2 example we also called it \(D_2\), this corresponds to the infinite family of diagrams \(D_n\) with \(n\) nodes connected in a line except it branches at the very end:
    \begin{equation}
        \tikzsetnextfilename{dynkin-Dn}
        \begin{tikzpicture}[baseline=(current bounding box), font=\small]
            \draw[dynkin single] (0, 0) -- (3, 0);
            \draw[dynkin single] (6, 0) -- ++ (45:1) coordinate (A);
            \draw[dynkin single] (6, 0) -- ++ (-45:1) coordinate (B);
            \draw[dashed] (3, 0) -- (5, 0);
            \draw[dynkin single] (5, 0) -- (6, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
            \node[dynkin root] at (5, 0) {};
            \node[dynkin root] at (6, 0) {};
            \node[dynkin root] at (A) {};
            \node[dynkin root] at (B) {};
            \node[anchor=base] at (0, -0.4) {1};
            \node[anchor=base] at (1, -0.4) {2};
            \node[anchor=base] at (2, -0.4) {3};
            \node[anchor=base] at (3, -0.4) {4};
            \node[anchor=base] at (5, -0.4) {\(n - 3\)};
            \node[right, xshift=0.05cm] at (6, 0) {\(n - 2\)};
            \node[right, xshift=0.05cm] at (A) {\(n - 1\)};
            \node[right, xshift=0.05cm] at (B) {\(n\)};
        \end{tikzpicture}
    \end{equation}
    So \(D_2 \isomorphic A_1 \oplus A_1\) is just the special case where we only have the last two nodes:
    \begin{equation}
        \tikzsetnextfilename{dynkin-D2}
        \begin{tikzpicture}[baseline=(current bounding box), font=\small]
            \node[dynkin root] at (0, 0.707) {};
            \node[dynkin root] at (0, -0.707) {};
        \end{tikzpicture}
    \end{equation}
    which is clearly just a rearrangement of \(A_1 \oplus A_1\).
    It can be shown that in general \(D_n\) corresponds to \(\specialOrthogonalLie(2n)\), so rotations in even dimensions.
    
    \subsection{Exceptional Lie Algebras}
    As well as the infinite families \(A_n\), \(B_n\), \(C_n\), and \(D_n\), there are five \define{exceptional Lie algebras}\index{exceptional Lie algebra}.
    These don't fit into any of the infinite families, and are completely classified by their root systems, and indeed are were discovered from their root systems and are named after them.
    The first we have already seen, it is rank 2, and is called \(G_2\), with the Lie algebra \(\lie{g}_2\).
    As we saw before this has two roots of lengths \(1\) and \(\sqrt{3}\), normalising the shorter root, \(\alpha\), with an angle of \(5\pi/6\) between them.
    So, we connect the two nodes with \(\abs{2\alpha \cdot \beta/\alpha^2} = \abs{2\sqrt{3}\cos(5\pi/6)} = 3\) lines with an arrow:
    \begin{equation}
        \tikzsetnextfilename{dynkin-G2}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin triple, dynkin arrow] (0, 0) -- (1, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
        \end{tikzpicture}
    \end{equation}
    
    There is a rank 4 exceptional Lie algebra, \(\lie{f}_4\), with corresponding root system \(F_4\) consisting of four roots of two different lengths forming the diagram
    \begin{equation}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (1, 0);
            \draw[dynkin double, dynkin arrow] (1, 0) -- (2, 0);
            \draw[dynkin single] (2, 0) -- (3, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
        \end{tikzpicture}
    \end{equation}
    
    There are then three similar exceptional Lie algebras of ranks 6, 7, and 8 all with all roots the same length.
    These are called \(\lie{e}_6\), \(\lie{e}_7\), and \(\lie{e}_8\) respectively and correspond to the diagrams \(E_6\), \(E_7\), and \(E_8\), given by
    \begin{gather}
        \tikzsetnextfilename{dynkin-E6}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (4, 0);
            \draw[dynkin single] (2, 0) -- (2, 1);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
            \node[dynkin root] at (4, 0) {};
            \node[dynkin root] at (2, 1) {};
        \end{tikzpicture}
        \\
        \tikzsetnextfilename{dynkin-E7}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (5, 0);
            \draw[dynkin single] (2, 0) -- (2, 1);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
            \node[dynkin root] at (4, 0) {};
            \node[dynkin root] at (5, 0) {};
            \node[dynkin root] at (2, 1) {};
        \end{tikzpicture}
        \\
        \tikzsetnextfilename{dynkin-E8}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (6, 0);
            \draw[dynkin single] (2, 0) -- (2, 1);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
            \node[dynkin root] at (4, 0) {};
            \node[dynkin root] at (5, 0) {};
            \node[dynkin root] at (6, 0) {};
            \node[dynkin root] at (2, 1) {};
        \end{tikzpicture}
    \end{gather}
    
    \begin{thm}{Classification of Semisimple Lie Algebras}{}
        Every semisimple Lie algebra (over an algebraically closed field of characteristic zero, which for our purposes is \(\complex\)) is a direct sum of simple Lie algebras, and these finite dimensional Lie algebras either fall into one of four infinite families, \(A_n\), \(B_n\), \(C_n\), and \(D_n\), corresponding to \(\specialUnitaryLie(n + 1)\), \(\specialOrthogonalLie(2n + 1)\), \(\symplecticLie(2n)\), and \(\specialOrthogonalLie(2n)\) respectively, or are one of the five exceptional Lie algebras \(\lie{g}_2\), \(\lie{f}_4\), \(\lie{e}_6\), \(\lie{e}_7\), or \(\lie{e}_8\).
    \end{thm}
    
    \subsection{\texorpdfstring{\(E_8\)}{E8} and Grand Unified Theories}
    One of the candidates for a grand unified theory is an \(E_8\) gauge theory.
    While grand unified theories are just slightly beyond the scope of this course we can motivate this as follows.
    It can be shown that certain symmetry breaking in an \(E_8\) theory corresponds to essentially removing one of the roots, and so one of the nodes.
    In this way it's possible to break the symmetry from \(E_8\) to \(E_7\) to \(E_6\).
    From here we can remove another node and we get
    \begin{equation}
        \tikzsetnextfilename{dynkin-D5-but-weird}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (3, 0);
            \draw[dynkin single] (2, 0) -- (2, 1);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
            \node[dynkin root] at (2, 1) {};
        \end{tikzpicture}
    \end{equation}
    which is just
    \begin{equation}
        \tikzsetnextfilename{dynkin-D5}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (2, 0);
            \draw[dynkin single] (2, 0) -- ++ (45:1) coordinate (A);
            \draw[dynkin single] (2, 0) -- ++ (-45:1) coordinate (B);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (A) {};
            \node[dynkin root] at (B) {};
        \end{tikzpicture}
    \end{equation}
    which is \(D_5\), which corresponds to \(\specialOrthogonalLie(10)\) and hence to \(\specialOrthogonal(10)\), which is a group in which many unified theories are constructed.
    Further breaking of the symmetry reduces this diagram to
    \begin{equation}
        \tikzsetnextfilename{dynkin-A4-but-weird}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (2, 0);
            \draw[dynkin single] (2, 0) -- ++ (45:1) coordinate (A);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (A) {};
        \end{tikzpicture}
    \end{equation}
    which is just
    \begin{equation}
        \tikzsetnextfilename{dynkin-A4}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (3, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (2, 0) {};
            \node[dynkin root] at (3, 0) {};
        \end{tikzpicture}
    \end{equation}
    which is \(A_4\), which corresponds to \(\specialUnitaryLie(5)\) and hence to \(\specialUnitary(5)\), which is a group in which many unified theories are constructed.
    One more symmetry breaking, this time removing one of the roots corresponding to a node not on the end gives
    \begin{equation}
        \tikzsetnextfilename{dynkin-A2xA1}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[dynkin single] (0, 0) -- (1, 0);
            \node[dynkin root] at (0, 0) {};
            \node[dynkin root] at (1, 0) {};
            \node[dynkin root] at (3, 0) {};
        \end{tikzpicture}
    \end{equation}
    which is \(A_2 \oplus A_1\), which is \(\specialUnitaryLie(3) \oplus \specialUnitaryLie(2)\).
    Now recall that a semisimple Lie algebra can always be written as an Abelian Lie algebra plus some simple Lie algebras, and we've been neglecting this Abelian Lie algebra, which can always be written as a sum of \(\unitaryLie(1)\)s, so we can introduce a \(\unitaryLie(1)\) to get \(\specialUnitaryLie(3) \oplus \specialUnitaryLie(2) \oplus \unitaryLie(1)\), corresponding to the group \(\specialUnitary(3) \otimes \specialUnitary(2) \otimes \unitary(1)\), which is the gauge group of the standard model!
    
    \subsection{Accidental Isomorphisms}
    We've seen several examples already of \enquote{accidental isomorphisms}, where two Dynkin diagrams, and hence Lie algebras, are isomorphic in the low rank cases.
    For example, \(A_1 \isomorphic B_1 \isomorphic C_1\), as all just consist of a single root, and so \(\specialUnitaryLie(2) \isomorphic \specialOrthogonalLie(3) \isomorphic \symplecticLie(2)\), we also have \(A_1 \oplus A_1 \isomorphic D_2\), corresponding to \(\specialUnitaryLie(2) \oplus \specialUnitaryLie(2) \isomorphic \specialOrthogonalLie(4)\), we also have \(B_2 \isomorphic C_2\), corresponding to \(\specialOrthogonalLie(5) \isomorphic \symplecticLie(4)\).
    There are also some we haven't seen yet, such as \(A_3 \isomorphic D_3\), corresponding to \(\specialUnitaryLie(4) \isomorphic \specialOrthogonalLie(6)\).
    
    Note that these isomorphisms are only on the level of the Lie algebras, the Lie groups are not necessarily isomorphic.
    For example we actually have \(\specialUnitary(2) \otimes \specialUnitary(2) \isomorphic \operatorname{Spin}(4)\), where \(\operatorname{Spin}(4)\) is the universal cover of \(\specialOrthogonal(4)\), which is connected where \(\specialOrthogonal(4)\) is not.
    
    \chapter{Representations of \texorpdfstring{\(\specialUnitaryLie(3)\)}{su(3)}}
    \section{The Gell-Mann Matrices}
    The Lie algebra \(\specialUnitaryLie(3)\) has a defining representation consisting of \(3 \times 3\) complex traceless Hermitian matrices.
    A basis for this vector space is given by the \defineindex{Gell-Mann matrices}:
    \begin{alignat}{3}
        \lambda_1 &= 
        \begin{pmatrix}
            0 & 1 & 0\\
            1 & 0 & 0\\
            0 & 0 & 0
        \end{pmatrix}
        , \quad & \lambda_2 &= \left(
        \begin{array}{@{}c@{\hskip 11pt}cc@{}}
            0 & \mathllap{-}i & 0\\
            i & 0 & 0\\
            0 & 0 & 0
        \end{array}
        \right)
        , \quad & \lambda_3 &= \left(
        \begin{array}{@{}c@{\hskip 11pt}cc@{}}
            1 & 0 & 0\\
            0 & \mathllap{-}1 & 0\\
            0 & 0 & 0
        \end{array}
        \right)
        \\
        \lambda_4 &= 
        \begin{pmatrix}
            0 & 0 & 1\\
            0 & 0 & 0\\
            1 & 0 & 0
        \end{pmatrix}
        , \quad & \lambda_5 &= \left(
        \begin{array}{@{}cc@{\hskip 11pt}c@{}}
            0 & 0 & \mathllap{-}i\\
            0 & 0 & 0\\
            i & 0 & 0
        \end{array}
        \right)
        , \quad & \lambda_6 &= 
        \begin{pmatrix}
            0 & 0 & 0\\
            0 & 0 & 1\\
            0 & 1 & 0
        \end{pmatrix}
        \\
        \lambda_7 &= \left(
        \begin{array}{@{}cc@{\hskip 11pt}c@{}}
            0 & 0 & 0\\
            0 & 0 & \mathllap{-}i\\
            0 & i & 0
        \end{array}
        \right)
        , \quad & \lambda_8 &= \frac{1}{\sqrt{3}} \left(
        \begin{array}{@{}cc@{\hskip 11pt}c@{}}
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & \mathllap{-}2
        \end{array}
        \right) .
    \end{alignat}
    We can immediately see that \(\specialUnitaryLie(3)\) is an 8-dimensional Lie algebra and since there are two diagonal, and therefore commuting, matrices the rank of \(\specialUnitaryLie(3)\) is at least 2, and it turns out that we can't simultaneously diagonalise more than two matrices in this basis, so the rank is exactly 2.
    
    Notice that \(\lambda_1\), \(\lambda_2\), and \(\lambda_3\) are just the Pauli matrices, \(\sigma_1\), \(\sigma_2\), and \(\sigma_3\), padded with an extra row and column.
    This corresponds to an \(\specialUnitaryLie(2)\) subalgebra.
    Similarly \(\lambda_4\) and \(\lambda_5\), as well as \(\lambda_6\) and \(\lambda_7\) are \(\sigma_1\) and \(\sigma_2\) padded with another row and column of zeros.
    
    These matrices are defined such that \(\tr(\lambda_i \lambda_j) = 2\delta_{ij}\).
    For this reason it's common to take the basis \(T_a = \lambda_a/2\).
    
    As a rank 2 Lie algebra we expect that there are two invariant tensors, and there are, they are \(\delta_{ij}\) and \(d_{ijk} = \tr(\lambda_i \anticommutator{\lambda_j}{\lambda_k})/4\).
    
    \section{A different Basis}
    Define a new basis
    \begingroup
    \allowdisplaybreaks
    \begin{alignat}{3}
        I_1 &= \frac{1}{2}\lambda_1, \quad & I_2 &= \frac{1}{2}\lambda_2, \quad & I_3 &= \frac{1}{2}\lambda_3,\\
        U_1 &= \frac{1}{2}\lambda_6, \quad & U_2 &= \frac{1}{2}\lambda_7, \quad & U_3 &= \frac{1}{2}\left( -\frac{1}{2}\lambda_3 + \frac{\sqrt{3}}{2}\lambda_8 \right),\\
        V_1 &= \frac{1}{2}\lambda_4, \quad & V_2 &= \frac{1}{2}\lambda_5, \quad & V_3 &= \frac{1}{2}\left( \frac{1}{2}\lambda_3 + \frac{\sqrt{3}}{2} \lambda_8 \right).
    \end{alignat}
    \endgroup
    So we have 
    \begin{equation}
        \lambda_3 = 2(V_3 - U_3), \qqand \lambda_8 = \frac{2}{\sqrt{3}}(U_3 + V_3).
    \end{equation}
    These linear combinations of \(\lambda_3\) and \(\lambda_8\) are chosen so that \(U_3\) and \(V_3\) look like \(\sigma_3/2\) with a row and column of zero padding.
    
    If we work out the commutation relations we find that
    \begin{align}
        \commutator{I_1}{I_j} &= i\varepsilon_{ijk}I_k,\\
        \commutator{U_1}{U_j} &= i\varepsilon_{ijk}U_k,\\
        \commutator{V_1}{V_j} &= i\varepsilon_{ijk}V_k.
    \end{align}
    From this we can see that there are actually three copies of \(\specialUnitaryLie(2)\) as a subalgebra of \(\specialUnitaryLie(3)\).
    
    We can define step operators for each copy of \(\specialUnitaryLie(2)\), \(I_{\pm} = I_1 \pm iI_2\), \(U_{\pm} = U_1 \pm iU_2\), and \(V_{\pm} = V_1 \pm iV_2\).
    These then give us the commutation relations
    \begin{equation}
        \commutator{I_+}{I_-} = 2I_3, \qquad \commutator{U_+}{U_-} = 2U_3, \qqand \commutator{V_+}{V_-} = 2V_3.
    \end{equation}
    
    The Cartan subalgebra of \(\specialUnitaryLie(3)\) is generated by \(\{\lambda_3, \lambda_8\}\).
    By definition we have \(\commutator{E_\alpha}{E_{-\alpha}} = \alpha_i H_i\) where \(H_i\) are the generators of the Cartan subalgebra and \(E_{\pm\alpha}\) are the step operators associated with the root \(\alpha\).
    Writing \(I_3\), \(U_3\), and \(V_3\) in terms of \(\lambda_3\) and \(\lambda_8\) we see that the positive root vectors in the \(\{\lambda_3, \lambda_8\}\) basis are
    \begin{equation}
        (2, 0), \qquad (-1, \sqrt{3}), \qqand (1, \sqrt{3}).
    \end{equation}
    Note that the last root vector is the sum of the first two, so there are two simple positive root vectors.
    We have already seen the root system for \(\specialUnitaryLie(3)\), which is \(A_2\), in terms of this basis it is
    \begin{equation}
        \tikzsetnextfilename{root-systsem-su3}
        \begin{tikzpicture}[
            baseline = (Iplus.base),
            >={Latex[width=1.5mm]}
            ]
            \draw[highlight!50] (1, 0) -- (60:1) -- (120:1) -- (180:1) -- (240:1) -- (300:1) -- cycle;
            \draw[->] (0, 0) -- (1, 0) node (Iplus) [right] {\(I_+\)};
            \draw[->] (0, 0) -- (-1, 0) node [left] {\(I_-\)};
            \draw[->] (0, 0) -- (60:1) node [above, xshift=0.2cm] {\(V_+\)};
            \draw[->] (0, 0) -- (-60:1) node [below, xshift=0.2cm] {\(U_-\)};
            \draw[->] (0, 0) -- (120:1) node [above] {\(U_+\)};
            \draw[->] (0, 0) -- (-120:1) node [below] {\(V_-\)};
            \fill (0, 0) circle [radius = 0.05];
        \end{tikzpicture}
    \end{equation}
    where the \(\lambda_3\) eigenvalue increases to the right and the \(\lambda_8\) eigenvalue increases upwards.
    
    There are also commutation relations between these copies of \(\specialUnitaryLie(2)\), which lead to the relations
    \begin{alignat}{3}
        \commutator{I_{\pm}}{U_{\pm}} &= \pm V_{\pm}, \qquad & \commutator{I_{\pm}}{V_{\pm}} &= 0, \qquad & \commutator{U_{\pm}}{V_{\pm}} &= 0,\\
        \commutator{I_{\pm}}{U_{\mp}} &= 0, \qquad & \commutator{I_{\pm}}{V_{\mp}} &= \mp U_{\mp}, \qquad & \commutator{U_{\pm}}{\mp} &= \pm I_{\mp}.
    \end{alignat}
    
    \section{Representations of \texorpdfstring{\(\specialUnitaryLie(3)\)}{su(3)}}
    Consider some representation with states \(\ket{\psi}\).
    Define the highest weight state, \(\ket{\psi_{\symrm{m}}}\), to be the one annihilated by all raising operators:
    \begin{equation}
        I_{+} \ket{\psi_{\symrm{m}}} = U_{+} \ket{\psi_{\symrm{m}}} = V_{+} \ket{\psi_{\symrm{m}}} = 0.
    \end{equation}
    We can find new states by acting on \(\ket{\psi_{\symrm{m}}}\) with the lowering operators \(I_-\), \(U_-\), and \(V_-\).
    The sequences of states \((I_-)^k \ket{\psi_{\symrm{m}}}\) for \(k = 1, 2, \dotsc, p\) and \((U_-)^n \ket{\psi_{\symrm{m}}}\) for \(n = 1, 2, \dotsc, q\) are non-degenerate, the sequences don't overlap or double back on themselves.
    We can map this out diagrammatically as
    \begin{equation}
        \tikzsetnextfilename{su3-rep-I-U-minus}
        \begin{tikzpicture}[
            >={Latex[width=1.5mm]},
            baseline = (current bounding box)
            ]
            \draw[->] (0, 0) node [above right] {\(\ket{\psi_{\symrm{m}}}\)} -- (-2, 0) node [left] {\((I_-)^k \ket{\psi_{\symrm{m}}}\)};
            \draw[->] (0, 0) -- (-60:2) node [right] {\((U_-)^n \ket{\psi_{\symrm{m}}}\)};
        \end{tikzpicture}
    \end{equation}
    
    We can use the commutation relations to rewrite these sequences, for example,
    \begin{equation}
        I_- \ket{\psi_{\symrm{m}}} = \commutator{U_+}{V_-} \ket{\psi_{\symrm{m}}} = (U_+ V_- - V_- U_+) \ket{\psi_{\symrm{m}}} = U_+ V_- \ket{\psi_{\symrm{m}}} + 0.
    \end{equation}
    So \(I_-\) and \(U_+ V_-\) produce the same state when acting on \(\ket{\psi_{\symrm{m}}}\).
    Diagrammatically we can write this as
    \begin{equation}
        \tikzsetnextfilename{su3-rep-I-UV-triangle}
        \begin{tikzpicture}[
            >={Latex[width=1.5mm]},
            baseline = (current bounding box)
            ]
            \draw[->] (0, 0) -- (-2, 0) node [midway, above] {\(I_-\)};
            \draw[->] (0, 0) -- (-1, -1.3) node [midway, below right] {\(V_-\)};
            \draw[->] (-1, -1.3) -- (-2, 0) node [midway, below left] {\(U_-\)};
        \end{tikzpicture}
    \end{equation}
    
    Repeatedly using \(\commutator{I_-}{U_+} = 0\) and \(\commutator{V_+}{I_-} = -U_+\) we find that
    \begin{equation}
        U_+(I_-)^k \ket{\psi_{\symrm{m}}} = V_+(I_-)^k \ket{\psi_{\symrm{m}}} = 0.
    \end{equation}
    Similarly using \(\commutator{I_+}{U_-} = 0\) and \(\commutator{V_+}{U_-} = I_+\) we find that
    \begin{equation}
        I_+ (U_-)^k \ket{\psi_{\symrm{m}}} = V_{+}(U_-)^k \ket{\psi_{\symrm{m}}} = 0.
    \end{equation}
    This tells us that the states \((I_-)^n\ket{\psi_{\symrm{m}}}\) and \((U_-)^n\ket{\psi_{\symrm{m}}}\) lie on the boundary of the representation, in the sense that in these diagrams there are no states to the right or above of these states.
    
    For a finite dimensional representation both sequences terminate after a finite number of steps.
    This means there are some states \(\ket{\psi_I}\) and \(\ket{\psi_U}\) such that \(I_-\ket{\psi_I} = U_-\ket{\psi_U} = 0\).
    We can generate new sequences, \((V_-)^k\ket{\psi_I}\) and \((V_-)^n\ket{\psi_U}\) from these by operating repeatedly with \(V_-\).
    This generates further states on the boundary.
    For a finite dimensional representation this process eventually closes and we get all boundary states.
    The result will be a hexagonal shape of the form
    \begin{equation}
        \tikzsetnextfilename{su3-rep-general-diagram}
        \begin{tikzpicture}[
            >={Latex[width=1.5mm]},
            baseline = (current bounding box)
            ]
            \draw (-2, 2) node [above left] {\(I\)} -- (2, 2) node [above right] {\(M\)} -- (3, 0.5) node [right] {\(U\)} -- (1, -2.5) -- (-1, -2.5) -- (-3, 0.5) -- cycle;
            \draw[->] (-3.5, 0) -- (3.5, 0) node [right] {\(\lambda_3\)};
            \draw[->] (0, -3) -- (0, 3) node [above] {\(\lambda_8\)};
        \end{tikzpicture}
    \end{equation}
    There are \(p + 1\) states between \(M\) and \(I\) and \(q + 1\) states between \(M\) and \(U\).
    This allows us to characterise each representation by a pair of non-negative integers, \((p, q)\).
    If either of these integers is zero then the hexagon is actually a triangle and we say the representation is triangular.
    There are three symmetry axes as each boundary \(MI\) and \(MU\) forms a complete \(I\)-spin or \(U\)-spin representation of \(\specialUnitaryLie(2)\) respectively, and so do the boundaries parallel to these.
    
    We can fill in the states in the middle using \((V_-)^k\ket{\psi_{\symrm{m}}}\).
    Doing so the next layer in is doubly degenerate as there are two ways to reach each state.
    For example, we can reach \(V_-\ket{\psi_{\symrm{m}}}\) as \(V_-\ket{\psi_{\symrm{m}}}\), \(U_- I_- \ket{\psi_{\symrm{m}}}\), or \(I_- U_- \ket{\psi_{\symrm{m}}}\).
    But we also have \(V_-\ket{\psi_{\symrm{m}}} \commutator{U_-}{I_-}\ket{\psi_{\symrm{m}}}\) and so only two of these states are linearly independent.
    Diagrammatically:
    \begin{equation}
        \tikzsetnextfilename{su3-rep-double-triangle}
        \begin{tikzpicture}[
            >={Latex[width=1.5mm]},
            baseline = (current bounding box)
            ]
            \draw[->] (0, 0) node [above right] {\(\ket{\psi_{\symrm{m}}}\)} -- (-2, 0) node [midway, above] {\(I_-\)};
            \draw[->] (0, 0) -- (-1, -1.3) node [midway, below right] {\(V_-\)};
            \draw[->] (-1, -1.3) -- (-2, 0) node [midway, below left] {\(U_-\)};
            \draw[->] (1, -1.3) -- (-1, -1.3) node [midway, below] {\(I_-\)};
            \draw[->] (0, 0) -- (1, -1.3) node [midway, right] {\(U_-\)};
        \end{tikzpicture}
    \end{equation}
    
    We can continue by defining some \(\ket{\psi_{\symrm{m}}'}\) orthogonal to \(V_-\ket{\psi_{\symrm{m}}}\) and starting the next layer.
    Carrying on this way we can generate all states.
    The degeneracy of the states increases at each layer.
    
    \section{Irreducible Representations of \texorpdfstring{\(\specialUnitaryLie(3)\)}{su(3)}}
    So far we have labelled states in a representation by the eigenvalues of \(\lambda_3\) and \(\lambda_8\).
    Now instead label them by the eigenvalues of the rescaled generators \(I_3 = \lambda_3/2\) and \(Y = \lambda_8/\sqrt{3}\).
    Notice that \(Y = 2(U_3 + V_3)/3\).
    Then we have the commutators
    \begin{alignat}{2}
        \commutator{I_3}{I_{\pm}} &= \pm I_{\pm}, \qquad & \commutator{Y}{I_{\pm}} = 0,\\
        \commutator{I_3}{U_{\pm}} &= \mp \frac{1}{2} U_{\pm}, \qquad & \commutator{Y}{U_{\pm}} = \pm U_{\pm},\\
        \commutator{I_3}{V_{\pm}} &= \pm \frac{1}{2} V_{\pm}, \qquad & \commutator{Y}{V_{\pm}} = \pm V_{\pm}.\\
    \end{alignat}
    Notice that the commutators of \(I_3\) with \(U_{\pm}\) and \(V_{\pm}\) produce a half step.
    The smallest irreducible representations are
    \begin{enumerate}
        \item The singlet, \(\rep{1}\), corresponding to \((p, q) = (0, 0)\), \(\lambda_3 = \lambda_8 = 0\), \(I_3 = Y = 0\).
        
        \item The triplet, \(\rep{3}\), corresponding to \((p, q) = (1, 0)\).
        There are three states, \((I_3, Y) = (1/2, 1/3), (-1/2, 1/3), (0, -2/3)\).
        We interpret these as an \(I\)-spin doublet (\(I_3 = \pm 1/2\)) with \(Y = 1/3\), and an \(I\)-spin singlet (\(I_3 = 0\)) with \(Y = -2/3\).
        \begin{equation}
            \tikzsetnextfilename{su3-triplet}
            \begin{tikzpicture}[
                >={Latex[width=1.5mm]},
                baseline = (current bounding box)
                ]
                \draw (30:1) node [above right] {\scriptsize\((1/2, 1/3)\)} -- (150:1) node [above left] {\scriptsize\((-1/2, 1/3)\)} -- (0, -1) node [below right] {\scriptsize\((0, -2/3)\)} -- cycle;
                \draw[->] (-1.5, 0) -- (1.5, 0) node [right] {\(I_3\)};
                \draw[->] (0, -1.5) -- (0, 1.5) node [left] {\(Y\)};
            \end{tikzpicture}
        \end{equation}
        
        \item The antitriplet, \(\rep{3}^*\), corresponding to \((p, q) = (0, 1)\).
        Notice that under complex conjugation \(\lambda_3 \mapsto -\lambda_3\) and \(\lambda_8 \mapsto -\lambda_8\), as \(\exp\{i\alpha^a \lambda_a/2\} \mapsto \exp\{-i\alpha^a \lambda_a/2\}\).
        So the three states are \((I_3, Y) = (-1/2, -1/3), (1/2, -1/3), (0, 2/3)\).
        So we have an \(I\)-spin doublet with \(Y = -1/3\) and an \(I\)-spin singlet with \(Y = 2/3\).
        \begin{equation}
            \tikzsetnextfilename{su3-antitriplet}
            \begin{tikzpicture}[
                >={Latex[width=1.5mm]},
                baseline = (current bounding box)
                ]
                \draw (-30:1) node [below right] {\scriptsize\((1/2, -1/3)\)} -- (-150:1) node [below left] {\scriptsize\((-1/2, -1/3)\)} -- (0, 1) node [right] {\scriptsize\((0, 2/3)\)} -- cycle;
                \draw[->] (-1.5, 0) -- (1.5, 0) node [right] {\(I_3\)};
                \draw[->] (0, -1.5) -- (0, 1.5) node [left] {\(Y\)};
            \end{tikzpicture}
        \end{equation}
        
        \item The sextet, \(\rep{6}\), corresponding to \((p, q) = (0, 2)\), and antisextet, \(\rep{6}^*\), corresponding to \((p, q) = (0, 2)\).
        These are triangular representations.
        The \(\rep{6}\) representation contains an \(I\)-spin triplet (\(I_3 = 0, \pm 1\)) with \(Y = 2/3\), an \(I\)-spin doublet with \(Y = -1/3\), and an \(I\)-spin singlet with \(Y = -4/3\).
        The \(\rep{6}^*\) states are given by negating the eigenvalues of the \(\rep{6}\) states.
        \begin{equation}
            \tikzsetnextfilename{su3-sextet}
            \begin{tikzpicture}[
                >={Latex[width=1.5mm]},
                baseline = (current bounding box)
                ]
                \node[below right] at (-1.7, 1.7) {\(\rep{6}\)};
                \draw (1, 2/3) -- (-1, 2/3) -- (0, -4/3) -- cycle;
                \draw (1/2, -1/3) -- (0, 2/3) -- (-1/2, -1/3) -- cycle;
                \draw[->] (-1.7, 0) -- (1.7, 0) node [right] {\(I_3\)};
                \draw[->] (0, -1.7) -- (0, 1.7) node [left] {\(I_3\)};
                
                \begin{scope}[xshift=4cm]
                    \node[below right] at (-1.7, 1.7) {\(\rep{6}^*\)};
                    \draw (1, -2/3) -- (-1, -2/3) -- (0, 4/3) -- cycle;
                    \draw (1/2, 1/3) -- (0, -2/3) -- (-1/2, 1/3) -- cycle;
                    \draw[->] (-1.7, 0) -- (1.7, 0) node [right] {\(I_3\)};
                    \draw[->] (0, -1.7) -- (0, 1.7) node [left] {\(I_3\)};
                \end{scope}
            \end{tikzpicture}
        \end{equation}
        
        \item The octet, \(\rep{8}\), corresponding to \((p, q) = (1, 1)\).
        This is the adjoint representation.
        This is a real representation, \(\rep{8} \isomorphic \rep{8}^*\).
        As such the diagram is invariant under \(\lambda_3 \mapsto -\lambda_3\) and \(\lambda_8 \mapsto \lambda_8\).
        There are two \(I\)-spin doublets, with \(Y = 1\) and \(Y = -1\), and one \(I\)-spin triplet, with \(Y = 0\), and one \(I\)-spin singlet with \(Y = 0\).
        Notice that this means there are two states with \((I_3, Y) = (0, 0)\), one from the triplet and one from the singlet.
        \begin{equation}
            \tikzsetnextfilename{su3-octet}
            \begin{tikzpicture}[
                >={Latex[width=1.5mm]},
                baseline = (current bounding box)
                ]
                \draw (1/2, 1) -- (1, 0) -- (1/2, -1) -- (-1/2, -1) -- (-1, 0) -- (-1/2, 1) -- cycle;
                \draw (1/2, 1) -- (-1/2, -1);
                \draw (-1/2, 1) -- (1/2, -1);
                \draw[->] (-1.5, 0) -- (1.5, 0) node [right] {\(I_3\)};
                \draw[->] (0, -1.5) -- (0, 1.5) node [left] {\(Y\)};
            \end{tikzpicture}
        \end{equation}
        \item The decuplet, \(\rep{10}\), corresponding to \((p, q) = (3, 0)\), and antidecuplet, \(\rep{10}^*\), corresponding to \((p, q) = (0, 3)\).
        These consist of an \(I\)-spin quadruplet, (\(I = \pm 1/2, \pm 3/2\)) with \(Y = 1\), an \(I\)-spin triplet with \(Y = 0\), an \(I\)-spin doublet with \(Y = -1\), and an \(I\)-spin singlet with \(Y = -2\).
        \begin{equation}
            \tikzsetnextfilename{su3-decuplet}
            \begin{tikzpicture}[
                >={Latex[width=1.5mm]},
                baseline = (current bounding box)
                ]
                \node[below right] at (-1.7, 2.2) {\(\rep{10}\)};
                \draw (-3/2, 1) -- (3/2, 1) -- (0, -2) -- cycle;
                \draw (-1, 0) -- (-1/2, 1) -- (0, 0) -- (1/2, 1) -- (1, 0);
                \draw (-1/2, -1) -- (0, 0) -- (1/2, -1) -- cycle;
                \draw[->] (-1.7, 0) -- (1.7, 0) node [right] {\(I_3\)};
                \draw[->] (0, -2.2) -- (0, 2.2) node [left] {\(Y\)};
                
                \begin{scope}[xshift=4.5cm]
                    \node[below right] at (-1.7, 2.2) {\(\rep{10}^*\)};
                    \draw (-3/2, -1) -- (3/2, -1) -- (0, 2) -- cycle;
                    \draw (-1, 0) -- (-1/2, -1) -- (0, 0) -- (1/2, -1) -- (1, 0);
                    \draw (-1/2, 1) -- (0, 0) -- (1/2, 1) -- cycle;
                    \draw[->] (-1.7, 0) -- (1.7, 0) node [right] {\(I_3\)};
                    \draw[->] (0, -2.2) -- (0, 2.2) node [left] {\(Y\)};
                \end{scope}
            \end{tikzpicture}
        \end{equation}
    \end{enumerate}
    
    \section{Young Tableaux}
    Consider \(\specialUnitary(N)\).
    Let \(V\) be a representation space for \(\specialUnitary(N)\).
    Then take \(U\colon V \to V\) as a representation of some particular group element of \(\specialUnitary(N)\).
    We can consider a three index tensor, \(T_{ijk}\), as an element of \(V \otimes V \otimes V = V^{\otimes 3}\).
    Then \(T\) transforms as
    \begin{equation}
        T_{ijk} \mapsto U_{il} U_{jm} U_{kn} T_{lmn}.
    \end{equation}
    
    We can use \define{Young diagrams}\index{Young diagram} to express the symmetries of \(T_{ijk}\).
    If \(T_{ijk}\) is symmetric in all of its indices then we denote this with as many boxes as indices in a row:
    \begin{equation}
        T_{(ijk)} \text{ totally symmetric} \longrightsquigarrow \ydiagram{3}.
    \end{equation}
    If \(T_{ijk}\) is antisymmetric in all of its indices then we denote this with as many boxes as indices in a column:
    \begin{equation}
        T_{[ijk]} \text{ totally antisymmetric} \longrightsquigarrow \ydiagram{1,1,1}.
    \end{equation}
    If \(T_{ijk}\) is antisymmetric in its first two indices and symmetric in the first and third indices then we represent this as
    \begin{equation}
        T_{ijk} = -T_{jik} = T_{kji} \longrightsquigarrow \ydiagram{2,1}.
    \end{equation}
    In general a row is symmetrised and a column antisymmetrised.
    
    For example, the electromagnetic field strength tensor is antisymmetric, so may be thought of as
    \begin{equation}
        F_{\mu\nu} \longrightsquigarrow \ydiagram{1,1}.
    \end{equation}
    The metric tensor is symmetric, so can be thought of as
    \begin{equation}
        g_{\mu\nu} \longrightsquigarrow \ydiagram{2}.
    \end{equation}
    
    It can be shown that each Young diagram with at most \(N - 1\) rows labels a irreducible representation of \(\specialUnitary(N)\).
    Further we can calculate the dimension of the representation labelled by the Young diagram \(Y\) with the following formula:
    \begin{equation}
        \frac{f_Y(d)}{\hook(Y)}
    \end{equation}
    where \(d\) is the dimension of the vector space \(V\).
    There are two components to this.
    
    The first is \(f_Y\), which is a polynomial given by taking \(Y\), putting \(d\) in the top left box, then increasing the number to the right and decreasing downwards.
    Then \(f_Y(d)\) is the product of all the numbers.
    Notice that when done correctly the leading diagonal of the Young tableau\footnote{a \defineindex{Young tableau} is a Young diagram with numbers written in the boxes.}
    For example, if \(Y\) is
    \begin{equation}
        Y = \ydiagram{3,2,1}
    \end{equation}
    then the numbered tableau is
    \begin{equation}
        \ytableausetup{boxsize=1.75em}
        \begin{ytableau}
            \scriptstyle d & \scriptstyle d + 1 & \scriptstyle d + 2\\
            \scriptstyle d - 1 & \scriptstyle d \\
            \scriptstyle d - 2
        \end{ytableau}
        \ytableausetup{boxsize=normal}
    \end{equation}
    and
    \begin{equation}
        f_Y(d) = d(d+1)(d+2)(d-1)d(d-2).
    \end{equation}
    
    The second component is the \defineindex{hook number}, which is defined as the product of the \define{hook lengths}\index{hook length}.
    The hook length of a box is the number of boxes to the right, plus the number of boxes below, plus one, it is the length of the \enquote{hook} such as
    \begin{equation}
        \ytableaushort{\none,\none,\none}*{3,2,1}*[*(highlight)]{0,2,1}
    \end{equation}
    which has length 3, and so the hook length of the first box in the second row is 3.
    The hook lengths of all boxes of \(Y\) are given below:
    \begin{equation}
        \ytableaushort{531,31,1}.
    \end{equation}
    The hook number is then
    \begin{equation}
        \hook(Y) = 5\cdot 3 \cdot 3 = 45.
    \end{equation}
    
    Consider the electromagnetic field strength tensor, \(F_{\mu\nu}\), in four spacetime dimensions, so \(d = 4\).
    Then we have
    \begin{equation}
        F_{\mu\nu} \longrightsquigarrow \ytableaushort{4,3} \implies \frac{4\cdot 3}{2} = 6,
    \end{equation}
    so \(F_{\mu\nu}\) is six dimensional, which is exactly what we would expect for a four by four real antisymmetric matrix.
    Similarly
    \begin{equation}
        g_{\mu\nu} \longrightsquigarrow \ytableaushort{45} \implies \frac{4 \cdot 5}{2} = 10,
    \end{equation}
    so \(g_{\mu\nu}\) is ten dimensional, which is exactly what we would expect for a four by four real symmetric matrix.
    
    Now specifically consider \(\specialUnitaryLie(3)\), so \(d = 3\).
    The defining representation, \(\rep{3}\), is three dimensional and is given by
    \begin{equation}
        \rep{3} \longrightsquigarrow \ydiagram{1}.
    \end{equation}
    Similarly the complex conjugate of this representation is given by
    \begin{equation}
        \rep{3}^* \longrightsquigarrow \ydiagram{1,1}.
    \end{equation}
    
    We want to compute tensor products of representations.
    We'll restrict ourselves to the case where one of the representations is the single box representation, in the case of \(\specialUnitaryLie(3)\), \(\rep{3}\).
    The tensor product of a diagram, \(Y\), with a single box is the sum of the diagrams given by attaching the single box to \(Y\) in such a way that each row of the resulting diagram has at most as many boxes as the row above, and the boxes remain left and top justified.
    For example,
    \begin{equation}
        \rep{3} \otimes \rep{3}^* = \ytableaushort{\none}*{1}*[*(highlight)]{1} \otimes \ydiagram{1,1} = \ytableaushort{\none,\none,\none}*{1,1,1}*[*(highlight)]{0,0,1} \oplus \ytableaushort{\none,\none}*{2,1}*[*(highlight)]{1+1,0} = \rep{1} \oplus \rep{8}
    \end{equation}
    where we've used
    \begin{equation}
        \dim \ydiagram{1,1,1} = \ytableausetup{smalltableaux} \frac{\ytableaushort{3,2,1}}{\ytableaushort{3,2,1}} \ytableausetup{nosmalltableaux} = \frac{3 \cdot 2}{3 \cdot 2} = 1,
    \end{equation}
    and
    \begin{equation}
        \dim \ydiagram{2,1} = \ytableausetup{smalltableaux} \frac{\ytableaushort{34,2}}{\ytableaushort{31,1}} \ytableausetup{nosmalltableaux} = \frac{3 \cdot 4 \cdot 2}{3} = 8.
    \end{equation}
    Since there's only one irreducible representation of dimension 1 and only one irreducible representation of dimension 8 we can identify these as \(\rep{1}\) and \(\rep{8}\) respectively.
    
    A term like \(\rep{3} \otimes \rep{3}^*\) corresponds to a meson, with \(\rep{3}\) being a quark and \(\rep{3}^*\) an antiquark, which we'll see more later.
    
    Another case we may want to consider is \(\rep{3} \otimes \rep{3} \otimes \rep{3}\), which corresponds to a baryon, with three quarks.
    This is given by
    \begin{align}
        \rep{3} \otimes \rep{3} \otimes \rep{3} &= \ytableaushort{\none}*{1}*[*(highlight)]{1} \otimes \ytableaushort{\none}*{1}*[*(my blue)]{1} \otimes \ydiagram{1}\\
        &= \ytableaushort{\none}*{1}*[*(highlight)]{1} \otimes \left( \ytableaushort{\none,\none}*{1,1}*[*(my blue)]{0,1} \oplus \ytableaushort{\none}*{2}*[*(my blue)]{1+1} \right)\\
        &= \ytableaushort{\none}*{1}*[*(highlight)]{1} \otimes \ytableaushort{\none,\none}*{1,1}*[*(my blue)]{0,1} \oplus \ytableaushort{\none}*{1}*[*(highlight)]{1} \otimes \ytableaushort{\none}*{2}*[*(my blue)]{1+1}\\
        &= \ytableaushort{\none,\none,\none}*{1,1,1}*[*(my blue)]{0,1,0}*[*(highlight)]{0,0,1} \oplus \ytableaushort{\none,\none}*{2,1}*[*(my blue)]{0,1}*[*(highlight)]{1+1,0} \oplus \ytableaushort{\none}*{3}*[*(my blue)]{1+1}*[*(highlight)]{2+1} \oplus \ytableaushort{\none,\none}*{2,1}*[*(my blue)]{1+1,0}*[*(highlight)]{0,1}\\
        &= \rep{1} \oplus \rep{8} \oplus \rep{10} \oplus \rep{8}\\
        &\isomorphic \rep{10} \oplus \rep{8} \oplus \rep{8} \oplus \rep{1}.
    \end{align}
    Here we've used
    \begin{equation}
        \dim \ydiagram{3} = \ytableausetup{smalltableaux} \frac{\ytableaushort{345}}{\ytableaushort{321}} \ytableausetup{nosmalltableaux} = \frac{5 \cdot 4 \cdot 3}{3 \cdot 2 \cdot 1} = 10.
    \end{equation}
    
    \subsection{General Case}
    Suppose we want to take the tensor product of two irreducible representations where one of the representations is \emph{not} \(\rep{3}\).
    We need a slightly more general method.
    Draw the two Young diagrams out.
    In the second place the label \(a_i\) in all boxes in the \(i\)th row, so all the boxes in the first row are labelled \(a_1\), all the boxes in the second row are labelled \(a_2\) etc.
    Then take the boxes of the second diagram and add them to the first such that the following rules are satisfied:
    \begin{itemize}
        \item The resulting diagram must be a valid Young diagram, meaning each row has at most as many boxes as the row above and the boxes are left and top justified.
        \item The number of boxes in the new diagram is equal to the sum of the number of boxes in the two initial diagrams.
        \item For \(\unitary(n)\) no diagram can have more than \(n\) rows, for \(\specialUnitary(n)\) no diagram can have more than \(n - 1\) rows.
        \item If we read the diagram top to bottom \emph{right} to \emph{left} then at any point the number of \(a_i\)s encountered must not exceed the number of \(a_{i-1}\)s encountered at that point.
        \item The numbers must not increase when reading from left to right along a row.
        \item The numbers must decrease when reading a column from top to bottom.
    \end{itemize}
    
    For example, if \(n \ge 3\) then we have
    \begin{equation}
        \ytableausetup{smalltableaux}
        \ydiagram{3} \otimes \ytableaushort{{a_1}{a_1},{a_2}} = \ytableaushort{{}{}{}{a_1}{a_1},{a_2}}
        \oplus \ytableaushort{{}{}{}{a_1},{a_1}{a_2}} \oplus \ytableaushort{{}{}{}{a_1},{a_1},{a_2}} \oplus \ytableaushort{{}{}{},{a_1}{a_1},{a_2}}
        \ytableausetup{nosmalltableaux}
    \end{equation}
    For \(n = 2\) we have
    \begin{equation}
        \ytableausetup{smalltableaux}
        \ydiagram{3} \otimes \ytableaushort{{a_1}{a_1},{a_2}} = \ytableaushort{{}{}{}{a_1}{a_1},{a_2}}
        \oplus \ytableaushort{{}{}{}{a_1},{a_1}{a_2}}
        \ytableausetup{nosmalltableaux}
    \end{equation}
    
    \subsection{Conjugate Diagrams}
    If we know the diagram corresponding to a particular representation, \(\rho\), then the diagram corresponding to \(\rho^*\) is the \defineindex{conjugate diagram}.
    This is given by the Young diagram required to complete an \(n \times r\) rectangle, where \(r\) is the number of boxes in the first row.
    Rotating this by \ang{180} then gives the conjugate diagram.
    For example, we've seen that
    \begin{equation}
        \rep{3} = \ydiagram{1} \longrightarrow \ytableaushort{\none,\none,\none}*{1,1,1}*[*(highlight)]{0,1,1}
    \end{equation}
    and we needed to add two blocks below this to get a \(3 \times 1\) rectangle, so
    \begin{equation}
        \rep{3}^* = \ydiagram{1,1}.
    \end{equation}
    As another example consider
    \begin{equation}
        \rep{6} = \ydiagram{2} \longrightarrow \ytableaushort{\none,\none,\none}*{2,2,2}*[*(highlight)]{0,2,2}.
    \end{equation}
    We needed a \(2 \times 2\) block to fill this out to a \(3 \times 2\) rectangle, so
    \begin{equation}
        \rep{6}^* = \ydiagram{2,2}.
    \end{equation}
    Finally, consider
    \begin{equation}
        \rep{8} = \ydiagram{2,1} \longrightarrow \ytableaushort{\none,\none,\none}*{2,2,2}*[*(highlight)]{0,1+1,2}.
    \end{equation}
    We needed another diagram of the same shape to form a \(3\times 2\) rectangle, so
    \begin{equation}
        \rep{8}^* = \ydiagram{2,1} = \rep{8},
    \end{equation}
    which is what we expect as \(\rep{8}\) is a real representation.
    
    \part{Internal Symmetries}
    \chapter{Flavour Symmetries}
    \section{\texorpdfstring{\(\specialUnitary(2)\)}{SU(2)} and \texorpdfstring{\(\specialUnitary(3)\)}{SU(3)} Flavour Symmetries}
    In this section we will look at approximate symmetries in nuclear physics.
    Be careful not to confuse these with the exact \(\specialUnitary(2)\) and \(\specialUnitary(3)\) symmetries arising as gauge symmetries for the weak and strong force respectively.
    The symmetries in this section are only good symmetries to the extent that the quarks can be treated as if they were massless.
    
    In the quark model the proton, \(\Pp\), is formed from two up quarks, \(\Pu\), and two down quarks, \(\Pd\), and the neutron, \(\Pn\), is formed from one up quark and two down quarks.
    The up quark has charge \(Q_{\Pu} = 2/3\), as a multiple of the magnitude of the charge of the electron, and the down quark charge \(Q_{\Pd} = -1/3\).
    We will also consider the strange quark, which also has charge \(Q_{\Ps} = -1/3\).
    These are the three lightest quarks, and as such are the quarks for which these symmetries hold best.
    
    \subsection{\texorpdfstring{\(\specialUnitary(2)\)}{SU(2)} Flavour Symmetry}
    We can package the up and down quark into an \(\specialUnitary(2)\) doublet, \(\rep{2}\), so we consider
    \begin{equation}
        \begin{pmatrix}
            \Pu\\ \Pd
        \end{pmatrix}
        .
    \end{equation}
    Hadrons are formed from three quarks, which in this model means that they are in the \(\rep{2} \otimes \rep{2} \otimes \rep{2}\) representation.
    We can decompose this as
    \begin{align}
        \rep{2} \otimes \rep{2} \otimes \rep{2} &= \ydiagram{1} \otimes \ydiagram{1} \otimes \ydiagram{1}\\
        &= \ydiagram{2,1} \oplus \ydiagram{3} \oplus \ydiagram{2,1}\\
        &= \rep{2} \oplus \rep{4} \oplus \rep{2}
    \end{align}
    The proton and neutron then transform under one of the doublets on the right hand side here and we consider
    \begin{equation}
        \begin{pmatrix}
            \Pp\\ \Pn
        \end{pmatrix}
        .
    \end{equation}
    
    We can also form quark-antiquark pairs, transforming under \(\rep{2} \otimes \rep{2}^*\):
    \begin{align}
        \rep{2} \otimes \rep{2}^* &= \ydiagram{1} \otimes \ydiagram{1}\\
        &= \ydiagram{1,1} \oplus \ydiagram{2}\\
        &= \rep{1} \oplus \rep{3}
    \end{align}
    The pions, \(\Ppip\), \(\Ppizero\), and \(\Ppim\), are in the \(\specialUnitary(2)\) triplet, \(\rep{3}\).    Their quark content is \(\Pu\APd\) for \(\Ppip\), \(\Pd\APu\) for \(\Ppim\), and a superposition of \(\Pu\APu\) and \(\Pd\APd\) for \(\Ppizero\).
    We then consider
    \begin{equation}
        \begin{pmatrix}
            \Ppip\\ \Ppizero\\ \Ppim
        \end{pmatrix}
        .
    \end{equation}
    
    \subsection{\texorpdfstring{\(\specialUnitary(3)\)}{SU(3)} Flavour Symmetry}
    If we also want to include the strange quark then we instead use \(\specialUnitary(3)\) in the place of \(\specialUnitary(2)\).
    We can then package the three light quarks as an \(\specialUnitary(3)\) triplet:
    \begin{equation}
        \begin{pmatrix}
            \Pu\\ \Pd\\ \Ps
        \end{pmatrix}
        .
    \end{equation}
    As we've already seen we then have mesons transforming under
    \begin{equation}
        \rep{3} \otimes \rep{3}^* = \rep{1} \oplus \rep{8}.
    \end{equation}
    Those particles which don't transform trivially form the meson octet of \(\specialUnitary(3)\):
    \begin{equation}
        \tikzsetnextfilename{meson-octet}
        \begin{tikzpicture}[
            baseline = (current bounding box),
            particle/.style = {inner sep=0pt, circle, draw=highlight, fill=highlight, minimum width=0.1cm},
            font=\scriptsize
            ]
            \node[particle] (A) at (1/2, 1) {};
            \node[particle] (B) at (1, 0) {};
            \node[particle] (C) at (1/2, -1) {};
            \node[particle] (D) at (-1/2, -1) {};
            \node[particle] (E) at (-1, 0) {};
            \node[particle] (F) at (-1/2, 1) {};
            \node[particle] (G) at (0, 0) {};
            \node[above] at (A) {\PKp};
            \node[right] at (B) {\Ppip};
            \node[below] at (C) {\APKzero};
            \node[below] at (D) {\PKm};
            \node[left] at (E) {\Ppim};
            \node[above] at (F) {\PKzero};
            \node[above] at (G) {\Peta};
            \node[below] at (G) {\Ppizero};
        \end{tikzpicture}
    \end{equation}
    Notice that we have two particles in the twice degenerate centre state.
    
    It is conventional not to use orthogonal axes here, but instead use the axes
    \begin{equation}
        \tikzsetnextfilename{meson-octet-axes}
        \begin{tikzpicture}[
            baseline = (current bounding box)
            ]
            \draw (1/2, 1) -- (1, 0) -- (1/2, -1) -- (-1/2, -1) -- (-1, 0) -- (-1/2, 1) -- cycle;
            \draw[->, rotate=30] (-1.5, 0) -- (1.5, 0) node [right] {\(Q\)};
            \draw[->] (0, -1.5) -- (0, 1.5) node [left] {\(S\)};
        \end{tikzpicture}
    \end{equation}
    labelling particles by their charge, \(Q\), and strangeness, \(S\).
    The particles on top, such as \(\PKp\), have strangeness \(S = 1\), the particles in the middle, such as \(\Ppim\), have strangeness \(S = 0\), and the particles on the bottom, such as \(\APKzero\), have strangeness \(S = -1\).
    These particles are all pseudoscalars, (spin \(1/2\) and negative parity).
    
    Sometimes the meson octet has an extra particle added in the centre, in which case we call it a nonet.
    This extra particle, \(\Peta'\), transforms under an \(\specialUnitary(3)\) singlet.
    There is also a vector (spin 1) meson octet in which \(\symrm{K}\) is replaced with \(\symrm{K}^*\), \(\uppi\) with \(\uprho\), \(\upeta\) with \(\upomega\), and \(\upeta'\) with \(\upvarphi\), which is again an \(\specialUnitary(3)\) singlet.
    
    Similarly, we can form baryons from three quarks,
    \begin{equation}
        \rep{3} \otimes \rep{3} \otimes \rep{3} = \rep{10} \oplus \rep{8} \oplus \rep{8} \oplus \rep{1}.
    \end{equation}
    We therefore also get a baryon octet:
    \begin{equation}
        \tikzsetnextfilename{baryon-octet}
        \begin{tikzpicture}[
            baseline = (current bounding box),
            particle/.style = {inner sep=0pt, circle, draw=highlight, fill=highlight, minimum width=0.1cm},
            font=\scriptsize
            ]
            \node[particle] (A) at (1/2, 1) {};
            \node[particle] (B) at (1, 0) {};
            \node[particle] (C) at (1/2, -1) {};
            \node[particle] (D) at (-1/2, -1) {};
            \node[particle] (E) at (-1, 0) {};
            \node[particle] (F) at (-1/2, 1) {};
            \node[particle] (G) at (0, 0) {};
            \node[above] at (A) {\Pp};
            \node[right] at (B) {\Psigmap};
            \node[below] at (C) {\Pxizero};
            \node[below] at (D) {\Pxim};
            \node[left] at (E) {\Psigmam};
            \node[above] at (F) {\Pn};
            \node[above] at (G) {\Psigmazero};
            \node[below] at (G) {\Plambda};
        \end{tikzpicture}
    \end{equation}
    There is also a baryon decuplet:
    \begin{equation}
        \tikzsetnextfilename{baryon-decuplet}
        \begin{tikzpicture}[
            particle/.style = {inner sep=0pt, circle, draw=highlight, fill=highlight, minimum width=0.1cm},
            font=\scriptsize,
            baseline = (current bounding box)
            ]
            \node[particle] (A) at (-3/2, 1) {};
            \node[particle] (B) at (-1/2, 1) {};
            \node[particle] (C) at (1/2, 1) {};
            \node[particle] (D) at (3/2, 1) {};
            \node[particle] (E) at (-1, 0) {};
            \node[particle] (F) at (0, 0) {};
            \node[particle] (G) at (1, 0) {};
            \node[particle] (H) at (-1/2, -1) {};
            \node[particle] (I) at (1/2, -1) {};
            \node[particle] (J) at (0, -2) {};
            \node[above] at (A) {\Pdeltam};
            \node[above] at (B) {\Pdeltazero};
            \node[above] at (C) {\Pdeltap};
            \node[above] at (D) {\Pdeltapp};
            \node[above] at (E) {\Psigmastarm};
            \node[above] at (F) {\Psigmastarzero};
            \node[above] at (G) {\Psigmastarp};
            \node[below] at (H) {\Pxistarm};
            \node[below] at (I) {\Pxistarzero};
            \node[below] at (J) {\Pomegam};
        \end{tikzpicture}
    \end{equation}
    Again, we use skew axes
    \begin{equation}
        \tikzsetnextfilename{baryon-decuplet-axes}
        \begin{tikzpicture}[
            >={Latex[width=1.5mm]},
            baseline = (current bounding box)
            ]
            \draw (-3/2, 1) -- (3/2, 1) -- (0, -2) -- cycle;
            \draw (-1, 0) -- (-1/2, 1) -- (0, 0) -- (1/2, 1) -- (1, 0);
            \draw (-1/2, -1) -- (0, 0) -- (1/2, -1) -- cycle;
            \draw (-1, 0) -- (1, 0);
            \draw[->, rotate=33.69] (-1.3, 0) -- (2.2, 0) node [right] {\(Q\)};
            \draw[->] (0, -2.2) -- (0, 1.7) node [left] {\(S\)};
        \end{tikzpicture}
    \end{equation}
    
    \subsection{General Points}
    \subsubsection{Charge}
    All particles in the same \(U\)-spin multiplet have the same charge, \(Q\).
    The charge generator, \(Q\), must therefore satisfy \(\commutator{Q}{U_{\pm}} = \commutator{Q}{U_3} = 0\), so that any way of creating a particle in a \(U\)-spin multiplet has the same charge.
    Writing \(Q = aI_3 + bY\) we can use the commutators we've already given and find that \(b = a/2\).
    We then make the choice that \(a = 1\) in order to get the expected charge of particles like the proton, which has \(Y = 1\) and \(I_3 = 1/2\) and we want \(Q = 1\) in this case.
    From this we get
    \begin{equation}
        Q = I_3 + \frac{1}{2}Y.
    \end{equation}
    Recall that \(I_3 = \lambda_3/2\) and \(Y = \lambda_8/\sqrt{3}\), so we have
    \begin{equation}
        Q = \frac{1}{3}
        \begin{pmatrix}
            2 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & -1
        \end{pmatrix}
    \end{equation}
    in the fundamental representation.

    Using the skew axes corresponds to defining
    \begin{equation}
        Y = S + B
    \end{equation}
    where \(B = 1\) for baryons and \(B = 0\) for mesons, and then \(S\) is the strangeness.
    
    \subsubsection{Symmetry Breaking}
    The assumption is that any particles in the same representation have the same mass.
    This is not the case, but it is close.
    We say that the \(\specialUnitary(3)\) symmetry is softly broken, since the symmetry breaking is negligible when the particle masses are negligible.
    
    If \(\Pq_r\) is a quark spinor with flavour \(r = 1, 2, 3\), or \(r = \Pu, \Pd, \Ps\), then we can write down a Lagrangian for a noninteracting quark field as
    \begin{equation}
        \lagrangianDensity = \diracadjoint{\Pq}_r \delta_{rs} i\slashed{\partial} \Pq_s - \diracadjoint{\Pq} M_{rs} \Pq_s
    \end{equation}
    with \(M\) a \(3 \times 3\) Hermitian mass matrix.
    
    For the \(\specialUnitary(3)\) symmetry we need all the masses to be the same, corresponding to \(M_{rs} = m \delta_{rs}\).
    Then the Lagrangian is invariant under the transformation \(\Pq \mapsto U \Pq\) for \(U \in \specialUnitary(3)\), since the antiquarks transform as \(\diracadjoint{\Pq} \to \diracadjoint{\Pq}U^{\hermit}\).
    
    Experimentally we know that the up quark (\(m_{\Pu} = \qty{2.3}{\mega\electronvolt}\)) and down quark (\(m_{\Pd} = \qty{4.8}{\mega\electronvolt}\)) have pretty much the same mass.
    The strange quark (\(m_{\Ps} = \qty{95}{\mega\electronvolt}\)) is more massive, and so as \(\abs{S}\) increases so too does the mass of the particles.
    
    If we say that both the up and down quark have mass \(m\) and then the strange quark has mass \(m' = m + \delta m\) we have
    \begin{equation}
        M_{rs} = \diag(m, m, m + \delta m) = \left( m + \frac{1}{3}\delta m \right) \delta_{rs} - \frac{1}{\sqrt{3}}\lambda_{rs}^8 \delta m.
    \end{equation}
    The symmetry is thus softly broken by a component proportional to \(\diracadjoint{\psi} \lambda^8 \psi\).
    In the case when \(\delta m\) is much smaller than all relevant energy scales this symmetry breaking becomes negligible and we say that the symmetry is restored.
    
    \subsection{Gell-Mann--Okubo Mass Formula}
    We can treat the symmetry breaking perturbatively, with the perturbation Hamiltonian
    \begin{equation}
        \delta H = \frac{1}{\sqrt{3}} \delta m_1 \lambda_8 + \frac{1}{4} \delta m_2 d_{8ab} \lambda_a \lambda_b
    \end{equation}
    where \(d_{abc} = \tr(\lambda_i \anticommutator{\lambda_j}{\lambda_k})/4\) is one of the invariant tensors of \(\specialUnitaryLie(3)\), and \(\delta m_i\) are free parameters.
    
    This works since \(\lambda_8\) and \(d_{8ab}\lambda_a\lambda_b\) are the only two independent octet 8th component operators.
    It can be shown that
    \begin{equation}
        d_{8ab} = -\frac{1}{2\sqrt{3}} \sum_{a = 1}^8 \lambda_a \lambda_a + \frac{3}{2\sqrt{3}} \sum_{i = 1}^{3} \lambda_i \lambda_i - \frac{1}{2\sqrt{3}} \lambda_8^2.
    \end{equation}
    The first term is a Casimir, and so gives the same value for all states in the same representation.
    Further it is simply a constant times the identity matrix in this representation by Schur's lemma, and we absorb this constant into the definition of \(m_0\).
    The sum in the second term is \(4I^2\), and \(\lambda_8^2 = 3Y^2\).
    So we have
    \begin{equation}
        m = m_0 + \delta m_1 Y + \delta m_2 (I (I + 1) - Y^2/4)
    \end{equation}
    with \(m_0\) the unperturbed mass and we absorb some constant factors into \(\delta m_i\).
    Applying this to the baryon octet we have four masses and three unknowns, so we're left with one equation:
    \begin{equation}
        \frac{1}{2}(m_{\symrm{N}} + m_{\upXi}) = \frac{3}{4}m_{\upLambda} + \frac{1}{4}m_{\upSigma}
    \end{equation}
    where \(\symrm{N}\) is either a neutron (\(\symrm{N}^0\)) or proton (\(\symrm{N}^+\)).
    This formula holds very well, with the left hand side equating to \qty{1127}{\mega\electronvolt} and the right hand side \qty{1134}{\mega\electronvolt}.
    
    Applying this formula to the baryon decuplet, and noting that \(Y = 2(I - 1)\) for the decuplet, the terms quadratic in \(I\) and \(Y\) cancel and we find that the \(\delta m_2\) term is proportional to \(Y\) plus a constant, and the mass is then \(m = m_0 + \delta m Y\).
    This is called the equal spacing rule, stating that moving up the decuplet, so up in strangeness, increases the mass by a fixed amount, \(\delta m\).
    Using the masses of \(\upDelta\), \(\upSigma^*\), and \(\upXi^*\) we find \(\delta m \approx \qty{145}{\mega\electronvolt}\).
    This lead to a prediction of the, at the time undiscovered, \(\Pomegam\), as having mass \qty{1675}{\mega\electronvolt}, and the measured value is \qty{1672}{\mega\electronvolt}.
    
    \subsection{Colour}
    The \(\Pdeltapp\) state is formed from three up quarks, \(\Pu\Pu\Pu\).
    The \(\Pdeltapp\) has spin \(3/2\), so all three quarks have spin \(1/2\).
    This contradicts the Pauli exclusion principle.
    The solution is to introduce another, exact, symmetry, called colour \(\specialUnitary(3)\).
    Note that it is a coincidence that we consider flavour \(\specialUnitary(3)\) and colour \(\specialUnitary(3)\), both \(\specialUnitary(N)\) with \(N = 3\).
    If we considered four quarks then we would use flavour \(\specialUnitary(4)\), but still colour \(\specialUnitary(3)\).
    However, flavour \(\specialUnitary(4)\) is not a very good symmetry as the charm quark is much heavier again.
    
    We can construct meson and baryon fields from quark fields with flavour indices, \(r = 1, 2, 3\) or \(r = \Pu, \Pd, \Ps\), and colour indices, \(a = 1, 2, 3\), or \(a = \text{red}, \text{green}, \text{blue}\).
    For mesons these fields are symmetric
    \begin{equation}
        \diracadjoint{\Pq}_r^a \Pq_s^b \delta_{ab},
    \end{equation}
    and for baryons the are antisymmetric:
    \begin{equation}
        \Pq_r^a \Pq_s^b \Pq_t^c \varepsilon_{abc}.
    \end{equation}
    This works as \(\delta_{ab}\) and \(\varepsilon_{abc}\) are both \(\specialUnitary(3)\) invariant tensors.
    
    The \(\Pdeltapp\) is then antisymmetric in exchange of the quarks, and Pauli's exclusion principle is no longer violated.
    Colour symmetry turns out to be important for describing strong interactions.
    One important fact is that physical states are singlets under colour \(\specialUnitary(3)\).
    This means physical states are colour neutral.
    
    \chapter{Internal Symmetry}
    \epigraph{A physicist theorem, so that should immediately raise your suspicion.}{Neil Turok}
    \section{Coleman--Mandula}
    \begin{dfn}{Internal Symmetry}{}
        An \defineindex{internal symmetry}, is a symmetry from a group \(G\) whose action commutes with the action of the Poincar\'e group.
    \end{dfn}
    
    In terms of group elements if \(g \in G\) with \(D\) a representation of \(G\) and \(U(a, \Lambda)\) is a Poincar\'e transformation in a representation of the same dimension then
    \begin{equation}
        D(g)U(a, \Lambda) = U(a, \Lambda)D(g).
    \end{equation}
    In terms of the Lie algebras if \(T_a\) are the generators of \(\lie{g}\), the Lie algebra of \(G\), then
    \begin{equation}
        \commutator{T_a}{P_\mu} = \commutator{T_a}{M_{\mu\nu}} = 0.
    \end{equation}
    
    Intuitively an internal symmetry is one with no spacetime dependence.
    
    The following theorem is provided without proof, but we will shortly give a simplified argument as to its truth.
    
    \begin{thm}{Coleman--Mandula}{}
        Under some technical assumptions if there exists nontrivial scattering between localised particle states then the only possible conserved charges are those associated with \(P^\mu\), \(M^{\mu\nu}\), and the geenrators of the Poincar\'e group and charges \(Q^a\) associated with a compact internal symmetry group, \(G\), and the \(Q^a\) commute with the \(P^\mu\) and \(M^{\mu\nu}\).
        
        More concisely:
        Under a general set of assumptions the symmetry group of any nontrivial theory is the direct product of the Poincar\'e group and a compact internal symmetry group, \(G\).
    \end{thm}
    
    Compactness means that the quantum numbers associated with the internal symmetry must be discrete.
    We consider a multiplet of states related by the internal symmetry group, \(G\): \(\{g\ket{\Phi} \mid g \in G\}\).
    If \(P_\mu \ket{\Phi} = p_\mu\ket{\Phi}\) then we have
    \begin{equation}
        P_\mu g \ket{\Phi} = gP_\mu \ket{\Phi} = p_\mu g\ket{\Phi}.
    \end{equation}
    Hence all states in the multiplet have the same spacetime properties, including having the same eigenvalues for the Casimir operators of the Poincar\'e group.
    This means that all of the states in the multiplet have the same mass and spin.
    
    Suppose these states, \(\ket{r}\), are created by a field, \(\Phi_r\), so \(\ket{r} = \Phi_r\ket{0}\).
    Then if the Lagrangian has the symmetry
    \begin{equation}
        \Phi_r \mapsto g \Phi_r g = D_{rs}(g)\Phi_s
    \end{equation}
    and if the vacuum is invariant under \(G\), so \(g \ket{0} = \ket{0}\), then under the action of \(G\)
    \begin{equation}
        \ket{r} \mapsto g\ket{r} = g\Phi_r g^{-1}\ket{-} = D_{rs}(g)\Phi_s \ket{0} = D_{rs}(g)\ket{s},
    \end{equation}
    and so the states are in the same representation as the field as they both transform in the same way:
    \begin{equation}
        \Phi_r \mapsto D_{rs}(g) \Phi_s, \qqand \ket{r} \mapsto D_{rs}(g)\ket{s}.
    \end{equation}
    
    If we work in a unitary representation the normalisation, \(\braket{r}{s} \propto \delta_{rs}\), is unchanged by the action of \(G\).
    For real fields we can instead choose orthogonal representations.
    
    \subsection{Simplified Argument}
    Consider a free massless theory with two fields, \(\varphi_1\) and \(\varphi_2\) forming a doublet
    \begin{equation}
        \varphi = 
        \begin{pmatrix}
            \varphi_1\\ \varphi_2
        \end{pmatrix}
        .
    \end{equation}
    Suppose that we have an \(\specialOrthogonal(2)\) internal symmetry,
    \begin{equation}
        \varphi \mapsto O \varphi
    \end{equation}
    for \(O \in \specialOrthogonal(2)\).
    The action for such a theory is given by
    \begin{equation}
        S = \int \dl{^4x} \, [-(\partial_\mu \varphi_1)(\partial^\mu \varphi_1) - (\partial_\mu \varphi_2)(\partial^\mu \varphi_2)].
    \end{equation}
    
    We have the Noether current associated with this \(\specialOrthogonal(2)\) symmetry:
    \begin{equation}
        J_\mu^{\symrm{N}} = (\partial_\mu \varphi_1) \varphi_2 - \varphi_1(\partial_\mu \varphi_2).
    \end{equation}
    It is simple to show this is conserved:
    \begin{equation}
        \partial^\mu J_\mu^{\symrm{N}} = (\dalembertian \varphi_1)\varphi_2 + (\partial_\mu \varphi_1)(\partial^\mu \varphi_2) - (\dalembertian \varphi_2)\varphi_1 - (\partial_\mu \varphi_2)(\partial^\mu \varphi_1) = 0.
    \end{equation}
    The first and third terms vanish by the equations of motion, which are simply \(\dalembertian \varphi_i = 0\), and the second and fourth terms cancel.
    
    We also have the conserved currents
    \begin{equation}
        J_{\mu\nu} = (\partial_\mu \varphi_1)(\partial_\nu \varphi_2) - \varphi_1 \partial_\mu \partial_\nu \varphi_2.
    \end{equation}
    We can show conservation of this current also:
    \begin{equation}
        \partial^\mu J_{\mu\nu} = (\dalembertian \varphi_1)(\partial_\nu \varphi_2) + (\partial_\mu \varphi_1)(\partial_\mu \partial_\nu \varphi_2) - (\partial^\mu \varphi_1)(\partial_\mu \partial_\nu \varphi_2) - \varphi_1 \dalembertian \partial_\nu \varphi_2 = 0.
    \end{equation}
    Again, the first and fourth terms vanish by the equations of motion and the second and third terms cancel.
    
    We also have the conserved currents
    \begin{equation}
        J_{\mu\nu\rho} = (\partial_\mu \varphi_1) (\partial_\nu \partial_\rho \varphi_2) - \varphi_1 \partial_\mu \partial_\nu \partial_\rho \varphi_2
    \end{equation}
    which we can show is conserved in exactly the same way.
    In general the currents
    \begin{equation}
        J_{\mu_1 \dotsm \mu_n} = (\partial_{\mu_1} \varphi_1) \left( \prod_{i=2}^{n} \partial_{\mu_i} \right) \varphi_2 - \varphi_1 \left( \prod_{i=1}^{n} \partial_{\mu_i} \right) \varphi_2
    \end{equation}
    are conserved.
    
    Now we can try to add an interaction to get nontrivial scattering, which we do through adding a potential \(V(\varphi_1, \varphi_2)\).
    For the Noether current as long as \(V(\varphi_1 \varphi_2)\) is invariant under \(\specialOrthogonal(2)\), which is the case if it is a constructed from terms like \(\varphi_1^2 + \varphi_2^2\), \((\varphi_1^2 + \varphi_2^2)^2\), etc.\@ then \(J_\mu^{\symrm{N}}\) is conserved.
    
    We cannot conserve the other current if the interaction is nontrivial, that is if \(V(\varphi_1, \varphi_2) \ne 0\).
    To see this suppose there exists some conserved charge \(Q_{\mu\nu}\) associated with one of the currents, which is symmetric in \(\mu\) and \(\nu\).
    We can also take \(Q_{\mu\nu}\) to be traceless since the trace is a scalar and not interesting in this argument.
    
    Consider the \(2 \to 2\) elastic scattering process with two incoming particles of momenta \(q_1\) and \(q_2\), and two outgoing particles of momenta \(p_1\) and \(p_2\).
    We can calculate the matrix elements of \(Q_{\mu\nu}\) before and after scattering, and since if we assume sufficient time passes between these measurements and the interaction then these should give the same result as both consist of the operator \(Q_{\mu\nu}\) acting on two free particles of the same type with the same total momentum:
    \begin{equation}
        \bra{p_1, p_2} Q_{\rho\sigma} \ket{p_1, p_2} = \bra{q_1, q_2} Q_{\rho\sigma} \ket{q_1, q_2}.
    \end{equation}
    This means that we must have a traceless symmetric quantity in terms of \(p_i\) on the left and \(q_i\) on the right.
    We can construct such a quantity easily.
    For \(p_i\) we have
    \begin{equation}
        p_{1\sigma}p_{1\sigma} - \frac{1}{D}\minkowskiMetric_{\rho\sigma} p_1^2 + p_{2\rho}p_{2\rho} - \frac{1}{D}\minkowskiMetric_{\rho\sigma}p_2^2
    \end{equation}
    and we can do exactly the same swapping \(p\) and \(q\).
    Note that in this expression \(D\) is the number of spacetime dimensions, which we've been taking as 4 so far, but this argument works in any number of dimensions.
    
    Conservation of four-momentum, or in terms of symmetries one of the conserved currents of the Poincar\'e group, gives
    \begin{equation}
        p_1^\mu + p_2^\mu = q_1^\mu + q_2^\mu.
    \end{equation}
    Conservation of \(Q_{\mu\nu}\) gives
    \begin{equation}
        p_1^\mu p_1^\nu + p_2^\mu p_2^\nu = q_1^\mu q_1^\nu + q_2^\mu q_2^\nu.
    \end{equation}
    
    Here we have \(2D\) unknowns.
    Conservation of four-momentum gives \(D\) equations.
    Conservation of \(Q_{\mu\nu}\) gives \(D(D + 1)/2 - 1\) equations, we have \(D(D + 1)/2\) components of a symmetric \(D \times D\) matrix, and we lose one degree of freedom by fixing \(\tr Q = 0\).
    
    If \(D = 2\) then we have 4 unknowns and 4 equations, and everything works out nicely.
    If \(D > 2\) then we have an overdetermined system, with more equations than unknowns.
    One solution in this case is \(p_1 = q_1\) and \(p_2 = q_2\).
    This just corresponds to nothing changing in the scattering, which is just the trivial case.
    Another solution is \(p_1 = q_2\) and \(p_2 = q_1\), but again this is just the trivial case if we swap the labels of the particles.
    There are no other solutions, so we see that we can only have conservation of \(Q_{\mu\nu}\) if scattering is trivial.
    
    There are ways around this argument, and around the theorem as a whole.
    One is supersymmetry, in which we get conserve charges with an anticommutator algebra, as opposed to the commutator algebra we get normally.
    
    In the case where all particles are massless it is also possible to have a conformal symmetry, where we can rescale lengths, called a dilation, \(x \mapsto \alpha x\) and translate, \(x \mapsto x + a\).
    Such a symmetry allows for so called conformal transformations
    \begin{equation}
        x \mapsto \frac{x - x^2 a}{1 - 2x \cdot a + a^2 x^2}.
    \end{equation}
    This is only a valid symmetry in the massless case, since the commutation relation of the generators of this conformal symmetry, \(K_{\mu\nu}\), as well as the dilations, \(D\), give something of the form \(\commutator{D}{P^2} \sim P^2\) and \(\commutator{K_{\mu\nu}}{P^2} \sim P^2\), which are zero only in the massless case.
    
    
    \chapter{Spontaneous Symmetry Breaking}
    Scalar fields can break internal symmetries spontaneously by taking a nonzero value in the vacuum.
    
    \section{Abelian Symmetry}\label{sec:abelian symmetry spontaneous symmetry breaking}
    Consider a complex scalar field, \(\varphi = \varphi_1 + i\varphi_2\) with \(\varphi_i\) real fields.
    Then \(\varphi\) has a global \(\unitary(1)\) symmetry, that is a symmetry independent of position, given by the action
    \begin{equation}
        \varphi \mapsto \varphi' = \e^{i\alpha} \varphi.
    \end{equation}
    
    A Lagrangian for which this is a symmetry is
    \begin{equation}
        \lagrangianDensity = -(\partial_\mu \varphi)(\partial^\mu \varphi) - \lambda(\abs{\varphi}^2 - v^2)^2
    \end{equation}
    with \(\abs{\varphi}^2 = \varphi^*\varphi\).
    The energy in this case is given by
    \begin{equation}
        E = \int \dl{^3\vv{x}} \, (\abs{\partial_0 \varphi}^2 + \abs{\grad \varphi}^2 + \lambda(\abs{\varphi}^2 - v^2)^2).
    \end{equation}
    The first term is the kinetic energy, the second is the gradient energy in the field, and the third is the potential energy.
    This potential is just a rewriting of the \enquote{sombrero potential} shown in \cref{fig:sombrero potential}.
    Notice that \(E \ge 0\) with \(E\) vanishing only if all three terms vanish independently.
    
    The set of minima of \(E\) are given by \(\varphi\) independent of position, so \(\abs{\partial_0 \varphi}^2 = \abs{\grad\varphi}^2 = 0\) and \(\abs{\varphi} = v\), and so \(E = 0\).
    This is the classical vacua.
    The set of values of \(\varphi\) minimising \(E\) is a circle given by \(\abs{\varphi} = v\) and \(\arg \varphi = \alpha\), that is \(\varphi = \e^{i\alpha}v\).
    
    We can write any value of \(\varphi\) as its vacuum value plus some term and with a different phase:
    \begin{equation}
        \varphi = \left( v + \frac{\delta v}{\sqrt{2}} \right) \e^{i(\alpha + \vartheta)}
    \end{equation}
    where \(\delta v\) and \(\vartheta\) are functions of position.
    We will take these to be small and consider perturbations about the minimum.
    
    Now consider \(\partial_\mu \varphi\):
    \begin{equation}
        \partial_\mu \varphi = \left[ \frac{1}{\sqrt{2}}\partial_\mu \delta v + i\left( v + \frac{\delta v}{\sqrt{2}} \right)(\partial_\mu \vartheta) \right] \e^{i(\alpha + \vartheta)}.
    \end{equation}
    The action is then
    \begin{equation}
        S = \int \dl{^4x} \, \left( -\frac{1}{2}(\partial_\mu \delta v)(\partial^\mu \delta v) - \frac{1}{2} v^2 (\partial_\mu \vartheta)(\partial^\mu \vartheta) - \frac{\lambda}{2} (2v)^2 \delta v^2 \right).
    \end{equation}
    We can define \(f = \vartheta v\) and the action becommes
    \begin{equation}
        S = \int \dl{^4x} \, \left( -\frac{1}{2}(\partial_\mu \delta v)(\partial^\mu \delta v) - \frac{1}{2}(\partial_\mu f)(\partial^\mu f) -\frac{1}{2} \lambda 4v^2 \delta v^2 \right).
    \end{equation}
    Now compare this with the Klein--Gordon action for a scalar field \(\Phi\):
    \begin{equation}
        S = \int \dl{^4x} \, \left( -\frac{1}{2}(\partial_\mu \Phi)(\partial^\mu \Phi) - \frac{1}{2}m^2 \Phi^2 \right).
    \end{equation}
    We can see that \(f\) corresponds to a massless scalar field and \(\delta v\) corresponds to a massive scalar field with mass \(m = 2v\sqrt{\lambda}\).
    
    The field \(f\) oscillates around the circle of the vacua.
    Hence it always stays in the minimum which is flat and so it has no mass as its potential energy doesn't change as it oscillates.
    The field \(\delta v\) oscillates radially across the circle of the minima.
    Hence it's potential energy varies as it oscillates and it corresponds to massive particles.
    
    We call the massless field, \(f\), a \defineindex{Goldstone boson}.
    Breaking global symmetries always produces Goldstone bosons, although they are often not physically observable due to gauge symmetries.
    The massive field, when this potential is considered in the context of the standard model\footnote{see \course{Particle Physics}.} corresponds to the Higgs boson.
    The entire field \(\varphi\) is the Higgs field, but we often only consider \(\delta v\).
    
    \section{Non-Abelian Symmetry}
    Consider a real fields, \(\varphi^a\), transforming under some representation, \(D\), of a Lie group, \(G\):
    \begin{equation}
        \varphi^a \mapsto \varphi'^a = D^{ab}(g)\varphi^b
    \end{equation}
    for \(g \in G\).
    In other words, \(\varphi \mapsto \varphi' = D(g)\varphi\), viewing \(\varphi\) as a vector with components \(\varphi^a\) and \(D(g)\) as a matrix acting on this vector.
    For simplicity we will restrict ourselves to real representations, but this all works for complex representations also.
    
    A Lagrangian for which this is a symmetry is
    \begin{equation}
        \lagrangianDensity = -\frac{1}{2}g_{ab}(\partial_\mu \varphi^a)(\partial_\mu \varphi^b) - V(\varphi^a)
    \end{equation}
    where \(g_{ab}\) is an invariant tensor, equal to \(\delta_{ab}\) in the case of a simple Lie group, and \(V(\varphi^a)\) is invariant under the action of \(G\).
    
    The set of values of \(\varphi\) minimising the potential, \(V\), is the vacuum:
    \begin{equation}
        V_{\min} \coloneqq \left\{ \varphi^a \,\middle\vert\, \varphi^a \text{ constant and } \diffp{V}{\varphi^a} = 0 \right\}.
    \end{equation}
    We will consider the case where \(V_{\min}\) is given by applying group elements to some particular \(\varphi \in V_{\min}\), that is \(V_{\min}\) is the orbit of \(\varphi\) under \(G\).
    
    Note that this isn't always the case, for example an order 8 polynomial generalising the Higgs potential, as seen in \cref{fig:octic higgs}, may have two concentric circles of minima and no way to map one to the other through the action of \(G\).
    This only happens if the parameters are tuned just right, so we won't worry about it.
    Coincidentally this is exactly the case in supersymmetric symmetries where the presence of the extra symmetry enforces this tuning.
    
    \begin{figure}
        \tikzsetnextfilename{octic-higgs-potential}
        \begin{tikzpicture}
            \begin{axis}[
                hide axis,
                samples=50,
                domain=0:360,
                y domain=0:1.97,
                view={90}{55}
                ]
                \addplot3 [surf, shader=flat, draw=highlight, fill=highlight!20, z buffer=sort, opacity=0.3] ({sin(x)*y}, {cos(x)*y}, {0.2*y^8 - 1.5*y^6 + 3.4258*y^4 - 2.3*y^2});
            \end{axis}
        \end{tikzpicture}
        \caption{A potential with two degenerate vacua not related by a group action.}
        \label{fig:octic higgs}
    \end{figure}
    
    The \defineindex{little group}, \(H\), of some state \(\varphi \in V_{\min}\), is
    \begin{equation}
        H \coloneqq \{g \in G \mid D(g)\varphi = \varphi\},
    \end{equation}
    that is it is the subgroup of \(G\) which leaves \(\varphi\) fixed.
    This is also called the stabiliser subgroup of \(G\) with respect to \(\varphi\).
    
    We can prove that this is a subgroup as follows.
    If \(g_1, g_2 \in H\) then \(\varphi = D(g_1) \varphi = D(g_2) \varphi\).
    Multiplying on the left by \(D(g_2)^{-1}\) we get \(\varphi = D(g_2)^{-1}D(g_1)\varphi = D(g_2^{-1})D(g_1) \varphi = D(g_2^{-1}g_1)\varphi\), and hence \(g_2^{-1}g_1 \in H\), which proves \(H\) is a subgroup of \(G\).
    
    We can then write \(g_1 = g_2 h\) for some \(h \in H\).
    This is an equivalence relation, \(g_1 \sim g_2\) if \(g_1 = g_2 h\) for some \(h \in H\).
    We can then form the coset space \(G/H\).
    This space will give \(V_{\min}\).
    Note that this space is not, in general, a group as the little group is not, in general, a normal subgroup.
    Instead this quotient occurs on the level of the manifolds underlying the two Lie groups.
    
    \begin{exm}{}{}
        Consider the case where \(G = \specialOrthogonal(3)\).
        So we're considering a rotationally invariant potentially.
        Take \(\varphi^a = (\varphi^1, \varphi^2, \varphi^3)\).
        Since the potential is spherically symmetric the minima must form a sphere, \(V_{\min} = S^2\).
        Given some point on this sphere the little group is given by picking some rotation, \(h \in \specialOrthogonal(3)\) and acting on this point repeatedly.
        This generates a circle, and so \(H = \specialOrthogonal(2)\) is the little group preserving this circle, acting as rotations about the origin in the plane of this circle.
        The coset space is \(\specialOrthogonal(3) / \specialOrthogonal(2)\), and it can be shown that this space is (isomorphic to) \(V_{\min}\).
    \end{exm}
    
    \begin{thm}{Goldstone Theorem}{}
        When a global symmetry, \(G\), is spontaneously broken to a subgroup, \(H\), these there are \(\dim G - \dim H\) massless fields called Goldstone bosons.
    \end{thm}
    
    \begin{exm}{}{}
        Consider the coset space \(\specialLinear(3, \reals) / \specialOrthogonal(3)\).
        The dimension of \(\specialLinear(3, \reals)\) is 8, since it is formed from three by three matrices (9 degrees of freedom) with one degree of freedom fixed by taking the determinant to be 1.
        The dimension of \(\specialOrthogonal(3)\) is three, since a rotation can be specified by three Euler angles.
        We then expect the dimension of \(\specialLinear(3, \reals) / \specialOrthogonal(3)\) to be five, and indeed it is.
        The coset space \(\specialLinear(3, \reals)/\specialOrthogonal(3)\) is formed from real, symmetric, unit determinant three by three matrices, and there are 5 degrees of freedom after fixing the determinant and fixing the matrix to be symmetric.
        Note that this coset space is not a group.
        
        The Lie algebra \(\specialLinearLie(3, \reals)\) is formed from traceless matrices.
        One basis for these generators, is
        \begin{alignat}{3}
            &
            \begin{pmatrix}
                0 & 1 & 0\\
                1 & 0 & 0\\
                0 & 0 & 0
            \end{pmatrix}
            , \qquad &&
            \begin{pmatrix}
                0 & 0 & 1\\
                0 & 0 & 0\\
                1 & 0 & 0
            \end{pmatrix}
            , \qquad && 
            \begin{pmatrix}
                0 & 0 & 0\\
                0 & 0 & 1\\
                0 & 1 & 0
            \end{pmatrix}
            \\
            &
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & -1 & 0\\
                0 & 0 & 0
            \end{pmatrix}
            , \qquad &&
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & 1 & 0\\
                0 & 0 & -2
            \end{pmatrix}
            , \qquad &&
            \begin{pmatrix}
                0 & 1 & 0\\
                -1 & 0 & 0\\
                0 & 0 & 0
            \end{pmatrix}
            \\
            &
            \begin{pmatrix}
                0 & 0 & 1\\
                0 & 0 & 0\\
                -1 & 0 & 0
            \end{pmatrix}
            , \qquad &&
            \begin{pmatrix}
                0 & 0 & 0\\
                0 & 0 & 1\\
                0 & -1 & 0
            \end{pmatrix}
            .
        \end{alignat}
        The Lie algebra \(\specialOrthogonalLie(3)\) is formed from traceless, antisymmetric matrices.
        It is generated by the last three matrices above.
        The symmetric matrices above linearly independent from the matrices generating \(\specialOrthogonalLie(3)\) can then be exponentiated to form \(\specialLinear(3, \reals)/\specialOrthogonal(3)\).
    \end{exm}
    
    \begin{exm}{Minkowski Space}{}
        Minkowski space, \(\minkowskiSpace\), can be characterised as the coset space of the Poincar\'e group by the Lorentz group.
        The Poincar\'e group, generated by the antisymmetric \(M_{\mu\nu}\) and \(P_\mu\), is 10 dimensional and the Lorentz group, generated by \(M_{\mu\nu}\), is 6 dimensional.
        The four generators \(P_\mu\) linearly independent from those of the Lorentz group exponentiate to generate the space
        \begin{equation}
            \minkowskiMetric = \ISO(1, 3)/\specialOrthogonal(1, 3).
        \end{equation}
    \end{exm}
    
    
    \part{Gauge Symmetry}
    \chapter{Abelian Gauge Symmetry}
    \section{Electromagnetism Recap}
    \begin{rmk}
        See \course{Classical Electrodynamics} for more details.
    \end{rmk}
    The \defineindex{Maxwell tensor}, \(F^{\mu\nu}\), is defined in terms of the electric and magnetic fields, \(\vv{E}\) and \(\vv{B}\), as follows:
    \begin{equation}
        F^{\mu\nu} =
        \begin{pmatrix}
            0 & -E^1 & -E^2 & -E^3\\
            E^1 & 0 & -B^3 & B^2\\
            E^2 & B^3 & 0 & -B^1\\
            E^3 & -B^2 & B^1 & 0
        \end{pmatrix}
        .
    \end{equation}
    So \(F^{0i} = E^i\) and \(F^{ij} = -\varepsilon^{ijk}B^k\),
    Lowering the indices gives
    \begin{equation}
        F_{\mu\nu} = 
        \begin{pmatrix}
            0 & E^1 & E^2 & E^3\\
            -E^1 & 0 & -B^3 & B^2\\
            -E^2 & B^3 & 0 & B^2\\
            E^3 & -B^2 & B^1 & 0
        \end{pmatrix}
        ,
    \end{equation}
    so \(F^{\mu\nu} \mapsto F_{\mu\nu}\) is equivalent to \(E^i \mapsto -E^i\).
    
    The \defineindex{dual tensor} is\footnote{following the convention that \(\varepsilon_{0123} = +1\)}
    \begin{equation}
        \tensor[^*]{F}{^{\mu\nu}} = \frac{1}{2}\varepsilon^{\mu\nu\rho\sigma} F_{\rho\sigma}
    \end{equation}
    Note that \(F^{\mu\nu} \mapsto \tensor[^*]{F}{^{\mu\nu}}\) is equivalent to \(\vv{E} \mapsto \vv{B}\) and \(\vv{B} \mapsto -\vv{E}\).
    The current is \(J^\mu = (\rho, \vv{J})\).
    
    Using these we can write Maxwell's equations as
    \begin{equation}
        \partial_\mu F^{\mu\nu} = J^\nu, \qqand \partial_\mu \tensor[^*]{F}{^{\mu\nu}} = 0.
    \end{equation}
    The first of these tells us that \(J^\mu\) is conserved, since
    \begin{equation}
        \partial_\nu J^\nu = \partial_\nu \partial_\mu F^{\mu\nu} = 0
    \end{equation}
    since \(\partial_\nu \partial_\mu\) is symmetric in \(\mu\) and \(\nu\) whereas \(F^{\mu\nu}\) is antisymmetric.
    The second says there are no magnetic monopoles, as we exchanging the electric and magnetic field (up to a sign) doesn't give us \enquote{magnetic current}.
    
    We can replace the second equation with the equivalent \defineindex{Binanchi identity}
    \begin{equation}
        \partial_\mu F_{\nu\rho} + \partial_\nu F_{\rho\mu} + \partial_\rho F_{\mu\nu} = 0.
    \end{equation}
    Note that this is \(\partial_\mu F_{\nu\rho}\) plus cyclic permutations of the indices.
    
    When the Bianchi identity, or equivalently the second of Maxwell's equations, holds we can, at least locally, write
    \begin{equation}
        F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu
    \end{equation}
    where \(A^\mu = (\varphi, \vv{A})\) is the four potential.
    This is the analogue of how \(\div\vv{B} = 0\) allows us to write \(\vv{B} = \curl \vv{A}\) in the first place.
    
    \section{Gauge Symmetry}
    The term gauge theory was coined in general relativity, where a similar idea lead to a fundamental length, or \enquote{gauge}.
    
    The electromagnetic field, \(A_\mu\), has a gauge symmetry, if \(A_\mu\) leads to the correct physics then so does
    \begin{equation}
        A'_\mu = A_\mu + \partial_\mu \chi
    \end{equation}
    for some differentiable function of spacetime, \(\chi\).
    
    This transformation doesn't change the field strength tensor:
    \begin{align}
        F'^{\mu\nu} &= \partial^\mu A'^\nu - \partial^\nu A'^\mu\\
        &= \partial^\mu A^\nu + \partial^\mu \partial^\nu \chi - \partial^\nu A^\mu - \partial^\nu \partial^\mu \chi\\
        &= \partial^\mu A^\nu - \partial^\nu A^\mu\\
        &= F^{\mu\nu}.
    \end{align}
    
    This symmetry shows that \(A^\mu\) has some redundancy.
    Classically this is fine since we cannot measure \(A^\mu\) anyway.
    We can reduce or eliminate this redundancy by fixing the gauge.
    This is done by using the gauge symmetry to fix one of the four components of \(A^\mu\).
    
    In addition to this we have Gauss' law, \(\partial_\mu F^{\mu 0} = \partial_i F^{i0} = \div \vv{E} = J^0 = \rho\).
    This is not a dynamical equation, as it doesn't have time derivatives, instead it is a constraint, an initial condition which any field must satisfy.
    This further reduces the degrees of freedom to leave only 2 actual degrees of freedom.
    These are what we take as the two polarisations of the photon.
    
    \section{Coupling Electromagnetic Fields to Particles}
    For a free nonrelativistic particle we have
    \begin{equation}
        E = \sqrt{\vv{p}^2 + m^2} \approx m + \frac{\vv{p}^2}{2m}
    \end{equation}
    for \(\vv{p}^2 \ll m^2\).
    We can ignore the constant \(m\) term and take the Hamiltonian to be
    \begin{equation}
        H = E = \frac{\vv{p}^2}{2m}.
    \end{equation}
    Hamilton's equations then give
    \begin{equation}
        \dot{x}_i = \diffp{H}{p_i} = \frac{p_i}{m} \implies \vv{p} = m\dot{\vv{x}},
    \end{equation}
    which is just the usual definition of momentum.
    
    We can couple this particle to the electromagnetic field through the principle of minimal coupling, which states that this coupling occurs through the replacement
    \begin{equation}
        p^\mu \mapsto p^\mu - q A^\mu
    \end{equation}
    where \(q\) is the classical electric charge and \(A^\mu\) is the electromagnetic potential.
    The Hamiltonian then becomes
    \begin{equation}
        H = p^0 = \frac{\vv{p}^2}{2m} \mapsto H - q\varphi = \frac{1}{2m}(\vv{p}^2 - q\vv{A})^2.
    \end{equation}
    
    \section{Quantising}
    We can quantise by taking the momentum operator to be \(\operator{p}_\mu = i\hbar \partial_\mu\), which gives the expected operators \(\operator{p}^0 = i\hbar \partial_t\) and \(\vecoperator{p} = -i\hbar\grad\).
    Then we have
    \begin{equation}
        \left( i\hbar\diffp{}{t} - q\varphi \right) \Psi = -\frac{\hbar^2}{2m}\left( \grad - i\frac{q}{\hbar}\vv{A} \right)^2 \Psi,
    \end{equation}
    which is just Schrdinger's equation.
    This becomes more obvious if we define the \defineindex{covariant derivative}
    \begin{equation}
        \covariantDerivative_\mu = \partial_\mu + i\frac{q}{\hbar}A_\mu
    \end{equation}
    and then this equation becomes
    \begin{equation}
        i\hbar \covariantDerivative_0\Psi = -\frac{\hbar^2}{2m}\covariantDerivative_i \covariantDerivative_i \Psi,
    \end{equation}
    which is Schrdinger's equation for a free particle if the covariant derivatives were just normal derivatives.
    
    \begin{rmk}
        There is a connection between the covariant derivative here, \(\covariantDerivative_\mu\), and the covariant derivative in general relativity, \(\nabla_\mu\).
        Both are of the form
        \begin{equation}
            \text{covariant derivative} = \text{normal derivative} + \text{extra term},
        \end{equation}
        in our case the extra term here is \(ieA_\mu\), where \(e = q/\hbar\) is the gauge coupling and \(A_\mu\) is the gauge field, so the covariant derivative of a scalar field is \(\covariantDerivative_\mu \varphi = \partial_\mu + ieA_\mu\).
        Note that while \(\varphi\) is a scalar with respect to Lorentz transformations it is a vector with respect to gauge transformations, so we compare to the derivative of a vector in GR.
        In general relativity the covariant derivative of a covariant four-vector field is defined as \(\nabla_\nu V^\mu = \partial_\nu V^\mu + \tensor{\Gamma}{^\mu_{\rho\nu}}V^\rho\) where \(\tensor{\Gamma}{^\mu_{\rho\nu}}\) is the affine connection.
        Note that \(\nabla_\mu f = \partial_\mu f\) if \(f\) is a Lorentz scalar and similarly \(\covariantDerivative_\mu f = \partial_\mu f\) if \(f\) is a scalar with respect to gauge transformations, that is \(f\) is gauge invariant.
        
        These two concepts are actually related at a fairly fundamental level in differential geometry.
        General relativity can be understood as the gauge theory of spacetime with gauge transformations being diffeomorphisms.
        Or in physics terms the gauge symmetry of general relativity is symmetry under changing reference frame.
    \end{rmk}
    
    If we want the Schrdinger equation to be invariant under gauge transformations then we need the wave function to transform at the same time according to
    \begin{equation}
        \Psi \mapsto \Psi' = \e^{-iq \chi / \hbar} \Psi
    \end{equation}
    since we then have
    \begin{align}
        (\covariantDerivative_\mu \Psi)' &= \left( \partial_\mu + i\frac{q}{\hbar} (A_\mu + \partial_\mu \chi) \right)\e^{-iq\chi/\hbar}\Psi\\
        &= \left( -i\frac{q}{\hbar} \e^{-iq\chi/\hbar}\Psi \partial_\mu \chi + \e^{-iq \chi/\hbar}\partial_\mu \Psi + i\frac{q}{\hbar} (A_\mu + \partial_\mu \chi) \e^{-iq\chi/\hbar}\Psi \right)\\
        &= \e^{-iq\chi/\hbar} \left( \partial_\mu + i\frac{q}{\hbar}A_\mu \right) \Psi\\
        &= \e^{-iq\chi/\hbar} \covariantDerivative_\mu \Psi.
    \end{align}
    This means that both sides of the Schrdinger equation transform in the same way and so the Schrdinger equation is valid in any gauge.
    
    The set of phases \(\{\e^{-iq\chi/\hbar}\}\) with \(\chi\) as any differentiable function of spacetime is the \defineindex{gauge group}, in this case where \(\chi(x)\) is just a real number we can think of the gauge group as being \(U(1)^{\infty^4}\), with \(\infty^4\) since we have a continuum of copies of \(U(1)\) for each of the four dimensions.
    This group is compact provided that \(\chi(x)\) is real and \(q = ne\hbar\) for some \(n \in \integers\) and some fundamental \defineindex{gauge coupling constant} \(e\) with \(0 < \chi(x) \le 2\pi/e\).
    
    The \defineindex{gauge principle} states that physics is independent of the phase of the wave function and we can choose this phase independently at every point of space.
    We can build gauge theories from quantities, quantum field operators, which transform like \(\Psi\) or \(\covariantDerivative_\mu \Psi\), or higher order tensors.
    
    Formally we can write the transformation of \(D_\mu\) as
    \begin{equation}
        \covariantDerivative_\mu' = \e^{-iq\chi/\hbar}\covariantDerivative_\mu \e^{iq\chi/\hbar}.
    \end{equation}
    Then we have
    \begin{equation}
        (\covariantDerivative_\mu \Psi)' = \underbrace{\e^{-iq\chi/\hbar} \covariantDerivative_\mu \e^{iq\chi/\hbar}}_{\covariantDerivative_\mu'} \underbrace{\vphantom{\covariantDerivative_\mu} \e^{-iq\chi/\hbar} \Psi}_{\Psi'} = \e^{-iq\chi/\hbar} \covariantDerivative_\mu \Psi,
    \end{equation}
    which is the same result we got before.
    
    The value \(q\) is the eigenvalue of the charge operator, \(\operator{Q}\), which is quantised in units of \(e\hbar\).
    If \(\operator{\Phi}\) is the field operator for a field with charge \(q\) then by definition we have
    \begin{equation}
        \commutator{\operator{Q}}{\operator{\Phi}(x)} = q\operator{\Phi}(x)
    \end{equation}
    and so
    \begin{equation}
        \e^{-i\operator{Q}\chi/\hbar} \operator{\Phi}(x) \e^{-i\operator{Q}\chi/\hbar} = \e^{iq\chi/\hbar}\operator{\Phi}(x).
    \end{equation}
    
    For a classical charged particle in an electromagnetic field we have
    \begin{equation}
        m\dot{x}_\mu = p_\mu + q A_\mu.
    \end{equation}
    The left hand side of this is gauge invariant in electromagnetism.
    After quantising this becomes
    \begin{equation}
        m\operator{\dot{x}}_\mu = i\hbar\left( \partial_\mu + i\frac{q}{\hbar}A_\mu \right) = i\hbar \covariantDerivative_\mu.
    \end{equation}
    We can compute the commutator of \(m\operator{\dot{x}}_\mu\) with itself:
    \begin{align}
        \commutator{m\operator{\dot{x}}_\mu}{m\operator{\dot{x}}_\nu} &= \commutator{i\hbar \covariantDerivative_\mu}{i\hbar \covariantDerivative_\nu}\\
        &= -\hbar^2 \commutator{\covariantDerivative_\mu}{\covariantDerivative_\nu}\\
        &= -\hbar^2\commutator*{\partial_\mu + i\frac{q}{\hbar}A_\mu}{\partial_\nu + i\frac{q}{\hbar} A_\nu}\\
        &= -iq\hbar(\partial_\mu A_\nu - \partial_\nu A_\mu)\\
        &= -iq\hbar F_{\mu\nu}.
    \end{align}
    This is gauge invariant, and so we can construct gauge invariant quantities by considering commutators of gauge covariant derivatives.
    
    We can express the transformation of \(F_{\mu\nu}\) as
    \begin{align}
        F'_{\mu\nu} &= \frac{\hbar}{iq} \commutator{\covariantDerivative'_\mu}{\covariantDerivative'_\nu}\\
        &= \frac{\hbar}{iq} \commutator{\e^{-iq\chi/\hbar \covariantDerivative_\mu} \e^{iq\chi/\hbar}}{\e^{-iq\chi/\hbar \covariantDerivative_\nu} \e^{iq\chi/\hbar}}\\
        &= \frac{\hbar}{iq} \e^{-iq\chi/\hbar} \commutator{\covariantDerivative_\mu}{\covariantDerivative_\nu} \e^{iq\chi/\hbar}\\
        &= \e^{-iq\chi/\hbar} F_{\mu\nu} \e^{iq\chi/\hbar}.
    \end{align}
    
    We also get the Binachi identity,
    \begin{equation}
        \partial_\lambda F_{\mu\nu} + \text{cycles} = 0
    \end{equation}
    from the Jacobi identity
    \begin{equation}
        \commutator{\covariantDerivative_\lambda}{\commutator{\covariantDerivative_\mu}{\covariantDerivative_\nu}} + \text{cycles} = 0 \implies \commutator{\covariantDerivative_\lambda}{F_{\mu\nu}} + \text{cycles} = 0.
    \end{equation}
    These formulae are key in generalising to non-Abelian gauge theories.
    
    \begin{rmk}
        There is another analogy with general relativity here.
        In GR the commutator of two covariant derivatives is used to define the Riemann curvature tensor, which measures the curvature of spacetime, and hence the strength of gravity.
        Turning this around we can think of \(F_{\mu\nu}\) as measuring the curvature of \(A_\mu\).
    \end{rmk}
    
    \chapter{Non-Abelian Gauge Theory}
    \section{Generalising from the Abelian Case}
    \subsection{Covariant Derivative and Gauge Fields}
    We can generalise from \(\unitary(1)\) to an arbitrary compact simple Lie group, \(G\).
    The non-simple case is then simply a direct sum of the simple cases, and the compactness requirement will be justified later.
    To make this generalisation we introduce matter fields, \(\varphi_i\), which transform under some representation, \(D\), of \(G\):
    \begin{equation}
        \varphi_i(x) \mapsto \varphi_i'(x) = D_{ij}(g(x)) \varphi_j(x)
    \end{equation}
    with \(i, j = 1, \dotsc, \dim D\).
    Notice that \(g\) can depend on position, and this dependence does not necessarily need to be smooth since this is not a parametrisation of \(G\) in the usual sense, but just a different copy of \(G\) acting on each point of space.
    Viewing \(\varphi\) as a vector with components \(\varphi_i\) we can write this as \(\varphi(x) \mapsto D(g)\varphi\).
    
    Now consider the transformation of \(\partial_\mu \varphi\):
    \begin{equation}
        \partial_\mu \varphi' = \partial_\mu (D(g)\varphi) = D(g)\partial_\mu \varphi + (\partial_\mu D(g)) \varphi.
    \end{equation}
    We can write \(\partial_\mu D(g) = D(\partial_\mu g)\) since \(D\) doesn't depend on \(x\) apart from through the dependence of \(g\).
    We then have
    \begin{equation}
        \partial_\mu \varphi' = D(g)\partial_\mu \varphi + D(\partial_\mu g)\varphi.
    \end{equation}
    We can then factorise this result as
    \begin{equation}
        \partial_\mu \varphi' = D(g)[\partial_\mu \varphi + D(g^{-1}\partial_\mu g)\varphi]
    \end{equation}
    having used \(D(g)D(g^{-1}) = D(gg^{-1}) = D(\ident) = \ident\).
    Notice that \(g^{-1}\partial_\mu g\) is an element of \(\lie{g}\), the Lie algebra of \(G\).
    This follows since we can write \(g(x) = \e^{i\chi(x)}\) where \(\chi\) is a Lie algebra valued function of space.
    Then we have \(g^{-1}\partial_\mu g = g^{-1}i(\partial_\mu \chi)g = i\partial_\mu \chi\), which is Lie algebra valued.
    
    We can compensate for the extra term bt defining the \defineindex{covariant derivative}
    \begin{equation}
        \covariantDerivative_\mu = \partial_\mu + ieD(W_\mu)
    \end{equation}
    where \(W_\mu \in \lie{g}\) is some Lie algebra valued gauge field and \(e = q/\hbar\) is the gauge coupling.
    As such we can expand \(W_\mu\) in terms of the Lie algebra generators, \(T_a\): \(W_\mu = W^a_\mu T_a\) where \(W_\mu^a\) are real or complex fields, depending on whether we consider a real or complex Lie algebra.
    Note that \(\mu = 0, 1, \dotsc, d\) in \(d\) spacetime dimensions and \(a = 1, \dotsc, \dim G\).
    By taking \(G\) to be compact we ensure that all representations of \(G\) are equivalent to a unitary representation, and so we may as well work in said unitary representation.
    The Lie algebra then consists of Hermitian operators, and so we can take \(T_a\), and all other Lie algebra valued quantities, to be Hermitian.
    We can treat \(W_\mu^a\) as classical fields or we can quantise so this is very general.
    
    The covariant derivative transforms as
    \begin{equation}
        D_\mu' = \partial_\mu + ieD(W_\mu')
    \end{equation}
    so we can choose to have \(\covariantDerivative_\mu \varphi\) transform as \(\varphi\), that is
    \begin{equation}
        (\covariantDerivative_\mu \varphi)' = D(g)\covariantDerivative_\mu \varphi,
    \end{equation}
    so long as
    \begin{equation}
        g^{-1}\partial_\mu g + ieg^{-1}W_\mu'g = ieW_\mu.
    \end{equation}
    This follows since we are demanding that
    \begin{align}
        \covariantDerivative_\mu'\varphi' &= \partial_\mu\varphi' + ieD(W_\mu')\varphi'\\
        &= \underbrace{D(g)\partial_\mu \varphi + D(\partial_\mu g)\varphi}_{\partial_\mu \varphi'} + ieD(W_\mu')\underbrace{D(g)\varphi}_{\varphi'}\\
        &= D(g)\covariantDerivative_\mu \varphi\\
        &= D(g)\partial_\mu \varphi + ieD(g)D(W_\mu)\varphi
    \end{align}
    which is true if
    \begin{equation}
        D(\partial_\mu g) + ieD(W'_\mu)D(g) = ieD(g)D(W_\mu).
    \end{equation}
    Multiplying on the left by \(D(g^{-1}) = D(g)^{-1}\) and using the fact that \(D \colon G \to \generalLinear(V)\) is a homomorphism we have
    \begin{equation}
        D(g^{-1}\partial_\mu g) + ieD(g^{-1}W'_\mu g) = ieD(W_\mu).
    \end{equation}
    Rearranging this we see that for \(\covariantDerivative_\mu \varphi\) to transform as \(\varphi\) we require that \(W_\mu\) transforms as
    \begin{equation}
        D(W_\mu') = D(gW_\mu g^{-1}) + \frac{i}{e}D((\partial_\mu g)g).
    \end{equation}
    We often don't explicitly write the representation, \(D\), and just work in some fixed representation and so we can write
    \begin{equation}
        W'_\mu = gW_\mu g^{-1} + \frac{i}{e}(\partial_\mu g)g^{-1}.
    \end{equation}
    
    We can check that this reduces to the expected result in the Abelian \(\unitary(1)\) case, where in some fixed representation \(g = \e^{-ie\chi}\) and so
    \begin{align}
        A'_\mu &= \e^{-ie\chi} A_\mu \e^{ie\chi} + \frac{i}{e}(\partial_\mu \e^{-ie\chi})\e^{ie\chi}\\
        &= A_\mu \e^{-ie\chi}\e^{ie\chi} + \frac{i}{e}(-ie\partial_\mu \chi)\e^{-ie\chi}\e^{ie\chi}\\
        &= A_\mu + \partial_\mu \chi,
    \end{align}
    which is the usual transformation law.
    
    We can write the transformation law for \(\covariantDerivative_\mu\) as
    \begin{equation}
        \covariantDerivative'_\mu = g \covariantDerivative_\mu g^{-1},
    \end{equation}
    which is the same as the transformation law for the covariant derivative in the Abelian case, and gives
    \begin{equation}
        \covariantDerivative'_\mu \varphi' = D(g)\covariantDerivative_\mu D(g^{-1}) D(g) \varphi = D(g) \covariantDerivative_\mu \varphi
    \end{equation}
    as required.
    
    \subsection{Field Strength}
    Again following the Abelian case we can use the commutator of the covariant derivative to define a \defineindex{field strength tensor}:
    \begin{align}
        ieD(F_{\mu\nu}) &\coloneqq D(\commutator{\covariantDerivative_\mu}{\covariantDerivative_\nu})\\
        &\hphantom{:}= \commutator{\partial_\mu + ieD(W_\mu)}{\partial_\nu + ieD(W_\nu)}\\
        &\hphantom{:}= ie\partial_\mu D(W_\nu) - ie\partial_\nu D(W_\mu) - e^2\commutator{D(W_\mu)}{D(W_\nu)}\\
        &\hphantom{:}= ieD(\partial_\mu W_\nu - \partial_\nu W_\mu + ie\commutator{W_\mu}{W_\nu}).
    \end{align}
    Clearly in the Abelian case the Lie bracket \(\commutator{W_\mu}{W_\nu}\) vanishes and so this reduces to the usual expression for the field strength tensor.
    Note that we can bring the derivatives inside the representation since we can write \(D(W_\mu) = D(W_\mu^a T_a) = W_\mu^a D(T_a)\) and then \(\partial_\nu D(W_\mu) = (\partial_\nu W_\mu^a)D(T_a) = D((\partial_\nu W_\mu^a) T_a) \eqqcolon D(\partial_\nu W_\mu)\).
    
    This extra commutator appearing only in the non-Abelian case is nonlinear in the gauge field.
    This leads to many interesting and important physical consequences in non-Abelian gauge theories, such as quark confinement in QCD.
    
    Note that \(F_{\mu\nu}\) is Lie algebra valued, \(F_{\mu\nu} \in \lie{g}\), and so we can write it as \(F_{\mu\nu} = F^a_{\mu\nu}T_a\).
    The transformation law for \(F_{\mu\nu}\) is
    \begin{align}
        ieD(F_{\mu\nu}') &= D(\commutator{\covariantDerivative'_\mu}{\covariantDerivative'_\nu})\\
        &= D(\commutator{g\covariantDerivative_\mu g^{-1}}{g\covariantDerivative_\nu g^{-1}})\\
        &= D(g\commutator{\covariantDerivative_\mu}{\covariantDerivative_\nu}g^{-1})\\
        &= ieD(gF_{\mu\nu}g^{-1}).
    \end{align}
    From this we can see that \(F_{\mu\nu}\) is gauge invariant only in the Abelian case.
    
    The simplest gauge invariant quantity which is also a Lorentz invariant scalar is given by contracting the field strength tensor with itself to get a Lorentz invariant and then taking the trace in the adjoint representation to get a gauge invariant quantity:
    \begin{equation}
        \tr_{\symrm{A}}(F^{\mu\nu}F_{\mu\nu}) = F^{a\mu\nu}F^a_{\mu\nu}.
    \end{equation}
    
    \subsection{Action and Equations of Motion}
    We can use this quantity to construct a Lagrangian or, in this case, an action:
    \begin{equation}
        S = -\frac{1}{4} \int \dl{^4x} \tr_{\symrm{A}}(F^{\mu\nu}F_{\mu\nu}) = -\frac{1}{4} \int \dl{^4x} \, F^{a\mu\nu}F^a_{\mu\nu}.
    \end{equation}
    The variation in \(S\) is then
    \begin{align}
        \delta S &= -\frac{1}{4} \int \dl{^4x} \, \delta(F^{a\mu\nu}F^a_{\mu\nu})\\
        &= -\frac{1}{4} \int \dl{^4x} \, [(\delta F^{a\mu\nu})F^a_{\mu\nu} + F^{a\mu\nu}(\delta F^a_{\mu\nu})]\\
        &= -\frac{1}{2} \int \dl{^4x} \, F^{a\mu\nu}\delta F^a_{\mu\nu}\\
        &= -\frac{1}{2} \int \dl{^4x} \, \tr_{\symrm{A}}(F^{\mu\nu} \delta F_{\mu\nu}).
    \end{align}
    The variation in \(F_{\mu\nu}\) is
    \begin{align}
        \delta F_{\mu\nu} &= \delta(\partial_\mu W_\nu - \partial_\nu W_\mu + ie\commutator{W_\mu}{W_\nu})\\
        &= \delta\partial_\mu W_\nu - \delta\partial_\nu W_\mu + ie\delta(W_\mu W_\nu - W_\nu W_\mu)\\
        &= \partial_\mu\delta W_\nu - \partial_\nu\delta W_\mu + ie((\delta W_\mu) W_\nu + W_\mu (\delta W_\nu) - (\delta W_\nu) W_\mu - W_\nu (\delta W_\mu)) \notag\\
        &= \partial_\mu \delta W_\nu \partial_\nu \delta W_\mu + ie \commutator{\delta W_\mu}{W_\nu} + ie \commutator{W_\mu}{\delta W_\nu}\\
        &= \covariantDerivative_\mu \delta W_\nu - \covariantDerivative_\nu \delta W_\mu.
    \end{align}
    This last step generalises the covariant derivative acting on scalar fields to act on vector fields, here \(\delta W_\mu\), according to
    \begin{equation}
        \covariantDerivative_\mu V_\nu \coloneqq \partial_\mu + ie\commutator{W_\mu}{V_\nu}.
    \end{equation}
    We then have
    \begin{align}
        F^{\mu\nu}\delta F_{\mu\nu} &= F^{\mu\nu}\covariantDerivative_\mu \delta W_\nu - F^{\mu\nu}\covariantDerivative_\nu \delta W_\mu\\
        &= F^{\mu\nu} \covariantDerivative_\mu \delta W_\nu - (-F^{\nu\mu})\covariantDerivative_\nu \delta W_\mu\\
        &= F^{\mu\nu} \covariantDerivative_\mu \delta W_\nu + F^{\mu\nu}\covariantDerivative_\mu \delta W_\nu \qquad \mu \leftrightarrow \nu \text{ in second term}\\
        &= 2F^{\mu\nu} \covariantDerivative_\mu \delta W_\nu
    \end{align}
    and the variation in the action is then
    \begin{align}
        \delta S &= -\int \dl{^4x} \tr_{\symrm{A}}(F^{\mu\nu} \covariantDerivative_\mu \delta W_\nu)\\
        &= -\int \dl{^4x} \, \partial\mu \tr_{\symrm{A}}(F^{\mu\nu} \delta W_\nu) + \int \dl{^4x} \, \tr_{\symrm{A}}((\covariantDerivative_\mu F^{\mu\nu}) \delta W^\nu)
    \end{align}
    where we've integrated by parts and used \(\covariantDerivative_\mu \tr({\dotsm}) = \partial_\mu \tr({\dotsm})\) since the trace is a group scalar.
    The first term here is a surface term so choosing the variation to vanish on the surface gives the result
    \begin{equation}
        \delta S = \int \dl{^4x} \, \tr_{\symrm{A}}((\covariantDerivative_\mu F^{\mu\nu}) \delta W_\nu).
    \end{equation}
    Taking \(\delta S = 0\) then gives
    \begin{equation}
        \covariantDerivative_\mu F^{\mu\nu} = 0,
    \end{equation}
    which is a non-Abelian generalisation of Maxwell's equation with no sources.
    This is what we would expect since we are working in a vacuum.
    
    We can include matter by including force terms such as
    \begin{equation}
        S_{\text{matter}} = -\int \dl{^4x} J_\mu^a W^{a\mu} = -\int \dl{^4x} \tr_{\symrm{A}}(J_\mu W^\mu)
    \end{equation}
    in analogy to the \(J^\mu A_\mu\) terms appearing in the action for an electromagnetic field with sources.
    Varying the action in the same way we get
    \begin{equation}
        \delta S = \int \dl{^4x} \tr_{\symrm{A}}((\covariantDerivative_\mu F^{\mu\nu} - J^\mu) \delta W_\nu )
    \end{equation}
    giving the equation of motion
    \begin{equation}
        \covariantDerivative_\mu F^{\mu\nu} = J^\nu,
    \end{equation}
    which again generalises Maxwell's equation, this time with sources.
    
    The Jacobi identity gives the non-Abelian version of the Binachi identity,
    \begin{equation}
        \covariantDerivative_\rho F_{\mu\nu} + \text{cycles} = 0,
    \end{equation}
    where
    \begin{equation}
        \covariantDerivative_\rho F_{\mu\nu} \coloneqq \partial_\rho F_{\mu\nu} + ie\commutator{W_\rho}{F_{\mu\nu}}.
    \end{equation}
    We can write the Binachi identity as
    \begin{equation}
        D_\mu \tensor[^*]{F}{^{\mu\nu}} = 0
    \end{equation}
    defining
    \begin{equation}
        \tensor[^*]{F}{^{\mu\nu}} = \frac{1}{2}\varepsilon^{\mu\nu\rho\sigma}F_{\rho\sigma}
    \end{equation}
    as in the Abelian case.
    
    \begin{exm}{Fermions}{}
        Consider the Dirac Lagrangian for massless fermions
        \begin{equation}
            \lagrangianDensity = i\diracadjoint{\psi} \slashed{\covariantDerivative} \psi.
        \end{equation}
        Here the covariant derivative of a spinor field is given by
        \begin{equation}
            \covariantDerivative_\mu \psi = (\partial_\mu + ieW^a_\mu D_{\mathrm{F}}(T^a))\psi
        \end{equation}
        where \(D_{\mathrm{F}}\) is the representation of the gauge group under which \(\psi\) transforms.
        The equations of motion in this case are then
        \begin{equation}
            (\covariantDerivative_\mu F^{\mu\nu})^a = e \diracadjoint{\psi} \gamma^\nu D_{\mathrm{F}}(T^a) \psi
        \end{equation}
    \end{exm}
    
    \begin{exm}{Higgs}{}
        Consider the Klein--Gordon Lagrangian for a scalar field, such as the Higgs,
        \begin{equation}
            \lagrangianDensity = (\covariantDerivative_\mu H)^\hermit (\covariantDerivative^\mu H).
        \end{equation}
        In this case the covariant derivative of the scalar field is
        \begin{equation}
            \covariantDerivative_\mu H = (\partial_\mu + ieW_\mu^a D_{\mathrm{H}}(T^a))H
        \end{equation}
        where \(D_{\mathrm{H}}\) is the representation of the gauge group under which \(H\) transforms.
        That is \(H' = D_{\mathrm{H}}(g)H\), and taking the gauge group to be compact we can assume that \(D_{\symrm{H}}\) is a unitary representation.
        Then we have
        \begin{equation}
            \covariantDerivative'^\mu H' = D(g)\covariantDerivative^\mu H, \qqand \covariantDerivative'_\mu H'^\hermit = \covariantDerivative_\mu H^\hermit D(g)^\hermit
        \end{equation}
        so
        \begin{equation}
            \lagrangianDensity' = (\covariantDerivative_\mu H^\hermit)D(g)^\hermit D(g) (\covariantDerivative^\mu H) = \lagrangianDensity,
        \end{equation}
        meaning the the Lagrangian is indeed invariant under the gauge transformation.
        
        Expanding the Lagrangian gives
        \begin{multline}
            \lagrangianDensity = (\partial_\mu H^\hermit)(\partial^\mu H) + ie W^a_\mu H^\hermit D_{\mathrm{H}}(T^a) \leftrightpartial_\mu H\\
            + \frac{e^2}{2}W^a_\mu W^{b\mu} H^{\hermit} \anticommutator{D(T^a)}{D(T^b)}H
        \end{multline}
        where we've used the fact that \(W^a_\mu W^{b \mu}\) is symmetric under exchange of \(a\) and \(b\) to replace \(T^aT^b\) with it's symmetric part, \(\anticommutator{T^a}{T^b}/2\).
        Here
        \begin{equation}
            f \leftrightpartial_\mu g \coloneqq f\partial_\mu g - g\partial_\mu f.
        \end{equation}
        We can interpret the last term of the Lagrangian as a mass term with a mass matrix,
        \begin{equation}
            m_{ab}^2 = e^2 H^{\hermit}D(\anticommutator{T_a}{T_b})H.
        \end{equation}
        This term gives mass to the gauge bosons \(W^a_\mu\), which would otherwise be massless.
        This is the Higgs mechanism, which we'll explore in more detail in the next chapter.
        
        The current in this case can be read off from the Lagrangian as
        \begin{equation}
            J^a_\mu = -ie H^{\hermit} D_{\mathrm{H}}(T^a) \leftrightpartial_\mu H.
        \end{equation}
    \end{exm}
    
    \chapter{Higgs Mechanism}
    \section{Abelian Higgs Model}
    \epigraph{They had very good arguments against it. They weren't quite good enough. They were wrong.}{Neil Turok}
    Consider an Abelian gauge theory described by Lagrangian\footnote{Note that we have rescaled \(v\) from the value used in \cref{sec:abelian symmetry spontaneous symmetry breaking}.}
    \begin{equation}
        \lagrangianDensity = -\frac{1}{4}F^{\mu\nu} F_{\mu\nu} + (\covariantDerivative_\mu \varphi)^\hermit (\covariantDerivative^\mu \varphi) - \lambda\left( \varphi^\hermit \varphi - \frac{v^2}{2} \right)^2
    \end{equation}
    where
    \begin{align}
        F^{\mu\nu} &\coloneqq \partial^\mu A^\nu - \partial^\nu A^\mu,\\
        \covariantDerivative_\mu \coloneqq \partial_\mu + ieA_\mu
    \end{align}
    with \(\varphi\) a complex scalar field and \(A_\mu\) a spin one field.
    Suppose further that under a gauge transformation these fields transform as
    \begin{equation}
        A_\mu \mapsto A_\mu + \partial_\mu \chi, \qqand \varphi \mapsto \e^{-ie\chi}\varphi.
    \end{equation}
    This leaves the Lagrangian gauge invariant.
    
    Consider \(V_{\min}\), the space of values of \(\varphi\) minimising the potential \(\lambda(\varphi^\hermit \varphi - v^2/2)^2\).
    Since this is a squared real quantity it is non-negative, and so is minimised when (and if) it achieves a value of zero, which it does precisely when \(\varphi^\hermit \varphi = v^2/2\), so
    \begin{equation}
        V_{\min} = \{\varphi \mid \varphi^\hermit \varphi = v^2/2\} = \{\e^{i\vartheta} v/\sqrt{2} \mid \vartheta \in [0, 2\pi)\}.
    \end{equation}
    If \(\varphi \ne 0\) then any nontrivial action of the gauge group will change it's value, and so the little group, \(H\), is trivial, \(H = \{\ident\}\).
    We then have \(V_{\min} = \unitary(1)/\{\ident\} \isomorphic \unitary(1) \isomorphic S^1\).
    So \(V_{\min}\) is a circle.
    
    Taking the two fields separately there are four degrees of freedom, \(A_\mu\) has two polarisations and \(\varphi = \varphi_1 + i\varphi_2\) with \(\varphi_i\) real fields has \(\varphi_i\) as degrees of freedom.
    However, it is often more useful to work with radial and angular degrees of freedom, taking say \(\varphi = v\e^{i\alpha}\) as the initial minimum point.
    For simplicity we choose \(\alpha = 0\).
    Then we can write \(\varphi\) as a perturbation about this point:
    \begin{equation}
        \varphi = \frac{1}{\sqrt{2}}(v + \delta v)\e^{i\vartheta}
    \end{equation}
    where \(\delta v\) and \(\vartheta\) are our degrees of freedom, and are both real fields.
    As we saw in \cref{sec:abelian symmetry spontaneous symmetry breaking} \(\delta v\) corresponds to a massive field, the Higgs boson, and \(\vartheta\) to a massless Goldstone boson.
    
    The kinetic term, with the covariant derivatives, is the most interesting term as it mixes the two fields.
    Computing the covariant derivative we get
    \begin{align}
        \covariantDerivative_\mu \varphi &= \frac{1}{\sqrt{2}}(\partial_\mu + ieA_\mu) [(v + \delta v) \e^{i\vartheta}]\\
        &= \frac{1}{\sqrt{2}} [(\partial_\mu \delta v) \e^{i\vartheta} + (v + \delta v)(i\partial_\mu \vartheta)\e^{i\vartheta} + ieA_\mu (v + \delta v)\e^{i\vartheta}]\\
        &= \frac{1}{\sqrt{2}} [\partial_\mu \delta v + i(\partial_\mu \vartheta + eA_\mu)(v + \delta v)]\e^{i\vartheta}
    \end{align}
    The kinetic term is then
    \begin{align}
        (\covariantDerivative_\mu \varphi)^\hermit (\covariantDerivative^\mu \varphi) &= \frac{1}{2} [\partial_\mu \delta v - i(v + \delta v)(\partial_\mu \vartheta + eA_\mu)] \notag\\
        &\qquad \qquad \times [\partial^\mu \delta v + i(v + \delta v)(\partial^\mu \vartheta + eA^\mu)]\\
        &= \frac{1}{2}(\partial_\mu \delta v)(\partial^\mu \delta v) + \frac{1}{2}(v + \delta v)^2(\partial_\mu \vartheta + eA_\mu)(\partial^\mu \vartheta + eA^\mu).\notag
    \end{align}
    Now define a new field
    \begin{equation}
        B_\mu \coloneqq A_\mu + \frac{1}{e}\partial_\mu \vartheta
    \end{equation}
    which is just a gauge transformed \(A_\mu\).
    Then
    \begin{equation}
        (\covariantDerivative_\mu \varphi)^\hermit (\covariantDerivative^\mu \varphi) = \frac{1}{2}(\partial_\mu \delta v)(\partial^\mu \delta v) + \frac{1}{2}(v + \delta v)^2 e^2B_\mu B^\mu
    \end{equation}
    
    Hence Lagrangian is
    \begin{equation}
        \lagrangianDensity = -\frac{1}{4}F^{\mu\nu}F_{\mu\nu} + \frac{1}{2}e^2v^2 B_\mu B^\mu + \frac{1}{2}(\partial_\mu \delta v)(\partial^\mu \delta v) - \lambda v^2 \delta v^2 + \order(\delta v^3, \delta v^4).
    \end{equation}
    Here we've redefined \(F^{\mu\nu} \coloneqq \partial^\mu B^\nu - \partial^\nu B^\mu\), which is not really a redefinition in this case since \(A_\mu\) and \(B_\mu\) are related by a gauge transformation and for an Abelian gauge theory \(F^{\mu\nu}\) is gauge invariant.
    
    Notice that there are no linear terms in the field as we are expanding about a minimum where, by definition, the linear terms vanish.
    
    Not every component of \(B_\mu\) corresponds to a particle.
    Consider the mass term for \(B_\mu\):
    \begin{equation}
        \frac{1}{2}e^2 v^2 B_\mu B^\mu = \frac{1}{2}e^2 v^2 B_0 B^0 - \frac{1}{2} e^2 v^2 B_i B^i.
    \end{equation}
    The second term here, \(B_i B^i\), is a mass term and each \(B_i\) corresponds to a massive spin one particle.
    The first term is not a mass term since Gauss's law fixes the value of \(\partial_i F^{0i} = J^0 = \rho = e^2 v^2 B^0\), since we can take the current to be the thing contracted with \(B^\mu\) in analogy to the usual \(A^\mu J_\mu\) term.
    We then have \(\partial_i F^{0i} = -\laplacian B^0 + \partial_0(\div \vv{B})\), which fixes the value of \(B^0\) so \(B^0\) is not a propagating degree of freedom.
    
    Thus we have three massive vector bosons corresponding to the three degrees of freedom of \(B_\mu\).
    We also have one massive scalar particle corresponding to \(\delta v\).
    There are no particles corresponding to the Goldstone boson, \(\vartheta\).
    We say that the Goldstone bosons are \enquote{eaten} by the gauge field to give it mass.
    All that has happened here is a reshuffling of degrees of freedom, initially we had two vector degrees of freedom and two scalar degrees of freedom, now we have three vector degrees of freedom and a single scalar degree of freedom.
    
    The interpretation of this Lagrangian is as follows:
    \begin{description}
        \item[\(-F^{\mu\nu}F_{\mu\nu}/4\)] This term corresponds to electromagnetic interactions mediated by the particles represented by field \(B_\mu\).
        We can interpret this as a mixture of kinetic and gradient terms for the \(B_\mu\) field:
        \begin{equation}
            -\frac{1}{4}F^{\mu\nu} F_{\mu\nu} = -\frac{1}{2}F_{\mu\nu}\partial^\mu B^\nu = \frac{1}{2}(\partial_0 B_i)(\partial^0 B^i) - \frac{1}{2}(\partial_j B_i)(\partial_j B_i) = \frac{1}{2}(\partial_\mu B_i)^2
        \end{equation}
        the \(\partial_0 B_i\) term is a kinetic term for the three vector bosons and the \(\partial_j B_i\) term is a gradient term for these bosons.
        Then the \(\partial_\mu B_i\) term is a kinetic term for these bosons.
        \item[\(e^2v^2 B_\mu B^\mu/2\)] This is a mass term for the particles represented by the \(B_\mu\) field.
        They have a mass of \(ev\).
        \item[\((\partial_\mu \delta v)(\partial^\mu \delta v)\)] This is a kinetic term for the \(\delta v\) field.
        \item[\(-\lambda v^2 \delta v^2\)] This is a mass term for the particles represented by the \(\delta v\) field.
        They have a mass of \(v\sqrt{\lambda}\).
        \item[\(\order(\delta v^3, \delta v^4)\)] These correspond to cubic and quartic interactions of the scalar field \(\delta v\).
        That is interactions between three or four particles represented by the field \(\delta v\).
    \end{description}

    \section{Non-Abelian Higgs Model}
    Consider a non-Abelian, compact Lie group, \(G\).
    Suppose that \(\varphi\) is a field transforming under some representation, \(D\), of \(G\).
    We have a radial mode, \(\delta v\), unaffected by the action of \(G\) since \(G\) moves the states around the minimum.
    This corresponds to a massive degree of freedom.
    We also have Goldstone modes corresponding to massless degrees of freedom.
    We suppose that the symmetry breaks and some value \(\varphi \in V_{\min}\) is picked.
    Then the symmetry breaks from \(G\) to \(H\), where \(H\) is the little group leaving \(\varphi\) invariant.
    There are then \(\dim G - \dim H\) Goldstone modes.
    
    The radial mode is always a degree of freedom, corresponding to a massive field.
    Before coupling through \(\covariantDerivative_\mu \varphi\) the other degrees of freedom consist of \(2 \dim G\) massless vectors, corresponding to \(\dim G\) copies of the two vector degrees of freedom in the Abelian case, as well as \(N\) radial modes of \(\varphi\), unaffected by \(G\), and \(\dim G - \dim H\) massless Goldstone bosons.
    After coupling the degrees of freedom consist of \(2 \dim H\) massless vectors and \(3(\dim G - \dim H)\) massive vectors and the \(N\) radial modes corresponding to Higgs bosons, in the case of electroweak theory this is \emph{the} Higgs boson.
    All the Higgs mechanism and coupling has done is rearrange the degrees of freedom.
    
    \begin{exm}{Electroweak Symmetry Breaking}{}
        \begin{rmk}
            See \course{Particle Physics} for the details of the Higgs mechanism in the case of electroweak theory.
        \end{rmk}
        Electroweak theory has the gauge group \(G = \specialUnitary(2) \times \unitary(1)\), which has dimension \(3\), since \(\specialUnitary(2)\) has dimension 3 and \(\unitary(1)\) has dimension 1.
        Symmetry breaking breaks this to the \(H = \unitary(1)\) of electromagnetism.
        So \(\dim G - \dim H = 2\).
        Before symmetry breaking the \(2 \dim G = 6\) massless vector degrees of freedom are the six degrees of freedom of the three \(W\) bosons.
        The \(\dim G - \dim H = 2\) massless Goldstone bosons are the degrees of freedom of the \(B_\mu\) field.
        After coupling the \(2\dim H = 2\) massless vector degrees of freedom are the two degrees of freedom of the photon, and the the \(3(\dim G - \dim H) = 6\) massive vector degrees of freedom are the two degrees of freedom of each of the \(\PWpm\) and \(\PZ\) bosons.
    \end{exm}
    
    We can see this in more detail by considering the field, \(\varphi\), after acting on it with \(G\):
    \begin{equation}
        \varphi = D(g(x))(\varphi_0 + \delta \varphi).
    \end{equation}
    Here \(\varphi_0 \in V_{\min}\) is the symmetry breaking value that the field takes before the action of \(G\), and \(\delta \varphi\) is a radial mode unaffected by the action of \(G\).
    Notice that \(g\) depends on \(x\) but \(\varphi_0\) doesn't.
    
    Computing the covariant derivative we have
    \begin{equation}
        \covariantDerivative_\mu \varphi = g(\partial_\mu + \underbrace{g^{-1} \partial_\mu g + g^{-1} ie W_\mu g}_{ieW'_\mu})(\varphi_0 + \delta \varphi).
    \end{equation}
    
    The analysis then proceeds very similarly to the Abelian case, we expand the \((\covariantDerivative_\mu \varphi)^\hermit (\covariantDerivative^\mu \varphi)\) term and see which extra terms appear in the Lagrangian and then interpret these as mass terms and interactions.
    
    There is one subtlety, which is that spontaneous symmetry breaking never happens in quantum mechanics, this is Elitzus's theorem, which states that it is impossible to break gauge symmetry in a finite volume.
    This is because quantum spreading means that the expectation value of \(\varphi\) is zero, so symmetry isn't broken.
    In QFT however we can have spontaneous symmetry breaking in the infinite volume limit.
    The homogeneous mode of \(\varphi\) in the action is
    \begin{equation}
        V \int \dl{t} \, \frac{1}{2} \dot{\varphi}^2
    \end{equation}
    where \(V\) is the volume of space.
    Then we can treat the field as if it had mass proportional to \(V\), and in the limit of \(V \to \infty\) there is no quantum spreading.
    That \(V\) gives the mass can be seen by the classical analogy of the action of a free particle
    \begin{equation}
        S = \int \dl{t} \, \frac{1}{2}m\dot{x}^2.
    \end{equation}
    Essentially quantum spreading stops being an issue because the Heisenberg uncertainty principle, \(\Delta x \, \Delta p \sim \hbar\), becomes \(\delta v \sim \hbar/(m \, \delta x)\), so as the mass becomes infinite faster than the uncertainty in the position vanishes then the uncertainty in velocity vanishes also, allowing for nonzero expectation values and hence symmetry breaking.
    
    \chapter{The Standard Model}
    The standard model has the gauge group
    \begin{equation}
        \specialUnitary(3)_{\mathrm{c}} \times \specialUnitary(2)_{\Left} \times \unitary(1)_{Y}.
    \end{equation}
    Here \(\mathrm{c}\) stands for colour, as this \(\specialUnitary(3)\) corresponds to the \(\specialUnitary(3)\) of QCD, \(\Left\) stands for left as this \(\specialUnitary(2)\) corresponds to the weak interaction between left handed particles, and \(Y\) stands for hypercharge.
    
    Actually, the standard model has gauge group
    \begin{equation}
        (\specialUnitary(3)_{\mathrm{c}} \times \specialUnitary(2)_{\Left} \times \unitary(1)_{Y})/\integers_6
    \end{equation}
    since we have
    \begin{equation}
        \begin{pmatrix}
            \omega_3 & 0 & 0\\
            0 & \omega_3 & 0\\
            0 & 0 & \omega_3
        \end{pmatrix}
        \in \specialUnitary(3), \qqand 
        \begin{pmatrix}
            \omega_2 & 0\\
            0 & \omega_2
        \end{pmatrix}
        \in \specialUnitary(2)
    \end{equation}
    with \(\omega_n^n = 1\), and then so the centre of the group \(\specialUnitary(3) \times \specialUnitary(2) \times \unitary(1)\) is \(\integers_3 \times \integers_2 \isomorphic \integers_6\), which we have to mod out by to avoid redundancy.
    However, this has no effect on the physics and so we generally ignore it.
    
    In the standard model quarks and leptons come in two types, left and right handed.
    The left handed quarks and leptons are \(\specialUnitary(2)\) doublets, such as
    \begin{equation}
        L_{\Left} = 
        \begin{pmatrix}
            \Pnue\\ \Penominus
        \end{pmatrix}
        _{\Left}, \qqand Q_{\Left} = 
        \begin{pmatrix}
            \Pup^a\\ \Pdown^a
        \end{pmatrix}
        _{\Left}.
    \end{equation}
    The right handed quarks and leptons are \(\specialUnitary(2)\) singlets,
    \begin{equation}
        \Penominus_{\Right}, \qquad \upnu_{\Penominus \Right}, \qquad \Pup_{\Right}^a, \qqand \Pdown_{\Right}^a.
    \end{equation}
    Here \(a = 1, 2, 3\) are colour labels, alternatively we could use \(a = r, g, b\).
    All of the quarks are in \(\specialUnitary(3)\) triplets.
    The second and third generations of particles are arranged similarly.
    
    We'll ignore QCD for now, since \(\specialUnitary(3)_{\mathrm{c}}\) is not broken by the Higgs mechanism, as the Higgs boson has no colour, and focus on the \(\specialUnitary(2)_{\Left} \times \unitary(1)_{Y}\) part of the standard model, which is electroweak theory.
    Without the Higgs mechanism \(\Penominus_{\Left}\) and \(\upnu_{\Penominus \Left}\) would be indistinguishable as they are rotated into each other by the \(\specialUnitary(2)_{\Left}\) symmetry.
    
    In the \(\rep{2}\) representation of \(\specialUnitary(2)_{\Left}\) we take the third generator to be \(T_3 = \sigma_3/2\).
    We define the electric charge, \(Q\), to be \(Q = T_3 + Y\), by which we mean that the electric charge of a particle is the eigenvalue of this operator acting on the field corresponding to that particle.
    In the \(\rep{1}\) representation of \(\specialUnitary(2)_{\Left}\) the third generator is \(T_3 = 0\), since then \(\e^{T_3} = 1\), meaning that the \(\rep{1}\) representation acts trivially, as expected.
    The result is that \(T_3 = 1/2\) for particles in the \(\rep{2}\) representation appearing in the top of the doublet, \(T_3 = -1/2\) for particles in the bottom of the doublet, and \(T_3 = 0\) for particles in the \(\rep{1}\) representation.
    Using this we can assign quantum numbers to all of the particles.
    Doing so gives \cref{tab:EW quantum numbers}.
    
    \begin{table}
        \caption[Quantum numbers for the electroweak part of the standard model.]{Quantum numbers for the electroweak part of the standard model. After accounting for three colours for each quark there are 16 unique particles, all transforming under either the \(\rep{2}\) or \(\rep{1}\) representation of \(\specialUnitary(2)\). We then get tree copies of this, one for each generation.}
        \label{tab:EW quantum numbers}
        \begin{tabular}{cccc}\toprule
            Particle & \(T_3\) & \(Y\) & \(Q\) \\ \midrule
            \(\upnu_{\Penominus \Left}\)  & \(1/2\)  & \(-1/2\) & \(0\)    \\
            \(\Penominus_{\Left}\)        & \(-1/2\) & \(-1/2\) & \(-1\)   \\
            \(\upnu_{\Penominus \Right}\) & \(0\)    & \(-1\)   & \(-1\)   \\
            \(\Penominus_{\Right}\)       & \(0\)    & \(0\)    & \(0\)    \\
            \(\Pup_{\Left}\)              & \(1/2\)  & \(1/6\)  & \(2/3\)  \\
            \(\Pdown_{\Left}\)            & \(-1/2\) & \(1/6\)  & \(-1/3\) \\
            \(\Pup_{\Right}\)             & \(0\)    & \(2/3\)  & \(2/3\)  \\
            \(\Pdown_{\Right}\)           & \(0\)    & \(-1/3\) & \(-1/3\) \\\bottomrule
        \end{tabular}
    \end{table}
    
    Take the Lagrangian
    \begin{equation}
        \lagrangianDensity = -\frac{1}{4} F^{i\mu\nu} F^i_{\mu\nu} - \frac{1}{4}B_{\mu\nu}B^{\mu\nu} - \frac{1}{4}F^{a\mu\nu}F^a_{\mu\nu}
    \end{equation}
    where \(i = 1, 2, 3\) and \(a = 1, \dotsc, 8\).
    The first term corresponds to \(\specialUnitary(2)_{\Left}\), the second to \(\unitary(1)_Y\), and the third to \(\specialUnitary(3)_{\mathrm{c}}\).
    
    The again neglecting the term for strong interactions the two field strength tensors are
    \begin{equation}
        B_{\mu\nu} = \partial_\mu B_\nu - \partial_\nu B_\mu
    \end{equation}
    with \(B_\mu\) being a vector field, distinguished from \(B_{\mu\nu}\) by the number of indices, and
    \begin{equation}
        F^i_{\mu\nu} = \partial_\mu W^i_\nu - \partial_\nu W^i_\mu - g_2 \varepsilon^{ijk} W^j_\mu W_\nu^k
    \end{equation}
    where \(g_2\) is some constant.
    Here we work in the adjoint representation, so \(\commutator{T^i}{T^j} = i\varepsilon^{ijk} T^k\) for \(\specialUnitary(2)\).
    
    The coupling then occurs through the term
    \begin{equation}
        \covariantDerivative_\mu \varphi = \left( \partial_\mu + ig_1 Y B_\mu + ig_2 \frac{\sigma_i}{2} W^i_\mu  \varphi \right)
    \end{equation}
    where \(g_1\) is another constant.
    The \(ig_1 YB_\mu\) term is the \(\unitary(1)_Y\) coupling, and the \(ig_2\sigma^i W^i_\mu/2\) term is the \(\specialUnitary(2)_{\Left}\) coupling.
    There is no \(\specialUnitary(3)\) coupling term as \(\varphi\) doesn't transform under \(\specialUnitary(3)_{\mathrm{c}}\), that is it is in the trivial representation of \(\specialUnitary(3)_{\mathrm{c}}\).
    
    The Higgs, \(\varphi\), is in the \(\rep{2}\) representation of \(\specialUnitary(2)_{\Left}\),
    \begin{equation}
        \varphi = 
        \begin{pmatrix}
            \varphi^+\\ \varphi^0
        \end{pmatrix}
        .
    \end{equation}
    For the Higgs \(Y = 1/2\), so the electric charge of \(\varphi^+\) is \(+1\) and the electric charge of \(\varphi^0\) is 0, since these terms have \(T_3 = 1/2\) and \(T_3 = -1/2\) respectively.
    Note that on any doublet field \(Q = Y + T_3\) can be replaced with
    \begin{equation}
        Q = g_1 Y + g_2 \frac{\sigma_3}{2}.
    \end{equation}
    
    As we've seen before after spontaneous symmetry breaking in the potential
    \begin{equation}
        V = \lambda(\varphi^\hermit \varphi - v^2/2)
    \end{equation}
    we have
    \begin{equation}
        \varphi_0 = 
        \begin{pmatrix}
            0\\ v/\sqrt{2}
        \end{pmatrix}
    \end{equation}
    for some constant \(v\).
    Then
    \begin{align}
        \covariantDerivative_\mu \varphi &= i\frac{g_1}{2} B_\mu 
        \begin{pmatrix}
            0\\ v/\sqrt{2}
        \end{pmatrix}
        + i\frac{g_2}{2}\sigma_i W^i_\mu
        \begin{pmatrix}
            0\\ v/\sqrt{2}
        \end{pmatrix}
        &= \frac{i}{2}
        \begin{pmatrix}
            g_1 B_\mu + g_2 W^3_\mu & g_2(W^1_\mu - iW^2_\mu)\\
            g_2(W^1_\mu + iW^2_\mu) & g_1B_\mu - g_2 W^3_\mu
        \end{pmatrix}
        \begin{pmatrix}
            0\\ v/\sqrt{2}
        \end{pmatrix}
        .
    \end{align}
    We diagonalise this to get the physical states.
    The term \(g_1 B_\mu + g_2 W_\mu^3\) then corresponds to a massless state, the photon.
    The \(g_1 B_\mu - g_2 W^3_\mu\) term, which is orthogonal to the photon state, becomes massive, and is the \PZ{} boson.
    The \(g_2 (W^1_\mu \pm i W^2_\mu)\) terms also become massive and correspond to the \PWpm{} bosons.
    
    If we consider the kinetic term
    \begin{equation}
        (\partial_\mu B_\nu - \partial_\nu B_\mu)^2 + (\partial_\mu W^3_\nu - \partial_\nu W^3_\mu)^2
    \end{equation}
    then we can see that this is a sum of squares, so is invariant under rotations of \(B_\mu\) and \(W_\mu\).
    Defining
    \begin{equation}
        Z_\mu = \frac{1}{\sqrt{g_1^2 + g_2^2}} (g_1 B_\mu - g_2 W^3_\mu)
    \end{equation}
    and
    \begin{equation}
        A_\mu = \frac{1}{\sqrt{g_1^2 + g_2^2}}(g_1 B_\mu + g_2 W^3_\mu)
    \end{equation}
    we get the fields corresponding to the \PZ{} boson and photon.
    We choose to normalise so that the kinetic term is 1, and we can then define
    \begin{equation}
        \cos\vartheta_{\PW} = \frac{g_1}{\sqrt{g_1^2 + g_2^2}}, \qqand \sin\vartheta_{\PW} \frac{g_2}{\sqrt{g_1^2 + g_2^2}},
    \end{equation}
    where \(\vartheta_{\PW}\) is the \defineindex{Weinberg angle}.
    We then find that the mass associated with \(A_\mu\) is 0 and the mass associated with \(Z_\mu\) is
    \begin{equation}
        m_{\PZ} = \frac{v}{2} \sqrt{g_1^2 + g_2^2}.
    \end{equation}
    This comes from computing \((\covariantDerivative_\mu \varphi)^\hermit (\covariantDerivative^\mu \varphi)\), which includes the term
    \begin{equation}
        \frac{1}{4}(g_1 B_\mu - g_2 W_\mu)^2 \frac{v^2}{2} = \frac{1}{8} (g_1^2 + g_2^2)v^2 Z_\mu Z^\mu,
    \end{equation}
    which is the mass term for \(Z_\mu\), and similarly for \(A_\mu\) the corresponding term vanishes.
    The masses of the \PWpm{} bosons follow similarly, although the algebra is more work.
    
    The generator leaving the symmetry broken Higgs field, \((0, v/\sqrt{2})^\trans\), unbroken is
    \begin{equation}
        Q = T_3 + Y = 
        \begin{pmatrix}
            1/2 & 0\\
            0 & -1/2
        \end{pmatrix}
        +
        \begin{pmatrix}
            1/2 & 0\\
            0 & 1/2
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 0\\
            0 & 0
        \end{pmatrix}
        .
    \end{equation}
    This generates a two dimensional representation of \(\unitary(1)\), in other words the Little group is \(H = \unitary(1)_{\mathrm{em}}\), where \(\mathrm{em}\) stands for electromagnetism, as this is the gauge group of QED.
    
    Electroweak theory is an attempt at unification, although as we can see here it cheats somewhat by keeping two couplings and two groups.
    This does have the advantage of making it fairly easy to recover electromagnetism.
    Symmetry breaks the term
    \begin{equation}
        ig_1 Y B_\mu + ig_2 T_3 W^3_\mu,
    \end{equation}
    which is in a two dimensional representation to the term
    \begin{equation}
        ieA_\mu Q
    \end{equation}
    which is also technically in a two dimensional representation, but reduces to a one dimensional representation direct summed with the trivial representation, which sends everything to 0 on the level of the Lie algebra, so it sends everything ot \(\e^0 = 1\) on the level of the Lie group.
    We can write the two electroweak fields as
    \begin{align}
        B_\mu &= \cos(\vartheta_{\PW}) Z_\mu + \sin(\vartheta_{\PW}) A_\mu,\\
        W^3_\mu &= -\sin(\vartheta_{\PW}) Z_\mu + \cos(\vartheta_{\PW}) A_\mu.
    \end{align}
    Comparing the term above before and after symmetry breaking we have
    \begin{equation}
        ig_1 Y B_\mu + ig_2 T_3 W^3_\mu = \frac{g_1 g_2}{\sqrt{g_1^2 + g_2^2}} iA_\mu(Y + T_3) = \frac{g_1g_2}{\sqrt{g_1^2 + g_2^2}} iA_\mu Q.
    \end{equation}
    From this we can identify
    \begin{equation}
        e = \frac{g_1 g_2}{\sqrt{g_1^2 + g_2^2}} = g_1 \sin \vartheta_{\PW} = g_2 \cos \vartheta_{\PW}.
    \end{equation}
    This means that \(e \le g_1\), so the weak force is actually stronger than electromagnetism.
    However, the weak force is suppressed by factors of \(1/m_{\PW}^2\).
    Experimental results give \(\sin^2\vartheta_{\PW} \approx 0.23\), and so \(g_2 \approx 2e\).
    
    
    
    
    
    \backmatter
%    \renewcommand{\glossaryname}{Acronyms}
%    \printglossary[acronym]
    \printindex
\end{document}
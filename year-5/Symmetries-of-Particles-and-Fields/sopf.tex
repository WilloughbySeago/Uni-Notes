% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

%% Packages
\usepackage{tensor}
\usepackage{csquotes}

% Tikz stuff
\usepackage{tikz}
%\tikzset{>=latex}
% External
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
% Other libraries
\usepackage{tikz-cd}

% References, should be last things loaded
\usepackage[pdfauthor={Willoughby Seago},pdftitle={Symmetries of Particles and Fields},pdfkeywords={symmetry, group theory, lie groups, lie algebras, representation theory, lorentz group, SU(2), quantum field theory},pdfsubject={Lie Groups, Representation Theory, and Quantum Field Theory}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}

% Highlight colour
\definecolor{highlight}{HTML}{710D78}
\definecolor{my blue}{HTML}{2A0D77}
\definecolor{my red}{HTML}{770D38}
\definecolor{my green}{HTML}{14770D}
\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Symmetries of Particles and Fields}
\author{Willoughby Seago}
\date{}
% \subtitle{}
% \subsubtitle{}

% Commands
% Text
\newcommand{\course}[1]{\textit{#1}}

% Maths
\newcommand{\cyclicGroupZ}[1][n]{\integers_{#1}}
\newcommand{\cyclicGroupC}[1][n]{C_{#1}}
\newcommand{\symmetricGroup}[1][n]{S_{#1}}
\newcommand{\dihedralGroup}[1][n]{D_{n}}
\newcommand{\subgroup}{\subseteq}
\newcommand{\properSubgroup}{\subset}
\DeclarePairedDelimiter{\cardinality}{\lvert}{\rvert}
\newcommand{\action}{\mathbin{.}}
\newcommand{\e}{\symrm{e}}
\newcommand{\ident}{1}
\newcommand{\sphere}[1][n]{S^{#1}}
\newcommand{\minkowskiSpace}{\reals^{1,3}}
\newcommand{\minkowskiMetric}{\eta}
\renewcommand{\lie}[1]{\symfrak{#1}}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1 , #2}
\DeclarePairedDelimiterX{\anticommutator}[2]{\{}{\}}{#1, #2}
\DeclarePairedDelimiterX{\innerproduct}[2]{(}{)}{#1, #2}
\newcommand{\isomorphic}{\cong}
\newcommand{\hermit}{\dagger}
\newcommand{\trans}{\top}
\renewcommand{\field}{\symbb{k}}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\symplectic}{Sp}
\ExplSyntaxOn
% Create LaTeX interface command
\NewDocumentCommand{\cycle}{ O{\,} m }{  % optional arg is separator, mandatory
    %arg is comma separated list
    (
    \willoughby_cycle:nn { #1 } { #2 }
    )
}

\clist_new:N \l_willougbhy_cycle_clist  % Create new clist variable
\cs_new_protected:Npn \willoughby_cycle:nn #1 #2 {  % create LaTeX3 function
    \clist_set:Nn \l_willougbhy_cycle_clist { #2 }  % set clist variable with
    %clist #2 passed by user
    \clist_use:Nn \l_willougbhy_cycle_clist { #1 }  % print list separated by #1
}
\ExplSyntaxOff
\newcommand{\order}{\symcal{O}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\spn}{span}
\newcommand{\parity}{\symcal{P}}
\newcommand{\timeReversal}{\symcal{T}}
\DeclareMathOperator{\artanh}{artanh}

\includeonly{}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{}
    \tableofcontents
    % \listoffigures
    \mainmatter
    
    \chapter{Introduction}
    Symmetries are incredibly important in physics.
    Both approximate and exact symmetries highly restrict the forms that our equations can take, to the point that often the correct equation can be written down from symmetries alone.
    The mathematical language expressing symmetries is group theory.
    In particular we are interested in continuous symmetries, which are modelled by Lie groups, and we can get a better understanding of Lie groups by studying the associated Lie algebras.
    
    The following is a selection of symmetries we'll discuss in this course, to demonstrate how broad a concept symmetry is:
    \begin{itemize}
        \item Spacetime translations and rotations, these give rise to conservation of energy, momentum, and angular momentum.
        \item Internal symmetries, these give rise to conservation of electric charge, and particle number to give a couple of examples.
        \item Charge conjugation, parity, and time reversal symmetries.
        \item Homogeneity and isotropy of the universe on a large scale, these allow us to write down a metric tensor which then describes the curvature of spacetime across the universe.
        \item Approximate internal symmetries, such as nuclear isospin and flavour \(\specialUnitary(3)\).
        \item General coordinate invariance (or invariance under diffeomorphisms) leads to an interpretation of general relativity as a gauge theory.
    \end{itemize}
    
    The course is structured as follows:
    \begin{enumerate}
        \item We introduce Lie groups and Lie algebras.
        \item We study spacetime symmetries and look at the actions that we can form obeying these symmetries.
        \item We study compact groups and their representations.
        \item We look at applications in particle physics, including Noether's theorem, isospin, the quark model, spontaneous symmetry breaking, chiral symmetry, gauge theories, QCD, electroweak theory, and the Higgs mechanism.
        \item We loo at applications in cosmology.
    \end{enumerate}
    
    \part{Lie Groups}
    \chapter{Groups and Lie Groups}
    \epigraph{What's the difference between theoretical physics and maths? The distinction is that thoretical physicists rarely prove anything, we cheat by looking at nature.}{Neil Turok}
    \section{Groups}
    \begin{rmk}
        For more details see the \course{Symmetries of Quantum Mechanics} course.
    \end{rmk}
    A group is the mathematical language used to describe symmetries.
    Roughly speaking a symmetry is something that we can do to something else, such that the second thing is unchanged.
    We should expect that chaining together symmetries should behave nicely.
    In particular, it should be possible to do nothing, and it should be possible to undo anything we do.
    Abstracting this gives us the definition of a group.
    \begin{dfn}{Group}{}
        A \defineindex{group}, \((G, \cdot)\), is a set, \(G\), along with an associative binary operation, \(\cdot \colon G\times G \to G\), such that there is an identity element and every element has an inverse.
        
        \define{Associativity}\index{associative} means that for all \(g, h, k \in G\) we have
        \begin{equation}
            g \cdot (h \cdot k) = (g \cdot h) \cdot k.
        \end{equation}
        The \defineindex{identity} element is the distinguished element \(1 \in G\) such that
        \begin{equation}
            1 \cdot g = g \cdot 1 = g
        \end{equation}
        for all \(g \in G\).
        For each \(g \in G\) we have some \(g^{-1} \in G\), called the \defineindex{inverse} of \(g\), such that
        \begin{equation}
            g \cdot g^{-1} = g^{-1} \cdot g = 1.
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Abelian}{}
        A group, \(G\), is \defineindex{Abelian} if \(gh = hg\) for all \(g, h \in G\).
    \end{dfn}
    
    \begin{ntn}{}{}
        We usually refer to a group, \(G\), rather than a group \((G, \cdot)\), with the operation either implicit or stated separately.
        
        We usually write the operation as juxtaposition, so \(gh\) instead of \(g \cdot h\).
        We may use multiple symbols for different group operations, such as \(+\), \(*\), or \(\circ\).
        We then write the inverse and identities with the appropriate notation, so \(-g\) for \(g\) inverse if we use \(+\), and 0 for the identity.
    \end{ntn}
    
    \begin{exm}{Groups}{}
        \begin{itemize}
            \item The single element set, \(\{e\}\), is a group if we define \(e \cdot e = e\).
            This is called the \defineindex{trivial group}.
            \item The symmetries of a cube form a group, with the operation being concatenation of symmetries.
            \item The sets \(\integers\), \(\rationals\), \(\reals\), and \(\complex\) all form groups under addition.
            \item The sets \(\rationals^{\times}\), \(\reals^{\times}\), and \(\complex^{\times}\) all form groups under multiplication where \(S^{\times} = S\setminus \{0\}\) denotes the set \(S\) with \(0\) removed.
            \item Invertible \(n \times n\) matrices over some ring, \(R\), form a group under matrix multiplication.
            \item The set \(\cyclicGroupZ = \cyclicGroupC = \{0, 1, \dotsc, n - 1\}\)\index{Zn@\(\cyclicGroupZ\)|see{cyclic group}}\index{Cn@\(\cyclicGroupC\)|see{cyclic group}} forms a group under addition modulo \(n\).
            This is called the \defineindex{cyclic group} of order \(n\).
            \item The set of all permutations on \(n\) elements forms a group, denoted \(\symmetricGroup\)\index{Sn@\(S_n\)|see{symmetric group}}.
            This is called the \defineindex{symmetric group} on \(n\) objects.
            \item The set of invertible functions, \(A \to A\), forms a group under function composition.
            \item The set of translations of space forms a group under repeated application of translations.
            \item The set of rotations of space forms a group under repeated application of rotations.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Subgroup}{}
        Let \(G\) be a group, and let \(H\) be a subset of \(G\).
        We say that \(H\) is a \defineindex{subgroup} of \(G\), and write \(H \subgroup G\) if \(H\) is a group under the operation of \(G\).
        
        If \(H \ne G\) we say that \(H\) is a \defineindex{proper subgroup}\index{subgroup!proper} of \(G\), and we write \(H \properSubgroup G\).
        If \(H \ne \{e\}\) then we say that \(H\) is a \index{nontrivial subgroup}\index{subgroup!nontrivial} of \(G\).
    \end{dfn}
    
    \begin{dfn}{Order}{}
        The order of a finite group, that is a group with a finite number of elements, is the number of elements.
        We write \(\cardinality{G}\) for the order of the group \(G\).
    \end{dfn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item The order of the trivial group is \(\cardinality{\{e\}} = 1\).
            \item The order of \(\cyclicGroupC\) is \(\cardinality{\cyclicGroupC} = n\).
            \item The order of \(\symmetricGroup\) is \(\cardinality{\symmetricGroup} = n!\).
        \end{itemize}
    \end{exm}
    
    We usually think of symmetries, and therefore groups, as acting on something.
    The group definition doesn't tell us how the group elements act on an object (mathematical or physical), this will come later.
    We typically don't distinguish much between a group acting on something, and a stand alone group with nothing to act on, since the later is not that interesting.
    
    \begin{dfn}{Group Action}{}
        Given a group, \(G\), and a set, \(S\), we define a \defineindex{left group action}\index{group action} as a function, \(\varphi \colon G \times S \to S\), such that
        \begin{itemize}
            \item \(\varphi(1, s) = s\) for all \(s \in S\); and
            \item \(\varphi(g, \varphi(h, s)) = \varphi(gh)\).
        \end{itemize}
        Writing \(\varphi(g, s) = g \action s\) these requirements become \(1 \action s = s\) and \(g \action (h \action s) = (gh) \action s\).
    \end{dfn}

    \begin{exm}{Group Actions}{}
        The group \(\reals^3\) of three-dimensional real vectors under vector addition acts on itself by translation:
        \begin{equation}
            \vv{a} \action \vv{x} = \vv{x} + \vv{a}.
        \end{equation}
        The group \(S_n\) acts on \((1, \dotsc, n)\) by permutation, say \(\sigma \in S_n\) swaps the first and second element, then
        \begin{equation}
            \sigma \action (1, 2, 3, \dotsc, n) = (2, 1, 3, \dotsc, n).
        \end{equation}
        The group \(\cyclicGroupZ[4] = \{0, 1, 2, 3\}\) acts on \(\reals^2\) by rotations by angles \(0\), \(\pi/2\), \(\pi\), and \(3\pi/2\), corresponding to \(0\), \(1\), \(2\), and \(3\) respectively.
        
        The group \(\cyclicGroupC[4] = \{1, i, -1, -i\}\), acts on \(\complex\) by complex multiplication:
        \begin{equation}
            z \action w = zw
        \end{equation}
        where \(z \in \cyclicGroupC[4]\) and \(w \in \complex\).
        However, notice that we can interpret \(\complex\) as a plane and rotation by multiples of \(i\) as rotations by multiplies of \(\pi/2\), so really these two groups are the same.
    \end{exm}

    Often two groups may look different, as in the case of \(\cyclicGroupZ[4]\) and \(\cyclicGroupC[4]\) above, but really they are the same on the level of the group structure.
    This is expressed through the notion of isomorphisms.
    
    \begin{dfn}{Morphisms}{}
        Let \(G\) and \(H\) be groups.
        A \defineindex{group homomorphism}\index{homomorphism!of groups} is a function \(\varphi \colon G \to H\) such that \(\varphi(g_1 g_2) = \varphi(g_1)\varphi(g_2)\) for all \(g_1, g_2 \in G\).
        If this function is invertible it is called a \defineindex{group isomorphism}\index{isomorphism!of groups}.
        
        If there exists an isomorphism \(G \to H\) then we say \(G\) and \(H\) are \defineindex{isomorphic}, and write \(G \isomorphic H\).
    \end{dfn}
    
    So, when we say that \(\cyclicGroupZ[4]\) and \(\cyclicGroupC[4]\) are the same, we really mean that the mapping defined by \(0 \mapsto 1\), \(1 \mapsto i\), \(2 \mapsto -1\), and \(3 \mapsto -i\) is a group isomorphism, and \(\cyclicGroupZ[4] \isomorphic \cyclicGroupC[4]\).
    In fact, this is a slightly cheaty example as \(\cyclicGroupZ[n] \isomorphic \cyclicGroupC[n]\) where \(\cyclicGroupC[n] = \{\e^{2\pi i m/n} \mid m = 0, \dotsc, n - 1\}\).
    Alternatively we can define \(\cyclicGroupC[n] = \cyclicGroupZ[n]\), and then say that the complex exponentials before are a representation of the group.
    
    When two groups are isomorphic, as far as group theory is concerned, they are equivalent and interchangeable.
    
    \begin{exm}{}{}
        \begin{itemize}
            \item The group \(\symmetricGroup[2]\) is isomorphic to the group \(\cyclicGroupC[2]\).
            \item The group of symmetries of the triangle is isomorphic to \(\symmetricGroup[3]\).
            \item All groups are isomorphic to some subgroup of \(\symmetricGroup[n]\) for some \(n\).
            \item All groups of order 1 are isomorphic, since they have a single element and that element must be the identity.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Direct Product}{}
        Let \(G\) and \(H\) be groups.
        Then there exists a group \(G \times H\) formed from pairs \((g, h)\) with \(g \in G\) and \(h \in H\) with the group operation just elementwise application of \(G\) and \(H\)'s operations:
        \begin{equation}
            (g, h)(g', h') = (gg', hh').
        \end{equation}
    \end{dfn}
    
    \begin{lma}{}{}
        The direct product as defined above is a group.
        \begin{proof}
            Let \(G\) and \(H\) be groups and \(G \times H\) their direct product.
            Then for all \(g, g', g'' \in G\) and \(h, h', h'' \in H\) we have
            \begin{align}
                (g, h)[(g', h')(g'', h'')] &= (g, h)(g'g'', h'h'')\\
                &= (g(g'g''), h(h'h''))\\
                &= ((gg')g'', (hh')h'')\\
                &= (gg', hh')(g'', h''')\\
                &= [(g, h)(g', h')](g'', h'').
            \end{align}
            So, the operation is associative.
            Let \(1_G \in G\) and \(1_H \in H\) be the identities in their respective groups, then
            \begin{equation}
                (1_G, 1_H)(g, h) = (1_Gg, 1_Hh) = (g, h) = (g1_G, h1_H) = (g, h)(1_G, 1_H),
            \end{equation}
            so \(1_{G\times H} = (1_G, 1_H)\) is the identity in \(G \times H\).
            Finally, take \(g \in G\) and \(h \in H\).
            Then we have
            \begin{equation}
                (g, h)(g^{-1}, h^{-1}) = (gg^{-1}, hh^{-1}) = (1_G, 1_H) = 1_{G\times H},
            \end{equation}
            so \(G \times H\) has inverses.
        \end{proof}
    \end{lma}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Considering \(\reals\) as a group under addition we have \(\reals \times \reals = \{(x, y) \mid x, y \in \reals\}\) with addition defined as \((x, y) + (x', y') = (x + x', y + y')\), so we identify this with the plane, \(\reals^2\), under vector addition.
            \item The product \(\cyclicGroupZ[3] \times \cyclicGroupZ[2]\) is the set \(\{(n, m) \mid n = 0, 1, 2 \text{ and } m = 0, 1\}\).
            This group is generated\footnote{A group, \(G\), is \defineindex{generated} by some \(g \in G\) if all \(h \in G\) are of the form \(g^n\) for some \(n \in \naturals\), where \(g^n \coloneqq g g \dotsm g\) with \(n\) factors and \(g^0 \coloneqq 1\). We call the smallest such \(n\) the \defineindex{order} of \(h\).} by \((1, 1)\), and \((1, 1)\) has order 6, so this group is isomorphic to \(\cyclicGroupZ[6]\).
            In fact, if \(p\) and \(q\) are relatively prime them \(\cyclicGroupZ[p] \times \cyclicGroupZ[q] \isomorphic \cyclicGroupZ[pq]\).
            \item The first noncyclic group is the Klein four-group, \(V = \cyclicGroupZ[2] \times \cyclicGroupZ[2]\).
            \item If \(G\) is a group and \(\{1\}\) is the trivial group then \(G \times \{1\} \isomorphic \{1\} \times G \isomorphic G\), so \(\{1\}\) acts as an identity for the direct product.
        \end{itemize}
    \end{exm}
    
    As well as direct products, which just pair up elements in a non-interacting way, we can act on one of the groups with the other, and then pair up the elements.
    This gives a new type of product, and so allows us to construct new groups.
    
    \begin{dfn}{Semidirect Product}{}
        Let \(N\) and \(H\) be groups, and let consider a group action of \(N\) on \(H\).
        We can define a group, \(N \rtimes H\), consisting of pairs \((n, h)\) with \(n \in N\) and \(h \in H\), and with the group operation
        \begin{equation}
            (n, h)(n', h') = (n(h \action n'), hh').
        \end{equation}
        This is called the \defineindex{semidirect product}.
    \end{dfn}
    That is, we act on the second element of \(N\) with the first element of \(H\), and then proceed with the usual direct product.
    
    \begin{exm}{Dihedral Group}{}
        The dihedral group, \(\dihedralGroup\), of order \(2n\) is the semidirect product of \(\cyclicGroupC\) and \(\cyclicGroupC[2]\) where \(\cyclicGroupC[2]\) acts on \(\cyclicGroupC\) by the nonidentity element of \(\cyclicGroupC[2]\) sending elements of \(\cyclicGroupC\) to their inverses.
        It can be shown that this is the symmetry group of the regular \(n\)-gon, where we interpret the action of \(\cyclicGroupC\) on the \(n\)-gon as rotations by \(2\pi/n\) and the action of \(\cyclicGroupC[2]\) as reflections.
    \end{exm}
    
    Groups can act on sets, and groups \emph{are} sets, so it makes sense to have groups act on groups, and in particular, its pretty obvious we can have a group act on itself.
    One example is the left action of \(G\) on \(G\), given by \(g \action h = gh\) for all \(g, h \in G\).
    Here we interpret this as \(g\) acting on \(h\).
    A slightly less trivial example is \defineindex{conjugation}, where \(g \action h = ghg^{-1}\) for all \(g, h \in G\).
    We can view conjugacy as a symmetry of the group, but what does it act on?
    Well, we can just act on the group, but this isn't that helpful.
    Instead, we define the \defineindex{conjugacy class} of some \(g \in G\) to be the set of all \(h \in G\) conjugate to \(G\), that is, all \(h\) such that there exists \(k \in G\) with \(khk^{-1} = g\).
    This partitions \(G\) into sets, since this defines an equivalence relation.
    
    Now that we have a particular symmetry of \(G\) we should ask if there are any invariants, and indeed there is.
    
    \begin{dfn}{Normal Subgroup}{}
        Let \(G\) be a group and \(N\) a subgroup of \(G\).
        Then we call \(N\) a \defineindex{normal subgroup}, or \define{invariant subgroup}\index{invariant subgroup|see{normal subgroup}} if \(N\) is invariant under conjugation.
        That is if \(n \in N\) then \(gng^{-1} \in N\) for all \(g \in G\).
    \end{dfn}

    \begin{exm}{Normal Subgroups}{}
        \begin{itemize}
            \item The trivial group, consisting of just the identity, is a normal subgroup of any group.
            \item The \defineindex{centre of a group}, defined as the set of all elements which commute with every element of the group is a normal subgroup.
            If the only normal subgroups are the trivial group and the centre then we call the group a \define{simple}\index{simple!group}.
            \item The group \(\{\cycle{}, \cycle{1,2,3}, \cycle{3,2,1}\}\) is a normal subgroup of \(\symmetricGroup[3]\).
        \end{itemize}
    \end{exm}
    
    We've seen products of groups, we now ask if there is a sensible notion of a quotient of groups, and indeed there is!
    The definition only makes sense if one of the groups is a normal subgroup of the other.
    But first, we need another definition.
    \begin{dfn}{Coset}{}
        Let \(G\) be a group and \(H\) a subgroup of \(G\).
        For each \(g \in G\) we define the \defineindex{left coset}\index{coset} to be \(gH \coloneqq \{gh \mid h \in H\}\).
        That is, it's the set of all elements of the form \(gh\) for \(h \in H\).
        Similarly the \defineindex{right coset} is \(Hg \coloneqq \{hg \mid h \in H\}\).
    \end{dfn}
    
    \begin{wrn}
        Cosets are not, in general, groups, since only the coset \(H\) contains the identity.
    \end{wrn}
    
    \begin{exm}{Cosets}{}
        \begin{itemize}
            \item Let \(\integers\) be the group of integers under addition.
            Then \(2\integers\) is the subgroup of even integers under addition.
            We can define cosets, \(0 + 2\integers = 2\integers\) and \(1 + 2\integers\) corresponding to the even integers and odd integers.
            \item Consider \(\reals^3\), then we can view \(\reals\) as a subgroup of this given by the \(x\)-axis.
            Then the cosets \(\vv{a} + \reals\) for \(\vv{a} \in \reals\) consist of lines parallel to the \(x\)-axis.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Quotient Group}{}
        Let \(G\) be a group, and \(N\) a normal subgroup of \(G\).
        Define \(G/N\) to be the set of all left cosets in \(G\), that \(G/N \coloneqq \{aN, a \in G\}\).
        We can define a group operation on this set: \((gN)(g'N) \coloneqq (gg')N\).
        We then call \(G/N\) the \defineindex{quotient group} of \(G\) by \(N\), also known as the \define{factor group}\index{factor group|see{quotient group}}.
    \end{dfn}
    
    \begin{wrn}
        This definition only defines a group if \(N\) is a normal subgroup of \(G\).
        If this isn't the case then the operation will not be well defined.
    \end{wrn}
    
    \begin{dfn}{Quotient Groups}{}
        \begin{itemize}
            \item Consider the group of integers, \(\integers\), under addition, and the normal subgroup \(2\integers\), of even integers.
            The quotient group \(\integers/2\integers\) consists of the cosets \(2\integers\) and \(1 + 2\integers\), where the second is all odd integers.
            Considering \(2\integers\) to be \(0 + 2\integers\) we see that, for example,
            \begin{equation}
                (1 + 2\integers)(0 + 2\integers) = (1 + 0) + 2\integers = 1 + 2\integers
            \end{equation}
            and
            \begin{equation}
                (1 + 2\integers)(1 + 2\integers) = (1 + 1) + 2\integers = 2 + 2\integers = 0 + 2\integers = 2\integers.
            \end{equation}
            We can conclude that \(\integers/2\integers \isomorphic \cyclicGroupZ[2]\), and indeed some people use \(\integers/n\integers\) to denote \(\cyclicGroupZ\).
            
            \item Consider the real numbers, \(\reals\), viewed as an additive group, with the subgroup \(\integers\).
            Each coset of \(\integers\) is then of the form \(a + \integers\) where \(a \in \reals\).
            When the noninteger parts of \(a\) and \(b\) are equal the cosets \(a + \integers\) and \(b + \integers\) are equal, and so we can subtract the integer parts of \(a\) and \(b\) and just consider the noninteger parts, identifying the groups with the noninteger part of their representative we notice that \(a + \integers\) is essentially covering the interval \([0, 1)\) over and over again as \(a\) is varied.
            In fact, \(\reals/\integers \isomorphic \sphere[1]\), identifying \(a + \integers\) with a rotation by \(2\pi\tilde{a}\), where \(\tilde{a}\) is the noninteger part of \(a\).
            
            \item The group \(G/G\) is the trivial group, and the group \(G/\{e\}\) is (isomorphic to) \(G\).
        \end{itemize}
    \end{dfn}
    
    \section{Lie Groups}
    Often the groups of interest in physics have an uncountable number of elements, in this case they are continuos groups.
    We can define this more rigorously by parametrising the elements in a continuous way.
    \begin{dfn}{Continuous Groups}{}
        A \defineindex{continuous group}, \(G\), is a group with an infinite number of elements parametrised by a (possibly infinite) set of parameters, \(\{\alpha\} = \{\alpha_1, \alpha_2, \dotsc\}\), such that each element of \(G\) can be written as a function of these parameters, \(g(\alpha) = g(\alpha_1, \alpha_2, \dotsc)\).
        
        The \defineindex{dimension} of \(G\) is the number of independent parameters.
    \end{dfn}
    
    \begin{exm}{}{}
        Spatial translations of \(\reals^3\) form a three dimensional continuous group.
        The translation \(\vv{x} \to \vv{x} + \vv{a}\) is parametrised by \(\{a_1, a_2, a_3\}\).
        
        Spatial rotations of \(\reals^3\) form a three dimensional continuous group.
        Consider the rotation \(x^i \to \tensor{R}{^i_j}x^j\).
        For a rotation we know that \(\tensor{R}{^i_j}\tensor{R}{^k_l} \delta_{ik} = \delta_{jl}\), or \(R^\trans R = \ident\).
        A rotation in three dimensions can be parametrised by 3 parameters, two used to pick out a unit vector in \(\reals^3\), the third component being fixed by normalisation, and one parameter to give the angle of rotation about said unit vector.
    \end{exm}
    
    As is usually the case in physics we assume that most things are analytic, they can be expanded in Taylor series.
    This assumption defines Lie groups, one of the main subjects of our study in this course.
    \begin{dfn}{Lie Group}{}
        A \defineindex{Lie group}\footnote{Named after Sophus Lie, no relation to lying, pronounced \emph{lee}.}, \(G\), is a continuous group for which the group multiplication has an analytic structure.
        That is, if \(g(\alpha) = g(\beta)g(\gamma)\) then \(\alpha_i = \varphi_i(\beta, \gamma)\) where \(\varphi_i\) are analytic functions of \(\beta\) and \(\gamma\).
    \end{dfn}
    
    Formally we say that the parameter space for a Lie group is a smooth manifold and multiplication is given by a smooth function on this manifold.
    
    Continuous groups can be either compact or noncompact, depending on the structure of the parameter space.
    For our purposes a \defineindex{compact space} is one which is closed and bounded, that is it contains its boundary and the parameters are restricted in their size.
    For example, \(\reals\) is noncomapct, since it isn't bounded, the intervals \([0, 1)\), \((0, 1]\), and \((0, 1)\) are noncompact as they don't contain their boundaries, and \([0, 1]\) is compact.
    The sphere, \(\sphere\), is compact.
    
    For example, the group of translations of \(\reals^3\) is noncompact, since we can have infinite translations.
    The group of rotations of \(\reals^3\) on the other hand is compact, the parameters defining the axis are constrained to \([0, 1]\) and the parameters defining the angle are constrained to \([0, 2\pi)\), which looks like it isn't closed but actually we identify 0 and \(2\pi\) as the same rotation, so really this is drawing from the circle, \(\sphere[1]\), which is closed.
    The parameter space is then \([0, 1]^2 \times \sphere[1]\), which is compact, since the product of compact spaces is compact.
    
    \section{Metric Spaces}
    \begin{rmk}
        This section is concerned with the notion of a metric on a real vector space.
        There is a more general mathematical notion of a metric on a topological space which we shall not discuss here, but when restricted to \(\reals^n\) these notions coincide.
    \end{rmk}

    Consider some finite dimensional real vector space, \(V\), and fix some basis.
    Take a vector with components \(x^\mu\) where \(\mu = 1, \dotsc, N\) where \(N = \dim V\).
    Alternatively, particularly when doing relativity, we may take \(\mu = 0, \dotsc, N - 1\), in which case we interpret \(x^0\) as the time.
    A \defineindex{metric}, \(g_{\mu\nu}\), is a real, symmetric, \(N \times N\) matrix.
    We define the length of a vector according to
    \begin{equation}
        \norm{x}^2 = x^2 = g_{\mu\nu}x^\mu x^\nu,
    \end{equation}
    where summation over \(\mu\) and \(\nu\) is implied by the Einstein summation convention, which we follow in this course: repeated indices appearing once raised and once lowered are summed over unless otherwise specified.
    
    It is possible to choose a basis such that:
    \begin{itemize}
        \item \(g_{\mu\nu}\) is diagonal, that is \(g_{\mu\nu} = \diag(\lambda_1, \dotsc, \lambda_N)\) where \(\lambda_i \in \reals\) are the eigenvalues of \(g_{\mu\nu}\); and
        \item the eigenvalues are either \(0\) or \(\pm 1\).
        Reordering the basis as required allows us to put the metric in the canonical form
        \begin{equation}
            g_{\mu\nu} = \diag(1, \dotsc, 1, 0, \dotsc, 0, -1, \dotsc, -1).
        \end{equation}
        We call this the \defineindex{canonical basis}.
    \end{itemize}

    The first point, that \(g_{\mu\nu}\) is diagonalisable, is not that surprising.
    The second point, that the eigenvalues are restricted to \(\{0, \pm 1\}\), is worth explaining.
    Suppose that \(g_{\mu\nu} = \diag(\lambda, \dotsc)\) where \(\lambda \ne 0\).
    We can rescale \(x^1 \mapsto sx^1\).
    Then, \(g_{\mu\nu} \mapsto \diag(\lambda/s^2, \dotsc)\), in order for \(x^2\) to remain invariant, since using the diagonal nature of \(g_{\mu\nu}\) we have
    \begin{multline}
        x^2 = g_{\mu\nu}x^\mu x^\nu = \lambda x^1 x^1 + \sum_{i = 2}^{N} g_{ii}x^ix^i\\
        \mapsto \frac{\lambda}{s^2}sx^1sx^1 + \sum_{i = 2}^{N} g_{ii}x^ix^i = \lambda x^1x^1 + \sum_{i = 2}^{N} g_{ii}x^ix^i = x^2.
    \end{multline}
    So, by choosing \(s = \sqrt{\abs{\lambda}}\) we can scale \(g_{\mu\nu}\) such that
    \begin{equation}
        g_{\mu\nu} = \diag(\sgn(\lambda), \dotsc)
    \end{equation}
    where
    \begin{equation}
        \sgn(\lambda) =
        \begin{cases}
            1 & \lambda > 0,\\
            -1 & \lambda < 0.
        \end{cases}
    \end{equation}
    Doing this same process for all nonzero eigenvalues of the original \(g_{\mu\nu}\) we can scale all diagonal elements to be \(0\) or \(\pm 1\).
    Reordering the basis then gives us the canonical form.
    
    \begin{exm}{Metrics}{}
        Euclidean space, \(\reals^n\), has the metric \(g_{ij} = \delta_{ij}\).
        In particular, when \(n = 3\) we have
        \begin{equation}
            x^2 = (x^1)^2 + (x^2)^2 + (x^3)^2,
        \end{equation}
        which is just the usual length-squared of a vector, \(\vv{x} = (x^1, x^2, x^3)\).
        
        Minkowski space, \(\minkowskiSpace\), has the metric \(g_{\mu\nu} = \diag(1, -1, -1, -1)\), so
        \begin{equation}
            x^2 = (x^0)^2 - (x^1)^2 - (x^2)^2 - (x^3)^2,
        \end{equation}
        which is just the usual scalar product of two four-vectors.
        \begin{wrn}
            The choice of \(g_{\mu\nu} = \diag(-1, 1, 1, 1)\) is also common, leading to lots of annoying sign discrepancies.
            In fact, given any metric, \(g\), the metric \(-g\) is equivalent.
        \end{wrn}
    \end{exm}
    
    Transformations preserving the metric in its canonical form are the symmetries that we are most interested in.
    This makes sense when you think about what we use the metric for.
    In Euclidean space the metric defines lengths, something that shouldn't change under symmetries.
    In special relativity transformations preserving the metric don't preserve lengths, at least in the traditional sense, they instead preserve the speed of light.
    In quantum mechanics the metric used to define the inner product between state vectors being preserved means that probability is preserved by metric preserving transformations.
    
    \begin{dfn}{Types of Metric}{}
        If the metric has only positive eigenvalues then we say that it is a \defineindex{positive definite} metric.
        If the metric has positive and negative eigenvalues then we say that it is \defineindex{indefinite}.
    \end{dfn}
    
    An indefinite metric allows for \(x^2\) to be positive, negative or zero, contrary to our normal intuition about length squared.
    
    The \defineindex{metric signature} is either a pair or triple of natural numbers giving the number of 1s, \(-1\)s, and 0s in the metric.
    For example, the the Euclidean metric on \(\reals^n\) is \((n, 0, 0)\) or \((n, 0)\), where no third number is taken to mean no zero eigenvalues.
    So the Minkowski metric is \((1, 3)\), or \((1, 3, 0)\).
    
    If we have a nonsingular metric, that is \(\det g \ne 0\), then we can use the metric, and its inverse, to raise and lower indices.
    Define the inverse metric \(g^{\mu\nu}\) to be such that
    \begin{equation}
        g^{\mu\rho}g_{\rho\nu} = \tensor{\delta}{^\mu_\nu}.
    \end{equation}
    Then we define
    \begin{equation}
        x_\mu = g_{\mu\nu}x^\nu, \quad T_{\mu\nu} = g_{\mu\rho}g_{\nu\sigma}T^{\rho\sigma},
    \end{equation}
    and so on for more indices.
    
    After lengths the next most important quantity we can define is volumes.
    Recall that a parallelepiped with sides \(\vv{a}, \vv{b}, \vv{c} \in \reals^3\) has volume
    \begin{equation}
        \abs{\vv{a} \cdot (\vv{b} \times \vv{c})} = \abs{a^ib^jc^k\varepsilon^{ijk}}.
    \end{equation}
    This suggests that we can use the Levi-Civita symbol to generalise volumes.
    Define the \defineindex{Levi-Civita symbol} on \(n\) indices to be
    \begin{equation}
        \varepsilon_{\mu_1 \mu_2 \dots \mu_n} \coloneqq
        \begin{cases}
            +1 & \text{if } \mu_1 \mu_2 \dots \mu_n \text{ is an even permutation of } 1 \dots n,\\
            -1 & \text{if } \mu_1 \mu_2 \dots \mu_n \text{ is an odd permutation of } 1 \dots n,\\
            0 & \text{otherwise}.
        \end{cases}
    \end{equation}
    For a nonsingular metric we can then define the Levi-Civita symbol with raised indices:
    \begin{equation}
        \varepsilon^{\mu_1\mu_2 \dots \mu_n} \coloneqq ^{\mu_1\nu_1}g^{\mu_2\nu_2} \dotsm g^{\mu_n\nu_n} \varepsilon_{\nu_1\nu_2 \dots \nu_n}.
    \end{equation}
    
    We can use this to define the determinant of a matrix, \(A\):
    \begin{equation}
        \det A \coloneqq \varepsilon_{\mu_1\mu_2 \dots \mu_n} \tensor{A}{^{\mu_1}_1} \tensor{A}{^{\mu_2}_2} \dotsm \tensor{A}{^{\mu_n}_n}
    \end{equation}
    
    In a general metric the infinitesimal volume element, which is invariant under metric preserving transformations, is
    \begin{equation}
        \dl{V} \coloneqq \sqrt{\abs{\det g}} \varepsilon_{\mu_1\mu_2 \dots \mu_n} \dd{x^{\mu_1}} \dd{x^{\mu_2}} \dotsm \dd{x^{\mu_n}}.
    \end{equation}	
    If \(g\) is the Minkowski metric then its determinant is negative in any coordinate system and so this is often written \(\sqrt{-\det g}\) instead.
    It's also common to use \(g\) as short for \(\det g\), and anywhere else the metric appears it has indices.
    
    \chapter{Matrix Groups}
    \section{General Theory}
    In this chapter we will study various metric preserving groups of linear transformations on some vector space, \(V\).
    If \(V\) is finite dimensional then we can choose a basis and identify these linear transformations with matrices, which gives a group.
    
    A \defineindex{linear transformation} is a function, \(T \colon U \to V\), where \(U\) and \(V\) are vector spaces over the same base field, \(\field\), such that
    \begin{equation}
        T(\alpha u_1 + \beta u_2) = \alpha T(u_1) + \beta T(u_2)
    \end{equation}
    for all \(\alpha, \beta \in \field\) and \(u_1, u_2 \in U\).
    Note that we often drop the brackets for linear transformations, and just write \(Tu = T(u)\), which reflects the notion of linear transformations as matrices in the finite dimensional case.
    
    The group operation in question is composition of linear transformation, which is matrix multiplication in the finite dimensional case.
    This inherits associativity from the underlying operation of function composition.
    Let \(f \colon A \to B\), \(g \colon B \to C\), and \(h \colon C \to D\) be functions.
    Then,
    \begin{multline}
        [(h \circ g) \circ f](a) = (h \circ g)(f(a)) = h(g(f(a)))\\
        = h((g \circ f)(a)) = [h \circ (g \circ f)](a).
    \end{multline}
    for all \(a \in A\), so \((h \circ g) \circ f = h \circ (g \circ f)\).
    
    The identity linear transformation, \(\ident \colon V \to V\), is given by \(\ident(v) = v\) for all \(v \in V\).
    Note that \(T \circ \ident_U = T\) and \(\ident_V \circ T = T\) for all \(T \colon U \to V\) where \(\ident_V\) and \(\ident_U\) are the identities on their respective vector spaces.
    
    The inverse linear transformation of \(T \colon U \to V\), if it exists, is the linear transformation \(T^{-1}\colon V \to U\) such that \(T \circ T^{-1} = \ident_V\) and \(T^{-1} \circ T = \ident_U\).
    We get around the \enquote{if it exists} problem by just defining our groups to be formed from invertible transformations.
    This works since if \(T \colon U \to V\) and \(S \colon V \to W\) are invertible linear transformations then the inverse of \(S \circ T\) is \(T^{-1} \circ S^{-1}\):
    \begin{equation}
        (S \circ T) \circ (T^{-1} \circ S^{-1}) = S \circ (T \circ T^{-1}) \circ S^{-1} = S \circ \ident_V \circ S = S \circ S^{-1} = \ident_W,
    \end{equation}
    and similarly \((T^{-1} \circ S^{-1}) \circ (S \circ T) = \ident_U\), so \(S \circ T \colon U \to W\) has an inverse.
    
    In our examples we will consider linear transformations from a space to itself, known as \define{endomorphisms}\index{endomorphism}.
    The set of all such functions is
    \begin{equation}
        \End(V) \coloneqq \{T \colon V \to V \mid T \text{is linear}\}.
    \end{equation}
    This same notion applies to other types of objects, such as groups, with linear transformations replaced with the appropriate type, so group homomorphisms for groups.
    With function composition as multiplication \(\End(V)\) forms a monoid, which is a group the requirement for inverses.
    Restricting ourselves to invertible linear transformations we consider all invertible linear transformations from a space to itself, known as \define{automorphisms}\index{automorphism}.
    The set of all such functions is
    \begin{equation}
        \Aut(V) \coloneqq \{T \colon V \to V \mid T \text{is linear and invertible}\} \subseteq \End(V).
    \end{equation}
    With function composition as multiplication \(\Aut(V)\) forms a group.
    
    In the finite dimensional case we are interested in transformations which can be expressed as invertible matrices, \(\tensor{D}{^\mu_\nu}(\alpha)\), where \(\alpha = \alpha_1, \dotsc, \alpha_{\dim G}\) parametrises the group.
    These act on vectors as
    \begin{equation}
        x^\mu \to x'^\mu = \tensor{D}{^\mu_\nu}(\alpha) x^\nu
    \end{equation}
    where \(\mu, \nu = 1, \dotsc, N\).
    Note that \(N \ne \dim G\) in general.
    
    \section{Matrix Groups}
    In this section we will define specific matrix groups of interest in physics.
    In all of these suppose we have a vector space, \(V\), of potentially infinite dimension over either the real or complex numbers.
    In all cases the elements of the groups are linear transformations and the group operation is composition of transformations.
    The identity is the identity transformation.
    Our first group is the broadest possible such group.
    
    \begin{dfn}{General Linear Group}{}
        The \defineindex{general linear group}, \(\generalLinear(V)\)\index{GL(V)@\(\generalLinear(V)\)|see{general linear group}}, is the group of all invertible transformations of \(V\), that is
        \begin{equation}
            \generalLinear(V) \coloneqq \{T \colon V \to V \mid T \text{ is invertible}\} = \Aut(V).
        \end{equation}
        
        If \(V\) is a vector space over the field \(\field\), and is of finite dimension \(N\), then by choosing a basis we can identify each linear transformation with a matrix, and we can identify \(\generalLinear(V)\) with the set of invertible \(N \times N\) matrices over \(\field\), denoted \(\generalLinear(N, \field)\).
        In this case invertible is equivalent to nonzero determinant, and so
        \begin{equation}
            \generalLinear(N, \field) \coloneqq \{T \in \matrices{N}{\field} \mid \det T \ne 0\}
        \end{equation}
        where \(\matrices{N}{\field}\) is the set of \(N \times N\) matrices over \(\field\).
    \end{dfn}
    
    There are a variety of notations for the general linear group, and the subsequent subgroups, such as \(\generalLinear_n(\field)\) or \(\generalLinear(n)\), leaving the precise field to context.
    
    All other groups to be defined are subgroups of \(\generalLinear(V)\), and so to show they are groups we need only show that they're closed under the induced operation and contain all inverses.
    This will require various linear algebra facts which we state without proof.
    We also won't worry too much about intricacies with definitions in the infinite dimensional case, we'll just work with matrices.
    
    \begin{dfn}{Special Linear Group}{}
        The \defineindex{special linear group}, \(\specialLinear(V)\)\index{SL(V)@\(\specialLinear(V)\)|see{special linear group}}, for finite dimensional \(V\), is the group of all invertible linear transformations with unit determinant:
        \begin{equation}
            \specialLinear(V) \coloneqq \{T \colon V \to V \mid T \text{ is invertible and } \det T = 1\} \subgroup \generalLinear(V).
        \end{equation}
        Choosing a basis we can identify each linear transformation with an \(N \times N\) matrix over \(\field\) and we get
        \begin{equation}
            \specialLinear(V) \coloneqq \{T \in \matrices{N}{\field} \mid \det T = 1\} \subgroup \generalLinear(N, \field).
        \end{equation}
    \end{dfn}
    
    We know that \(\det(S \circ T) = \det(S) \det(T)\), so if \(\det T = \det S = 1\) then \(\det(S \circ T) = 1\), so \(\specialLinear(V)\) is closed under composition.
    Since \(\det(T^{-1}) = 1/\det(T)\) if \(\det T = 1\) then \(\det(T^{-1}) = 1\), so \(\specialLinear(V)\) contains all inverses.
    Hence, \(\specialLinear(V)\) is a group.
    
    \begin{dfn}{Orthogonal Group}{}
        The \defineindex{orthogonal group}, \(\orthogonal(N)\)\index{O(n)@\(\orthogonal(n)\)|see{orthogonal group}}, is the group preserving the Euclidean metric.
        It consists of all orthogonal \(N \times N\) matrices over \(\reals\):
        \begin{equation}
            \orthogonal(N) \coloneqq \{O \in \matrices{N}{\reals} \mid O^\trans O = OO^\trans = \ident\} \subgroup \generalLinear(N, \reals).
        \end{equation}
    \end{dfn}
    
    Note that \(O^\trans O = \ident\) can be written as \(O^\trans \ident O = \ident\), where the \(\ident\) on the left is understood as the matrix form of the Euclidean metric, so the Euclidean metric is invariant under the action of the Orthogonal group
    \begin{equation}
        \delta_{\mu\nu} \xrightarrow{D \in \orthogonal(N)} \delta_{\alpha\beta} \tensor{D}{^\alpha_\mu}\tensor{D}{^\beta_\nu} = \delta_{\mu\nu}.
    \end{equation}
    This allows us to interpret orthogonal transformations as rotations, since they preserve distances and angles.
    
    Suppose \(O_1, O_2 \in \orthogonal(N)\), then \(O^\trans O = \ident\).
    Then \((O_1O_2)^{\trans}(O_1O_2) = O_2^\trans O_1^\trans O_1 O_2 = O_2^\trans \ident O_2 = \ident\), so \(\orthogonal(N)\) is closed under composition.
    We claim that if \(O \in \orthogonal(N)\) then \(O^{-1} \in \orthogonal(N)\), first note that \(O^{-1} = O^{\trans}\), then we have \((O^{-1})^\trans O^{-1} = (O^{\trans})^{\trans}O^{\trans} = OO^{\trans} = \ident\), so \(O^{-1} \in \orthogonal(N)\).
    Hence \(\orthogonal(N)\) contains all inverses.
    
    \begin{dfn}{Special Orthogonal Group}{}
        The \defineindex{special orthogonal group}, \(\specialOrthogonal(N)\)\index{SO(n)@\(\specialOrthogonal(n)\)|see{special orthogonal group}}, is the subgroup of the orthogonal group given by restricting to \(O \in \orthogonal(N)\) with \(\det O = 1\):
        \begin{equation}
            \specialOrthogonal(N) \coloneqq \{O \in \orthogonal(N) \mid \det O = 1\}.
        \end{equation}
        Note that \(\specialOrthogonal(N) \subgroup \orthogonal(N)\) and \(\specialOrthogonal(N) \subgroup \specialLinear(N, \reals)\).
    \end{dfn}
    
    The special orthogonal group is a group for exactly the same reasons that \(\orthogonal(N)\) and \(\specialLinear(N, \field)\) are.
    
    \begin{dfn}{Unitary Group}{}
        The \defineindex{unitary group}, \(\unitary(N)\)\index{U(n)@\(U(n)\)|see{unitary group}}, is the group preserving the standard inner product on a complex vector space:
        \begin{equation}
            \innerproduct{x}{y} = \sum_i x_i^* y_i.
        \end{equation}
        \begin{wrn}
            We follow the physics convention that an inner product is conjugate linear in its first argument, mathematicians often define it to be conjugate linear in the second instead.
        \end{wrn}
        The unitary group consists of all \(N \times N\) unitary matrices over \(\complex\):
        \begin{equation}
            \unitary(N) \coloneqq \{U \in \matrices{N}{\complex} \mid U^\hermit U = UU^\hermit = \ident\} \subgroup \generalLinear(N, \complex).
        \end{equation}
    \end{dfn}
    
    Note that \(U^\hermit U = \ident\) can be written as \(U^\hermit \ident U = \ident\), with the \(\ident\) on the left understood as the matrix form of the metric on this complex vector space, so the metric is invariant under the action of the unitary group
    \begin{equation}
        \delta_{\mu\nu} \xrightarrow{D \in \unitary(N)} \delta_{\alpha\beta} (\tensor{D}{^\alpha_\mu})^* \tensor{D}{^\beta_\nu} = \delta_{\mu\nu}.
    \end{equation}
    
    The same logic used to show the orthogonal group is a group works for the unitary group if we just replace transposes with Hermitian conjugates.
    
    \begin{dfn}{Special Unitary Group}{}
        The \defineindex{special unitary group}, \(\specialUnitary(N)\)\index{SU(n)@\(\specialUnitary(n)\)|see{special unitary group}}, is the subgroup of the unitary group given by restricting to \(U \in \unitary(N)\) with \(\det U = 1\):
        \begin{equation}
            \specialUnitary(N) \coloneqq \{U \in \unitary(N) \mid \det U = 1\}.
        \end{equation}
        Note that \(\specialUnitary(N) \subgroup \unitary(N)\) and \(\specialUnitary(N) \subgroup \specialLinear(N, \complex)\).
    \end{dfn}
    
    The special unitary group is a group for exactly the same reasons that \(\unitary(N)\) and \(\specialLinear(N, \field)\) are.
    
    \begin{exm}{\(\specialUnitary(2)\)}{exm:SU(2) = three sphere}
        Consider \(\specialUnitary(2)\).
        Start with some \(2\times 2\) matrix over \(\complex\),
        \begin{align}
            U = 
            \begin{pmatrix}
                a & b\\ c & d
            \end{pmatrix}
            .
        \end{align}
        The requirement that \(U^\hermit U = \ident\) tells us that
        \begin{equation}
            U^{-1} = U^{\hermit} \implies 
            \begin{pmatrix}
                d & -b\\
                -c & a
            \end{pmatrix}
            =
            \begin{pmatrix}
                a^* & c^*\\
                b^* & d^*
            \end{pmatrix}
        \end{equation}
        so \(a = d^*\) and \(c^* = -b\).
        Now add in the requirement that \(\det U = 1\) and we have
        \begin{equation}
            ad - bc = 1 \implies aa^* + bb^* = 1.
        \end{equation}
        Now write \(a = \alpha_1 + i \alpha_2\) and \(b = \alpha_3 + i\alpha_4\) for \(\alpha_i \in \reals\).
        Then we have
        \begin{equation}
            \alpha_1^2 + \alpha_2^2 + \alpha_3^2 + \alpha_4^2 = 1.
        \end{equation}
        This defines the three sphere,
        \begin{equation}
            \sphere[3] \coloneqq \{\vv{x} \in \reals^4 \mid x_1^2 + x_2^2 + x_3^2 + x_4^2 = \norm{\vv{x}}^2 = 1\}.
        \end{equation}
        This is a three-dimensional real manifold which is the parameter space for \(\specialUnitary(2)\).
        Note that \(\sphere[3]\) is simply connected, that is any loop can be contracted to a point.
    \end{exm}
    
    \begin{dfn}{Pseudo-Orthogonal Group}{}
        The \defineindex{pseudo-orthogonal group}, \(\orthogonal(n, m)\)\index{O(n, m)@\(\orthogonal(n, m)\)|see{pseudo-orthogonal group}}, is the group preserving the metric with signature \((n, m)\).
    \end{dfn}
    
    \begin{exm}{Lorentz Group}{}
        The \defineindex{proper Lorentz group}, \(\specialOrthogonal(3, 1)\), is the group of Lorentz transformations, \(\Lambda\), preserving the Minkowski metric:
        \begin{equation}
            \Lambda^\trans \minkowskiMetric \Lambda = \minkowskiMetric \iff \tensor{\Lambda}{_\mu^\rho} \minkowskiMetric_{\rho\sigma} \tensor{\Lambda}{^\sigma_\nu} = \minkowskiMetric_{\mu\nu},
        \end{equation}
        and with \(\det \Lambda = 1\).
        
        Note that we can also consider the general \defineindex{Lorentz group} \(\orthogonal(3, 1)\)\index{O(3,1)@\(\orthogonal(3, 1)\)|see{Lorentz group}}, which allows transformations inverting spacetime with \(\det \Lambda \ne 1\), and the \defineindex{proper orthochronous Lorentz group}, \(\specialOrthogonal^+(3, 1)\)\index{SO+(3,1)@\(\specialOrthogonal^+(3, 1)\)|see{proper orthochronous Lorentz group}}, which has \(\tensor{\Lambda}{^0_0} \ge 1\).
    \end{exm}
    
    \begin{dfn}{Pseudo-Unitary Group}{}
        The \defineindex{pseudo-unitary group} is the group preserving an indefinite metric on a complex vector space.
    \end{dfn}
    
    \begin{dfn}{Symplectic Groups}{}
        The \defineindex{symplectic group} \(\symplectic(2N, \field)\)\index{Sp(2n, k)@\(\symplectic(2n, \field)\)|see{symplectic group}}, preserves an antisymmetric metric given by the block diagonal matrix
        \begin{equation}
            g = 
            \begin{pmatrix}
                0 & \ident_N\\
                -\ident_N & 0
            \end{pmatrix}
        \end{equation}
        on a \(2N\)-dimensional vector space over \(\field\).
        
        The \defineindex{compact symplectic group}, \(\symplectic(2N)\)\index{Sp(2n)@\(\symplectic(2n)\)|see{compact symplectic group}}, is \(\symplectic(2N) = \symplectic(2N, \complex) \cap \unitary(2N)\).
        We can think of it as being the result of taking matrices in \(\symplectic(2N), \complex\) and replacing complex numbers with \(2\times 2\) real matrices according to
        \begin{equation}
            x + iy \mapsto 
            \begin{pmatrix}
                x & y\\
                -y & x
            \end{pmatrix}
            .
        \end{equation}
    \end{dfn}
    
    The symplectic group arises in physics when we consider phase space.
    In classical mechanics in three spatial dimensions phase space is the six dimensional space spanned by the three components of position and three components of momentum, so a point in phase space is \((q_1, q_2, q_3, p_1, p_2, p_3)\).
    Symplectic transformations, \(\symplectic(6, \reals)\), are the set of transformations preserving Hamilton's equations:
    \begin{equation}
        \dot{q}_i = \diff{H}{p_i}, \qquad \dot{p}_i = -\diff{H}{q_i}.
    \end{equation}
    
    With some restrictions most Lie groups fit into one of the previously mentioned categories.
    However, there are five Lie groups that don't, these are called the \define{exceptional groups}\index{exceptional group}.
    There is no particularly simple definition of any of them, we just note here that they exist, and are called \(F_4\), \(G_2\), \(E_6\), \(E_7\), and \(E_8\).
    These names come from the classification of semisimple Lie algebras (to be defined later), where we call the Lie algebras of \(\specialLinear(n + 1, \complex)\), \(\specialOrthogonal(2n + 1)\), \(\symplectic(2n)\), and \(\specialOrthogonal(2n)\) by the names \(A_n\), \(B_n\), \(C_n\), and \(D_n\) respectively.
    Here \(n\) is the rank of the Lie algebra (to be defined later).
    
    \section{One Dimensional Groups}
    In this section we will discuss one-dimensional groups, these are groups parametrised by a single value.
    \begin{exm}{}{}
        \begin{itemize}
            \item Translations along \(\reals\) form a one-dimensional Lie group parametrised by the size of the translation, \(a \in \reals\), with the action \(x \mapsto x + a\).
            This group is just \(\reals\).
            This example is noncompact.
            \item Rotations about some fixed axis form a one-dimensional Lie group parametrised by the size of the rotation, \(\vartheta \in [0, 2\pi)\), with the action
            \begin{equation}
                \begin{pmatrix}
                    x\\ y
                \end{pmatrix}
                \mapsto
                \begin{pmatrix}
                    \cos\vartheta & \sin\vartheta\\
                    -\sin\vartheta & \cos\vartheta
                \end{pmatrix}
                \begin{pmatrix}
                    x\\ y
                \end{pmatrix}
                .
            \end{equation}
            This group is \(\specialOrthogonal(2)\).
            This example is compact.
            \item Multiplication by a phase factor forms a one-dimensional Lie group parametrised by the argument of the phase factor, \(\varphi \in [0, 2\pi)\), with the action \(z \mapsto \e^{i\varphi}z\).
            This group is \(\unitary(1)\).
            This example is compact.
        \end{itemize}
        These last two examples are actually isomorphic, using \(\e^{i\varphi} = \cos \varphi + i\sin \varphi\) we can map from \(\unitary(1)\) to \(\specialOrthogonal(2)\) with
        \begin{equation}
            \e^{i\varphi} \mapsto 
            \begin{pmatrix}
                \cos\varphi & \sin\varphi\\
                -\sin\varphi & \cos\varphi
            \end{pmatrix}
            .
        \end{equation}
        So, \(\unitary(1) \isomorphic \specialOrthogonal(2)\).
    \end{exm}
    
    One-dimensional Lie groups are, unsurprisingly, some of the simplest Lie groups.
    In fact, they're so simple that they're not that interesting, a theory we'll develop rigorously through a collection of theorems.
    
    \begin{thm}{}{thm:one dim lie group parameters add}
        All one-dimensional Lie groups can be parametrised so that
        \begin{equation}
            g(a) g(b) = g(a + b)
        \end{equation}
        for all \(a\) and \(b\).
        \begin{proof}
            Consider a one-dimensional Lie group, \(G\), parametrised by some value in the interval \(I\).
            Associativity tells us that
            \begin{equation}
                g(x) [g(y)g(z)] = [g(x)g(y)]g(z)
            \end{equation}
            for all \(x, y, z \in I\).
            We can write \(g(y)g(z)\) and \(g(x)g(y)\) as single group elements \(g(\varphi(y, z))\) and \(g(\varphi(x, y))\) for some analytic function \(\varphi \colon I^2 \to I\), this is just the definition of a Lie group.
            Doing so we have
            \begin{equation}
                g(x) g(\varphi(y, z)) = g(\varphi(x, y))g(z).
            \end{equation}
            Now we can use analyticity again to write \(g(x)g(\varphi(y, z)) = g(x)g(a) = g(\varphi(x, a)) = g(\varphi(x, \varphi(y, z)))\) and \(g(\varphi(x, y))g(z) = g(\varphi(\varphi(x, y), z))\):
            \begin{equation}
                g(\varphi(x, \varphi(y, z))) = g(\varphi(\varphi(x, y), z)).
            \end{equation}
            Hence, we must have
            \begin{equation}
                \varphi(x, \varphi(y, z)) = \varphi(\varphi(x, y), z).
            \end{equation}
            
            Now, take the derivative of this expression with respect to \(z\), the right hand side gives
            \begin{equation}
                \diffp{}{z}\varphi(\varphi(x, y), z),
            \end{equation}
            and the chain rule applied to the left hand side gives
            \begin{equation}
                \diffp{}{z} \varphi(x, \varphi(y, z)) = \diffp{\varphi(x, \varphi(y, z))}{{\varphi(y, z)}} \diffp{\varphi(y, z)}{z}.
            \end{equation}
            
            We are free to shift our parametrisation interval around, so imagine we choose it to contain zero and choose a parametrisation such that \(g(0) = 1\).
            Consider the case where \(z = 0\).
            Then \(g(y)g(z) = g(y)g(0) = g(y)1 = g(y)\), but we also have \(g(y)g(z) = g(\varphi(y, z)) = g(\varphi(y, 0))\), so we must have \(\varphi(y, 0) = y\).
            Thus, evaluating are derivatives at \(z = 0\), we have
            \begin{equation}
                \diffp{\varphi(x, y)}{y} \psi(y) = \psi(\varphi(x, y))
            \end{equation}
            where
            \begin{equation}
                \psi(y) \coloneqq \diffp{\varphi(y, z)}{z} \bigg|_{z = 0}.
            \end{equation}
            This differential equation can be solved by writing it as
            \begin{equation}
                \frac{1}{\psi(\varphi(x, y))} \diffp{\varphi(x, y)}{y} = \frac{1}{\psi(y)}.
            \end{equation}
            We can then integrate with respect to \(y\) and we get
            \begin{equation}
                \rho(\varphi(x, y)) = \rho(y) + c(x)
            \end{equation}
            where \(c\) is an arbitrary function of \(x\), taking the role of our integration constant, but not constant as \(x\) is allowed to vary, and 
            \begin{equation}
                \rho(x) \coloneqq \int_0^x \frac{1}{\psi(t)} \dd{t}
            \end{equation}
            with \(\rho(0) = 0\).
            
            We can determine \(c(x)\) by choosing \(y = 0\), which gives \(\rho(\varphi(x, 0)) = \rho(x) = \rho(0) + c(x) = 0 + c(x)\), so \(c(x) = \rho(x)\).
            Hence,
            \begin{equation}
                \rho(\varphi(x, y)) = \rho(x) + \rho(y).
            \end{equation}
            
            Now we just reparametrise our group from \(g(x)\) to \(\overbar{g}(\rho(x))\), we then have
            \begin{equation}
                \overbar{g}(\rho(x))\overbar{g}(\rho(y)) = g(x)g(y) = g(\varphi(x, y)) = \overbar{g}(\rho(\varphi(x, y))) = \overbar{g}(\rho(x) + \rho(y)).
            \end{equation}
            Hence, our group operation becomes addition in parameter space and we are finished.
        \end{proof}
    \end{thm}
    
    \begin{crl}{}{}
        All one dimensional Lie algebras are Abelian.
        \begin{proof}
            Let \(G\) be a one-dimensional Lie group parametrised such that \(g(a)g(b) = g(a + b)\) for all \(a\) and \(b\).
            Then
            \begin{equation}
                g(a)g(b) = g(a + b) = g(b + a) = g(b)g(a),
            \end{equation}
            and so \(G\) is Abelian.
        \end{proof}
    \end{crl}
    
    The following theorem won't be proved here, but intuitively all it says is that every compact connected Abelian Lie group can be parametrised by phase factors.
    \begin{thm}{}{}
        All compact connected Abelian Lie groups are isomorphic to
        \begin{equation}
            \bigotimes_{i = 1}^{n} \unitary(1) = \underbrace{\unitary(1) \otimes \dotsb \otimes \unitary(1)}
        \end{equation}
        for some \(n \in \naturals\).
    \end{thm}
    Identifying each copy of \(\unitary(1)\) with a circle we see that every compact connected Abelian Lie group is a product of circles, which is a Torus.
    
    This means that in most of the course we'll be interested in non-Abelian Lie groups.
    
    \chapter{Representations}
    \section{What is a Representation}
    Intuitively a representation of a group, \(G\), is a set of \(N \times N\) matrices, \(D\), parametrised by group elements, \(g\), such that matrix multiplication is compatible with the group operation:
    \begin{equation}
        D(g) D(h) = D(gh) \iff \tensor{D}{^\mu_\nu}(g) \tensor{D}{^\nu_\rho}(h) = \tensor{D}{^\mu_\rho}(g h)
    \end{equation}
    for all \(g, h \in G\).
    
    \begin{exm}{\(\symmetricGroup[3]\)}{}
        The symmetric group on 3 objects has a representation, called the \defineindex{permutation representation}, given by the matrices
        \begin{alignat*}{3}
            \cycle{} &\mapsto 
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & 1 & 0\\
                0 & 0 & 1
            \end{pmatrix}
            , \quad & \cycle{1,2} &\mapsto
            \begin{pmatrix}
                0 & 1 & 0\\
                1 & 0 & 0\\
                0 & 0 & 1
            \end{pmatrix}
            , \quad & \cycle{1,3} &\mapsto
            \begin{pmatrix}
                0 & 0 & 1\\
                0 & 1 & 0\\
                1 & 0 & 0
            \end{pmatrix}
            \\
            \cycle{2,3} &\mapsto 
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & 0 & 1\\
                0 & 1 & 0
            \end{pmatrix}
            , \quad & \cycle{1,2,3} &\mapsto
            \begin{pmatrix}
                0 & 1 & 0\\
                0 & 0 & 1\\
                1 & 0 & 0
            \end{pmatrix}
            , \quad & \cycle{3,2,1} &\mapsto
            \begin{pmatrix}
                0 & 0 & 1\\
                1 & 0 & 0\\
                0 & 1 & 0
            \end{pmatrix}
            .
        \end{alignat*}
        To motivate this suppose that the three objects are the basis vectors
        \begin{equation}
            e_1 = 
            \begin{pmatrix}
                1\\0\\0
            \end{pmatrix}
            , \quad e_2 =
            \begin{pmatrix}
                0\\1\\0
            \end{pmatrix}
            , \qand e_3 = 
            \begin{pmatrix}
                0\\0\\1
            \end{pmatrix}
            .
        \end{equation}
        Then this representation acts by permuting the vectors in the obvious way.
        For example, the matrix representing \(\cycle{1,2}\) sends \(e_1\) to \(e_2\), \(e_2\) to \(e_1\), and \(e_3\) to itself.
        
        Another representation of \(\symmetricGroup[3]\) is given by the \defineindex{trivial representation}, which represents all group elements with the \(1\times 1\) zero matrix, \((0)\), sending all vectors to the zero vector.
        
        A third representation of \(\symmetricGroup[3]\) is given by sending each element either to \((1)\) or \((-1)\), depending on the sign of the permutation.
    \end{exm}
    
    There are two ways to formalise the notion of a representation, they are as follows.
    \begin{dfn}{Representation}{}
        Let \(G\) be a group.
        Then a group \define{representation}\index{representation!of a group}, \((D, V)\), is a vector space, \(V\), and a homomorphism, \(D \colon G \to \generalLinear(V)\).
    \end{dfn}
    
    \begin{dfn}{Representation}{}
        Let \(G\) be a group and \(V\) a vector space.
        A group \define{representation}, \((D, V)\), is a vector space, \(V\), and a group action, \(G \times V \to V\) given by \(g \action v = D(g)v\) where \(D(g)\) is some linear transformation.
    \end{dfn}
    
    \begin{ntn}{}{}
        We're generally pretty loose with the exact language as to what objects make up a representation.
        People will refer to \((D, V)\), \(D\), \(V\), and the set of matrices \(\{D(g) \mid g \in G\}\) as the representation interchangeably, with context telling us which we care about.
    \end{ntn}
    
    \begin{dfn}{Representation Dimension}{}
        The dimension of a representation is the dimension of the vector space, \(V\), which is also the number of rows in the matrix.
    \end{dfn}
    
    \begin{wrn}
        The dimension of the representation is, in general, \emph{not} the same as the dimension of a continuous group.
    \end{wrn}
    
    \begin{exm}{\(\symmetricGroup[3]\)}{}
        The permutation representation is of dimension 3, the trivial representation is of dimension 1, as is the sign representation.
    \end{exm}
    
    \section{Properties of Representations}
    \begin{exm}{Faithful Representation}{}
        A representation is faithful if \(\rho\) is injective.
    \end{exm}
    
    \begin{exm}{\(\symmetricGroup[3]\)}{}
        The permutation representation is faithful, the trivial and sign permutations are not.
    \end{exm}
    
    Representations give us another way to think about the matrix groups of the last chapter.
    Instead of defining them as matrices we can define them by symmetries, so, for example, \(\orthogonal(N)\) is the group preserving the Euclidean metric.
    Then the usual interpretation as \(N \times N\) orthogonal matrices is just a representation.
    We call it the \defineindex{defining representation}, since it can be used to define the representation.
    The defining representation must be an isomorphism in order for it to contain all group elements exactly once.
    
    The \defineindex{fundamental representation} is one from which all other all other representations can be built from tensor products.
    
    \begin{dfn}{Equivalence of Representations}{}
        Two representations are \define{equivalent}\index{equivalent representations} if they are related by a similarity transformation.
        That is, if \(G\) is a group with representations \(D\) and \(D'\) then there exists some \(S\), independent of \(g\), such that for all \(G\)
        \begin{equation}
            D'(g) = SD(g)S^{-1}.
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Reducible}{}
        A representation, \((D, V)\), is \define{reducible}\index{reducible representation} if there exists some subspace \(U \subset V\) with \(U \ne V\) and \(U \ne \{0\}\) such that
        \begin{equation}
            D(g) u \in U \text{ for all } u \in U \text{ and } g \in G.
        \end{equation}
        If this is the case we call \(U\) an \defineindex{invariant subspace}.
        A representation is \define{irreducible}\index{irreducible representation} if it is not reducible.
    \end{dfn}
    
    \begin{ntn}{}{}
        People often shorten \enquote{irreducible representation} to \define{irrep}\index{irrep|see{irreducible representation}}.
    \end{ntn}
    
    Notice that if \(D\) is a representation of \(G\) then so is \(D^*\), since
    \begin{equation}
        D(g)^*D(h)^* = (D(g)D(h))^* = D(gh)^*
    \end{equation}
    for all \(g, h \in G\).
    
    \begin{dfn}{}{}
        Let \(D\) be a representation of some group \(G\).
        
        If \(D(g)^* = D(g)\) for all \(g \in G\) then we say that \(D\) is a \defineindex{real representation}.
        
        If \(D\) is not equivalent to \(D^*\) then we say that \(D\) is a \defineindex{complex representation}.
        
        If \(D \ne D^*\) but \(D\) is equivalent to \(D^*\) then we say that \(D\) is a \defineindex{pseudo-real representation}.
    \end{dfn}
    
    Suppose that we have some symmetry described by the group \(G\).
    Then we classify different types of objects based on which representation they transform under.
    If
    \begin{equation}
        s \mapsto s,
    \end{equation}
    that is there is no transformation, then \(s\) transforms under the trivial representation and we call \(s\) a \defineindex{scalar}.
    If
    \begin{equation}
        v^\mu \mapsto \tensor{D}{^\mu_\nu}(g)v^\nu,
    \end{equation}
    where \(D\) is the fundamental representation we say that \(v\) is a \defineindex{vector}.
    If
    \begin{equation}
        T^{\mu_1\mu_2\dots \mu_n} \mapsto \tensor{D}{^{\mu_1}_{\nu_1}}(g) \tensor{D}{^{\mu_2}_{\nu_2}}(g) \dotsm \tensor{D}{^{\mu_n}_{\nu_n}}(g) T^{\nu_1\nu_2 \dots \nu_n}
    \end{equation}
    then we say that \(T\) is a \defineindex{tensor}, note that both scalars and vectors are special cases of tensors.
    
    We can write this last line more succinctly as
    \begin{equation}
        T^\alpha = \tensor{D}{^\alpha_\beta}(g) T^\beta.
    \end{equation}
    We understand this as \(T\) transforming under the tensor product
    \begin{equation}
        \tensor{(D(g) \otimes D(g) \otimes \dotsb \otimes D(g))}{^{\mu_1\mu_2 \dots \mu_n}}_{\nu_1\nu_2\dots \nu_n} = \tensor{D}{^{\mu_1}_{\nu_1}}(g) \tensor{D}{^{\mu_2}_{\nu_2}}(g).
    \end{equation}
    
    \section{Unitary Representations}
    
    \begin{dfn}{Unitary Representation}{}
        Let \(G\) be a group and \((D, V)\) a representation of \(G\).
        We say that \(D\) is a \defineindex{unitary representation} if it preserves the inner product on \(V\), that is if
        \begin{equation}
            \innerproduct{u}{v} = \innerproduct{D(g)u}{D(g)v}
        \end{equation}
        for all \(g, h \in G\).
    \end{dfn}
    
    \begin{thm}{Maschke's Theorem}{thm:maschke}
        Any representation of a finite group is equivalent to a unitary representation.
        \begin{proof}
            Let \(G\) be a finite group, \(V\) a vector space, and \((D, V)\) a representation.
            Let \(\innerproduct{-}{-}\) be the inner product on \(V\).
            Define a new inner product,
            \begin{equation}
                \braket{x}{y} \coloneqq \sum_{g \in G} \innerproduct{D(g)x}{D(g)y}.
            \end{equation}
            Then we claim that \((D, V)\) is a unitary representation with respect to this new inner product and since different inner products are related by similarity transformations \((D, V)\) is equivalent to a unitary representation.
            
            Take some fixed \(h \in G\).
            The rearrangement theorem states that if the elements of the group are listed, \((g_1, \dotsc, g_n)\), then the action \((g_1, \dotsc, g_n) \mapsto (g_1h, \dotsc, g_nh)\) is just a permutation.
            In particular, if \(f \colon G \to \complex\) then
            \begin{equation}
                \sum_{g \in G} f(g) = \sum_{g\in G} f(gh),
            \end{equation}
            since multiplying each argument by \(h\) just permutes the terms.
            
            Using this we can equivalently write the inner product, \(\braket{-}{-}\), as
            \begin{align}
                \braket{x}{y} &= \sum_{g\in G} \innerproduct{D(gh)x}{D(gh)y}\\
                &= \sum_{g\in G} \innerproduct{D(g)D(h)x}{D(g)D(h)y}\\
                &= \sum_{g\in G} \innerproduct{D(g)u}{D(g)v}\\
                &= \braket{u}{v},
            \end{align}
            where \(u = D(h)x\) and \(v = D(h)y\).
            Then we have
            \begin{equation}
                \braket{x}{y} = \braket{u}{v} = \braket{D(h)x}{D(h)y} = \bra{x}D^\hermit(h)D(h)\ket{y},
            \end{equation}
            so for this to hold true for all \(x, y \in V\) we must have \(D^\hermit(h)D(h) = \ident\).
            Hence, \(D\) is unitary with respect to this new inner product.
        \end{proof}
    \end{thm}
    
    A very similar theorem holds for compact Lie groups.
    The proof is almost identical but the sum is replaced with an integral.
    We won't prove it here as it requires some measure theory to make everything work.
    It comes down to the compactness requirement ensuring convergence of the integrals at every step.
    Without this requirement the integrals may not converge, in which case there is no reason for an equivalent unitary representation to exist.
    \begin{thm}{}{}
        Every representation of a compact Lie group is equivalent to a unitary representation.
    \end{thm}
    
    Why do we care about unitary representations?
    Mostly for quantum mechanics.
    If a group action on a Hilbert space of states is to preserve probability then it must be either unitary, so \(\innerproduct{Ux}{Uy} = \innerproduct{x}{y}\), or \defineindex{antiunitary}, that is \(\innerproduct{Ux}{Uy} = \innerproduct{x}{y}^*\).
    An example of an antiunitary transformation is time reversal.
    Consider a system evolving from state \(\ket{\psi}\) to \(\ket{\varphi}\).
    The amplitude for this is \(\braket{\psi}{\varphi}\).
    The time reversed system evolves from state \(\ket{\varphi}\) to state \(\ket{\psi}\).
    The amplitude for this is \(\braket{\varphi}{\psi} = \braket{\psi}{\varphi}^*\).
    
    \part{Lie Algebras}
    \chapter{Lie Algebras}
    There are, broadly, two approaches to Lie algebras.
    One can derive their properties and then abstract, or abstract and then show that there is a connection to Lie groups.
    The first is how a typical physics course goes about it, and fits the historical process of discovering Lie algebras, but I prefer to abstract and then look at applications where possible, so that's what these notes will do.
    
    We'll define Lie algebras as abstract objects, and briefly discuss some properties which follow immediately.
    Then we'll discuss the link to Lie groups, and then go into more detail about the properties of Lie algebras.
    
    \section{What is an Algebra?}
    An algebra is a vector space equipped with a product compatible with the vector space structure.
    Formally, the definition is as follows.
    
    \begin{dfn}{Algebra}{}
        Let \(\field\) be a field and \(A\) a vector space over \(\field\).
        Equip \(A\) with a binary operation \(A \times A \to A\), denoted by juxtaposition here.
        If this product has the following properties then \(A\) is an \defineindex{algebra}:
        \begin{itemize}
            \item Left distributivity: \(x(y + z) = xy + xz\) for all \(x, y, z \in A\);
            \item Right distributivity: \((x + y)z = xz + yz\) for all \(x, y, z \in A\);
            \item Compatibility with scalar multiplication: \((\alpha x)(\beta y) = (\alpha\beta)(xy)\) for all \(\alpha, \beta \in \field\) and \(x, y \in A\).
        \end{itemize}
        We call \(A\) an \defineindex{associative algebra} if the product is associative.
    \end{dfn}
    
    \begin{exm}{Algebra}{}
        \begin{itemize}
            \item The real numbers are a vector space over themselves, and also an algebra over themselves with the product being normal multiplication.
            \item The complex numbers are a vector space over themselves, and also an algebra over themselves with the product being normal multiplication.
            \item The complex numbers are a vector space over \(\reals\), and also an algebra over \(\reals\) with the product being normal complex multiplication when we think of complex numbers as ordered pairs of real numbers.
            \item The vector space \(\reals^3\) when equipped with the cross product forms an algebra over \(\reals\).
            \item The vector space \(\reals^4\) when identified with the quaternions forms an algebra over \(\reals\) with the product being the quaternion product.
            \item Polynomials over \(\reals\), that is \(\reals[x]\), form a vector space over \(\reals\) and when equipped with polynomial multiplication this is an algebra.
            \item The set of \(m \times n\) matrices over \(\reals\) forms an \(mn\)-dimensional vector space.
            This forms an algebra over \(\reals\) when equipped with the normal matrix product.
            \item The set of \(m \times n\) matrices over \(\reals\) forms an \(mn\)-dimensional vector space.
            This forms an algebra over \(\reals\) when equipped with the \defineindex{commutator}, \(\commutator{A}{B} \coloneqq AB - BA\).
        \end{itemize}
    \end{exm}
    
    \section{What is a Lie Algebra}
    A Lie algebra is an algebra where the product is given by a Lie bracket, which is just a particular form of product with a couple of properties which we write as a bracket, so \(\commutator{a}{b}\) instead of \(ab\).
    
    \begin{dfn}{Lie Algebra}{}
        A \defineindex{Lie algebra}, \(\lie{g}\), is a vector space over some field, \(\field\), equipped with a binary operation, \(\commutator{-}{-} \colon \lie{g} \times \lie{g} \to \lie{g}\), satisfying the following:
        \begin{itemize}
            \item \define{Bilinearity}\index{bilinear}: for all \(\alpha, \beta \in \field\) and \(x, y, z \in \lie{g}\) we have
            \begin{equation*}
                \commutator{\alpha x + \beta y}{z} = \alpha\commutator{x}{z} + \beta \commutator{y}{z}, \qand \commutator{z}{\alpha x + \beta y} = \alpha\commutator{z}{x} + \beta \commutator{z}{y}.
            \end{equation*}
            \item \define{Alternativity}\index{alternating}: for all \(x \in \lie{g}\) we have
            \begin{equation}
                \commutator{x}{x} = 0,
            \end{equation}
            where \(0\) is the zero vector.
            \item The \defineindex{Jacobi identity}: for all \(x, y, z \in \lie{g}\) we have
            \begin{equation}
                \commutator{x}{\commutator{y}{z}} + \commutator{y}{\commutator{z}{x}} + \commutator{z}{\commutator{x}{y}} = 0.
            \end{equation}
            Note that this is just \(\commutator{x}{\commutator{y}{z}}\) plus cyclic permutations.
        \end{itemize}
    \end{dfn}
    \begin{ntn}{}{}
        Lie algebras are, unsurprisingly, related to Lie groups.
        If we have a Lie group denoted with capital letters, such as \(G\), \(\generalLinear\), \(\specialOrthogonal\), \(\unitary\), and so on, then we denote the associated Lie algebra by the same letter but lowercase and in the Fraktur script, so \(\lie{g}\), \(\generalLinearLie\), \(\specialOrthogonalLie\), \(\unitaryLie\), and so on.
        For future reference here is the alphabet, in order, in Fraktur:
        \begin{equation}
            \lie{a \, b \, c \, d \, e \, f \, g \, h \, i \, j \, k \, l \, m \, n \, o \, p \, q \, r \, s \, t \, u \, v \, w \, x \, y \, z}.
        \end{equation}
        It is also common to use lowercase, non-Fraktur letters for Lie algebras, such as \(g\), \(\symrm{gl}\), \(\symrm{so}\), \(\symrm{u}\), and so on.
    \end{ntn}
    
    Note that some sources define the Lie bracket to be anticommutative, instead of alternating.
    That is, \(\commutator{x}{y} = -\commutator{y}{x}\) for all \(x, y \in \lie{g}\).
    This is necessarily the case for an alternating Lie bracket:
    \begin{multline}
        0 = \commutator{x + y}{x + y} = \commutator{x}{x + y} + \commutator{y}{x + y}\\
        = \commutator{x}{x} + \commutator{x}{y} + \commutator{y}{x} + \commutator{y}{y} = \commutator{x}{y} + \commutator{y}{x},
    \end{multline}
    and so \(\commutator{x}{y} = -\commutator{y}{x}\).
    However, the reverse implication, that an anticommutative product is alternating, only holds if the characteristic\footnote{the \defineindex{characteristic} of a ring, and hence field, is the number of times you have to add 1 to itself to get 0, or 0 if you never get 0 this way.} of the field is not 2, so our definition is \emph{slightly} more general.
    However, we'll almost entirely work over \(\reals\) and \(\complex\) which have characteristic 0, so it's not an important distinction.
    So, suppose that we work over a field with characteristic not equal to 2, then \(\commutator{x}{x} = -\commutator{x}{x}\) by anticommutativity (in the second bracket we swapped the \(x\)s, you just can't tell).
    Hence, we have
    \begin{equation}
        2 \commutator{x}{x} = \commutator{x}{x} + \commutator{x}{x} = \commutator{x}{x} - \commutator{x}{x} = 0,
    \end{equation}
    and so long as we can divide by 2 we have \(\commutator{x}{x} = 0\).
    It is this dividing by 2 step that fails in a field with characteristic 2.
    
    The Jacobi identity is a bit weird as a requirement.
    Define \(D_x(y) = \commutator{x}{y}\), then we can write the Jacobi identity as
    \begin{equation}
        D_x(\commutator{y}{z}) = \commutator{D_x(y)}{z} + \commutator{y}{D_x(z)},
    \end{equation}
    which looks a lot like the product rule.
    Going back to writing Lie brackets this is
    \begin{equation}
        \commutator{x}{\commutator{y}{z}} = \commutator{\commutator{x}{y}}{z} + \commutator{y}{\commutator{x}{z}}.
    \end{equation}
    Using the anticommutativity of the Lie bracket (in fields of characteristic other than 2, an implicit assumption from now on) and the bilinearity we can rewrite
    \begin{equation}
        \commutator{\commutator{x}{y}}{z} = -\commutator{z}{\commutator{x}{y}}
    \end{equation}
    and
    \begin{equation}
        \commutator{y}{\commutator{x}{z}} = \commutator{y}{-\commutator{z}{x}} = -\commutator{y}{\commutator{z}{x}}.
    \end{equation}
    Hence,
    \begin{equation}
        \commutator{x}{\commutator{y}{z}} = -\commutator{z}{\commutator{x}{y}} - \commutator{y}{\commutator{z}{x}},
    \end{equation}
    which is just the Jacobi identity with some terms moved to the other side.
    
    \begin{exm}{Lie Algebra}{}
        \begin{itemize}
            \item Consider the set of \(m \times n\) matrices over \(\reals\).
            This forms a Lie algebra when equipped with the commutator, \(\commutator{A}{B} = AB - BA\).
            \item Consider \(\reals^3\).
            This forms a Lie algebra when equipped with the cross product, \(\commutator{\vv{v}}{\vv{u}} = \vv{v} \times \vv{u}\).
        \end{itemize}
    \end{exm}
    
    \section{Subalgebras}
    As with groups we can often find Lie algebras hiding inside other Lie algebras, and certain such Lie algebras are special.
    \begin{dfn}{Lie Subalgebra and Ideals}{}
        Let \(\lie{g}\) be a Lie algebra and \(\lie{h}\) a subspace of \(\lie{g}\) (as vector spaces).
        Then \(\lie{h}\) is a \defineindex{Lie subalgebra}, or simply a \defineindex{subalgebra}, of \(\lie{g}\) if \(\commutator{x}{y} \in \lie{h}\) for all \(x, y \in \lie{h}\), where \(\commutator{-}{-}\) is the Lie bracket of \(\lie{g}\).
        
        An \defineindex{ideal}, or an \defineindex{invariant subalgebra}, is a Lie subalgebra \(\lie{i}\) with the stronger condition that \(\commutator{g}{i} \in \lie{i}\) for all \(g \in \lie{g}\).
    \end{dfn}
    
    The definition of an ideal here is analogous to that of a normal subgroup.
    A normal subgroup is invariant under conjugation with any group element and an ideal is invariant under Lie brackets with any algebra element.
    Note that if \(\commutator{g}{i} \in \lie{i}\) then \(\commutator{i}{g} = -\commutator{g}{i} \in \lie{i}\) also as \(\lie{i}\) is a subspace of \(\lie{g}\).
    Similar to the group case a Lie algebra is \define{simple}\index{simple!Lie algebra} if it has no nontrivial ideals, that is the only ideals are itself and the zero dimensional vector space, and it is not Abelian.
    A Lie algebra is \define{Abelian}\index{Abelian!Lie algebra} if the Lie bracket vanishes, that is \(\commutator{x}{y} = 0\) for all \(x, y \in \lie{g}\).
    Note that this is equivalent to saying \(\commutator{x}{y} = \commutator{y}{x}\), the more usual notion of being Abelian, but anticommutativity means \(\commutator{y}{x} = -\commutator{x}{y}\), so we have \(\commutator{x}{y} = -\commutator{x}{y}\), and so \(\commutator{x}{y} = 0\).
    
    \section{Morphisms Between Lie Algebras}
    As often happens in maths after defining an object we should define maps between these objects.
    These maps should preserve the structure of the object, and if such a map is invertible then the structure, at least at the Lie algebra level, of the two objects is the same, and so, when thinking about Lie algebras, we treat the two objects as if they were identical.
    \begin{dfn}{Morphisms}{}
        Let \(\lie{g}\) and \(\lie{h}\) be Lie algebras.
        A \defineindex{Lie algebra homomorphism}\index{homomorphism!of Lie algebras} is a function \(\varphi \colon \lie{g} \to \lie{h}\) preserving the bracket, that is
        \begin{equation}
            \varphi(\commutator{x}{y}) = \commutator{\varphi(x)}{\varphi(y)}
        \end{equation}
        for all \(x, y \in \lie{g}\).
        Here the Lie bracket on the left is that of \(\lie{g}\), and on the right the Lie bracket is that of \(\lie{h}\).
        
        An \defineindex{Lie algebra isomorphism}\index{isomorphism!of Lie algebras} is an invertible Lie algebra homomorphism.
    \end{dfn}
    
    \section{Generators and Structure Constants}
    Lie algebras are vector spaces, and as such have lots of nice things, like bases, that we can use to make calculations easier.
    \begin{dfn}{Generators}{}
        Let \(\lie{g}\) be a Lie algebra.
        A set of elements of \(\lie{g}\) are said to \define{generate}\index{generator} \(\lie{g}\) if the smallest subalgebra containing these elements is \(\lie{g}\) itself.
        That is, if all elements of \(\lie{g}\) can be be generated from linear combinations of the generators, and Lie brackets of the generators, and Lie brackets of Lie brackets of the generators and so on.
        The generators are the basis for \(\lie{g}\) as a vector space.
    \end{dfn}
    
    \begin{ntn}{}{}
        Generators are commonly denoted \(T_a\), where \(a\) indexes the generating set.
    \end{ntn}
    
    Let \(\lie{g}\) be a Lie algebra with generators \(T_a\).
    Since A Lie algebra is a vector space, and any element of it can be expressed as a linear combination of basis vectors, which for a Lie algebra are the generators.
    In particular, the Lie bracket of two elements of the Lie algebra is another element of the Lie algebra, and so can be expressed in this way:
    \begin{equation}
        \commutator{x}{y} = f^a(x, y) T_a
    \end{equation}
    where \(f^a(x, y)\) is the coefficient of \(T_a\) in this expansion, and there is an implicit sum over \(a\).
    Naturally we can consider the case when \(x\) and \(y\) are themselves generators, and this leads to the following definition.
    \begin{dfn}{Structure Constants}{}
        Let \(\lie{g}\) be a Lie algebra with generators \(T_a\).
        Then the \defineindex{structure constant} \(\tensor{c}{^c_{ab}}\) is defined as the coefficient in one of the following:
        \begin{equation}
            \commutator{T_a}{T_b} = \tensor{c}{^c_{ab}}T_c, \qqor \commutator{T_a}{T_b} = \textcolor{highlight}{i}\tensor{c}{^c_{ab}}T_c,
        \end{equation}
        where there is an implicit sum over the index \(c\).
    \end{dfn}
    
    Which of these two statements defines the structure constants depends on convention.
    For Lie algebras formed from real matrices we usually choose the first, and for Lie algebras formed from complex matrices the second.
    If we include the factor of \(i\) here then there are other places we need to include it later, but it can make the generators slightly nicer for physics, such as making them Hermitian when they would otherwise have been anti-Hermitian.
    
    We can use the Jacobi identity to derive an equivalent identity for the structure constants.
    Start with the Jacobi identity applied to three generators:
    \begin{equation}
        0 = \commutator{T_a}{\commutator{T_b}{T_c}} + \commutator{T_b}{\commutator{T_c}{T_a}} + \commutator{T_c}{\commutator{T_a}{T_b}}.
    \end{equation}
    Replacing the inner Lie bracket with structure constants then applying linearity we have
    \begin{align}
        0 &= \commutator{T_a}{\tensor{c}{^d_{bc}}T_d} + \commutator{T_b}{\tensor{c}{^d_{ca}}T_d} + \commutator{T_c}{\tensor{c}{^d_{ab}}T_d}\\
        &= \tensor{c}{^d_{bc}}\commutator{T_a}{T_d} + \tensor{c}{^d_{ca}}\commutator{T_b}{T_d} + \tensor{c}{^d_{ab}}\commutator{T_c}{T_d}.
    \end{align}
    Now replace the Lie brackets with structure constants again:
    \begin{equation}
        0 = \tensor{c}{^d_{bc}}\tensor{c}{^e_{ad}}T_e + \tensor{c}{^d_{ca}}\tensor{c}{^e_{bd}}T_e + \tensor{c}{^d_{ab}}\tensor{c}{^e_{cd}}T_e.
    \end{equation}
    Requiring this to hold for initial choices of generators the coefficients here must vanish, and so
    \begin{equation}
        0 = \tensor{c}{^d_{bc}}\tensor{c}{^e_{ad}} + \tensor{c}{^d_{ca}}\tensor{c}{^e_{bd}} + \tensor{c}{^d_{ab}}\tensor{c}{^e_{cd}}.
    \end{equation}
    
    \begin{lma}{}{}
        Let \(\lie{g}\) be the Lie algebra generated by \(\{T_a\}\) with structure constants \(\tensor{c}{^c_{ab}}\).
        If \(T_a\) are Hermitian then the structure constants are real.
        \begin{proof}
            If \(\{T_a\}\) generates the Lie algebra then so does \(\{-T_a^*\}\).
            Taking the complex conjugate of the defining relation for the structure constants we have
            \begin{equation}
                \commutator{T_a}{T_b}^* = (i\tensor{c}{^c_{ab}}T_a)^* \implies \commutator{T_a^*}{T_b^*} = -i\tensor{c}{^c_{ab}}^*T_c^*.
            \end{equation}
            Linearity tells us that \(\commutator{A}{B} = \commutator{-A}{-B}\), and so
            \begin{equation}
                \commutator{-T_a^*}{T_b^*} = i\tensor{c}{^c_{ab}}^*(-T_c^*).
            \end{equation}
            The structure constants are independent of the choice of generators, so we must have \(\tensor{c}{^c_{ab}} = \tensor{c}{^c_{ab}}^*\), that is \(\tensor{c}{^c_{ab}} \in \reals\).
        \end{proof}
    \end{lma}

    \section{Representations}
    Representations of Lie algebras aren't that different to representations of groups.
    The idea is to find some set of matrices, \(D\), parametrised by the elements of the Lie algebra, say \(x, y \in \lie{g}\), such that the Lie bracket in \(\lie{g}\) corresponds to the commutator of these matrices, that is
    \begin{equation}
        D(\commutator{x}{y}) = D(x)D(y) - D(y)D(x) = \commutator{D(x)}{D(y)},
    \end{equation}
    where the first \(\commutator{-}{-}\) is an abstract Lie bracket, which may or may not take the form of a commutator, and the second \(\commutator{-}{-}\) is the normal commutator.
    The formal definition of a representation of a Lie algebra is as follows.
    
    \begin{dfn}{Representation}{}
        Let \(\lie{g}\) be a Lie algebra.
        Then a \define{Lie algebra representation}\index{representation!of a Lie algebra}, \((D, V)\), is a vector space, \(V\), and a homomorphism of Lie algebras
        \begin{equation}
            D \colon \lie{g} \to \generalLinearLie(V)
        \end{equation}
        where \(\generalLinearLie(V) = \End(V)\) is made into a Lie algebra by equipping it with the commutator as a Lie bracket.
    \end{dfn}
    
    Representations essentially equate to a choice of generators.
    We have already met one of the most important representations of any Lie group, the structure constants themselves.
    
    \begin{dfn}{Adjoint Representation}{}
        The \define{adjoint representation}\index{adjoint representation!of a Lie algebra} of a Lie algebra, \(\lie{g}\), is the representation on \(\lie{g}\) itself, given by choosing the generators to satisfy
        \begin{equation}
            \tensor{(T_a)}{^b_c} = i\tensor{c}{^b_{ac}}.
        \end{equation}
    \end{dfn}
    
    To show that this is indeed a representation we need to show that \(T_a\) defined in this way satisfy \(\commutator{T_a}{T_b} = i\tensor{c}{^c_{ab}}T_c\).
    We can use the antisymmetry of the structure constants to write the Jacobi identity as
    \begin{equation}
        \tensor{(T_a)}{^d_e} \tensor{(T_b)}{^e_c} - \tensor{(T_b)}{^d_e}\tensor{(T_a)}{^e_c} = i\tensor{(T_e)}{^d_c} \tensor{c}{^e_{ab}},
    \end{equation}
    which in matrix notation gives
    \begin{equation}
        \commutator{T_a}{T_b} = i\tensor{c}{^e_{ab}}T_e.
    \end{equation}
    
    \chapter{Lie Algebras and Lie Groups}
    In this chapter we discuss the relationship between a Lie group and the associated Lie algebra.
    Starting with how we can get a Lie algebra from a Lie group by linearisation, then we'll see some examples, then we'll look at how to get back to the Lie group (or close to it) from the Lie algebra.
    
    \section{Linearising}
    Let \(G\) be a Lie group.
    An important idea is that every point in a Lie group is essentially equivalent, since if we study a neighbourhood of \(g \in G\) then we can transform this into a neighbourhood of \(h \in G\) by multiplying on the left by \(hg^{-1}\).
    This suggests that we can learn about a Lie group by studying just a single neighbourhood, and if we have to select a single point in the group it makes sense to choose the identity.
    
    If we want to study the behaviour of something near a fixed point it makes sense to expand about this point, which we're allowed to do for Lie groups by the analyticity part of the definition.
    Doing so up to first order we \defineindex{linearise} the group.
    If the group elements are \(g(\alpha)\), where \(\alpha = \{\alpha^a\}\) is parametrising the group, then expanding about the identity, 1, we get
    \begin{equation}
        g(\alpha) = 1 + i\alpha^a T_a + \order(\alpha^2)
    \end{equation}
    where
    \begin{equation}
        T_a = -i \diffp{g(\alpha)}{\alpha^a}[\alpha = 0]
    \end{equation}
    and \(a\) runs from 1 to \(\dim G\).
    We call \(T_a\) the \define{generators}\index{generator}.
    \begin{wrn}
        The factor of \(i\) here in these definitions is convention.
        It allows us to work with Hermitian matrices rather than anti-Hermitian matrices.
        Not everyone follows this convention, and we won't include it if we're working with real matrices.
    \end{wrn}
    
    \section{Lie Bracket}
    In a group we can measure how Abelian the group is using the \defineindex{group commutator}, defined for \(g, h \in G\) to be the product \(f = ghg^{-1}h^{-1}\).
    Notice that this gives the identity if \(g\) and \(h\) commute.
    If \(G\) is a Lie group then we can expand each term to evaluate this:
    \begin{align}
        g &= 1 + i\alpha^aT_a + \frac{1}{2}(i\alpha^aT_a)^2 + \order(\alpha^3),\\
        h &= 1 + i\beta^aT_a + \frac{1}{2}(i\beta^aT_a)^2 + \order(\beta^3),\\
        f &= 1 + i\gamma^aT_a + \frac{1}{2}(i\gamma^aT_a)^2 + \order(\gamma^3).
    \end{align}
    The inverse of \(g\) can similarly be expanded as
    \begin{equation}
        g^{-1} = 1 - i\alpha^aT_a + \frac{1}{2}(i\alpha^aT_a)^2 + \order(\alpha^3),
    \end{equation}
    this can be checked by expanding \(gg^{-1}\) and showing that we get 1.
    Write \(\alpha\) for \(\alpha^aT_a\) and \(\beta\) for \(\beta^aT_a\), then we have
    \begin{align}
        ghg^{-1}h^{-1} &\approx \left( 1 + i\alpha + \frac{1}{2}(i\alpha)^2 \right)\left( 1 + i\beta + \frac{1}{2}(i\beta)^2 \right)\\
        &\qquad\left( 1 - i\alpha + \frac{1}{2}(i\alpha)^2 \right)\left( 1 - i\beta + \frac{1}{2}(i\beta)^2 \right)\\
        &= 1 + i\alpha + i\beta - i\alpha - i\beta\\
        &- \alpha\beta + \alpha\alpha + \alpha\beta + \beta\alpha + \beta\beta -\alpha\beta\\
        &+ \frac{1}{2}[(i\alpha)^2 + (i\beta)^2 + (i\alpha)^2 + (i\beta)^2] + \order(\alpha^3, \beta^3)\\
        &= -\alpha\beta + \beta\alpha + \order(\alpha^3, \beta^3).
    \end{align}
    That is,
    \begin{equation}
        f = -\alpha^aT_a \beta^bT_b + \beta^bT_b\alpha^aT_a + \order(\alpha^3, \beta^3) = -\commutator{\alpha^aT_a}{\beta^bT_b} + \order(\alpha^3, \beta^3)
    \end{equation}
    where \(\commutator{-}{-}\) is the usual commutator.
    
    From here we define the Lie algebra of the Lie group \(G\) as the Lie algebra, \(\lie{g}\), generated by
    \begin{equation}
        T_a = -i\diffp{g(\alpha)}{\alpha^a}[\alpha = 0]
    \end{equation}
    with the Lie bracket given by the commutator, which is such that
    \begin{equation}
        \commutator{T_a}{T_b} = i\tensor{c}{^c_{ab}}T_c
    \end{equation}
    
    \section{Exponential Map}
    Let \(G\) be a Lie group.
    Consider some one-dimensional Lie subgroup, \(G_1 \subgroup G\).
    We can think of \(G_1\) as a path on the manifold \(G\) parametrised by some parameter \(t\), so
    \begin{equation}
        G_1 = \{g(\alpha(t)) t \in \reals\},
    \end{equation}
    we then write \(g(t)\) for \(g(\alpha(t))\).
    
    Since this is a one-dimensional Lie group by \cref{thm:one dim lie group parameters add} we can choose the parametrisation to be such that \(g(t)g(s) = g(t + s)\) and \(g(0) = 1\).
    Now differentiate this with respect to \(s\) and then set \(s = 0\), we get
    \begin{equation}
        g(t)g'(0) = g'(t).
    \end{equation}
    A solution to this is
    \begin{equation}
        g(t) = \e^{g'(0)t}.
    \end{equation}
    Now, define \(g'(0) = i\gamma^aT_a\), since this is what we get if \(g(s) = 1 + i\gamma^aT_a + \order(\gamma^2)\) and we find that we can write an arbitrary element of \(G_1\) as
    \begin{equation}
        g(t) = \e^{it\gamma^aT_a}.
    \end{equation}
    
    This motivates the following theorem, which we won't prove.
    \begin{thm}{}{}
        Let \(G\) be a compact Lie group with Lie algebra \(\lie{g}\).
        Let \(g \in G\) be continuously connected to the identity.
        Then
        \begin{equation}
            g = \e^{i\gamma^aT_a}
        \end{equation}
        for some \(\gamma^a\) and \(T_a\) the generators of \(\lie{g}\).
    \end{thm}
    In words, if \(G\) is a compact Lie group then every element connected to the identity can be obtained by exponentiating the Lie algebra.
    
    A compact Lie group is a Riemannian manifold.
    The exponential map, \(t \mapsto \exp[it\gamma^aT_a]\), then gives geodesics through the origin on this manifold.
    Every point connected to the identity is then on one of these geodesics.
    Intuitively this makes sense because by definition such a point is connected to the identity and so by simply minimising the path connecting it we get a geodesic.
    
    \subsection{Noncompact Lie Groups}
    The compact requirement is important, for noncompact Lie groups the theorem does not hold, which we demonstrate now by example.
    The group \(\specialLinear(2, \reals)\) is noncompact, it is formed of elements of the form
    \begin{equation}
        \begin{pmatrix}
            a & b\\ c & d
        \end{pmatrix}
    \end{equation}
    with \(ab - cd = 1\).
    The values of \(a\), \(b\), \(c\), and \(d\) are unbounded, and so \(\specialLinear(2, \reals)\) is not compact.
    
    Suppose that all \(g \in \specialLinear(2, \reals)\) can be expressed as \(\exp[l]\) for some \(l \in \specialLinearLie(2, \reals)\).
    Then we can define a one-dimensional path in \(\specialLinear(2, \reals)\) as the image of the map \(t \mapsto \exp[tl]\) with \(t \in [0, 1]\).
    In particular, \(h = \exp[l/2]\) satisfies \(h^2 = g\), so we can talk of \enquote{square roots} of group elements.
    
    We now proceed to demonstrate that such an \(h\) does not exist for all choices of \(g\).
    Let
    \begin{equation}
        g = 
        \begin{pmatrix}
            -4 & 0\\
            0 & -1/4
        \end{pmatrix}
    \end{equation}
    Then suppose that 
    \begin{equation}
        h = 
        \begin{pmatrix}
            a & b\\ c & d
        \end{pmatrix}
        .
    \end{equation}
    We then have
    \begin{equation}
        h^2 = 
        \begin{pmatrix}
            a^2 + bc & b(a + d)\\
            c(a + d) & d^2 + bc
        \end{pmatrix}
        .
    \end{equation}
    Since \(g\) has zero off diagonal elements we have \(b(a + d) = c(a + d) = 0\), meaning either \(b\) and \(c\) are both zero or \(a + d = 0\).
    Suppose that \(b = c = 0\), then looking at the first entry in \(g\) we have \(a^2 + bc = a^2 = -4\), but this can't be the case as \(a\) is real.
    Suppose then that \(a + d = 0\), then \(a = -d\) and so \(a^2 = d^2\).
    The first entry in \(g\) gives us \(a^2 + bc = -4\), but the last gives \(d^2 + bc = a^2 + bc = -1/4\).
    It is not possible for \(a^2 + bc\) to have two separate values so we conclude that there is no way to satisfy the conditions and there is no \(h \in \specialLinear(2, \reals)\) satisfying \(h^2 = g\).
    
    Note that \(\specialLinear(2, \reals)\) is connected, so it's the lack of compactness that is causing the problems here.
    We can show that \(g\) is connected to the identity by considering the piecewise path first defined by
    \begin{equation}
        \begin{pmatrix}
            \cos\vartheta & -\sin\vartheta\\
            \sin\vartheta & \cos\vartheta
        \end{pmatrix}
    \end{equation}
    for \(\vartheta \in [0, \pi]\), which gets us from the identity to
    \begin{equation}
        \begin{pmatrix}
            -1 & 0\\
            0 & -1
        \end{pmatrix}
        ,
    \end{equation}
    and then the path given by
    \begin{equation}
        \begin{pmatrix}
            -\lambda & 0\\ 0 & -\frac{1}{\lambda}
        \end{pmatrix}
        ,
    \end{equation}
    for \(\lambda \in [1, 4]\), which ends us at
    \begin{equation}
        \begin{pmatrix}
            -4 & 0\\
            0 & -\frac{1}{4}
        \end{pmatrix}
        .
    \end{equation}
    That is, the path given by the image of
    \begin{equation}
        t \mapsto
        \begin{cases}
            \begin{pmatrix}
                \cos\vartheta & -\sin\vartheta\\
                \sin\vartheta & \cos\vartheta
            \end{pmatrix}
            & \vartheta = t \in [0, \pi],\\
            \begin{pmatrix}
                -\lambda & 0\\
                0 & -\frac{1}{\lambda}
            \end{pmatrix}
            & \lambda = t + 1 - \pi, t \in [\pi, \pi + 3].
        \end{cases}
    \end{equation}
    This path is continuous (but not differentiable at \(t = \pi\)).
    
    \subsection{Baker--Campbell--Hausdorff}
    For two matrices, \(A\) and \(B\), we have
    \begin{equation*}
        \exp[A]\exp[B] = \exp\left[ A + B + \frac{1}{2}\commutator{A}{B} + \frac{1}{12}(\commutator{A}{\commutator{A}{B}} + \commutator{\commutator{A}{B}}{B}) + \dotsb \right]
    \end{equation*}
    If we know that \(A, B \in \lie{g}\) for some Lie algebra \(\lie{g}\) then we can evaluate the commutators and hence can determine the product of \(\exp[A]\) and \(\exp[B]\) in \(G\).
    
    \section{Matrix Lie Algebras}
    \subsection{General Linear}
    Consider the general linear group, \(\generalLinear(n, \field)\).
    This consists of all invertible \(n \times n\) matrices over \(\field\).
    Now let \(A\) be any \(n \times n\) matrix over \(\field\).
    Then \(\exp[A]\) is an invertible \(n\times n\) matrix over \(\field\), in particular \(\exp[A]^{-1} = \exp[-A]\).
    This shows that the Lie algebra of \(\generalLinear(n, \field)\), denoted \(\generalLinearLie(n, \field)\), is the set of all \(n \times n\) matrices over \(\field\).
    Note that this can be identified with the set of all linear transformations, \(\generalLinearLie(n, \field) \isomorphic \End(\field^n)\).
    Since we have \(n^2\) entries into an \(n \times n\) matrix, and no conditions, we conclude that \(\dim\generalLinearLie(n, \reals) = n^2\).
    Similarly, if \(\field = \complex\) then each entry has two real parameters and so we have \(2n^2\) degrees of freedom so the dimension of \(\generalLinearLie(n, \complex)\) as a real vector space is \(2n^2\).
    
    Now consider the special linear group, \(\specialLinear(n, \field)\).
    We then have the extra condition that \(\det M = 1\) for all \(M \in \specialLinear(n, \field)\).
    To proceed we need the following lemma
    \begin{lma}{}{}
        Let \(A\) be an \(n \times n\) matrix and \(I\) the \(n\)-dimensional identity matrix.
        Then
        \begin{equation}
            \det(I + \varepsilon A) = 1 + \varepsilon\tr A
        \end{equation}
        to first order in \(\varepsilon\).
        \begin{proof}
            Let \(a_i\) be the eigenvalues of \(A\).
            Then the characteristic polynomial of \(A\) is
            \begin{equation}
                \det(t I - A) = (t - a_1)(t - a_2) \dotsm (t - a_n).
            \end{equation}
            Setting \(t = -1\) we get
            \begin{align}
                \det(-I - A) &= (-1)^n \det(I + A)\\
                &= (-1 - a_1)(-1 - a_2) \dotsm (-1 - a_n)\\
                &= (-1)^n(1 + a_1)(1 + a_2) \dotsm (1 + a_n)\\
                &= (-1)^n[1 + a_1 + a_2 + \dotsb + a_n + \order(a_ia_j)]\\
                &= (-1)^n[1 + \tr A + \order(a_ia_j)].
            \end{align}
            Hence \(\det(I + A) = 1 + \tr A + \order(a_ia_j)\)
            Rescaling so that \(A \to \varepsilon A\) we get the result \(\det(I + \varepsilon A) = 1 + \varepsilon\tr A + \order(\varepsilon^2)\).
        \end{proof}
    \end{lma}
    Using this we can see that if \(M \in \specialLinear(n, \field)\) we can expand \(M = I + i\alpha^aT_a + \order(\alpha^2)\) and we get
    \begin{equation}
        1 = \det M \approx \det(I + i\alpha^aT_a) \approx 1 + \alpha^a \tr T_a,
    \end{equation}
    and so the generators of \(\specialLinearLie(n, \field)\) must be traceless, and hence all matrices in \(\specialLinearLie(n, \field)\) are traceless.
    So, \(\specialLinearLie(n, \field)\) consists of all traceless \(n \times n\) matrices.
    We have \(n^2\) entries, but being traceless fixes one, since we can force a matrix to be traceless by setting \(A_{11} = -A_{22} - A_{33} - \dotsb - A_{nn}\), and so \(\dim \specialLinearLie(n, \reals) = n^2 - 1\) and being traceless in both the real and imaginary components fixes two degrees of freedom in the complex case, so \(\dim \specialLinearLie(n, \complex) = 2(n^2 - 1)\) as a real vector space.
    
    Now consider the orthogonal group, \(\orthogonal(n)\).
    Writing some generic \(O \in \orthogonal(n)\) as \(I + \varepsilon A + \order(\varepsilon^2)\) and expanding the defining condition, \(O^\trans O = I\), we get
    \begin{multline}
        I = (I + \varepsilon A + \order(\varepsilon^2)) (I + \varepsilon A^\trans + \order(\varepsilon^2))\\
        = I + \varepsilon A + \varepsilon A^\trans + \order(\varepsilon^2) = I + \varepsilon(A + A^\trans) + \order(\varepsilon^2).
    \end{multline}
    So we must have that \(A = -A^{\trans}\), that is \(A\) is antisymmetric.
    So the Lie algebra, \(\specialOrthogonalLie(n)\) consists of all \(n \times n\) real antisymmetric matrices.
    Being antisymmetric sets all values below the diagonal, and the diagonal must be zero, so we get an upper triangle to fill in of base \(n - 1\), the number of entries in this triangle is \(n(n - 1)/2\).
    
    Consider the special orthogonal group, \(\specialOrthogonal(n)\).
    As with \(\orthogonal(n)\) the matrices in the Lie algebra must be antisymmetric.
    But, this then enforces that these matrices are traceless, since an antisymmetric matrix has zeros down the diagonal, and hence \(\orthogonalLie(n) \isomorphic \specialOrthogonalLie(n)\).
    
    Now consider the unitary group, \(\unitary(n)\).
    Suppose that \(H \in \unitaryLie(n)\), that is \(\exp[i\varepsilon H] \in \unitary(n)\), then \(\exp[i\varepsilon H]\exp[i\varepsilon H]^\hermit = I\) by definition, and we have
    \begin{equation}
        I = \exp[i\varepsilon H]\exp[i\varepsilon H]^\hermit \approx (I + i\varepsilon H)(I - i\varepsilon H^\hermit) = I + i\varepsilon(H - H^\hermit) + \order(\varepsilon^2),
    \end{equation}
    and so we must have \(H = H^\hermit\), so \(H\) is Hermitian.
    Note that if we had not included the factor of \(i\) we would instead have found \(H\) to be anti-Hermitian, which isn't as nice and this is why we include the factor of \(i\).
    So, \(\unitaryLie(n)\) consists of all \(n \times n\) Hermitian matrices.
    Being Hermitian fixes the values below the diagonal, removing \(n(n - 1)/2\) entries, but \(n(n - 1)\) degrees of freedom, since these are complex numbers with two real parameters.
    Being Hermitian also forces the diagonal to be real, removing another \(n\) parameters from the imaginary part of the diagonal.
    This leaves \(n(n - 1)\) real parameters defining the upper triangle and \(n\) defining the diagonal, for a total of \(n(n - 1) + n = n^2\) real parameters, so \(\dim \unitaryLie(n) = n^2\).
    
    Finally, consider \(\specialUnitary(n)\).
    This consists of all Hermitian, traceless, \(n \times n\) matrices.
    Being traceless fixes one real parameter on the diagonal, since the diagonal is real anyway, and so \(\dim \specialUnitaryLie(n) = n^2 - 1\).
    
    \section{Universal Covering Group}
    Every Lie group has a unique Lie algebra which we find by linearising the group.
    Every Lie algebra can be exponentiated to form a Lie group.
    However, this Lie group is not necessarily the same one that we linearised to find the Lie algebra.
    The exponentiated Lie algebra forms a simply connected Lie group, even if the original Lie group wasn't simply connected.
    This means that multiple Lie groups can have the same Lie algebra, but only one of these groups is attained by exponentiating the Lie algebra, this Lie group is called the \defineindex{universal covering group}.
    
    If two Lie groups have the same Lie algebra it is because they are indistinguishable in the neighbourhood of the identity.
    
    \begin{exm}{}{}
        Consider \(\unitary(1)\).
        This has elements \(\e^{i\vartheta}\) for some \(\vartheta \in [0, 2\pi)\).
        Since we identify \(0\) and \(2\pi\) this group is compact, and topologically equivalent to the circle, which is not simply connected.
        The Lie algebra is then just \([0, 2\pi)\).
        
        Consider instead the set of elements of the form \(\e^{\vartheta}\) for \(\vartheta \in \reals\).
        This gives \(\reals_{>0}\), the set of positive real numbers.
        However, the Lie algebra is, up to an unimportant factor of \(i\), the same as for \(\unitary(1)\).
        The ray consisting of the positive real numbers is simply connected, and hence exponentiating this Lie algebra gives this group, rather than \(\unitary(1)\).
    \end{exm}
    
    \begin{exm}{}{exm:SO(n) = O(n)/Z2}
        We've seen that both \(\orthogonal(n)\) and \(\specialOrthogonal(n)\) have the same Lie algebra.
        This is because the requirement of unit determinant is not relevant in the neighbourhood of the identity, where it is satisfied in both groups.
        
        The orthogonal group, \(\orthogonal(n)\), is formed of two disconnected components, one with determinant \(+1\) and one with determinant \(-1\).
        The piece with determinant \(+1\) is just the subgroup \(\specialOrthogonal(n)\).
        The characterisation of the piece determinant \(-1\) depends on the dimension.
        If \(n\) is odd then we can form pairs \((A, +1)\) and \((A, -1)\) for all \(A \in \specialOrthogonal(n)\).
        Then identifying \((A, +1) = A \in \orthogonal(n)\) and \((A, -1) = -A \in \orthogonal(n)\) we get a one-to-one correspondence between these pairs and \(\orthogonal(n)\).
        That means we have
        \begin{equation}
            \orthogonal(n) \isomorphic \specialOrthogonal(n) \times \integers_2 \qquad n \text{ odd}.
        \end{equation}
        
        If instead \(n\) is even then, for example, both \(I\) and \(-I\) are in \(\specialOrthogonal(n)\), and so we cannot do the same thing with pairs.
        The result is that the two components don't commute with each other.
        
        Consider the case where \(n = 2\).
        We can then split \(\orthogonal(2)\) into two parts, the first formed of matrices of the form
        \begin{equation}
            g = 
            \begin{pmatrix}
                \cos \vartheta & -\sin \vartheta\\
                \sin \vartheta & \cos \vartheta
            \end{pmatrix}
            \in \specialOrthogonal(2),
        \end{equation}
        and the second formed of matrices of the form
        \begin{equation}
            \overbar{g} = 
            \begin{pmatrix}
                -\cos \psi & \sin \psi\\
                \sin \psi & \cos \psi
            \end{pmatrix}
            \in \overline{\specialOrthogonal(2)}.
        \end{equation}
        That is, \(\specialOrthogonal(2)\) consists of all matrices in \(\orthogonal(2)\) with determinant 1 and \(\overline{\specialOrthogonal(2)}\) consists of all matrices in \(\orthogonal(2)\) with determinant \(-1\).
        Notice that if \(\overbar{g}, \overbar{h} \in \overline{\specialOrthogonal(2)}\) then we have \(\det(\overbar{g}\overbar{h}) = \det(\overbar{g})\det(\overbar{h}) = (-1)(-1) = 1\), so \(\overbar{g}\overbar{h} \in \specialOrthogonal(2)\).
        
        It can be shown that \(\specialOrthogonal(2)\) is a normal subgroup of \(\orthogonal(2)\), that is \(ghg^{-1} \in \specialOrthogonal(2)\) for all \(g \in \orthogonal(2)\) and \(h \in \specialOrthogonal(2)\).
        In particular, if \(g \in \specialOrthogonal(2)\) and \(\overbar{g} \in \overline{\specialOrthogonal(2)}\) then \(\overbar{g} g \overbar{g}^{-1} \in \specialOrthogonal(2)\).
        
        If we have a normal subgroup then we can write the original group as a semidirect product of the normal subgroup and some other group, in this case
        \begin{equation}
            \orthogonal(2) \isomorphic \specialOrthogonal(2) \rtimes \integers_2,
        \end{equation}
        where the action of \(\integers_2\) on \(\specialOrthogonal(2)\) is
        \begin{equation}
            0 \action g =
            \begin{pmatrix}
                1 & 0\\
                0 & 1
            \end{pmatrix}
            g, \qqand 1 \action g = 
            \begin{pmatrix}
                -1 & 0\\
                0 & 1
            \end{pmatrix}
            g.
        \end{equation}
    \end{exm}
    
    \begin{exm}{}{}
        Perhaps the most important example, at least for physics, of two Lie groups with the same Lie algebras is \(\specialOrthogonal(3)\) and \(\specialUnitary(2)\).
        Start with \(\specialUnitary(2)\), this has a Lie algebra formed from all Hermitian traceless \(2 \times 2\) matrices.
        A basis for these matrices is given by the Pauli matrices, although to we include a factor of \(1/2\) as a matter of convention, giving
        \begin{equation}
            \specialUnitaryLie(2) = \spn_{\reals} \left\{ \frac{1}{2}\sigma_a \right\}.
        \end{equation}
        This Lie algebra then satisfies
        \begin{equation}
            \commutator{\sigma_a/2}{\sigma_b/2} = i\varepsilon_{cab}\sigma_c/2,
        \end{equation}
        that is, the structure constants are \(\tensor{c}{^c_{ab}} = \varepsilon_{cab}\).
        Note that we are free to raise and lower indices here since the Killing form (defined later) is positive definite.
        
        For \(\specialOrthogonal(3)\) the Lie algebra is formed from the matrices \(T_a\) with components \((T_a)_{bc} = i\varepsilon_{bac}\).
        This is simply the adjoint representation of \(\specialOrthogonalLie(2)\) as defined above, so clearly these two Lie algebras are the same.
        
        Consider what happens when we exponentiate them.
        We can consider
        \begin{equation}
            \exp\left[ i\omega \vh{n} \cdot \vv{\sigma}/2 \right]
        \end{equation}
        where our parameters, \(\omega\vh{n}\), are formed from a unit vector, \(\vh{n} \in \sphere[2]\), and some \(\omega \in \reals\).
        Note that a unit vector in \(n\) dimensions requires \(n - 1\) parameters to define.
        Using the identity \(\sigma_i \sigma_j = \delta_{ij}I + i\varepsilon_{ijk}\sigma_k\) we can expand the exponential and collect terms to show that
        \begin{equation}
            \exp[i\omega \vh{n} \cdot \vv{\sigma} /2] = \cos\left( \frac{\omega}{2} \right) I + i\vh{n} \cdot \vv{\sigma} \sin\left( \frac{\omega}{2} \right).
        \end{equation}
        Thus we have to take \(\omega \in [0, 4\pi)\) in order to cover every element of \(\specialUnitary(2)\), at least those which can be reached in this way.
        
        Instead we can consider \(\specialOrthogonal(3)\) and exponentiate
        \begin{equation}
            \exp[i\omega\vh{n}(i\varepsilon_{bac})] = n_bn_c + (\delta_{bc} - n_bn_c)\cos\omega + \varepsilon_{abc} n_a \sin \omega.
        \end{equation}
        This requires a lot of algebra to show, but in the \course{Vectors, Tensors, and Continuum Mechanics} part of the \course{Methods of Theoretical Physics} course we show that this is the general form of a rotation.
        Note then that if \(\omega \in [0, 4\pi)\) as for \(\specialUnitary(2)\) we will hit every element of \(\specialOrthogonal(3)\) twice.
        We say that \(\specialUnitary(2)\) is a double cover of \(\specialOrthogonal(3)\).
        
        As a manifold \(\specialOrthogonal(3)\) corresponds to the three-dimensional ball of radius \(\pi\), where a vector in this ball picks out an axis, and its length picks out the angle of rotation.
        In particular, a vector, \(\vv{x}\), of length \(\pi\) is equivalent as a rotation to the vector \(-\vv{x}\), and so we identify opposite points on the ball.
        This means that a line going from one side of the ball to the other is technically a loop, but is not contractible to a point so \(\specialOrthogonal(3)\) is not simply connected.
        
        On the other hand, we showed in \cref{exm:SU(2) = three sphere} that \(\specialUnitary(2)\) as a manifold is the three sphere, \(\sphere[3]\), which is simply connected.
        We conclude that both the Lie algebras of \(\specialUnitary(2)\) and \(\specialOrthogonal(3)\) both exponentiate to give \(\specialUnitary(2)\).
    \end{exm}
    
    \chapter{Invariant Tensors}
    \section{Structure Constants}
    Recall that for a Lie algebra generated by \(T_a\) the structure constants, \(\tensor{c}{^c_{ab}}\) are defined as
    \begin{equation}
        \commutator{T_a}{T_b} = i\tensor{c}{^c_{ab}}T_c.
    \end{equation}
    These are independent of the representation chosen.
    For example, we saw that the structure constants of \(\specialUnitaryLie(2) \isomorphic \specialOrthogonalLie(3)\) are \(\varepsilon_{ijk}\) whether we think of this Lie algebra as \(2\times 2\) traceless Hermitian matrices (\(\specialUnitaryLie(2)\)) or as antisymmetric traceless real matrices (\(\specialOrthogonalLie(3)\)).
    
    If the generators are Hermitian, so \(T_a^\hermit = T_a\), then the structure constants are real, to show this consider the conjugate of the defining equation:
    \begin{equation}
        \commutator{T_a}{T_b}^\hermit = -i\tensor{c}{^c_{ab}}^*T_c^\hermit,
    \end{equation}
    the conjugate of the commutator is the negative of the commutator of the conjugates:
    \begin{multline}
        \commutator{A}{B}^\hermit = (AB - BA)^\hermit = (AB)^\hermit - (BA)^\hermit\\
        = B^\hermit A^\hermit - A^\hermit B^\hermit = \commutator{B^\hermit}{A^\hermit} = -\commutator{A^\hermit}{B^\hermit}
    \end{multline}
    Hence, we have
    \begin{equation}
        \commutator{T_a}{T_b}^\hermit = -\commutator{T_a^\hermit}{T_b^\hermit} = -\commutator{T_a}{T_b},
    \end{equation}
    and so we have
    \begin{equation}
        \commutator{T_a}{T_b} = i\tensor{c}{^c_{ab}}^* T_c.
    \end{equation}
    Hence we must have \(\tensor{c}{^c_{ab}}^* = \tensor{c}{^c_{ab}}\), and so \(\tensor{c}{^c_{ab}}\) is real.
    This is useful because in a compact Lie group we can always choose finite dimensional representations to be unitary by Maschke's theorem (\cref{thm:maschke}), and the corresponding Lie algebra representation will be Hermitian.
    
    If the generators are purely imaginary, so \(iT_a\) is real, this follows since \(T_a\) being purely imaginary implies \(\commutator{T_a}{T_b}\) is real, and so in the defining equation we have something real, \(\commutator{T_a}{T_b}\) is equal to the structure constants, \(\tensor{c}{^c_{ab}}\), times something real, \(iT_a\), and hence \(\tensor{c}{^c_{ab}}\) must be real.
    This is useful because we can often choose the generators of a noncompact group to be purely imaginary.
    
    The structure constants are antisymmetric in the second two indices, since
    \begin{equation}
        i\tensor{c}{^c_{ab}}T_c = \commutator{T_a}{T_b} = -\commutator{T_b}{T_a} = -i\tensor{c}{^c_{ba}}T_c.
    \end{equation}
    
    If we rescale one of the generators, say \(T_a \mapsto \mu T_a\) for some fixed \(a\), then the structure constants rescale accordingly, for \(b \ne a\)
    \begin{equation}
        \commutator{T_a}{T_b} \mapsto \commutator{\mu T_a}{T_b} = \mu\commutator{T_a}{T_b} = i\mu\tensor{c}{^c_{ab}}T_c.
    \end{equation}
    This can be useful to swap between the physics and maths conventions, where the generators differ by a factor of \(i\).
    
    \subsection{Adjoint Representation}
    The structure constants define the adjoint representation of the Lie algebra.
    There is an associated representation of the Lie group on the Lie algebra.
    This is fully determined by the action of the Lie group on the generators.
    Consider the transformation \(T_a \mapsto gT_ag^{-1}\), where \(g = 1 + i\alpha^aT_a + \dotsb\) and so \(g^{-1} = 1 - i\alpha^aT_a + \dotsb\).
    Expanding this to first order we have
    \begin{align}
        gT_ag^{-1} &\approx (1 + i\alpha^bT_b)T_a(1 - i\alpha^cT_c)\\
        &= T_a + i\alpha^bT_bT_a - i\alpha^cT_aT_c + \order(\alpha^2)\\
        &= T_a + i\alpha^b \commutator{T_b}{T_a} + \order(\alpha^2),
    \end{align}
    where we reindex \(c \to b\) in the last step.
    It turns out that this holds to all orders, and so it can be shown that
    \begin{equation}
        gT_ag^{-1} = T_b \tensor{D}{^b_a}(g)
    \end{equation}
    where \(D(g)\) are some matrices.
    We call \(D\) the \define{adjoint representation}\index{adjoint representation!of a Lie group}.
    
    We need to check that \(D\) really is a representation, that is, we need to check if the following holds for all \(g, h \in G\):
    \begin{equation}
        \tensor{D}{^b_c}(g) \tensor{D}{^c_d}(h) = \tensor{D}{^b_d}(gh).
    \end{equation}
    To do this multiply by \(T_b\) to give
    \begin{align}
        T_b \tensor{D}{^b_c}(g) \tensor{D}{^c_d}(h) &= gT_cg^-1 \tensor{D}{^c_d}(h)\\
        &= gT_c\tensor{D}{^c_d}(h)g^{-1}\\
        &= ghT_ch^{-1}g^{-1}\\
        &= (gh)T_d(gh)^{-1}\\
        &= T_b \tensor{D}{^b_d}(gh).
    \end{align}
    Note that while \(D(h)\) is a matrix \(\tensor{D}{^c_d}(h)\) is a number, so commutes with \(g^{-1}\).
    Since this holds for all \(T_b\) we have
    \begin{equation}
        \tensor{D}{^b_c}(g) \tensor{D}{^c_d}(h) = \tensor{D}{^b_d}(gh),
    \end{equation}
    and so \(D\) is a representation.
    
    For either a real Lie group or a compact Lie group the adjoint representation is real.
    This leads to many nice properties that make it a useful representation.
    
    \section{Invariant Tensors}
    Consider some vector space \(V\), and some vector \(x \in V\) with components \(x^i\).
    If \(G\) is a Lie group with some action on \(V\) defined by matrix multiplication with the representation \(D\) then \(x^i\) transforms under \(g \in G\) as
    \begin{equation}
        x^i \mapsto (g\action x)^i = \tensor{D}{^i_j}(g)x^j.
    \end{equation}
    Similarly, a vector in the dual space, \(\bar{V}\), has components \(x_i\), which transform as
    \begin{equation}
        x_i \mapsto (g\action x)_i = \tensor{D}{_i^j}(g)x_j.
    \end{equation}
    This extends to some tensor, \(T \in V^{\otimes n} \otimes \overbar{V}^{\otimes m}\) with components \(\tensor{T}{^{i_1i_2\dotso i_n}_{j_1j_2\dotso j_m}}\) transforming as
    \begin{multline*}
        \tensor{T}{^{i_1i_2\dotso i_n}_{j_1j_2\dotso j_m}} \mapsto \tensor{(g\action T)}{^{i_1i_2\dotso i_n}_{j_1j_2\dotso j_m}}\\
        = \tensor{D}{^{i_1}_{k_1}}(g)\tensor{D}{^{i_2}_{k_2}}(g) \dotsm \tensor{D}{^{i_n}_{k_n}}(g) \tensor{D}{_{j_1}^{l_1}}(g) \tensor{D}{_{j_2}^{l_2}}(g) \tensor{D}{_{j_m}^{l_m}}(g) \tensor{T}{^{k_1k_2\dotso k_n}_{l_1l_2\dotso l_m}}.
    \end{multline*}
    
    Some special tensors have the property that their components don't change under any transformation in \(G\), these are called invariant tensors, and they can tell us a lot about the group in question.
    
    \begin{dfn}{Invariant Tensor}{}
        An \defineindex{invariant tensor} of rank \(n\), with components \(d_{a_1a_2\dotso a_n}\) is a tensor whose components are invariant under the action of some Lie group in some representation.
        That is, for \(g \in G\) and representation \(D\) we have
        \begin{align}
            d_{a_1a_2\dotso a_n} \mapsto (g \action d)_{a_1a_2\dotso a_n} &= \tensor{D}{^{b_1}_{a_1}}(g)\tensor{D}{^{b_1}_{a_1}}(g) \dotsm \tensor{D}{^{b_1}_{a_1}}(g) d_{b_1b_2\dotso b_n}\notag\\
            &= d_{a_1a_2\dotso a_n}.
        \end{align}
    \end{dfn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Any scalar is an invariant tensor.
            \item Consider the usual representation of \(\orthogonal(n)\) on \(\reals^n\), that is \(\orthogonal(n)\) just acts by matrix multiplication.
            An invariant tensor in this case is \(\delta_{ij}\).
            \item Now restrict things to \(\specialOrthogonal(n)\), we get an additional invariant tensor, \(\varepsilon_{i_1i_2\dotso i_n}\).
            \item Given invariant tensors we can combine them to get other invariant tensors, such as \(\varepsilon_{ijk}\delta_{lm}\) or \(\delta_{ij}\delta_{kl}\delta_{mn}\).
            \item The tensor \(\eta^{\mu\nu}\) is invariant under \(\specialOrthogonal^+(1, 3)\).
            \item For \(\unitary(n)\) acting on \(\complex^n\) in the obvious way \(\delta_{ij}\) is \emph{not} an invariant tensor, but \(\tensor{\delta}{^i_j}\) is, since
            \begin{equation}
                \tensor{\delta}{^i_j} \mapsto \tensor{U}{^i_k}\tensor{\delta}{^k_l}\tensor{U}{_j^l} = \tensor{U}{^i_l}\tensor{U}{_j^l} = \delta^i_j
            \end{equation}
            for some unitary \(U \in \unitary(n)\).
            \item Now restrict things to \(\specialUnitary(n)\), and we get two additional invariant tensors, \(\varepsilon_{i_1i_2\dotso i_n}\) and \(\varepsilon^{i_1i_2\dotso i_n}\).
        \end{itemize}
    \end{exm}
    
    One simple way to create invariant tensors is to take traces of products of generators.
    For example, define some tensor \(d \in V^{\otimes 3}\) with components
    \begin{equation}
        d_{abc} \coloneqq \tr(T_aT_bT_c).
    \end{equation}
    Then this transforms in the adjoint representation as
    \begin{align}
        d_{abc} &\mapsto \tr(gT_ag^{-1}gT_bg^{-1}gT_cg^{-1})\\
        &= \tr(gT_aT_bT_cg^{-1})\\
        &= \tr(g^{-1}gT_aT_bT_c)\\
        &= \tr(T_aT_bT_c)\\
        &= d_{abc}
    \end{align}
    using the cyclic property of the trace.
    
    \section{Killing Form}
    \epigraph{Set 2 equal to 1}{Neil Turok}
    \begin{dfn}{Killing Form}{}
        The \defineindex{Killing form}\footnote{named after Wilhelm Killing, no relation to murder.} is defined as the tensor with components
        \begin{equation}
            g_{ab} \coloneqq \tr_{\symrm{A}}(T_aT_b)
        \end{equation}
        where \(T_a\) are in the adjoint representation, which is what the subscript \(\symrm{A}\) tells us.
    \end{dfn}
    
    The Killing form is symmetric, since
    \begin{equation}
        g_{ab} = \tr_{\symrm{A}}(T_aT_b) = \tr(T_bT_a) = g_{ba}
    \end{equation}
    by the cyclic property of the trace.
    
    If the structure constants are real then the Killing form is a real symmetric matrix, so can be used as a metric on the Lie group.
    We can write the Killing form in terms of the structure constants:
    \begin{equation}
        g_{ab} = -\tensor{c}{^d_{ae}} \tensor{c}{^e_{bd}} = g_{ba}.
    \end{equation}

    For a compact Lie algebra it is possible to diagonalise the Killing form by some orthogonal transformation.
    That is, there is some basis such that \(g_{ab} = \lambda_a\delta_{ab}\) (no sum on \(a\)).
    We can then rescale the basis and reorder it to get a result in the canonical form:
    \begin{equation}
        g = 
        \begin{pmatrix}
            1 & & & & & & & & &\\
            & \ddots & & & & & & &\\
            & & 1 & & & & & &\\
            & & & 0 & & & & & \\
            & & & & \ddots & & & & \\
            & & & & & 0 & & & & \\
            & & & & & & \mathllap{-}1 & & \\
            & & & & & & & \ddots & \\
            & & & & & & & & \mathllap{-}1
        \end{pmatrix}
        .
    \end{equation}

    \begin{exm}{}{}
        In \(\specialUnitaryLie(2)\) the structure constants are \(\varepsilon_{ijk}\), and so in the adjoint representation the generators have components \(\tensor{(T_i)}{^k_l} = i\varepsilon_{kil}\).
        The Killing form is then
        \begin{equation}
            g_{ij} = -\varepsilon_{kil}\varepsilon_{ljk} = 2\delta_{ij},
        \end{equation}
        so the Killing form is already diagonal in this case.
        We can rescale the generators \(T_a \mapsto T_a/\sqrt{2}\), and so \(\tensor{(T_i)}{^k_l} = i\varepsilon_{kil}/\sqrt{2}\) and \(g_{ij} = \delta_{ij}\).
    \end{exm}
    
    The Killing form is an invariant tensor, since it transforms under the adjoint representation as
    \begin{equation}
        g_{ab} = \tr_{\symrm{A}}(T_aT_b) \mapsto \tr_{\symrm{A}}(gT_ag^{-1}gT_bg) = \tr_{\symrm{A}}(T_aT_b) = g_{ab},
    \end{equation}
    having used the cyclic property of the trace.
    
    We can use the Killing form to lower indices, so for example, we can define structure constants with all lower indices
    \begin{equation}
        c_{abc} = g_{ad}\tensor{c}{^d_{bc}}.
    \end{equation}
    The structure constants with lowered indices are invariant tensors, since the normal structure constants and the Killing form are invariant tensors.
    We can write these lowered index structure constants in terms of traces of products of generators as follows:
    \begin{align}
        \tr_{\symrm{A}}(T_a \commutator{T_b}{T_c}) &= \tr_{\symrm{A}}(T_a i\tensor{c}{^d_{bc}}T_d)\\
        &= i\tensor{c}{^d_{bc}} \tr_{\symrm{A}}(T_aT_d)\\
        &= i\tensor{c}{^d_{bc}} g_{ad}\\
        &= ic_{abc}.
    \end{align}
    
    This form form is useful since we have the identity
    \begin{multline}
        \tr(A \commutator{B}{C}) = \tr(ABC - ACB) = \tr(ABC) - \tr(ACB)\\
        = \tr(ABC) - \tr(BAC) = \tr(ABC - BAC) = \tr(\commutator{A}{B}C)
    \end{multline}
    and similarly
    \begin{equation}
        \tr(A\commutator{B}{C}) = \tr(\commutator{C}{A}B).
    \end{equation}
    This means we have
    \begin{equation}
        ic_{abc} = \tr(T_a\commutator{T_b}{T_c}) = \tr(\commutator{T_a}{T_b}T_c) = \tr(T_c\commutator{T_a}{T_b}) = ic_{cab}.
    \end{equation}
    So, we have \(c_{abc} = c_{cab}\), and we know that \(c_{abc}\) is antisymmetric in the last two indices, so \(c_{abc} = -c_{acb}\).
    This means that exchanging the first two indices must also pick up a negative sign to cancel with this one.
    We conclude that \(c_{abc}\) is antisymmetric in all three indices.
    This result holds for compact and noncompact groups.
    
    We can form another similar invariant tensor by replacing the commutator with the \defineindex{anticommutator}, \(\anticommutator{A}{B} = AB + BA\), and then we have
    \begin{equation}
        d_{abc} \coloneqq \tr_{\symrm{A}}(T_a\anticommutator{T_b}{T_c}).
    \end{equation}
    This is symmetric in all three indices.
    
    In general for \(n > 3\) if we take the expression \(\tr(T_{a_1} \dotsm T_{a_n})\) we can always write the antisymmetric part in terms of \(c_{abc}\).
    This means that each time we increment \(n\) we only get one independent extra invariant tensor given by the totally symmetric tensor on \(n\) indices.
    This holds up to a point where we stop getting new tensors at all as we can even decompose the totally symmetric tensors in terms of lower rank symmetric tensors.
    
    \begin{dfn}{Rank}{}
        The \defineindex{rank} of a Lie algebra is the number of invariant tensors under the universal covering group.
    \end{dfn}
    
    \part{Simplicity and Compact Groups}
    \chapter{Simplicity}
    \section{Definitions}
    We repeat some definitions here for ease of reference in this section.
    \begin{dfn}{Lie Subalgebras and Ideals}{}
        Let \(\lie{g}\) be a Lie algebra and \(\lie{h}\) a subspace of \(\lie{g}\) (as vector spaces).
        Then \(\lie{h}\) is a \defineindex{Lie subalgebra}, or simply a \defineindex{subalgebra} of \(\lie{g}\) if \(\commutator{x}{y} \in \lie{h}\) for all \(x, y \in \lie{h}\), where \(\commutator{-}{-}\) is the Lie bracket of \(\lie{g}\).
        
        A \defineindex{proper subalgebra} of \(\lie{g}\) is a subalgebra of \(\lie{g}\) with a smaller set of generators than \(\lie{g}\).
        
        An \defineindex{ideal}, \(\lie{i}\), or \defineindex{invariant subalgebra} of \(\lie{g}\) is a subalgebra of \(\lie{g}\) such that \(\commutator{g}{i} \in \lie{i}\) for all \(g \in \lie{g}\) and \(i \in \lie{i}\).
    \end{dfn}
    Compare these definitions with those of subgroups.
    A subgroup is one which is closed under the group operation, and a subalgebra is one which is closed under the Lie bracket.
    A proper subgroup is one which is not equal to the original group, and a proper subalgebra is one in which the set of generators is not equal to the set of generators of the original Lie algebra.
    An invariant subgroup is one which is closed under conjugation with any group element, and an invariant subalgebra is one which is closed under the Lie bracket with any Lie algebra element.
    
    \begin{dfn}{Simple and Semisimple}{}
        Let \(\lie{g}\) be a Lie group.
        If \(\lie{g}\) has no proper invariant subalgebras it is \defineindex{simple}.
        If \(\lie{g}\) has no proper invariant Abelian subalgebras it is \defineindex{semisimple}.
        Recall that a Lie algebra, \(\lie{h}\) is Abelian if \(\commutator{x}{y} = 0\) for all \(x, y \in \lie{h}\).
    \end{dfn}
    
    \begin{exm}{}{}
        The Lie algebra \(\specialUnitaryLie(2)\) is simple.
        The Lie algebra \(\specialUnitaryLie(2) \times \specialUnitaryLie(2)\) is semisimple, having a subalgebra isomorphic to \(\specialUnitaryLie(2)\), but no Abelian proper subalgebras.
    \end{exm}
    
    \section{Nonsemisimple Case}
    Consider a Lie algebra, \(\lie{l}\), which is \emph{not} semisimple.
    This means that there exists some Lie algebra, \(\lie{s}\), which is an Abelian subalgebra of \(\lie{l}\).
    In this section we'll use the indices \(a, b, c, \dotsc\) to index elements of \(\lie{s}\).
    We'll use the indices \(i, j, k, \dotsc\) to index elements of \(\lie{l}\), and we'll use the indices \(p, q, r, \dotsc\) to index elements of \(\lie{p} = \lie{l} \setminus \lie{s}\), that is the complement of \(\lie{s}\) in \(\lie{l}\), which is itself a vector space.
    Denote by \(S_i\) the generators of \(\lie{s}\), \(P_p\) the generators of \(\lie{p}\), and \(T_a\) the generators of \(\lie{l}\), note that \(T_a\) coincide with \(S_i\) and \(P_p\).
    Indices \(a, b, c, \dotsc\) run from \(1\) to \(\dim \lie{l}\).
    Indices \(i, j, k, \dotsc\) run from \(1\) to \(\dim \lie{s}\).
    Indices \(p, q, r, \dotsc\) run from \(\dim\lie{s} + 1\) to \(\dim\lie{l}\).
    
    We then have two facts:
    \begin{enumerate}
        \item \(\commutator{S_i}{S_j} = i\tensor{c}{^a_{ij}}T_a = 0\) since \(\lie{s}\) is Abelian.
        Hence \(\tensor{c}{^a_{ij}} = 0\) since \(T_a\) are linearly independent so can only sum to zero if the coefficients vanish.
        \item \(\commutator{S_i}{T_a} = i\tensor{c}{^b_{ia}}T_b \in \lie{s}\) since \(\lie{s}\) is an invariant subalgebra.
        This implies that \(\tensor{c}{^p_{ia}} = 0\) as any element of \(\lie{s}\) can be expressed as a linear combination of \(S_i\) with no \(P_p\) terms.
    \end{enumerate}
    
    Now, consider the elements of the Killing form with one index restricted to \(\lie{s}\).
    By definition we have
    \begin{equation}
        g_{ia} = \tr_{\symrm{A}}(T_iT_a) = -\tensor{c}{^c_{id}} \tensor{c}{^d_{ac}}.
    \end{equation}
    Using the second fact we see that \(\tensor{c}{^c_{id}}\) vanishes unless \(c\) is between 1 and \(\dim\lie{s}\), and so we can set \(c = j\), giving
    \begin{equation}
        g_{ia} = -\tensor{c}{^j_{id}}\tensor{c}{^d_{aj}}.
    \end{equation}
    Using the second fact again we see that \(\tensor{c}{^d_{aj}}\) vanishes unless \(d\) is between 1 and \(\dim\lie{s}\), and so we can set \(d = k\), giving
    \begin{equation}
        g_{ia} = -\tensor{c}{^j_{ik}}\tensor{c}{^k_{aj}} = 0.
    \end{equation}
    Here we've used the first fact which tells us that \(\tensor{c}{^j_{ik}} = 0\) identically.
    Symmetry means that \(g_{ai} = 0\) also, and so we see that the metric takes the form
    \begin{equation}
        \begin{pmatrix}
            0 & 0\\
            0 & g_{pq}
        \end{pmatrix}
    \end{equation}
    where \(g_{pq}\) is a \((\dim\lie{l} - \dim\lie{s}) \times (\dim\lie{l} - \dim\lie{s})\) matrix and the zeros are all block matrices of zeros.
    That is, the metric is zero apart from on the subspace \(\lie{p}\).
    The metric has \(\dim\lie{s}\) zero eigenvalues.
    
    \subsection{Semisimple Case}
    Consider a semisimple Lie algebra, \(\lie{g}\).
    Then the Killing form has no zero eigenvalues, and so is invertible.
    We define the inverse to be the tensor \(g^{ab}\) such that
    \begin{equation}
        g^{ab}g_{bc} = \tensor{\delta}{^a_c}.
    \end{equation}
    For a semisimple Lie algebra the killing form is a pseudo-Riemannian metric, this means we can choose a basis in which \(g_{ab}\) and \(g^{ab}\) are diagonal and have only 1 and \(-1\) on the diagonal.
    
    \section{Casimir Operators}
    \begin{dfn}{Casimir Operator}{}
        A \defineindex{Casimir operator} is anything\footnote{the technical definition being any element of the centre of the universal enveloping algebra, which is basically the associative algebra with the same representations as the Lie algebra, so in the case of matrix Lie algebras its simply the set of matrices with matrix multiplication as the associative product in the algebra, this is what allows us to work with the universal enveloping algebra informally. The formal definition of the universal enveloping algebra is to construct the tensor algebra, \(T(\lie{g}) \coloneqq \field \oplus \lie{g} \oplus (\lie{g} \otimes \lie{g}) \oplus (\lie{g} \otimes \lie{g} \otimes \lie{g}) \oplus \dotsb\), then define recursively the bracket on \(T^n(\lie{g}) \coloneqq \lie{g}^{\otimes n}\) by \(\commutator{x\otimes y}{z} \coloneqq x \otimes \commutator{y}{z} + \commutator{x}{y}\otimes z\) and extend this linearly, note that the Lie brackets on the right correspond to the Lie brackets on \(T^k(\lie{g})\) and \(T^{n - k}(\lie{g})\) for some \(k < n\). The universal enveloping algebra is then \(U(\lie{g}) = T(\lie{g})/{\sim}\) where \(\sim\) is the equivalence relation \(x \otimes y - y \otimes x = \commutator{x}{y}\)} which commutes with all elements of the Lie algebra.
    \end{dfn}
    
    \begin{dfn}{Quadratic Casimir}{}
        The \defineindex{quadratic Casimir} of a semisimple Lie algebra with Killing form \(g_{ab}\) is defined as
        \begin{equation}
            C^{(2)} \coloneqq g^{ab}T_aT_b.
        \end{equation}
        Note that this definition is independent of the representation we choose.
    \end{dfn}
    
    For a nonsemisimple Lie algebra we can define a quadratic Casimir in a similar way but we have to choose a different bilinear form, since \(g^{ab}\) doesn't exist.
    
    Having claimed that \(C^{(2)}\) is a Casimir operator, and so commutes with all elements of the Lie algebra we should prove it.
    It is sufficient to show that \(C^{(2)}\) commutes with all generators of the Lie algebra.
    
    \begin{lma}{}{}
        Let \(\lie{g}\) be a semisimple Lie algebra with Killing form \(g_{ab}\).
        Then the quadratic Casimir, \(C^{(2)} = g^{ab}T_aT_b\), commutes with all generators of \(\lie{g}\).
        
        \begin{proof}
            We'll prove this two ways, first using the Lie group structure, then using the Lie algebra structure.
            Let \(T_a\) be the generators of \(\lie{g}\).
            Consider the universal covering group, \(G\), of \(\lie{g}\), that is we get \(G\) by exponentiating \(\lie{g}\).
            Take some \(g \in G\).
            Then consider \(gC^{(2)}g^{-1}\).
            If \(C^{(2)}\) commutes with all the group elements then it must commute with the generators, since we can expand the group elements about the identity as a series of generators, so if \(gC^{(2)}g = C^{(2)}\) then \(g\) must have commuted with \(C^{(2)}\) and we will have proven the statement.
            Inserting the definition of \(C^{(2)}\) we have
            \begin{equation}
                gC^{(2)}g^{-1} = g^{ab} gT_aT_bg^{-1}.
            \end{equation}
            Here we've pulled the inverse Killing form out since it is just a number.
            We can insert an identity in the form \(g^{-1}g\) to get
            \begin{equation}
                gC^{(2)}g^{-1} = g^{ab}gT_ag^{-1}gT_bg^{-1}.
            \end{equation}
            Now we can recognise \(gT_ag = T_c \tensor{D}{^c_a}(g)\) as the adjoint representation acting on \(T_a\), and so
            \begin{equation}
                gC^{(2)}g^{-1} = g^{ab} T_c \tensor{D}{^c_a}(g) T_d\tensor{D}{^d_b}(g).
            \end{equation}
            Now using the fact that \(g^{ab}\) is an invariant tensor we have
            \begin{equation}
                g^{cd} \mapsto g^{ab} \tensor{D}{^c_a}(g)\tensor{D}{^d_b}(g) = g^{cd},
            \end{equation}
            and so
            \begin{equation}
                gC^{(2)}g^{-1} = g^{cd}T_cT_d = C^{(2)}.
            \end{equation}
            This completes the first proof.\\[1.5ex]
            We have the identity
            \begin{equation}
                \commutator{AB}{C} = A\commutator{B}{C} + \commutator{A}{C}B.
            \end{equation}
            We then have
            \begin{align}
                \commutator{C^{(2)}}{T_c} &= \commutator{g^{ab}T_aT_b}{T_c}\\
                &= g^{ab}\commutator{T_aT_b}{T_c}\\
                &= g^{ab}T_a\commutator{T_b}{T_c} + g^{ab}\commutator{T_a}{T_c}T_b\\
                &= g^{ab}T_ai\tensor{c}{^d_{bc}}T_d + g^{ab}i\tensor{c}{^d_{ac}}T_dT_b\\
                &= i\tensor{c}{^d_{bc}}T^bT_d + i\tensor{c}{^d_{ac}}T_dT^a\\
                &= i\tensor{c}{_{dbc}}T^bT^d + i\tensor{c}{_{dac}}T^dT^a \qquad b \to a\text{ in first term}\\
                &= i\tensor{c}{_{dac}}T^aT^d + i\tensor{c}{_{dac}}T^dT^a\\
                &= i\tensor{c}{_{dac}}\anticommutator{T^a}{T^d}\\
                &= 0
            \end{align}
            where in the last step we notice that \(\tensor{c}{_{dac}}\) is antisymmetric in \(a\) and \(d\), and \(\anticommutator{T^a}{T^d}\) is symmetric in \(a\) and \(d\), so their product vanishes.
            This completes the second proof.
        \end{proof}
    \end{lma}
    
    Consider the trace of two generators in some representation, \(R\):
    \begin{equation}
        \tr_{R}(T_aT_b) \eqqcolon \overbar{g}_{ab}.
    \end{equation}
    Obviously choosing \(R\) to be the adjoint representation recovers the definition of the Killing form.
    This is an invariant tensor, meaning that
    \begin{equation}
        D^\trans \overbar{g} D = \overbar{g}
    \end{equation}
    for all \(D\) in the adjoint representation.
    Now suppose that we are working in a semisimple Lie algebra.
    Then the inverse of the Killing form exists.
    Consider the product \(g^{ab}\overbar{g}_{bc}\), or in terms of matrices \(g^{-1}\overbar{g}\).
    Since both \(g\) and \(\overbar{g}\) are invariant tensors we have \(g = D^\trans g D\) and \(\overbar{g} = D^\trans \overbar{g} D\) for representation \(D\) evaluated at some arbitrary group element.
    Inverting the first of these we have \(g^{-1} = D^{-1}g^{-1}(D^\trans)^{-1}\).
    Putting these together we have
    \begin{equation}
        g^{-1} \overbar{g} = D^{-1} g^{-1} (D^\trans)^{-1} D^{\trans} g D = D^{-1}g^{-1}\overbar{g}D.
    \end{equation}
    Multiplying on the left by \(D\) we get
    \begin{equation}
        D g^{-1} \overbar{g} D = g^{-1}\overbar{g},
    \end{equation}
    so \(g^{-1}\overbar{g}\) commutes with all \(D\) in the adjoint representation.
    
    \begin{lma}{Schur's Lemma}{}
        Let \(G\) be a Lie group with finite dimensional irreducible representations \(D, D' \colon G \to \generalLinear(V)\).
        Let \(T \colon V \to V'\) be a linear map satisfying \(T \circ D(g) = D'(g) \circ T\) for all \(g \in G\).
        Then \(T = \lambda \ident\) for some \(\lambda \in \complex\).
    \end{lma}
    
    Schur's lemma basically tells us that any matrix which commutes with all the matrices in an irreducible representation of a group is simply a multiple of the identity.
    The adjoint representation of a semisimple Lie group is an irreducible representation, and so we must have that \(g^{-1}\overbar{g} = \lambda \ident\) for some \(\lambda \in \complex\).
    This means that, up to a scale constant, \(\overbar{g}\) is the inverse to \(g^{-1}\), so up to a scale constant \(\overbar{g}\) is the Killing form, i.e., \(\overbar{g}_{ab} = \lambda g_{ab}\).
    
    This means that the trace of two generators is given by the Killing form times some, representation dependent, number, \(C(R)\):
    \begin{equation}
        \tr_R(T_aT_b) = C(R)g_{ab}.
    \end{equation}
    Now consider the quadratic Casimir \(C^{(2)} = g^{ab}T_aT_b\), which we can now write as
    \begin{equation}
        C^{(2)} = C_R^{(2)}\ident
    \end{equation}
    by Schur's lemma, where \(C_R^{(2)}\) is another representation dependent number.
    
    Now consider \(g^{ab}\tr_R(T_aT_b) = C(R)g^{ab}g_{ab}\).
    The left hand side is 
    \begin{equation}
        g^{ab}\tensor{(T_a)}{^c_d}\tensor{(T_b)}{^d_c} = C_R^{(2)}\dim R.
    \end{equation}
    The right hand side is
    \begin{equation}
        g^{ab}g_{ab}C(R) = \tensor{\delta}{^a_a}C(R) = C(R) \dim G.
    \end{equation}
    This means we can work out the eigenvalue of the quadratic Casimir operator by evaluating
    \begin{equation}
        C_R^{(2)} = C(R)\frac{\dim G}{\dim R}
    \end{equation}
    for some representation \(R\).
    
    \begin{exm}{}{}
        Consider \(\specialUnitary(2)\).
        The adjoint representation is \(3 \times 3\) matrices \(\tensor{(T_a)}{^b_c} = \varepsilon_{bac}\).
        Some basic identities give us \(\tr_{\symrm{A}}(T_aT_b) = 2\delta_{ab}\).
        The Casimir operator in this case is \(\vv{J}^2\), and has eigenvalue \(j(j + 1)\), this should be familiar from quantum mechanics, see \course{Principles of Quantum Mechanics}, and \course{Symmetries of Quantum Mechanics} has the group theory details.
        So we have the eigenvalue \(2\), meaning \(j = 1\).
        
        There is also a two dimensional representation given by \(T_a = \sigma_a/2\).
        Then \(\tr_2(T_aT_b) = \tr(\sigma_a\sigma_b)/4 = \delta_{ab}/2\).
        We then have \(C_R^{(2)} = (1/2)(3/2) = 3/4\), which is \(j(j + 1)\) with \(j = 1/2\).
    \end{exm}
    
    \part{Spacetime Symmetries}
    \chapter{Lorentz Group}
    \section{Relativity Recap}
    \begin{rmk}
        We assume the reader is familiar with the basics of relativity.
        Should this not be the case see notes from any and all of \course{Relativity Nuclear and Particle Physics} (basics), \course{Classical Electrodynamics}, \course{Quantum Theory}, or \course{Quantum Field Theory}.
    \end{rmk}
    
    An event in spacetime is given by a four vector with components \(x^\mu = (x^0, \vv{x})\), where \(x^0 = ct\).
    Four vectors are defined by their transformation rule:
    \begin{equation}
        x^\mu \mapsto x'^\mu = \tensor{\Lambda}{^\mu_\nu}x^\nu \iff x \mapsto x' = \Lambda x,
    \end{equation}
    where \(\Lambda\) is a Lorentz transformation, which we'll define shortly.
    
    There is a metric tensor, \(\minkowskiMetric_{\mu\nu}\), which is real, symmetric, and invertible (\(\det\minkowskiMetric \ne 1\)).
    The inverse of this metric is \(\minkowskiMetric^{\mu\nu}\), and is such that \(\minkowskiMetric^{\mu\nu}\minkowskiMetric_{\nu\rho} = \tensor{\delta}{^\mu_\rho}\).
    We can use this to raise and lower indices, \(\minkowskiMetric_{\mu\nu}x^\mu = x_\nu\), and \(\minkowskiMetric^{\mu\nu}x_\nu = x^\mu\).
    We can define an inner product on four-vectors:
    \begin{equation}
        x \cdot y \coloneqq x^\mu y^\nu \minkowskiMetric_{\mu\nu} = x^\mu y_\mu = x^\trans \minkowskiMetric y.
    \end{equation}
    We can choose an orthonormal, with respect to this inner product, metric such that
    \begin{equation}
        \minkowskiMetric_{\mu\nu} = \minkowskiMetric^{\mu\nu} = \diag(1, -1, -1, -1).
    \end{equation}
    Then \(x_\mu = (x^0, -\vv{x})\).
    
    Formally the space of all position four-vectors forms a Lorentzian manifold, meaning the metric signature is \((1, D - 1)\) in \(D\) dimensions, here the first number is the number of positive eigenvalues of the metric and the second the number of negative eigenvalues.
    This Lorentzian manifold is what we call \defineindex{Minkowski space}, \(\minkowskiSpace\)\index{R13@\(\minkowskiMetric\)|see{Minkowski space}}.
    
    \section{The Lorentz Group}
    \begin{dfn}{Lorentz Transformation}{}
        A \defineindex{Lorentz transformation}, \(\Lambda\), is a transformation of Minkowski space, \(\minkowskiSpace\), preserving the metric.
        That is, \(\Lambda\) is a \(4 \times 4\) real matrix such that
        \begin{equation}
            \Lambda^\trans \minkowskiMetric \Lambda = \minkowskiMetric.
        \end{equation}
        
        The \defineindex{Lorentz group}, \(\orthogonal(1, 3)\)\index{O(1,3)@\(\orthogonal(1, 3)\)|see{Lorentz group}}, is the group of all Lorentz transformations.
    \end{dfn}
    
    A consequence of this definition is that Lorentz transformations preserve the inner product, if \(x \mapsto \Lambda x\) and \(y \mapsto \Lambda y\) then
    \begin{equation}
        x \cdot y \mapsto (\Lambda x)^\trans \minkowskiMetric (\Lambda y) = x^\trans (\Lambda^\trans \minkowskiMetric \Lambda) y = x^\trans \minkowskiMetric y = x \cdot y
    \end{equation}
    
    We have the determinant identities
    \begin{equation}
        \det(AB) = \det(A)\det(B), \qqand \det A^\trans = \det A.
    \end{equation}
    Using these we have
    \begin{equation}
        \det \minkowskiMetric = \det(\Lambda^\trans \minkowskiMetric \Lambda) = \det(\Lambda^\trans)\det(\minkowskiMetric)\det(\Lambda) = \det(\Lambda)^2\det(\minkowskiMetric)
    \end{equation}
    and so
    \begin{equation}
        \det(\Lambda)^2 = 1 \implies \det(\Lambda) = \pm 1.
    \end{equation}
    We call Lorentz transformations with \(\det \Lambda = 1\) \define{proper Lorentz transformations}\index{proper Lorentz transformation}.
    They are analogous to proper rotations.
    
    \begin{dfn}{Proper Lorentz Group}{}
        The \defineindex{proper Lorentz group}, \(\specialOrthogonal(1, 3)\), is the subgroup of \(\orthogonal(1, 3)\) formed from all Lorentz transformations with unit determinant.
    \end{dfn}
    
    Consider the defining property of Lorentz transformations, written out in terms of components:
    \begin{equation}
        \minkowskiMetric_{\mu\nu} \tensor{\Lambda}{^\mu_\rho} \tensor{\Lambda}{^\nu_\sigma} = \minkowskiMetric_{\rho\sigma}.
    \end{equation}
    Setting \(\rho = \sigma = 0\) gives
    \begin{equation}
        \minkowskiMetric_{\mu\nu} \tensor{\Lambda}{^\mu_0} \tensor{\Lambda}{^\nu_0} = 1.
    \end{equation}
    The term on the left is zero unless \(\mu = \nu\), and so we get
    \begin{equation}
        (\tensor{\Lambda}{^0_0}) - \sum_i (\tensor{\Lambda}{^i_0})^2 = 1.
    \end{equation}
    Hence, we have \((\tensor{\Lambda}{^0_0})^2 \ge 1\), since \(\Lambda\) is a real matrix, so \(\sum_i(\tensor{\Lambda}{^i_0})^2 \ge 0\).
    Hence we either have \(\tensor{\Lambda}{^0_0} \ge 1\) or \(\tensor{\Lambda}{^0_0} \le -1\).
    We call Lorentz transformations with \(\tensor{\Lambda}{^0_0} \ge 1\) \defineindex{orthochronous}, since they preserve the direction of time.
    
    \begin{dfn}{Proper Orthochronous Lorentz Group}{}
        The \defineindex{proper orthochronous Lorentz group}, \(\specialOrthogonal^+(1, 3)\), is the subgroup of \(\specialOrthogonal(1, 3)\) formed from all Lorentz transformations, \(\Lambda\), with \(\tensor{\Lambda}{^0_0} \ge 1\).
    \end{dfn}
    
    It is common refer to the proper orthochronous Lorentz group as simply \emph{the Lorentz group}, since it is the group of symmetries that we require our theories to be invariant under.
    
    The Lorentz group \(\orthogonal(1, 3)\), has four connected components:
    \begin{itemize}
        \item \(\det \Lambda = +1\) and \(\tensor{\Lambda}{^0_0} \ge +1\);
        \item \(\det \Lambda = -1\) and \(\tensor{\Lambda}{^0_0} \ge +1\);
        \item \(\det \Lambda = -1\) and \(\tensor{\Lambda}{^0_0} \le -1\);
        \item \(\det \Lambda = +1\) and \(\tensor{\Lambda}{^0_0} \le -1\).
    \end{itemize}
    The first of these, which is \(\specialOrthogonal^+(1, 3)\), contains the identity.
    The second contains the parity operator, \(\parity \coloneqq \diag(1, -1, -1, -1)\).
    The third contains the time reversal operator, \(\timeReversal \coloneqq \diag(-1, 1, 1, 1)\).
    The fourth contains the product \(\parity \timeReversal = \timeReversal \parity = \diag(-1, -1, -1, -1)\).
    We can use these to move between the different components as follows:
    \begin{equation}
        \tikzexternaldisable
        \begin{tikzcd}[sep=2.5cm, arrows={<->}]
            ++ \arrow[r, "\displaystyle\timeReversal"] \arrow[d, "\displaystyle\parity"'] \arrow[dr, "\displaystyle\parity\timeReversal"'] & -- \arrow[d, "\displaystyle\parity"]\\
            -+ \arrow[r, "\displaystyle\timeReversal"] & +-
        \end{tikzcd}
        \tikzexternalenable
    \end{equation}
    where \(\pm\pm\) corresponds to the sign of \(\det \Lambda\) and then the sign of \(\tensor{\Lambda}{^0_0}\), and since \(\parity\) and \(\timeReversal\) are self inverses we can move either way along the arrow using them.
    
    This is analogous to how \(\orthogonal(n)\) has two disconnected components, one with \(\det R = +1\) and one with \(\det R = -1\), and we can move between them using \(-\ident_n\).
    As in this case elements of the disconnected parts of the Lorentz groups do not generally commute with each other.
    
    The proper orthochronous Lorentz group is formed from rotations and boosts.
    A general rotation is of the form
    \begin{equation}
        \Lambda_R = 
        \begin{pmatrix}
            1 & 0\\
            0 & R
        \end{pmatrix}
        =
        \left(
            \begin{array}{cccc}
                1 & 0 & 0 & 0 \\ \cline{2-4}
                0 & \multicolumn{3}{|c|}{} \\
                0 & \multicolumn{3}{|c|}{R} \\
                0 & \multicolumn{3}{|c|}{} \\ \cline{2-4}
            \end{array}
        \right).
    \end{equation}
    The property that \(\Lambda^\trans \Lambda = 1\) implies \(R^\trans R = 1\), and similarly \(\det \Lambda = 1\) implies \(\det R = 1\), so \(R \in \specialOrthogonal(3)\), which is how we interpret this as a rotation.
    It simply leaves time, the first coordinate, unchanged.
    There are three parameters needed to specify a rotation.
    For example, we can specify a unit vector with two (the third component being fixed by the normalisation condition) and then the size of the rotation is the third, or we can use three Euler angles to specify a rotation.
    If we go with the first specification then the parameters specifying two coordinates of a unit vector are taken from \([-1, 1]\), and the angle from \([0, 2\pi)\), identifying \(0\) and \(2\pi\).
    Similarly in the third case the Euler angles are all taken from closed intervals, exactly which interval depending on you convention for defining Euler angles.
    Either way we see that rotations form a compact subgroup of the Lorentz group, and this subgroup is (isomorphic to) \(\specialOrthogonal(3)\).
    
    A general boost is of the form
    \begin{equation*}
        \Lambda_{\vartheta, \vh{n}} = 
        \begin{pmatrix}
            \cosh(\vartheta) & \sinh(\vartheta) \vh{n}^\trans\\
            \sinh(\vartheta) \vh{n}  & \ident_3 + \cosh(\vartheta) \vh{n} \vh{n}^\trans
        \end{pmatrix}
        =
        \begin{pmatrix}
            c & n_1 s & n_2 s & n_3 s \\
            s n_1 & 1 + c n_1^2 & cn_1n_2 & cn_1n_3 \\
            s n_2 & n_2n_1 & 1 + n_2^2 & n_2n_3 \\
            s n_3 & n_3n_1 & n_3n_2 & 1 + n_3^2
        \end{pmatrix}
    \end{equation*}
    where \(c = \cosh \vartheta\) and \(s = \sinh \vartheta\).
    Here \(\vh{n}\) specifies the direction of the boost, so \(\vh{n} = \vh{x}\) for the standard Lorentz boost, and \(\vartheta\) is a parameter called the \defineindex{rapidity}, defined as \(\artanh(v/c)\).
    While the components defining \(\vh{n}\) are bounded between \([-1, 1]\) the rapidity is unbounded, since \(\artanh\) maps the interval \([0, 1]\) to \([0, \infty]\).
    This means that the boosts form a noncompact subgroup of the Lorentz group.
    
    A general Lorentz transformation can then be written as a combination of a rotation and a boost, \(\Lambda_R\Lambda_B\).
    There are then 6 parameters needed to specify this Lorentz transformation, the simplest being 3 specifying \(\Lambda_R\) and 3 specifying \(\Lambda_B\), as discussed above.
    The Lorentz group is noncompact, since it has a noncompact subgroup (the boosts).
    
    \chapter{Lorentz Algebra}
    \section{Deriving the Algebra}
    The Lorentz group is a Lie group, and the component connected to the identity is the proper orthochronous Lorentz group, \(\specialOrthogonal^+(1, 3)\).
    The Lie algebra of the full Lorentz group and this subgroup coincide, as does the Lie algebra of the in-between-subgroup \(\specialOrthogonal(1, 3)\), which is analogous to \(\orthogonal(n)\) and \(\specialOrthogonal(n)\) having the same Lie algebras.
    
    We can derive the Lorentz algebra, \(\specialOrthogonalLie^+(1, 3)\), in the same way as we did for the other matrix groups.
    We consider some Lorentz transformation, \(\Lambda \in \specialOrthogonal^+(1, 3)\), close to the identity and expand.
    It's easier to do this working with the components, rather than the full matrices, so in our expansion the identity matrix becomes \(\tensor{\delta}{^\mu_\nu}\) and we have
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = \tensor{\delta}{^\mu_\nu} + \tensor{\omega}{^\mu_\nu} + \order(\omega^2)
    \end{equation}
    where \(\tensor{\omega}{^\mu_\nu}\) are the components of some matrix and \(\abs{\tensor{\omega}{^\mu_\nu}} \ll 1\).
    
    We now use the defining property of a Lorentz transformation,
    \begin{equation}
        \minkowskiMetric_{\mu\nu} = \minkowskiMetric_{\sigma\rho} \tensor{\Lambda}{^\sigma_\mu} \tensor{\Lambda}{^\rho_\nu},
    \end{equation}
    and expanding this we get
    \begin{align}
        \minkowskiMetric_{\mu\nu} &= \minkowskiMetric_{\sigma\rho} \tensor{\Lambda}{^\sigma_\mu} \tensor{\Lambda}{^\rho_\nu}\\
        &= \minkowskiMetric_{\sigma\rho}(\tensor{\delta}{^\sigma_\mu} + \tensor{\omega}{^\sigma_\mu} + \order(\omega^2)) (\tensor{\delta}{^\rho_\nu} + \tensor{\omega}{^\rho_\nu} + \order(\omega^2))\\
        &= \minkowskiMetric_{\sigma\rho}(\tensor{\delta}{^\sigma_\mu}\tensor{\delta}{^\rho_\nu} + \tensor{\delta}{^\sigma_\mu} \tensor{\omega}{^\rho_\nu} + \tensor{\omega}{^\sigma_\mu}\tensor{\delta}{^\rho_\nu} + \order(\omega^2))\\
        &= \minkowskiMetric_{\mu\nu} + \minkowskiMetric_{\mu\rho}\tensor{\omega}{^\rho_\nu} + \minkowskiMetric_{\sigma\nu}\tensor{\omega}{^\sigma_\mu} + \order(\omega^2)\\
        &= \minkowskiMetric_{\mu\nu} + \omega_{\mu\nu} + \omega_{\nu\mu} +\order(\omega^2).
    \end{align}
    So, we see that \(\omega\) must be antisymmetric, \(\omega_{\mu\nu} = -\omega_{\nu\mu}\), so that the two linear terms in \(\omega\) cancel.
    
    \section{Exponentiating the Lorentz Algebra}
    We start with the assumption that a general infinitesimal Lorentz transformation, \(\Lambda\), can be written as
    \begin{equation}
        \tensor{\Lambda}{^\gamma_\delta} = \tensor{\delta}{^\gamma_\delta} + i\frac{\omega^{\alpha\beta}}{2}\tensor{(M_{\alpha\beta})}{^\gamma_\delta}
    \end{equation}
    where \(M_{\alpha\beta}\) is a generator of the Lie algebra of Lorentz transformations and is labelled by \(\alpha\) and \(\beta\), and above we look at the component in row \(\gamma\) and column \(\delta\).
    Think of \(M_{\alpha\beta}\) as being \(T_a\), but we use two labels, \(\alpha\) and \(\beta\), instead of just one, \(a\), because this will help us interpret the generators later.
    Here \(\omega_{\alpha\beta}\) is just some parameter, like \(\alpha^a\) in a generic Lie algebra.
    In terms of matrices, rather than components, we can write this as
    \begin{equation}
        \Lambda = \ident + i\frac{\omega^{\alpha\beta}}{2}M_{\alpha\beta}.
    \end{equation}
    We previously wrote an infinitesimal Lorentz transformation as
    \begin{equation}
        \tensor{\Lambda}{^\gamma_\delta} = \tensor{\delta}{^\gamma_\delta} + \tensor{\omega}{^\gamma_\delta},
    \end{equation}
    so we have
    \begin{equation}
        \tensor{\omega}{^\gamma_\delta} = i\frac{\omega^{\alpha\beta}}{2}\tensor{(M_{\alpha\beta})}{^\gamma_\delta}.
    \end{equation}
    We can solve this to show that in the defining representation
    \begin{equation}
        \tensor{(M_{\alpha\beta})}{^\gamma_\delta} = -i(\tensor{\delta}{^\gamma_\alpha}\minkowskiMetric_{\beta\delta} - \tensor{\delta}{^\gamma_\beta}\minkowskiMetric_{\alpha\delta}).
    \end{equation}
    Notice that this is antisymmetric in \(\alpha\) and \(\beta\), which makes sense since \(\omega^{\alpha\beta}\) is also antisymmetric in \(\alpha\) and \(\beta\) so no symmetric part of \(M_{\alpha\beta}\) can contribute.
    This means there are only \(6\) independent generators, \(M_{\alpha\beta}\).
    This also explains the choice of the factor of 2, we don't want to double count.
    
    Consider the case where \(\alpha\) and \(\beta\) are taken as spatial indices \(i\) and \(j\), and we consider the spatial components of \(M_{ij}\).
    From the definition we find that
    \begin{align}
        \tensor{(M_{ij})}{^k_l} &= -i(\tensor{\delta}{^k_i} \minkowskiMetric_{jl} - \tensor{\delta}{^k_j} \minkowskiMetric_{il})\\
        &= -i(-\delta_{ki}\delta_{jl} + \delta_{kj}\delta_{il})\\
        &= i(\delta_{ki}\delta_{jl} - \delta_{kj}\delta_{il}).
    \end{align}
    notice that this is antisymmetric under exchange of \(k\) and \(l\), and so accounting for the factor of \(i\) we see that \(M_{ij}\) are Hermitian.
    This is what we would expect for the generators of a rotation.
    
    Similarly we can consider the case when \(\alpha = 0\), and \(\beta\) is spatial:
    \begin{align}
        \tensor{(M_{0i})}{^\gamma_\delta} &= -i(\tensor{\delta}{^\gamma_0} \minkowskiMetric_{i\delta} - \tensor{\delta}{^\gamma_i} \minkowskiMetric_{0\delta})\\
        &= -i(-\tensor{\delta}{^\gamma_0} \delta_{i\delta} - \tensor{\delta}{^\gamma_i}\delta_{0\delta})\\
        &= i(\tensor{\delta}{^\gamma_0} \delta_{i\delta} + \tensor{\delta}{^\gamma_i}\delta_{0\delta}).
    \end{align}
    Notice that this is symmetric under exchange of \(\gamma\) and \(\delta\), so accounting for the factor of \(i\) we find that \(M_{0i}\) are anti-Hermitian.
    This is what we would expect for the generators of boosts.
    
    From these results we see that
    \begin{equation}
        \Lambda = \exp\left[ i\frac{\omega^{ij}}{2} M_{ij} \right]
    \end{equation}
    is unitary.
    We also see that the space of real parameters \(\omega^{ij}\) is compact, since the group element, \(\Lambda\), is a periodic function of \(\omega^{ij}\).
    On the other hand
    \begin{equation}
        \Lambda = \exp\left[ i\frac{\omega^{0i}}{2}M_{0i} \right]
    \end{equation}
    is not unitary and the space of parameters is not compact.
    
    This seems like a problem.
    We want to be able to do relativistic quantum mechanics.
    However, quantum mechanics requires unitary transformations, and we've seen here that the Lorentz group is not compact, and therefore isn't guaranteed to have any nontrivial finite dimensional faithful\footnote{A \defineindex{faithful representation} is one in which the homomorphism \(D \colon G \to \generalLinear(V)\) is injective.} unitary representations, and in fact doesn't.
    We'll come back to this later.
    
    \section{The Lie Bracket of the Lorentz Algebra}
    Now that we've seen what elements of the Lorentz algebra are we should work out what the Lie bracket is.
    We could just work in the defining representation and compute commutators of \(M_{\alpha\beta}\), but there's a less brute-force method.
    Consider some infinitesimal Lorentz transformation \(\Lambda\) and also some other infinitesimal Lorentz transformation which we can write as \(\ident + \omega\).
    Then consider the following, where we make use of the fact that a representation is a homomorphism:
    \begin{align}
        D(\Lambda)D(1 + \omega) D(\Lambda)^{-1} &= D(\Lambda)D(1 + \omega)D(\Lambda^{-1})\\
        &= D(\Lambda(1 + \omega)\Lambda^{-1})\\
        &= D(1 + \Lambda\omega\Lambda^{-1})\\
        &= D(1 + \Lambda\omega\Lambda^{\trans}).
    \end{align}
    In the last step we used the fact that for an infinitesimal Lorentz transformation \(\Lambda^{-1} = \Lambda^\trans\), or in terms of components \((\Lambda^{-1})^{\mu\nu} = (\Lambda^{\trans})^{\nu\mu}\).
    This follows from the defining relation, \(\Lambda^\trans \minkowskiMetric \Lambda = \minkowskiMetric\), we can take the inverse of to get
    \begin{equation}
        \Lambda^{-1}\minkowskiMetric^{-1}(\Lambda^{\trans})^{-1} = \Lambda^{-1}\minkowskiMetric(\Lambda^\trans)^{-1} = \eta,
    \end{equation}
    which follows since the inverse of a Lorentz transformation is also a Lorentz transformation.
    Multiplying by \(\Lambda^\trans\) on the right we get
    \begin{equation}
        \Lambda^{-1}\minkowskiMetric = \minkowskiMetric \Lambda^\trans.
    \end{equation}
    In terms of components this is
    \begin{equation}
        \tensor{(\Lambda^{-1})}{^\nu_\rho} \minkowskiMetric^{\rho\mu} = \minkowskiMetric^{\nu\rho} \tensor{(\Lambda^\trans)}{^\mu_\rho} \implies (\Lambda^{-1})^{\mu\nu} = (\Lambda^{\trans})^{\mu\nu}.
    \end{equation}
    
    By definition we have
    \begin{equation}
        D(1 + \omega) = 1 + i\frac{\omega^{\alpha\beta}}{2}M_{\alpha\beta} + \order(\omega^2).
    \end{equation}
    Hence,
    \begin{align}
        D(\Lambda)D(1 + \omega)D(\Lambda)^{-1} &= D(\Lambda)\left[ 1 + i \frac{\omega^{\alpha\beta}}{2}M_{\alpha\beta} \right]D(\Lambda)^{-1}\\
        &= 1 + i\frac{\omega^{\alpha\beta}}{2}D(\Lambda)M_{\alpha\beta}D(\Lambda)^{-1}.
    \end{align}
    Hence, we have
    \begin{equation}
        D(1 + \Lambda\omega\Lambda^\trans) = 1 + \frac{i}{2}\underbrace{\tensor{\Lambda}{^\alpha_\mu}\omega^{\mu\nu}\tensor{\Lambda}{^\beta_\nu}}_{(\Lambda\omega\Lambda^\trans)^{\alpha\beta}}M_{\alpha\beta}.
    \end{equation}
    This must be true for any choice of our parameters, so, comparing our two results and renaming some indices we have
    \begin{equation}\label{eqn:deriving commutator in Lorentz algebra}
        D(\Lambda)M_{\alpha\beta}D(\Lambda)^{-1} = \tensor{\Lambda}{^\mu_\alpha}\tensor{\Lambda}{^\nu_\beta}M_{\mu\nu}.
    \end{equation}
    That is, \(M_{\alpha\beta}\) transforms as a rank 2 tensor.
    
    Now consider some Lorentz transformation which we can write as
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = \tensor{\delta}{^\mu_\nu} + \tensor{\varepsilon}{^\mu_\nu} + \order(\varepsilon^2).
    \end{equation}
    We then have
    \begin{equation}
        D(\Lambda) = 1 + i\frac{\varepsilon^{\mu\nu}}{2}M_{\mu\nu}, \qqand D(\Lambda)^{-1} = 1 - i\frac{\varepsilon^{\mu\nu}}{2}M_{\mu\nu}.
    \end{equation}
    Substituting this into the left hand side of \cref{eqn:deriving commutator in Lorentz algebra} we get
    \begin{align}
        D(\lambda)M_{\alpha\beta}D(\Lambda) &= \left[ 1 + i\frac{\varepsilon^{\mu\nu}}{2}M_{\mu\nu} \right] M_{\alpha\beta} \left[ 1 - i\frac{\varepsilon^{\mu\nu}}{2}M_{\mu\nu} \right]\\
        &= M_{\alpha\beta} + \frac{i}{2}\varepsilon^{\mu\nu}M_{\mu\nu}M_{\alpha\beta} - \frac{i}{2}\varepsilon^{\mu\nu}M_{\alpha\beta}M_{\mu\nu} + \order(\varepsilon^2)\\
        &= M_{\alpha\beta} + \frac{i}{2}\varepsilon^{\mu\nu}\commutator{M_{\mu\nu}}{M_{\alpha\beta}} + \order(\varepsilon^2).
    \end{align}
    Substituting \(\tensor{\Lambda}{^\mu_\nu} = \tensor{\delta}{^\mu_\nu} + \tensor{\varepsilon}{^\mu_\nu}\) into the right hand side we get
    \begin{align}
        \tensor{\Lambda}{^\mu_\alpha}\tensor{\Lambda}{^\nu_\beta}M_{\mu\nu} &= (\tensor{\delta}{^\mu_\alpha} + \tensor{\varepsilon}{^\mu_\nu}) (\tensor{\delta}{^\nu_\beta} + \tensor{\varepsilon}{^\nu_\beta})M_{\mu\nu}\\
        &= M_{\alpha\beta} + \tensor{\delta}{^\mu_\alpha}\tensor{\varepsilon}{^\nu_\beta}M_{\mu\nu} + \tensor{\delta}{^\nu_\beta}\tensor{\varepsilon}{^\mu_\nu}M_{\mu\nu} + \order(\varepsilon^2)\\
        &= M_{\alpha\beta} + \tensor{\varepsilon}{^\nu_\beta}M_{\alpha\nu} + \tensor{\varepsilon}{^\mu_\nu}M_{\mu\beta} + \order(\varepsilon^2).
    \end{align}
    Now we can use the antisymmetry of \(\varepsilon^{\mu\nu}\) to write
    \begin{equation}
        \tensor{\varepsilon}{^\nu_\beta} = \varepsilon^{\nu\gamma}\minkowskiMetric_{\gamma\beta} = -\varepsilon^{\gamma\nu}\minkowskiMetric_{\gamma\beta} = -\tensor{\varepsilon}{_\gamma^\nu}
    \end{equation}
    and so
    \begin{equation}
        \tensor{\Lambda}{^\mu_\alpha}\tensor{\Lambda}{^\nu_\beta}M_{\mu\nu} = M_{\alpha\beta} + \tensor{\varepsilon}{^\mu_\nu} M_{\mu\beta} - \tensor{\varepsilon}{_\beta^\nu}M_{\alpha\nu}.
    \end{equation}
    We can then identify
    \begin{equation}
        \frac{i}{2}\varepsilon^{\mu\nu}\commutator{M_{\mu\nu}}{M_{\alpha\beta}} = \tensor{\varepsilon}{^\mu_\nu} M_{\mu\beta} - \tensor{\varepsilon}{_\beta^\nu}M_{\alpha\nu}.
    \end{equation}
    The right hand side can be rewritten as
    \begin{equation}
        \frac{1}{2}\varepsilon^{\mu\nu}(M_{\mu\beta} \minkowskiMetric_{\nu\alpha} - M_{\nu\beta}\minkowskiMetric_{\mu\alpha} - M_{\alpha\nu}\minkowskiMetric_{\mu\beta} + M_{\alpha\mu}\minkowskiMetric_{\nu\beta})
    \end{equation}
    which can be shown by expanding this and using the antisymmetry of \(\varepsilon^{\mu\nu}\).
    Note that this result is antisymmetric in \(\mu\) and \(\nu\), as well as \(\alpha\) and \(\beta\).
    This must hold for all \(\varepsilon^{\mu\nu}\) and so
    \begin{equation}
        \commutator{M_{\alpha\beta}}{M_{\mu\nu}} = i(M_{\alpha_\mu} \minkowskiMetric_{\beta\nu} - M_{\alpha\nu} \minkowskiMetric_{\beta\mu} - M_{\beta\mu} \minkowskiMetric_{\alpha\nu} + M_{\beta\nu} \minkowskiMetric_{\alpha\mu}).
    \end{equation}
    Note that we can simply remember the first term here and then the other terms follow by antisymmetrising \(\mu\) and \(\nu\), as well as \(\alpha\) and \(\beta\).
    This Lie bracket defines the \(\specialOrthogonalLie(1, 3)\) Lie algebra of the Lorentz group, \(\specialOrthogonal^+(1, 3)\).
    
    \section{Rotations and Boosts}
    We can write the generators of the Lorentz algebra in a more familiar way by first considering 
    \begin{equation}
        J_i = -\frac{1}{2}\varepsilon_{ijk}M_{jk}.
    \end{equation}
    The commutation relations for \(J_i\) can be computed using those for \(M_{jk}\):
    \begin{align}
        \commutator{J_i}{J_j} &= \frac{1}{4} \varepsilon_{imn}\varepsilon_{jpq} \commutator{M_{mn}}{M_{pq}}\\
        &= \frac{i}{4}\varepsilon_{imn}\varepsilon_{jpq} (M_{mp}\minkowskiMetric_{nq} - M_{mq}\minkowskiMetric_{np} - M_{np}\minkowskiMetric_{mq} + M_{nq}\minkowskiMetric_{mp})\\
        &= -\frac{i}{4}\varepsilon_{imn}\varepsilon_{jpq} (M_{mp}\delta_{nq} - M_{mq}\delta_{np} - M_{np}\delta_{mq} + M_{nq}\delta_{mp})\\
        &= -\frac{1}{4}(\varepsilon_{imq}\varepsilon_{jpq}M_{mp} - \varepsilon_{imp}\varepsilon_{jpq}M_{mq} - \varepsilon_{iqn}\varepsilon_{jpq}M_{np} + \varepsilon_{ipn}\varepsilon_{jpq}M_{nq}) \notag\\
        &= -\frac{1}{4}((\delta_{ij}\delta_{mp} - \delta_{ip}\delta_{mj})M_{mp} + (\delta_{ij}\delta_{mq} - \delta_{mj}\delta_{iq})M_{mq}\\
        &\quad+ (\delta_{ij}\delta_{np} - \delta_{ip}\delta_{nj})M_{np} + (\delta_{ij}\delta_{nq} - \delta_{iq}\delta_{nj})M_{nq})\\
        &= -i(\delta_{ij}\delta_{mp} - \delta_{ip}\delta_{mj})M_{mp}\\
        &= -i(\delta_{ij}M_{mm} - M_{ji})\\
        &= -iM_{ij}\\
        &= i\varepsilon_{ijk}J_k
    \end{align}
    where in the last step we use \(M_{ij} = -\varepsilon_{ijk}J_{k}\), which follows from considering
    \begin{multline}
        -\frac{1}{2}\varepsilon_{ijk}J_k = -\frac{1}{2}\varepsilon_{ijk}\varepsilon_{klm}M_{lm}\\
        = -\frac{1}{2}(\delta_{il}\delta_{jm} - \delta_{im}\delta_{lj})M_{lm} = -\frac{1}{2}M_{ij} + \frac{1}{2}M_{ji} = M_{ij}.
    \end{multline}
    
    So, we find that
    \begin{equation}
        \commutator{J_i}{J_j} = i\varepsilon_{ijk}J_k,
    \end{equation}
    which is exactly the Lie algebra of \(\specialUnitaryLie(2) \isomorphic \specialOrthogonalLie(3)\).
    So, we can interpret \(J_i\) as the generators of rotations.
    
    If we similarly let
    \begin{equation}
        K_i = M_{0i}
    \end{equation}
    then we can interpret these as the generators of boosts.
    Similar calculations give the result that
    \begin{equation}
        \commutator{K_i}{K_j} = \commutator{M_{0i}}{M_{0j}} = iM_{ij} = -i\varepsilon_{ijk}J_k.
    \end{equation}
    We can also compute the commutator of a rotation and boost:
    \begin{multline}
        \commutator{J_i}{K_j} = -\frac{1}{2}\varepsilon_{imn} \commutator{M_{mn}}{M_{0j}}\\
        = -\frac{1}{2}\varepsilon_{imn}i(M_{m0}\minkowskiMetric_{nj} - M_{n0}\minkowskiMetric_{mj}) = i\varepsilon_{ijk}M_{0k} = i\varepsilon_{ijk}K_k.
    \end{multline}
    
    \section{Casimirs}
    We can construct Casimir operators for \(\specialOrthogonalLie(1, 3)\) with products of the generators and the invariant tensors, \(\delta_{ij}\) and \(\varepsilon_{ijk}\).
    There are two linearly independent Casimir operators, meaning \(\specialOrthogonalLie(1, 3)\) has rank 2.
    One possible basis for the Casimir operators is\footnote{Compare this to the products \(F^{\mu\nu}F_{\mu\nu}\) and \(F^{\mu\nu}F^*_{\mu\nu}\) in \course{Classical Electrodynamics}.}
    \begin{equation}
        \frac{1}{2} M_{\mu\nu}M^{\mu\nu} = \vv{J}^2 - \vv{K}^2, \qqand \frac{1}{2}\varepsilon_{\mu\nu\rho\sigma} M^{\mu\nu}M^{\rho\sigma} = -\vv{J} \cdot \vv{K}.
    \end{equation}
    
    
    \chapter{Relation to \texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)}}
    \section{Decoupling the Algebra}
    We've seen that \(\specialOrthogonalLie(1, 3)\) is generated by \(\{J_i, K_i\}\).
    These are, in a sense, coupled, in that Lie brackets of \(K_i\)s give \(J_i\)s.
    We can decouple them by defining two new operators
    \begin{align}
        N_i &\coloneqq \frac{1}{2}(J_i + i K_i),\\
        N_i^\hermit &= \frac{1}{2}(J_i - i K_i).
    \end{align}
    Note that, since \(K_i\) is not Hermitian, \(N_i^\hermit\) is \emph{not} the Hermitian conjugate of \(N_i\).
    These are linearly independent combinations of \(J_i\) and \(K_i\) so also span the Lie algebra \(\specialOrthogonalLie(1, 3)\).
    
    These decoupled generators have the commutation relations
    \begin{equation}
        \commutator{N_i}{N_j} = i\varepsilon_{ijk}N_k, \qquad \commutator{N_i^\hermit}{N_j^\hermit} = i\varepsilon_{ijk}N_k^\hermit, \qqand \commutator{N_i}{N_j^\hermit} = 0.
    \end{equation}
    We can identify the subalgebras generated by \(N_i\) and \(N_i^\hermit\) separately as two copies of \(\specialUnitaryLie(2)\) which don't interact.
    This means that \(\specialOrthogonalLie(1, 3)\) is a direct sum of \(\specialUnitaryLie(2)\) algebras:
    \begin{equation}
        \specialOrthogonalLie(1, 3) \isomorphic \specialUnitaryLie(2) \oplus \specialUnitaryLie(2).
    \end{equation}
    
    While these two copies of \(\specialUnitaryLie(2)\) don't interact they aren't completely independent, since they're related by parity transformations given by taking the Hermitian conjugate, we have \(J_i \mapsto J_i\), \(K_i \mapsto -K_i\), and so \(N_i \mapsto N_i^\hermit\).
    
    \section{\texorpdfstring{\(\specialLinearLie(2, \complex) \isomorphic \specialOrthogonalLie(1, 3)\)}{sl(2, C) isomorphic to so(1, 3)}}
    Consider the Lie algebra \(\specialLinearLie(2, \complex)\).
    This consists of complex \(2 \times 2\) traceless matrices.
    One basis generating this algebra is \(\{\tfrac{1}{2}\sigma_i, -\tfrac{i}{2}\sigma_i\}\), where \(\sigma_i\) are the Pauli matrices.
    After some playing around we find that \(\sigma_i/2\) have the same commutation relations as \(J_i\) and \(-i\sigma_i/2\) have the same commutation relations as \(K_i\).
    This means that
    \begin{equation}
        \specialOrthogonalLie(1, 3) \isomorphic \specialLinearLie(2, \complex).
    \end{equation}
    
    This extends to an isomorphism of groups:
    \begin{equation}
        \specialOrthogonal(1, 3) \isomorphic \specialLinear(2, \complex) / \cyclicGroupZ[2].
    \end{equation}
    This is analogous to \(\specialOrthogonal(n) \isomorphic \orthogonal(n)/\cyclicGroupZ[2]\) in \cref{exm:SO(n) = O(n)/Z2}.
    
    We can construct an explicit map \(\specialOrthogonal(1, 3) \to \specialLinear(2, \complex)\) by considering the four-vectors \(\sigma_\mu = (1, \vv{\sigma})\) and \(\overbar{\sigma}_\mu = (1, -\vv{\sigma})\), then defining
    \begin{equation}
        X = x^\mu \sigma_\mu = 
        \begin{pmatrix}
            x^0 + x^3 & x^1 - ix^2\\
            x^1 + ix^2 & x^0 - x^3
        \end{pmatrix}
        .
    \end{equation}
    Consider the determinant of this matrix:
    \begin{align}
        \det X &= (x^0 + x^3)(x^0 - x^3) - (x^1 - ix^2)(x^1 + ix^2)\\
        &= (x^0)^2 - (x^1)^2 - (x^2)^2 - (x^3)^2\\
        &= x \cdot x.
    \end{align}
    So, \(\det X\) is invariant under Lorentz transformations of \(x\).
    
    We now study the transformation of \(X\) under a Lorentz transformation of \(x\).
    As a \(2 \times 2\) complex matrix \(X\) will transform under a subgroup of \(\generalLinear(2, \complex)\), and transforms as
    \begin{equation}
        X \mapsto A(\Lambda) X A^\hermit(\Lambda)
    \end{equation}
    where \(A(\Lambda)\) is a \(2 \times 2\) complex matrix depending on the Lorentz transformation, \(\Lambda\), applied to \(x\).
    Since \(\det X\) doesn't change we have
    \begin{equation}
        \det(AXA^\hermit) = \det(A)\det(X)\det(A^\hermit) = \det(X).
    \end{equation}
    We can see that this is certainly satisfied if \(A \in \specialLinear(2, \complex)\), and so \(\det A = 1\) and \(\det A^\hermit = (\det A^\trans)^* = (\det A)^* = 1^*\).
    So, we define a map \(A \colon \specialOrthogonal(1, 3) \to \specialLinear(2, \complex)\).
    We can also \enquote{invert} this map by defining
    \begin{equation}
        \tensor{\Lambda}{^\mu_\nu} = \frac{1}{2} \tr(\overbar{\sigma}^\mu A \sigma_\nu A^\hermit).
    \end{equation}
    This has two solutions for \(A\):
    \begin{equation}
        A = \pm \frac{\sigma_\mu \tensor{\Lambda}{^\mu_\nu}\overbar{\sigma}^\nu}{\sqrt{\tensor{\Lambda}{^\mu_\mu}}}.
    \end{equation}
    It is this ambiguous \(\pm\) which is modded out by the \(\cyclicGroupZ[2]\) factor.
    
    \backmatter
%    \renewcommand{\glossaryname}{Acronyms}
%    \printglossary[acronym]
    \printindex
\end{document}
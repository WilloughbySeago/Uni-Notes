% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

%% Packages
\usepackage{xfrac}
\usepackage{csquotes}
\usepackage{multienum}
\newenvironment{multiitem}{%
    \multienumerate\renewcommand{\labelname}{\textbullet}%
}{%
    \endmultienumerate%
}

% Tikz stuff
\usepackage{tikz}
%\tikzset{>=latex}
% External
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
% Other libraries
\undef{\eth}
\undef{\digamma}
\undef{\backepsilon}
\usetikzlibrary{positioning, decorations.pathreplacing, calligraphy, calc, shapes.geometric}
\usepackage{tikz-cd}

\BeforeBeginEnvironment{tikzcd}{\tikzexternaldisable}
\AfterEndEnvironment{tikzcd}{\tikzexternalenable}

% ZX calculus
\tikzset{Z spider/.style={fill=Green!50, draw=Green, text=black, circle, minimum width=1mm, inner sep=2pt, thick}}
\tikzset{X spider/.style={fill=Red!50, draw=Red, text=black, circle, minimum width=1mm, inner sep=2pt, thick}}

% Graphical calculus
\tikzset{wire/.style={thick}}
\tikzset{morphism/.style={thick, draw, inner sep=2pt}}
\tikzset{triangle/.style={regular polygon, regular polygon sides=3}}
\tikzset{state/.style={morphism, triangle, shape border rotate=180}}
\tikzset{effect/.style={morphism, triangle}}

\RequirePackage[%
sorting=none,  % Don't sort the references, they will appear in the order they are first cited
style=numeric-comp,  % citations like [1] and citing 1, 2, and 3 gives [1-3]
giveninits=true,  % style author names as J. Doe
language=british  % Make dates dd/mm/yyyy
]{biblatex}
\addbibresource{references.bib}
\usepackage{xurl}

% References, should be last things loaded
\usepackage[pdfauthor={Willoughby Seago},pdftitle={Categories and Quantum Information},pdfkeywords={category theory, quantum information, quantum computing, monoidal category},pdfsubject={Gauge Theories}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}

% Highlight colour
\definecolor{Red}{HTML}{D00000}
\definecolor{Yellow}{HTML}{FFBA08}
\definecolor{Blue}{HTML}{3F88C5}
\definecolor{Navy}{HTML}{032B43}
\definecolor{Green}{HTML}{136F63}

\colorlet{highlight}{Green}

\colorlet{codeTextColor}{Navy}

% Title page info
\title{Categories and Quantum Informatics}
\author{Willoughby Seago}
\date{January 17, 2023}
% \subtitle{}
% \subsubtitle{}

% Commands
\lstdefinestyle{haskell}{
    language=haskell,
    literate=
        {->}{$\to$}{1}
        {=>}{$\Rightarrow$}{1}
}

% Text
\newcommand*{\course}[1]{\textit{#1}}
\newcommand{\Haskell}{\textit{Haskell}}

% Maths
\newcommand{\e}{\symrm{e}}
\DeclarePairedDelimiter{\denotes}{\lBrack}{\rBrack}
\newcommand{\cat}[1]{\symbfup{#1}}
\newcommand{\category}[1]{\symbfup{#1}}
\newcommand{\Set}{\category{Set}}
\newcommand{\Rel}{\category{Rel}}
\renewcommand{\field}{\symbb{k}}
\newcommand{\Mat}[1][\field]{{\category{Mat}_{#1}}}
\newcommand{\Vect}[1][\field]{{\category{Vect}_{#1}}}
\newcommand{\FVect}[1][\field]{{\category{FVect}_{#1}}}
\newcommand{\Hilb}{\category{Hilb}}
\newcommand{\FHilb}{\category{FHilb}}
\newcommand{\Mon}{\category{Mon}}
\newcommand{\Grp}{\category{Grp}}
\newcommand{\Ring}{\category{Ring}}
\newcommand{\CRing}{\category{CRing}}
\newcommand{\Field}{\category{Field}}
\newcommand{\RMod}[1][R]{#1{-}\category{Mod}}
\newcommand{\Top}{\category{Top}}
\newcommand{\pointedTop}{\category{Top}_{\bullet}}
\newcommand{\Cat}{\category{Cat}}
\newcommand{\Ab}{\category{Ab}}
\newcommand{\Pos}{\category{Pos}}
\newcommand{\LieGrp}{\category{LieGrp}}
\newcommand{\LieAlg}{\category{LieAlg}}
\newcommand{\one}{\category{1}}
\DeclareMathOperator{\Ob}{Ob}
\DeclareMathOperator{\id}{id}
\newcommand{\isomorphic}{\cong}
\renewcommand{\ve}[1]{e_{#1}}
\newcommand{\topology}{\symcal{T}}
\newcommand{\powerset}{\symcal{P}}
\newcommand{\phantomrlap}[2]{\mathrlap{#1}\phantom{#2}}
\newcommand{\naturalTransformation}{\Rightarrow}
\DeclarePairedDelimiterX{\functorCategory}[2]{[}{]}{#1, #2}
\newcommand{\hermit}{\dagger}
\undef\det
\DeclareMathOperator{\det}{det}
\AtBeginDocument{\renewcommand{\mapstochar}{\rule[0.5pt]{0.5pt}{4.3pt}\mkern-1mu}}

\includeonly{parts/maths-defs}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{}
    \tableofcontents
    \listoffigures
    \mainmatter
    
    \chapter{Introduction}
    The material delivered in the lectures is included in these notes, but so is extra material pulled from the textbook \cite{heunen}, as well as other sources, such as \cite{leinster}.
    I include various additional examples, some which require some mathematical background to understand.
    Some definitions are included in the appendices, but those unfamiliar with the material in these examples should feel free to just skip them.
    I've also included pointers to notes from other courses where appropriate, particularly the courses \course{Symmetries of Quantum Mechanics} and \course{Symmetries of Particles and Fields}, which both cover the areas of representation theory and Lie theory.
    The notes from these courses and others can be found at \url{https://github.com/WilloughbySeago/Uni-Notes}.
    Again, any unfamiliar material from these courses can be skipped.
    A fair few of the examples simply come from relevant Wikipedia pages, and I haven't performed detailed checks of all the facts in these cases.
    
    There are a few notational things which don't align with the course.
    A big one is leaving out brackets for functors, writing \(FA\) and \(Ff\) instead of \(F(A)\) and \(F(f)\).
    The use of \(-\) as a blank to be filled in with some object is also not used in the course.
    
    \part{Introduction}
    
    \chapter{ZX Calculus}
    In this chapter we will introduce \defineindex{ZX calculus}.
    This is a diagramatic notation for performing calculations.
    ZX calculus is mathematically rigorous, and developing the maths explaining this is a large part of this course.
    ZX calculus provides a higher level of abstraction that a quantum circuit, focusing less on implementation and more on what the circuit is doing.
    ZX calculus is built from a relatively small number of building blocks.
    It is the freedom we have in combining these that makes ZX calculus so powerful.
    
    We'll introduce ZX calculus in a seemingly backwards manner, first introducing which sorts of diagrams we can have, then how to manipulate the diagrams then what the diagrams mean.
    
    \section{Types of Diagrams}
    A diagram in ZX calculus is somewhat like a flowchart.
    The playing field is the two-dimensional page.
    We imagine that time goes upwards and space extends to the left and right.
    This means that a process described by a ZX calculus starts by entering the bottom of the diagram and ends when we leave the top of the diagram.
    Qubits are represented by wires, which are just lines.
    Processes are represented by boxes, for now we won't focus on what the process might be.
    The following diagram represents a process which takes in three qubits and produces two qubits:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-example-process}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, 0) -- ++ (1.5, 0) -- ++ (-0.25, 0.75) -- ++ (-1.25, 0) -- cycle;
            \draw (0.3, 0) -- ++ (0, -1);
            \draw (0.75, 0) -- ++ (0, -1);
            \draw (1.2, 0) -- ++ (0, -1);
            \draw (0.4, 0.75) -- ++ (0, 1);
            \draw (0.85, 0.75) -- ++ (0, 1);
        \end{tikzpicture}
        .
    \end{equation}
    
    In diagrams it isn't important exactly how we draw the wires, so long as they are connected in the same way, so in the same order both on the box and along the top and bottom, the diagram corresponds to the same equation.
    For example, the following is equivalent to the previous diagram
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-example-process-with-weird-wires}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, 0) -- ++ (1.5, 0) -- ++ (-0.25, 0.75) -- ++ (-1.25, 0) -- cycle;
            \draw (0.3, 0) -- ++ (0, -0.25) arc (0:-180:0.3) arc (0:180:0.3) -- ++ (0, -0.75);
            \draw[rounded corners] (0.75, 0) -- ++ (0, -0.15) -- ++ (0.5, -0.2) -- ++ (0, -0.35) -- ++ (-0.5, -0.2) -- ++ (0, -0.15);
            \draw[rounded corners] (1.2, 0) -- ++ (0, -0.15) -- ++ (-0.5, -0.2) -- ++ (0, -0.35) -- ++ (0.5, -0.2) -- ++ (0, -0.15);
            \draw[rounded corners] (0.4, 0.75) -- ++ (0, 0.2) -- ++ (-1, 0.6) -- ++ (0, 0.2);
            \draw[rounded corners] (0.85, 0.75) -- ++ (0, 0.1) -- ++ (0.2, 0.2) -- ++ (0, 0.1) coordinate (A);
            \draw (A) arc (0:330:0.3) coordinate (B);
            \draw[rounded corners] (B) -- ++ (0.15, 0.25) -- ++ (0, 0.5);
        \end{tikzpicture}
        .
    \end{equation}
    
    We are also free to change the orientation of the box, so long as the the connectivity stays the same.
    This is why we draw the box as a trapezium without rotational symmetry.
    For example, the following diagram is equivalent to both of the previous diagrams.
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-example-process-with-rotated-box}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, 0) -- ++ (-0.25, 0.75) -- ++ (1.5, 0) -- ++ (0, -0.75) -- cycle;
            \draw (0.05, 0.75) -- ++ (0, 0.3) arc (0:180:0.3) -- ++ (0, -1.15) arc (180:270:0.3) -- ++ (1.5, 0) arc (90:0:0.3) -- ++ (0, -0.3);
            \draw (0.5, 0.75) -- ++ (0, 0.3) arc (180:90:0.3) -- ++ (1, 0) arc (90:0:0.3) -- ++ (0, -1.4) arc (0:-90:0.3) -- ++ (-0.6, 0) arc (90:180:0.3) -- ++ (0, -0.05);
            \draw (0.95, 0.75) -- ++ (0, 0.1) arc (180:0:0.3) -- ++ (0, -1.1) arc (0:-90:0.3) -- ++ (-0.5, 0) arc (90:180:0.3) -- ++ (0, -0.15);
            \draw (0.4, 0) -- ++ (0, -0.1) arc (0:-90:0.2) -- ++ (-0.3, 0) arc (270:180:0.2) -- ++ (0, 1.45) arc (180:90:0.2) -- ++ (0.7, 0) arc (-90:0:0.2);
            \draw (0.85, 0) -- ++ (0, -0.1) arc (180:270:0.2) -- ++ (0.5, 0) arc (-90:0:0.2) -- ++ (0, 1.45) arc (0:90:0.2) -- ++ (-0.7, 0) arc (270:180:0.2);
        \end{tikzpicture}
        .
    \end{equation}

    A sensible question to ask now is what process does this box represent.
    We'll get to this.
    For now we'll just say that the process can be built up of fundamental process.
    There are four processes which we use to build any diagram in ZX calculus.
    They are
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-building-blocks}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[Z spider] (A) at (0, 0) {\(\alpha\)};
            \draw (A) -- ++ (0, 1);
            \node[Z spider] (B) at (1.5, 0) {\(\alpha\)};
            \draw (B) -- ++ (0, 1);
            \draw[rounded corners=7pt] (B) -- ++ (-0.5, 0) -- ++ (0, -0.75);
            \draw[rounded corners=7pt] (B) -- ++ (0.5, 0) -- ++ (0, -0.75);
            \node[X spider] (C) at (3, 0) {\(\alpha\)};
            \draw (C) -- ++ (0, 1);
            \node[X spider] (D) at (4.5, 0) {\(\alpha\)};
            \draw (D) -- ++ (0, 1);
            \draw[rounded corners=7pt] (D) -- ++ (-0.5, 0) -- ++ (0, -0.75);
            \draw[rounded corners=7pt] (D) -- ++ (0.5, 0) -- ++ (0, -0.75);
        \end{tikzpicture}
        .
    \end{equation}
    Actually, \(\alpha\) can take any value in \([0, 2\pi)\), so there are really an uncountable number of these building blocks.
    For short if the phase is zero then we omit the label:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-zero-phase-1}
        \begin{tikzpicture}[baseline=(A.base)]
            \node[X spider] (A) at (0, 0) {\(0\)};
        \end{tikzpicture}
        =
        \tikzsetnextfilename{ZXcalculus-zero-phase-2}
        \begin{tikzpicture}
            \node[X spider] at (0, 0) {};
        \end{tikzpicture}
        , \qqand
        \tikzsetnextfilename{ZXcalculus-zero-phase-3}
        \begin{tikzpicture}[baseline=(A.base)]
            \node[Z spider] (A) at (0, 0) {\(0\)};
        \end{tikzpicture}
        =
        \tikzsetnextfilename{ZXcalculus-zero-phase-4}
        \begin{tikzpicture}
            \node[Z spider] at (0, 0) {};
        \end{tikzpicture}
    \end{equation}
    
    We call these \define{spiders}\index{spider}.
    In particular, the green is a \(Z\) spider and the red is an \(X\) spider.
    
    Combining these pieces we can quickly build up fairly complex diagrams.
    For example, the diagram in \cref{fig:example ZX diagram} is a process which takes in two qubits and outputs two qubits.
    
    \begin{figure}
        \tikzsetnextfilename{ZXcalculus-example-using-building-blocks}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[X spider] (A) at (0, 0) {};
            \node[X spider, above=of A] (B) {\(\tfrac{\pi}{2}\)};
            \node[Z spider, below right=of A] (C) {\(\tfrac{\pi}{3}\)};
            \node[Z spider, below=of C] (D) {\(\pi\)};
            \node[Z spider, above right=of C] (E) {\(\pi\)};
            \node[Z spider, above=of E] (F) {\(\tfrac{\pi}{3}\)};
            \node[X spider, below right=of E] (G) {};
            \node[Z spider, below=of G] (H) {};
            \node[X spider, above right=of G] (I) {\(\tfrac{\pi}{4}\)};
            \node[Z spider, above right=of I] (J) {\(\tfrac{\pi}{2}\)};
            \node[Z spider, below right=of H] (K) {\(\tfrac{\pi}{4}\)};
            \node[Z spider, below=of K] (L) {\(\tfrac{\pi}{2}\)};
            
            \draw (A) -- (B);
            \draw (A) -- (C);
            \draw (C) -- (D);
            \draw (C) -- (E);
            \draw (E) -- (F);
            \draw (E) -- (G);
            \draw (G) -- (H);
            \draw (G) -- (I);
            \draw (I) -- (J);
            \draw (H) -- (K);
            \draw (K) -- (L);
            \draw[rounded corners] (K) -- ++ (2.5, 0) -- ++ (0, 5.55) -- (J);
            \draw[rounded corners] (I) -- ++ (-0.75, 0.75) -- ++ (0, 2) coordinate (M);
            \draw (J) -- (J |- M);
            \draw[rounded corners] (A) -- ++ (-1, -1) -- ++ (0, -2) -- ++ (2.5, -2.5) -- ++ (0, -1) coordinate (N);
            \draw[rounded corners] (G) -- ++ (-1, -1) -- ++ (0, -2) -- ++ (-3.5, 0) coordinate (O) -- (O |- N);
        \end{tikzpicture}
        \caption{A diagram in ZX calculus taking two qubits to two qubits.}
        \label{fig:example ZX diagram}
    \end{figure}
    
    \section{Simplifying Diagrams}
    There are two types of rules by which we might manipulate diagrams.
    The first, which we've already seen, are \define{graphical rules}\index{graphical rule} which allow us to move different pieces around so long as we don't change the connectivity.
    More formally two diagrams are equivalent if the are isotopic as graphs, a concept we'll make precise later, but for now two diagrams are isotopic if fixing all of the inputs and outputs as well as the points at which they connect it is possible to continuously morph one into the other.
    We allow the wires to pass through each other in this process.
    
    The second type of rule corresponds to specific properties of the basic building blocks.
    There are quite a few of these, and for now we'll just list them without much explanation.
    First we have the \define{monoid rules}\index{monoid rule} which are
    \begin{equation*}
        \tikzsetnextfilename{ZXcalculus-monoid-rules-X}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
            \node[Z spider] (A) {};
            \draw (A) -- ++ (0, 0.5);
            \draw (B) -- (-0.5, -1);
            \draw (0.2, -0.8) coordinate (E) arc (180:0:0.3) coordinate (F);
            \node[Z spider] (D) at (C) {};
            \draw (E) -- (E |- 0, -1);
            \draw (F) -- (F |- 0, -1);
            \node at (1.1, 0) {\(=\)};
            \begin{scope}[xshift=2.2cm, xscale=-1]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[Z spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \draw (B) -- (-0.5, -1);
                \draw (0.2, -0.8) coordinate (E) arc (180:0:0.3) coordinate (F);
                \node[Z spider] (D) at (C) {};
                \draw (E) -- (E |- 0, -1);
                \draw (F) -- (F |- 0, -1);
            \end{scope}
            
            \begin{scope}[xshift=3.89cm]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[Z spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \draw (B) -- ++ (0, -0.5);
                \draw (C) -- ++ (0, -0.5);
                \node at (0.8, 0) {\(=\)};
                \begin{scope}[xshift=1.6cm]
                    \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                    \node[Z spider] (A) {};
                    \draw (A) -- ++ (0, 0.5);
                    \draw[rounded corners] (B) -- ++ (0, -0.15) -- ++ (1, -0.2) -- ++ (0, -0.15);
                    \draw[rounded corners] (C) -- ++ (0, -0.15) -- ++ (-1, -0.2) -- ++ (0, -0.15);
                \end{scope}
            \end{scope}
            
            \begin{scope}[xshift=7.4cm]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[Z spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \node[Z spider] at (B) {};
                \draw (C) -- ++ (0, -0.5);
                \node at (0.75, 0) {\(=\)};
                \draw (1.125, -1) -- (1.125, 0.5);
                \node at (1.5, 0) {\(=\)};
                \node[Z spider] (C) at (2.1, -0.6) {};
                \node[Z spider] (D) at (2.1, 0.1) {};
                \draw (D) -- ++ (0, 0.4);
                \draw (C) -- ++ (0, -0.4);
                \draw (D) to[out=-45, in=45] (C);
                \draw (D) to[out=-135, in=135] (C);
            \end{scope}
        \end{tikzpicture}
        ,
    \end{equation*}
    and the same for the other colour:
    \begin{equation*}
        \tikzsetnextfilename{ZXcalculus-monoid-rules-Z}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
            \node[X spider] (A) {};
            \draw (A) -- ++ (0, 0.5);
            \draw (B) -- (-0.5, -1);
            \draw (0.2, -0.8) coordinate (E) arc (180:0:0.3) coordinate (F);
            \node[X spider] (D) at (C) {};
            \draw (E) -- (E |- 0, -1);
            \draw (F) -- (F |- 0, -1);
            \node at (1.1, 0) {\(=\)};
            \begin{scope}[xshift=2.2cm, xscale=-1]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[X spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \draw (B) -- (-0.5, -1);
                \draw (0.2, -0.8) coordinate (E) arc (180:0:0.3) coordinate (F);
                \node[X spider] (D) at (C) {};
                \draw (E) -- (E |- 0, -1);
                \draw (F) -- (F |- 0, -1);
            \end{scope}
            
            \begin{scope}[xshift=3.89cm]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[X spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \draw (B) -- ++ (0, -0.5);
                \draw (C) -- ++ (0, -0.5);
                \node at (0.8, 0) {\(=\)};
                \begin{scope}[xshift=1.6cm]
                    \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                    \node[X spider] (A) {};
                    \draw (A) -- ++ (0, 0.5);
                    \draw[rounded corners] (B) -- ++ (0, -0.15) -- ++ (1, -0.2) -- ++ (0, -0.15);
                    \draw[rounded corners] (C) -- ++ (0, -0.15) -- ++ (-1, -0.2) -- ++ (0, -0.15);
                \end{scope}
            \end{scope}
            
            \begin{scope}[xshift=7.4cm]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[X spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \node[X spider] at (B) {};
                \draw (C) -- ++ (0, -0.5);
                \node at (0.75, 0) {\(=\)};
                \draw (1.125, -1) -- (1.125, 0.5);
                \node at (1.5, 0) {\(=\)};
                \node[X spider] (C) at (2.1, -0.6) {};
                \node[X spider] (D) at (2.1, 0.1) {};
                \draw (D) -- ++ (0, 0.4);
                \draw (C) -- ++ (0, -0.4);
                \draw (D) to[out=-45, in=45] (C);
                \draw (D) to[out=-135, in=135] (C);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation*}
    
    The next set of rules are called the \define{Frobenius rules}\index{Frobenius rule}, they are
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-Frobenius-rules}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, -0.2) -- ++ (0, 0.7) arc (180:0:0.3) arc (-180:0:0.3) -- ++ (0, 0.7);
            \node[Z spider] (A) at (0.3, 0.8) {};
            \node[Z spider] (B) at (0.9, 0.2) {};
            \draw (A) -- ++ (0, 0.4);
            \draw (B) -- ++ (0, -0.4);
            \node at (1.5, 0.5) {\(=\)};
            \draw (1.8, 1.2) -- ++ (0, -0.7) arc (-180:0:0.3) arc (180:0:0.3) -- ++ (0, -0.7);
            \node[Z spider] (C) at (2.1, 0.2) {};
            \node[Z spider] (D) at (2.7, 0.8) {};
            \draw (D) -- ++ (0, 0.4);
            \draw (C) -- ++ (0, -0.4);
            
            \begin{scope}[xshift=4.5cm]
                \draw (0, -0.2) -- ++ (0, 0.7) arc (180:0:0.3) arc (-180:0:0.3) -- ++ (0, 0.7);
                \node[X spider] (A) at (0.3, 0.8) {};
                \node[X spider] (B) at (0.9, 0.2) {};
                \draw (A) -- ++ (0, 0.4);
                \draw (B) -- ++ (0, -0.4);
                \node at (1.5, 0.5) {\(=\)};
                \draw (1.8, 1.2) -- ++ (0, -0.7) arc (-180:0:0.3) arc (180:0:0.3) -- ++ (0, -0.7);
                \node[X spider] (C) at (2.1, 0.2) {};
                \node[X spider] (D) at (2.7, 0.8) {};
                \draw (D) -- ++ (0, 0.4);
                \draw (C) -- ++ (0, -0.4);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    The \define{fusion rules}\index{fusion rule} allows us to combine multiple nodes of the same colour in some circumstances:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-fusion-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, -0.5) coordinate (A) arc (180:0:0.5) coordinate (B);
            \node[Z spider] (C) at (-0.5, -0.9) {\(\smash{\mathrlap{\alpha}}\phantom{\beta}\)};
            \node[Z spider] (D) at (0.5, -0.9) {\(\beta\)};
            \node[Z spider] (E) at (0, 0) {};
            \draw (C) -- (A);
            \draw (D) -- (B);
            \draw (E) -- ++ (0, 0.5);
            \node (equals) at (1, -0.2) {\(=\)};
            \node[Z spider] (F) at (2, -0.75) {\(\alpha + \beta\)};
            \draw (F.north) -- (F |- 0, 0.5);
            \begin{scope}[xshift=4.5cm]
                \draw (-0.5, -0.5) coordinate (A) arc (180:0:0.5) coordinate (B);
                \node[X spider] (C) at (-0.5, -0.9) {\(\smash{\mathrlap{\alpha}}\phantom{\beta}\)};
                \node[X spider] (D) at (0.5, -0.9) {\(\beta\)};
                \node[X spider] (E) at (0, 0) {};
                \draw (C) -- (A);
                \draw (D) -- (B);
                \draw (E) -- ++ (0, 0.5);
                \node at (1, -0.2) {\(=\)};
                \node[X spider] (F) at (2, -0.75) {\(\alpha + \beta\)};
                \draw (F.north) -- (F |- 0, 0.5);
            \end{scope}
        \end{tikzpicture}
        ,
    \end{equation}
    where addition is taken modulo \(2\pi\).
    
    Before introducing the next set of rules we introduce some shorthand notation:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-2-parity-spiders}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[Z spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.75);
            \draw (A) -- ++ (0, -0.75);
            \node (equals) at (0.5, 0) {\(\coloneqq\)};
            \draw (1, -0.2) coordinate (B) arc (180:0:0.5) coordinate (C);
            \node[Z spider] (D) at (1.5, 0.3) {};
            \node[Z spider] at (B) {\(\alpha\)};
            \draw (C) -- ++ (0, -0.5);
            \draw (D) -- ++ (0, 0.45);
            \begin{scope}[xshift=3.25cm]
                \node[X spider] (A) {\(\alpha\)};
                \draw (A) -- ++ (0, 0.75);
                \draw (A) -- ++ (0, -0.75);
                \node (equals) at (0.5, 0) {\(\coloneqq\)};
                \draw (1, -0.2) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[X spider] (D) at (1.5, 0.3) {};
                \node[X spider] at (B) {\(\alpha\)};
                \draw (C) -- ++ (0, -0.5);
                \draw (D) -- ++ (0, 0.45);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    We also define the \defineindex{Hadamard}:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-hadamard}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[draw] (H) {\(H\)};
            \node (equals) at (0.7, 0) {\(\coloneqq\)};
            \node[Z spider] (A) at (1.5, 1) {\(\scriptstyle\frac{\pi}{2}\)};
            \node[X spider] (B) at (1.5, 0) {\(\scriptstyle\frac{\pi}{2}\)};
            \node[Z spider] (C) at (1.5, -1) {\(\scriptstyle\frac{\pi}{2}\)};
            \draw (A) -- (B);
            \draw (B) -- (C);
            \draw (A) -- ++ (0, 0.65) coordinate (D);
            \draw (C) -- ++ (0, -0.65) coordinate (E);
            \draw (H) -- (H |- D);
            \draw (H) -- (H |- E);
        \end{tikzpicture}
        .
    \end{equation}
    It's safe to give this a square symbol, with rotational symmetry, since we can see from the definition that it is rotationally symmetric.
    
    We then have two \define{identity rules}\index{identity rule}, the first is just a repeat of one of the monoid rules, but now in this new shorthand, the second is new:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-identity-rules}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[Z spider] (A) {};
            \node[X spider] (B) at (1.5, 0) {};
            \draw (A) -- ++ (0, 0.5);
            \draw (A) -- ++ (0, -0.5);
            \draw (B) -- ++ (0, 0.5);
            \draw (B) -- ++ (0, -0.5);
            \draw (0.75, -0.5) -- (0.75, 0.5);
            \node (equals) at (0.4, 0) {\(=\)};
            \node at (1.1, 0) {\(=\)};
            
            \begin{scope}[xshift=3cm]
                \draw (-0.5, -0.5) arc (180:0:0.5);
                \node[Z spider] (A) at (0, 0) {};
                \node[Z spider] (B) at (0, 0.5) {};
                \draw (A) -- (B);
                \begin{scope}[xshift=1.4cm]
                    \draw (-0.5, -0.5) arc (180:0:0.5);
                    \node[X spider] (A) at (0, 0) {};
                    \node[X spider] (B) at (0, 0.5) {};
                    \draw (A) -- (B);
                \end{scope}
                \node at (0.7, 0) {\(=\)};
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    The next rule allows us to change the colour of a node, at the cost of some Hadamards, it is appropriately called the \defineindex{colour change rule}:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-colour-change-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, -0.5) coordinate (A) arc (180:0:0.5) coordinate (B);
            \node[X spider] (C) {};
            \node (equals) at (1, 0) {\(=\)};
            \draw (1.6, -0.5) coordinate (D) arc (180:0:0.5) coordinate (E);
            \node[Z spider] (F) at (2.1, 0) {};
            \node[above = 0.35cm of F, draw] (H1) {\(H\)};
            \node[below, draw] (H2) at (D) {\(H\)};
            \node[below, draw] (H3) at (E) {\(H\)};
            \draw (F) -- (H1);
            \draw (H1) -- ++ (0, 0.7) coordinate (G);
            \draw (H2) -- ++ (0, -0.5);
            \draw (H3) -- ++ (0, -0.5) coordinate (I);
            \draw (C) -- (C |- G);
            \draw (A) -- (A |- I);
            \draw (B) -- (B |- I);
        \end{tikzpicture}
        .
    \end{equation}
    The next rule is called the \defineindex{copy rule}, since it allows us to make two diagrams out of one:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-copy-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, -0.5) arc (180:0:0.5);
            \node[Z spider] (A) at (0, 0) {};
            \node[X spider] (B) at (0, 0.75) {};
            \draw (A) -- (B);
            \node (equals) at (1, 0) {\(=\)};
            \node[X spider] (C) at (1.5, 0.75) {};
            \node[X spider] (D) at (2, 0.75) {};
            \draw (C) -- ++ (0, -1.25);
            \draw (D) -- ++ (0, -1.25);
        \end{tikzpicture}
        .
    \end{equation}
    Our next rule allows for an \(X\) spider with a phase of \(\pi\) to be copied pulling it through a \(Z\) spider.
    It is called the \define{\(\symbf{\pi}\)-copy rule}\index{\(\pi\)-copy rule}:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-pi-copy-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, -0.5) coordinate (C) arc (180:0:0.5) coordinate (D);
            \node[Z spider] (A) at (0, 0) {\(\alpha\)};
            \node[X spider] (B) at (0, 0.75) {\(\pi\)};
            \draw (A) -- (B);
            \draw (B) -- ++ (0, 0.75);
            \node (equals) at (1, 0) {\(=\)};
            \draw (C) -- ++ (0, -0.5);
            \draw (D) -- ++ (0, -0.5);
            \draw (1.7, -0.3) coordinate (E) -- ++ (0, 0.3) arc (180:0:0.5) -- ++ (0, -0.3) coordinate (F);
            \node[Z spider] (G) at (2.2, 0.5) {\(\alpha\)};
            \node[X spider] (H) at (E) {\(\pi\)};
            \node[X spider] (I) at (F) {\(\pi\)};
            \draw (G) -- (G |- 0, 1.5);
            \draw (H) -- (H |- 0, -1);
            \draw (I) -- (I |- 0, -1);
        \end{tikzpicture}
        .
    \end{equation}
    The next rule is called the \defineindex{bialgebra rule}:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-bialgebra-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, 1) arc (-180:0:0.5);
            \draw (-0.5, -1) arc (180:0:0.5);
            \node[Z spider] (A) at (0, 0.5) {};
            \node[X spider] (B) at (0, -0.5) {};
            \draw (A) -- (B);
            \node (equals) at (1, 0) {\(=\)};
            \draw (2, 0.5) arc (90:270:0.5);
            \draw (3, 0.5) arc (90:-90:0.5);
            \node[X spider] (C) at (2, 0.5) {};
            \node[X spider] (D) at (3, 0.5) {};
            \node[Z spider] (E) at (2, -0.5) {};
            \node[Z spider] (F) at (3, -0.5) {};
            \draw (C) -- ++ (0, 0.5);
            \draw (D) -- ++ (0, 0.5);
            \draw (E) -- ++ (0, -0.5);
            \draw (F) -- ++ (0, -0.5);
            \draw[rounded corners] (C) -- ++ (0.25, 0) -- ++ (0.5, -1) -- (F);
            \draw[rounded corners] (E) -- ++ (0.25, 0) -- ++ (0.5, 1) -- (D);
        \end{tikzpicture}
        .
    \end{equation}
    The final rule is rather simple, it's simply that we can ignore overall factors, called the \defineindex{scalar rule}, it corresponds to the following:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-scalar-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[Z spider] (A) {\(\phantom{\beta}\smash{\mathllap{\alpha}}\)};
            \node[X spider, above=1cm of A] (B) {\(\beta\)};
            \draw (A) -- (B);
            \node (equals) at (0.7, 0.7) {\(=\)};
            \draw[dashed, thick, Navy] (1.2, 0) rectangle (2.5, 1.6);
            \node at (3, 0.7) {\(=\)};
            \node at (4.3, 0) {};
        \end{tikzpicture}
        .
    \end{equation}
    Here the dashed box as well as the empty space both represent the empty diagram, which is simply the trivial identity process taking in no qubits, doing nothing, and outputting no qubits.
    
    \section{Interpretation}
    We'll see in more detail what these rules mean, where they come from, and why they have the names they do.
    For now it is enough to know that combined the monoid rules, Frobenius rules, fusion rules, and identity rules tell us that it doesn't matter how the dots of the same colour are connected, so long as the phases in the dots add to the same value modulo \(2\pi\).
    
    We can represent a qubit as an element of \(\complex^2\).
    Then the rules about \(Z\) spiders tell us how to multiply matrices which are diagonal in the computational basis, \(\{\ket{0}, \ket{1}\}\), with eigenvalues \(\e^{i\alpha}\), and the rules about \(X\) spiders tell us how to multiply matrices which are diagonal in the Hadamard transformed basis formed from
    \begin{equation}
        \ket{+} = H\ket{0} = \frac{1}{\sqrt{2}}(\ket{0} + \ket{1}), \qqand \ket{-} = H\ket{1} = \frac{1}{\sqrt{2}}(\ket{0} - \ket{1}).
    \end{equation}
    The colour change rule tells us how to convert one basis to the other.
    The bialgebra rule tells us that these bases are complimentary, that they are at the maximal angle to each other.
    The copy and \(\pi\)-copy rules are just artefacts of nicely chosen bases.
    
    Using this we can develop the \defineindex{standard model} of ZX calculus, which represents each diagram as a matrix acting on the input qubits.
    More formally we can define a map
    \begin{equation}
        \denotes{-} \colon (n\text{-to-}m \text{ qubit ZX diagram}) \to (2^n \times 2^m \text{ complex matrices}).
    \end{equation}
    Then a process which takes a single qubit, does nothing to it, and immediately outputs it is represented as
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-identity}
        \begin{tikzpicture}[baseline=0.4cm]
            \draw (0, 0) -- (0, 1);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1 & 0\\
            0 & 1
        \end{pmatrix}
        .
    \end{equation}
    Unsurprisingly doing nothing to a qubit gives the identity.
    
    The following diagram takes in two qubits, swaps them, and then returns them:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-swap}
        \begin{tikzpicture}[baseline=0.4cm]
            \draw[rounded corners] (0, 0) -- ++ (0, 0.2) -- ++ (0.5, 0.6) -- ++ (0, 0.2);
            \draw[rounded corners] (0.5, 0) -- ++ (0, 0.2) -- ++ (-0.5, 0.6) -- ++ (0, 0.2);
        \end{tikzpicture}
        \longmapsto 
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    In terms of matrices this acts as follows:
    \begin{equation}
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        \left[
            \begin{pmatrix}
                a\\ b
            \end{pmatrix}
            \otimes
            \begin{pmatrix}
                c\\ d
            \end{pmatrix}
        \right]
        =
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            ac\\ ad\\ bc\\ bd
        \end{pmatrix}
        =
        \begin{pmatrix}
            ac\\ bc\\ ad\\ bd
        \end{pmatrix}
        =
        \begin{pmatrix}
            c\\ d
        \end{pmatrix}
        \otimes
        \begin{pmatrix}
            a\\ b
        \end{pmatrix}
        .
    \end{equation}
    
    It is possible to have diagrams which create qubits from nothing.
    In this case we should take the input to simply be 1.
    The following diagram creates a pair of qubits:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-create-qubits}
        \begin{tikzpicture}[baseline=-0.35cm]
            \draw (0, 0) arc (-180:0:0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1\\ 0\\ 0\\ 1
        \end{pmatrix}
        .
    \end{equation}
    Similarly we can destroy qubits:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-destroy-qubits}
        \begin{tikzpicture}[baseline=0.15cm]
            \draw (0, 0) arc (180:0:0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    The Hadamard gives the \define{Hadamard matrix}\index{Hadamard!matrix}, note that we're ignoring an overall scalar, there's usually a factor of \(1/\sqrt{2}\):
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-hadamard}
        \begin{tikzpicture}[baseline=(H.base)]
            \node[draw] (H) {\(H\)};
            \draw (H) -- ++ (0, 0.5);
            \draw (H) -- ++ (0, -0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1 & 1\\
            1 & -1
        \end{pmatrix}
        .
    \end{equation}
    We can create a single qubit:
    \begin{equation}
         \tikzsetnextfilename{ZXcalculus-denotates-identity-one-arity-Z}
        \begin{tikzpicture}[baseline=(A.base)]
            \node[Z spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1\\ \e^{i\alpha}
        \end{pmatrix}
        , \qqand
        \tikzsetnextfilename{ZXcalculus-denotes-one-arity-X}
        \begin{tikzpicture}[baseline=(A.base)]
            \node[X spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1 + \e^{i\alpha}\\ 1 - \e^{i\alpha}
        \end{pmatrix}
    \end{equation}
    The three-arity spiders give the following matrices
    \begin{align}
        \tikzsetnextfilename{ZXcalculus-denotates-3-arity-Z}
        \begin{tikzpicture}[baseline=(A.base)]
            \draw (-0.5, -0.5) arc (180:0:0.5);
            \node[Z spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.5);
        \end{tikzpicture}
        &\longmapsto
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 0 & 0 & \e^{i\alpha}
        \end{pmatrix}
        ,\\ 
        \tikzsetnextfilename{ZXcalculus-denotes-three-arity-X}
        \begin{tikzpicture}[baseline=(A.base)]
            \draw (-0.5, -0.5) arc (180:0:0.5);
            \node[X spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.5);
        \end{tikzpicture}
        &\longmapsto
        \begin{pmatrix}
            1 + \e^{i\alpha} & 1 - \e^{i\alpha} & 1 - \e^{i\alpha} & 1 + \e^{i\alpha}\\
            1 - \e^{i\alpha} & 1 + \e^{i\alpha} & 1 + \e^{i\alpha} & 1 - \e^{i\alpha}
        \end{pmatrix}
    \end{align}
    
    Writing two processes next to each other gives their tensor product:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-tensor-product}
        \begin{tikzpicture}[baseline=(f.base)]
            \draw (0, 0) -- ++ (1, 0) -- ++ (-0.25, 0.75) -- ++ (-0.75, 0) -- cycle;
            \node (f) at (0.4, 0.375) {\(f\)};
            \draw (0.375, 0) -- ++ (0, -0.5);
            \draw (0.375, 0.75) -- ++ (0, 0.5);
            \begin{scope}[xshift=1.5cm]
                \draw (0, 0) -- ++ (1, 0) -- ++ (-0.25, 0.75) -- ++ (-0.75, 0) -- cycle;
                \node at (0.4, 0.375) {\(g\)};
                \draw (0.375, 0) -- ++ (0, -0.5);
                \draw (0.375, 0.75) -- ++ (0, 0.5);
            \end{scope}
        \end{tikzpicture}
        \longmapsto f \otimes g
    \end{equation}
    Writing two processes one after the other connected up represents doing them in the order they are connected from bottom to top, which is composition:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-composition}
        \begin{tikzpicture}[baseline=1cm]
            \draw (0, 0) -- ++ (1, 0) -- ++ (-0.25, 0.75) -- ++ (-0.75, 0) -- cycle;
            \node (f) at (0.4, 0.375) {\(f\)};
            \draw (0.375, 0) -- ++ (0, -0.5);
            \draw (0.375, 0.75) -- ++ (0, 0.75);
            \begin{scope}[yshift=1.5cm]
                \draw (0, 0) -- ++ (1, 0) -- ++ (-0.25, 0.75) -- ++ (-0.75, 0) -- cycle;
                \node at (0.4, 0.375) {\(g\)};
                \draw (0.375, 0.75) -- ++ (0, 0.5);
            \end{scope}
        \end{tikzpicture}
        \longmapsto g \circ f
    \end{equation}
    Note that for processes represented by matrices composition is just matrix multiplication.
    
    This mapping makes precise what any one ZX diagram represents.
    What is important is that this isn't changed when we apply the rules of ZX calculus.
    This is the crux of the following theorem.
    
    \begin{thm}{ZX Calculus is Sound}{}
        Let \(D_1\) and \(D_2\) be diagrams in ZX calculus.
        If \(D_1 = D_2\) according to the rules of ZX calculus then \(\denotes{D_1} = \denotes{D_2}\).
    \end{thm}
    
    As well as being a rigorous way to manipulate objects ZX calculus can also approximate any process from \(m\) qubits to \(n\) qubits to arbitrary precision.
    \begin{thm}{ZX Calculus is Approximately Universal}{}
        For any \(2^m \times 2^n\) matrix, \(f\), and any error margin, \(\varepsilon > 0\), there exists a diagram, \(D\), in ZX calculus built only from terms with phases an integer multiple of \(\pi/4\) such that \(\norm{\denotes{D} - f} < \varepsilon\) for some appropriate matrix norm \(\norm{-}\).
    \end{thm}
    This is one of the reasons that ZX calculus is so powerful.
    
    Another desirable quality for a notation like ZX calculus is that it be complete.
    By this we mean that if two matrices are equal and both given by some ZX diagram then there should be a graphical proof of this using only the rules of ZX calculus.
    This is the case if we assume the following two axioms, which are sound under the standard interpretation:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-extra-completeness-rule-1-lhs}
        \begin{tikzpicture}[font=\scriptsize, baseline=2cm]
            \draw (-1, 1) arc (-180:0:1);
            \node[Z spider] (A) at (0, 0) {};
            \node[X spider] (B) at (-1, 1) {};
            \node[X spider] (C) at (1, 1) {};
            \node[Z spider] (D) at (-2, 1) {\(\varphi\)};
            \node[Z spider] (E) at (-1, 2) {\(\varphi\)};
            \node[X spider] (F) at (0, 2) {\(\vartheta\)};
            \node[Z spider] (G) at (2, 1) {\(\psi\)};
            \node[Z spider] (H) at (1, 2) {\(\psi\)};
            \node[X spider] (I) at (-1, 3) {\(-\vartheta\)};
            \node[Z spider] (J) at (0, 3) {};
            
            \draw (0, -0.5) -- (A);
            \draw (B) -- (D);
            \draw (B) -- (E);
            \draw (C) -- (G);
            \draw (C) -- (H);
            \draw (E) -- (F);
            \draw (F) -- (H);
            \draw (E) -- (I);
            \draw (I) -- (J);
            \draw (J) -- (F);
            \draw (H) -- (1, 4);
            \draw (I) -- (-1, 4);
            \draw (J) -- (0, 4);
        \end{tikzpicture}
        =
        \tikzsetnextfilename{ZXcalculus-extra-completeness-rule-1-rhs}
        \begin{tikzpicture}[font=\scriptsize, baseline=2cm]
            \draw (-1, 1) arc (-180:0:1);
            \node[Z spider] (A) at (0, 0) {};
            \node[X spider] (B) at (-1, 1) {};
            \node[X spider] (C) at (1, 1) {};
            \node[Z spider] (D) at (-2, 1) {\(\varphi\)};
            \node[Z spider] (E) at (-1, 2) {\(\varphi\)};
            \node[X spider] (F) at (0, 2) {\(-\vartheta\)};
            \node[Z spider] (G) at (2, 1) {\(\psi\)};
            \node[Z spider] (H) at (1, 2) {\(\psi\)};
            \node[X spider] (I) at (1, 3) {\(\vartheta\)};
            \node[Z spider] (J) at (0, 3) {};
            
            \draw (0, -0.5) -- (A);
            \draw (B) -- (D);
            \draw (B) -- (E);
            \draw (C) -- (G);
            \draw (C) -- (H);
            \draw (E) -- (F);
            \draw (F) -- (H);
            \draw (H) -- (I);
            \draw (F) -- (J);
            \draw (J) -- (I);
            \draw (E) -- (-1, 4);
            \draw (J) -- (0, 4);
            \draw (I) -- (1, 4);
        \end{tikzpicture}
    \end{equation}
    for any phases \(\varphi\), \(\psi\), and \(\vartheta\) which are integer multiples of \(\pi/4\), and the second axiom
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-extra-completeness-rule-2-lhs}
        \begin{tikzpicture}[baseline=3cm]
            \draw (0, -0.5) -- (0, 6.5);
            \draw (2, 6) arc (90:270:1);
            \draw (2, 2) arc (90:270:1);
            \node[Z spider] (E) at (0, 1) {};
            \node[X spider] (A) at (0, 2) {};
            \node[Z spider] at (0, 3) {\(-\sfrac{\pi}{2}\)};
            \node[X spider] (B) at (0, 4) {};
            \node[Z spider] (F) at (0, 5) {};
            \node[Z spider] (C) at (-1, 2) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] (D) at (-1, 4) {\(\tfrac{\pi}{4}\)};
            \node[X spider] (G) at (1, 1) {};
            \node[X spider] (H) at (1, 5) {};
            \node[Z spider] at (2, 2) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] at (2, 0) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] at (2, 4) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] at (2, 6) {\(\sfrac{\pi}{4}\)};
            
            \draw (A) -- (C);
            \draw (B) -- (D);
            \draw (E) -- (G);
            \draw (F) -- (H);
        \end{tikzpicture}
        \quad = \quad
        \tikzsetnextfilename{ZXcalculus-extra-completeness-rule-2-rhs}
        \begin{tikzpicture}[baseline=3cm]
            \draw (0, -0.5) -- (0, 6.5);
            \draw (2, 5) arc (90:270:1);
            \node[X spider] at (0, 1) {\(\sfrac{\pi}{2}\)};
            \node[Z spider] at (0, 2) {\(\sfrac{\pi}{4}\)};
            \node[X spider] (A) at (0, 3) {\(\pi\)};
            \node[Z spider] (C) at (0, 4) {};
            \node[Z spider] (B) at (-1, 3) {\(\sfrac{\pi}{4}\)};
            \node[X spider] (D) at (1, 4) {\(\pi\)};
            \node[Z spider] at (2, 5) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] at (2, 3) {\(\sfrac{\pi}{4}\)};
            
            \draw (A) -- (B);
            \draw (C) -- (D);
        \end{tikzpicture}
        .
    \end{equation}
    Call ZX calculus with these rules added \define{\(\symbf{\pi/4}\)-ZX calculus}\index{\(\pi/4\)-ZX calculus}.
    
    \begin{thm}{\(\pi/4\)-ZX Calculus is Complete}{}
        Let \(D_1\) and \(D_2\) be diagrams in \(\pi/4\)-ZX calculus.
        If \(\denotes{D_1} = \denotes{D_2}\) then \(D_1 = D_2\) under the axioms of \(\pi/4\)-ZX calculus.
    \end{thm}
    
    The third thing making ZX calculus powerful is how well it can be automated.
    All a ZX calculation is is a finite labelled graph.
    Once you've implemented a way of applying the rules this can then be done very efficiently on a computer.
    
    A common use of ZX calculus is in quantum circuit optimisation.
    Given some quantum algorithm as a quantum circuit it is often possible to optimise the circuit.
    For example, the \(T\) gate,
    \begin{equation}
        T = 
        \begin{pmatrix}
            1 & 0\\
            0 & \e^{-i\pi/4}
        \end{pmatrix}
        ,
    \end{equation}
    is typically expensive to implement, so reducing the number of \(T\) gates in a circuit is usually desirable.
    Given some circuit making use of \(T\) gates we can use the universality of ZX calculus to convert the circuit into a ZX diagram, then manipulate the ZX diagram and then convert it back to a, hopefully, more optimised circuit with fewer \(T\) gates.
    
    \chapter{Semantics}
    \section{Types of Semantics}
    Consider the two following pseudocode fragments:
    \begin{align}
        P &= \lstinline|if 1 = 1 then F else G|,\\
        Q &= \lstinline|if 1 = 0 then F else F|.
    \end{align}
    Are \(P\) and \(Q\) the same program?
    There are two schools of thought:
    \begin{itemize}
        \item No. Clearly looking at them both programs are implemented differently, \(P\) makes reference to \lstinline|G|, \(Q\) makes reference to 0.
        \item Yes. Both programs take no input and output \lstinline|F|.
    \end{itemize}
    Whether or not we count these as the same program depends on what we are interested in.
    
    To aid in our analysis we assign the code fragments their meanings, encoded in some appropriate mathematical object.
    This gives a mapping
    \begin{equation}
        \denotes{-} \colon \text{Programs} \to \text{Mathematical Objects}.
    \end{equation}
    For now we'll leave the details of exactly what mathematical objects alone.
    If we are interested in implementation details then we assign \(P\) and \(Q\) to objects encoding these details.
    This is called \defineindex{operational semantics}.
    If we aren't interested in implementation details then we assign \(P\) and \(Q\) to objects which treat them as black boxes with inputs and outputs.
    This is called \defineindex{denotational semantics}.
    If this is what we do then we find that \(\denotes{P} = \denotes{Q} = \denotes{\lstinline|F|}\).
    
    We want this mapping to preserve the structure of our programs.
    For example, suppose that we have two processes, \lstinline|F| and \lstinline|G|, which can be composed by running them one after another.
    We might write this as \(\lstinline|F; G|\) in a language using semicolons to terminate a line.
    In order to reason about our program, regardless of which type of semantics we are interested in, we want the result to be the same if we compose the programs and then look at the semantics or look at the semantics and then compose the programs.
    That is we want
    \begin{equation}
        \denotes{\lstinline|F; G|} = \denotes{\lstinline|F|} \circ \denotes{\lstinline|G|}.
    \end{equation}
    Here \(\circ\) is some method of composing the semantics of two programs.
    Another structure which we may want to preserve is the ability to compute things in parallel, for example
    \begin{equation}
        \denotes{\lstinline|paralell(F, G)|} = \denotes{\lstinline|F|} \otimes \denotes{\lstinline|G|}.
    \end{equation}
    
    \section{Motivation}
    Why might we care about this sort of analysis?
    This reasoning can be used to ground assumptions and correct erroneous assumptions.
    We can also use semantics to justify transformations of programs, for example, to demonstrate that a compiled program does the same thing as the original program.
    It is also often the case that it is easier to reason about the mathematical objects encoding the programs, rather than the programs themselves, in fact sometimes it isn't possible to reason directly about the programs, such as in quantum computing where many operations are like black boxes, even if we can't look at the operational semantics we can still consider the denotational semantics and the flow of information through the program to compare programs.
    
    Choosing different semantics also allows us to focus on different details.
    Operational semantics focus on implementation details, such as memory usage and running time, which is useful if we want to improve the efficiency of our programs.
    Denotational semantics focus on results, which is useful if we want to check that our program does what we want.
    
    \section{Mathematical Objects}
    There are many possible mathematical objects to encode programs.
    Simple programs can be modelled as set theoretical functions from some set of possible inputs to some set of possible outputs.
    More specifically we can use something like \(\lambda\)-calculus to represent programs.
    In this way we can represent both operational semantics, by writing our functions as expressions in the input variables, and denotational semantics, by focussing on the input and output values.
    
    In quantum computing operational semantics are often not an option.
    So we will mostly stick to denotational semantics.
    The objects we choose to represent programs are categories.
    Category theory supports combing programs as we saw in the examples above and gives us a powerful graphical language for computations.
    
    \part{Categories}
    \chapter{Categories}
    \section{Motivation}
    We want an abstract mathematical formalism in which the meaning of a program lives, allowing us to abstract away implementation details and reason about computations.
    Such a formalism provides a map
    \begin{equation}
        \denotes{-} \colon \text{programs} \to \text{mathematical objects}.
    \end{equation}
    There are several features which are desirable of such a formalism, including but not limited to,
    \begin{itemize}
        \item a notion of composition, if \lstinline|F| and \lstinline|G| are programs and \lstinline|F; G| is running \lstinline|F| then \lstinline|G| then we should have \(\denotes{\lstinline|F; G|} = \denotes{\lstinline|G|} \circ \denotes{\lstinline|F|}\) where \(\circ\) represents composition in this mathematical formalism;
        \item a notion of concurrency, if \lstinline|F par G| corresponds to running \lstinline|F| and \lstinline|G| at the same time then we should have \(\denotes{\lstinline|F par G|} = \denotes{\lstinline|F|} \otimes \denotes{\lstinline|G|}\);
        \item a notion of calling programs recursively, if \lstinline|X| is some code calling \lstinline|F| then we should be able to compute \lstinline|F(X)|, and this requires our mathematical formalism to have structures of the form \(\denotes{\lstinline|F(X)|} = \denotes{\lstinline|F|}(\denotes{\lstinline|X|})\).
    \end{itemize}
    More concisely, our formalism should preserve composition, concurrency, and function application.
    
    The question we have to ask is what sort of mathematical objects we're considering.
    There are multiple options, each with their own advantages and disadvantages.
    Some common options are
    \begin{itemize}
        \item \(\lambda\)-calculus is an algebraic notation for functions. It is similar in syntax to \Haskell{}.
        It works with anonymous functions, or lambda functions, such as the function \lstinline[literate={lambda}{$\lambda$}{1}, mathescape]|lambda x. * 2 x| which takes an argument, \lstinline|x|, and multiplies it by 2, using prefix notation.
        Compare this to the \Haskell{} function
        \begin{lstlisting}[language=haskell, gobble=12]
            timesThree x = (*) 2 x
        \end{lstlisting}
        This choice is good for analysing implementation details and is simply a precise notation for applying functions, a common mathematical operation.
        \item Partially ordered sets, or posets, where the elements of the poset are partially completed calculations, any two elements are comparable if they are the same calculation at, potentially, different levels of completion, and the more complete calculation is greater.
        This choice is good for analysing computations step by step without worrying about how each step is implemented.
        \item Categories, which is what we'll use.
        This option subsumes both \(\lambda\)-calculus, as a method of defining functions, and posets, which can be regarded as a special case of a category.
    \end{itemize}
    
    Our goal will be to work in a general category to develop theory, imposing only the required restrictions for things to work out.
    Then we can pick a particular category to analyse our work in, with the interpretation depending on the category we pick.
    Often we can work in a generic category and then specialise the result to a category to perform either classical or quantum computations.
    
    \section{Categories: The Idea}
    A category consists of two pieces of data:
    \begin{itemize}
        \item Objects, \(A, B, C, \dotsc\);
        \item Morphisms, \(f \colon A \to B\), between objects.
    \end{itemize}

    Given some specific category there are various ways to think of computations occurring in this category.
    Some examples are given here:
    \begin{itemize}
        \item We can think of the objects as physical systems and the morphisms as processes.
        For example,
        \begin{itemize}
            \item Two objects may be a full cup and an empty cup and a process may be drinking the drink or making a new drink.
            \item Two objects might be a plate and pieces of broken pottery and a process may be dropping the plate on the floor.
        \end{itemize}
        \item We can think of objects as data types, and morphisms as functions between these types.
        Borrowing \Haskell{} notation some examples are
        \begin{itemize}
            \item One object might be \lstinline|Int|, and a morphism \lstinline[style=haskell]|f :: Int -> Int| defined by \lstinline|f n = 2 * n|.
            \item Another object might be \lstinline|String|, and a morphism \lstinline|len :: String -> Int| defined by
            \begin{lstlisting}[style=haskell, gobble=16]
                len [] = 0
                len (x : xs) = 1 + len xs
            \end{lstlisting}
            \item Another object might be \lstinline[style=haskell]|Num a => [a]|, and a morphism
            \begin{lstlisting}[style=haskell, gobble=16]
                mySum :: (Num a) => [a] -> a
                mySum [] = 0
                mySum (x : xs) = x + mySum xs
            \end{lstlisting}
        \end{itemize}
        \item Objects are algebraic structures and morphisms are structure preserving maps.
        For example,
        \begin{itemize}
            \item Objects are sets and morphisms are functions.
            \item Objects are groups and morphisms are homomorphisms.
            \item Objects are topological spaces and morphisms are continuous functions.
            \item Objects are vector spaces and morphisms are linear maps.
        \end{itemize}
        \item Objects are logical propositions and morphisms are implications between them.
        For example, we could take the objects \enquote{it rains} and \enquote{I get wet} and then we might have a morphism \(\text{\enquote{it rains}} \implies \text{\enquote{I get wet}}\).
        But what if we're indoors?
        We might introduce another object, \enquote{I am outside}, and then we may have a morphism \(\text{\enquote{it rains}} \land \text{\enquote{I am outside}} \implies \text{\enquote{I get wet}}\).
        Here we've implicitly defined another object \(\text{\enquote{it rains}} \land \text{\enquote{I am outside}}\) using logical conjunction, \(\land\).
        This is actually an example of a product in this category, we'll see what this means later.
    \end{itemize}
    
    It turns out that the second and fourth examples, programs/algorithms and propositions/implications are actually the same!
    This is known as the Curry--Howard isomorphism, and allows us to write proofs as programs and vice versa.
    
    The mindset that one should have when doing category theory is
    \begin{important}
        Morphisms are more important than objects.
    \end{important}
    \noindent This might seem backwards at first, for example we spend a lot of time thinking about groups, and homomorphisms are only one aspect that we consider, but it turns out that we can learn a lot about groups by studying how they relate to other groups, and this is done through homomorphisms.
    This mindset is particularly useful for cases such as quantum computing where we \emph{can't} look at internal structure, and can only look at how systems relate to each other.
    
    \section{Categories: The Definition}
    Categories are objects and morphisms.
    The definition of a category simply states what we mean by this and the properties that morphisms are expected to have.
    
    \begin{dfn}{Category}{}
        A \defineindex{category}, \(\cat{C}\), consists of the following data
        \begin{itemize}
            \item a collection of \define{objects}\index{object}, \(\Ob(\cat{C})\) (often denoted \(\mathop{\operatorname{Obj}}(\cat{C})\) or simply \(\cat{C}\));
            \item for every pair of objects, \(A, B \in \Ob(\cat{C})\) a collection of \define{morphisms}\index{morphism} (also known as \define{maps}\index{map|see{morphism}} or \define{arrows}\index{arrow|see{morphism}}), \(\cat{C}(A, B)\) (often denoted \(\hom_{\cat{C}}(A, B)\), \(\mathop{\operatorname{Mor}}_{\cat{C}}(A, B)\), possibly without the subscript \(\cat{C}\) when the category is clear, this collection is often called a \defineindex{hom set}), where for \(f \in \cat{C}(A, B)\) we write \(f \colon A \to B\) or \(A \xrightarrow{f} B\);
            \item a map \(\circ \colon \cat{C}(B, C) \times \cat{C}(A, B) \to \cat{C}(A, C)\) which assigns to each \(f \colon A \to B\) and \(g \colon B \to C\) some \define{composite}\index{composition} \((g \circ f) \colon A \to C\);
            \item for every object \(A \in \Ob(\cat{C})\) a morphism \(\id_A \colon A \to A\), that is \(\id_A \in \cat{C}(A, A)\), called the \defineindex{identity morphism}.
        \end{itemize}
        This data is subject to the following conditions:
        \begin{itemize}
            \item \defineindex{associativity} of \(\circ\): for all objects \(A, B, C, D \in \Ob(\cat{C})\) and for all morphisms \(f \colon A \to B\), \(g \colon B \to C\), and \(h \colon C \to D\) we have
            \begin{equation}
                h \circ (g \circ f) = (h \circ g) \circ f,
            \end{equation}
            so we can unambiguously write \(h \circ g \circ f\) for both of these;
            \item \defineindex{identity} law: for all objects \(A, B \in \Ob(\cat{C})\) and for all morphisms \(f \colon A \to B\) we have
            \begin{equation}
                f \circ \id_A = f = \id_B \circ f.
            \end{equation}
        \end{itemize}
    \end{dfn}
    
    \subsection{Technicality}
    Notice that in the definition we use the word \enquote{collection}.
    It is tempting to replace this with \enquote{set}, but this can cause issues.
    For example, the set of all sets is not a set, due to Russell's paradox.
    However, we will shortly see that we have categories where the objects are all sets, and so \(\Ob(\cat{C})\) cannot be a set in this case.
    We aren't going to worry too much about these types of issues, and may erroneously refer to these collections as sets.
    A category is \define{small}\index{small category} if both \(\Ob(\cat{C})\) and the collection of all morphisms between any two objects are sets.
    A category is \define{locally small}\index{locally small category} if for all objects \(A\) and \(B\) \(\cat{C}(A, B)\) is a set.
    Many statements we make throughout will apply only to small, or more likely locally small categories.
    
    \section{Categories: The Examples}
    \subsection{\texorpdfstring{\(\Set\)}{Set}}
    \begin{dfn}{\(\Set\)}{}
        The category \(\Set\)\index{Set@\(\Set\)} has sets as objects.
        A morphism \(A \to B\) is simply a function from \(A\) to \(B\).
        Composition of morphisms is composition of functions, defined for \(f \colon A \to B\) and \(g \colon B \to C\) by \((g \circ f) \colon A \to C\) given by \((g \circ f)(a) = g(f(a))\) for all \(a \in A\).
        The identity morphism is the identity function, \(\id_A \colon A \to A\), given by \(\id_A(a) = a\) for all \(a \in A\).
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Set\) is a category.
        \begin{proof}
            The collection \(\Set(A, B)\) of functions \(A \to B\) exists for all sets \(A\) and \(B\).
            It will be empty if \(B = \emptyset\) and \(A \ne \emptyset\) which is allowed, if \(A = \emptyset\) and \(B = \emptyset\) then there is a unique function \(f \colon \emptyset \to \emptyset\) which is also the identity on the empty set.
            Function composition is associative.
            Take sets \(A\), \(B\), \(C\), and \(D\), and morphisms \(f \colon A \to B\), \(g \colon B \to C\), and \(h \colon C \to D\).
            Then
            \begin{equation}
                (h \circ (g \circ f))(x) = h((g \circ f)(x)) = h(g(f(x))) = (h \circ g)(f(x)) = ((h \circ g) \circ f)(x)
            \end{equation}
            for all \(x \in A\), so \(h \circ (g \circ f) = (h \circ g) \circ f\).
            The identity function is then such that
            \begin{equation}
                (f \circ \id_A)(x) = f(\id_A(x)) = f(x) = \id_B(f(x)) = (\id_B \circ f)(x)
            \end{equation}
            and so \(f \circ \id_A = f = \id_B \circ f\).
        \end{proof}
    \end{lma}
    
    We can think of a function \(f \colon A \to B\) as dynamically indicating how elements of \(A\) transform into elements of \(B\).
    Pictorially, we can represent \(f \colon A \to B\) and \(g \colon B \to C\) as
    \begin{equation}
        \tikzsetnextfilename{categories-set-function-example}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B3) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(f\)};
            \begin{pgfonlayer}{behind}
                \draw (A1) -- (B1);
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
            
            \begin{scope}[xshift=4cm]
                \fill[highlight] (0, 0.25) coordinate (B1) circle [radius=0.075cm];
                \fill[highlight] (0, 0.75) coordinate (B2) circle [radius=0.075cm];
                \fill[highlight] (0, 1.25) coordinate (B3) circle [radius=0.075cm];
                \fill[highlight] (2, 0.25) coordinate (C1) circle [radius=0.075cm];
                \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                \fill[highlight] (2, 1.25) coordinate (C3) circle [radius=0.075cm];
                \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \node (B) at (0, 2) {\(B\)};
                \node (C) at (2, 2) {\(C\)};
                \draw[->] (B) -- (C) node [midway, above] {\(g\)};
                \begin{pgfonlayer}{behind}
                    \draw (B1) -- (C1);
                    \draw (B2) -- (C3);
                    \draw (B3) -- (C3);
                \end{pgfonlayer}
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    Then composition is just following the lines between sets:
    \begin{equation}
        \tikzsetnextfilename{categories-set-function-composition}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B3) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(f\)};
            \begin{pgfonlayer}{behind}
                \draw (A1) -- (B1);
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
            
            \begin{scope}[xshift=2cm]
                \fill[highlight] (2, 0.25) coordinate (C1) circle [radius=0.075cm];
                \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                \fill[highlight] (2, 1.25) coordinate (C3) circle [radius=0.075cm];
                \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \node (C) at (2, 2) {\(C\)};
                \draw[->] (B) -- (C) node [midway, above] {\(g\)};
                \begin{pgfonlayer}{behind}
                    \draw (B1) -- (C1);
                    \draw (B2) -- (C3);
                    \draw (B3) -- (C3);
                \end{pgfonlayer}
            \end{scope}
            
            \begin{scope}[yshift=-3cm]
                \fill[highlight] (0, 0) coordinate (A1) circle [radius=0.075cm];
                \fill[highlight] (0, 0.5) coordinate (A2) circle [radius=0.075cm];
                \fill[highlight] (0, 1) coordinate (A3) circle [radius=0.075cm];
                \fill[highlight] (0, 1.5) coordinate (A4) circle [radius=0.075cm];
                \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
                \node (A) at (0, 2) {\(A\)};
                
                \begin{scope}[xshift=2cm]
                    \fill[highlight] (2, 0.25) coordinate (C1) circle [radius=0.075cm];
                    \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                    \fill[highlight] (2, 1.25) coordinate (C3) circle [radius=0.075cm];
                    \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                    \node (C) at (2, 2) {\(C\)};
                    \draw[->] (A) -- (C) node [midway, above] {\(g \circ f\)};
                    \begin{pgfonlayer}{behind}
                        \draw (A1) -- (C1);
                        \draw (A2) -- (C1);
                        \draw (A3) -- (C1);
                        \draw (A4) -- (C3);
                    \end{pgfonlayer}
                \end{scope}
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    As a process composition in set is just doing one thing and then the other.
    
    \(\Set\) is the prototypical category.
    It is often tempting to think of other examples of categories, which we'll meet shortly, as simply sets with extra structure, and indeed this is often, but not always, the case.
    A \defineindex{concrete category} is a category in which all objects can be mapped to sets and morphisms can be mapped to functions between these sets.
    This mapping is done with something called a functor, which we'll define later (\cref{def:functor}).
    
    We'll list some properties of \(\Set\) here, some of which won't make sense until later.
    \begin{itemize}
        \item \(\Set\) is a concrete category.
        \item \(\Set\) has the empty set as an initial object, and the singleton as a terminal object.
        \item \(\Set\) is complete and co-complete, in particular the product is the Cartesian product and the coproduct is the disjoint union.
        \item \(\Set\) is a monoidal category with the monoidal product given by the Cartesian product.
    \end{itemize}
    
    \subsection{\texorpdfstring{\(\Rel\)}{Rel}}
    \begin{dfn}{Relation}{}
        Let \(A\) and \(B\) be sets.
        A \defineindex{relation}, \(R\), is a subset of \(A \times B\).
        If \((a, b) \in R\) we write \(a R b\).
    \end{dfn}
    Relations are morphisms in a category we will defined shortly (\cref{def:Rel}), so for \(R \subset A \times B\) we write \(R \colon A \to B\).
    In a similar manner to functions between sets being dynamic transformations we can think of relations as being non-deterministic transformations, where each element can transform into multiple objects, or possibly don't map across at all.
    For example, if we take \(A = \{a, b, c, d\}\), \(B = \{1, 2, 3\}\), and \(C = \{\alpha, \beta, \gamma\}\) then the relations
    \begin{align}
        R &= \{(b, 2), (c, 2), (d, 2), (d, 3)\} \subseteq A \times B, \qquad \text{and}\\
        S &= \{(1, \beta), (3, \beta), (3, \gamma)\} \subseteq B \times C
    \end{align}
    can be represented pictorially as
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-example}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(R\)};
            \begin{pgfonlayer}{behind}
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
            
            \begin{scope}[xshift=4cm]
                \fill[highlight] (0, 0.25) coordinate (B3) circle [radius=0.075cm];
                \fill[highlight] (0, 0.75) coordinate (B2) circle [radius=0.075cm];
                \fill[highlight] (0, 1.25) coordinate (B1) circle [radius=0.075cm];
                \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
                \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
                \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \node (B) at (0, 2) {\(B\)};
                \node (C) at (2, 2) {\(C\)};
                \draw[->] (B) -- (C) node [midway, above] {\(S\)};
                \begin{pgfonlayer}{behind}
                    \draw (B1) -- (C2);
                    \draw (B3) -- (C2);
                    \draw (B3) -- (C3);
                \end{pgfonlayer}
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    As with \(\Set\) we then compose relations by joining up these lines:
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-composition}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(R\)};
            \begin{pgfonlayer}{behind}
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
            
            \begin{scope}[xshift=2cm]
                \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
                \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
                \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \node (C) at (2, 2) {\(C\)};
                \draw[->] (B) -- (C) node [midway, above] {\(S\)};
                \begin{pgfonlayer}{behind}
                    \draw (B1) -- (C2);
                    \draw (B3) -- (C2);
                    \draw (B3) -- (C3);
                \end{pgfonlayer}
            \end{scope}
            
            \begin{scope}[yshift=-3cm]
                \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
                \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
                \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
                \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
                \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
                \node (A) at (0, 2) {\(A\)};
                
                \begin{scope}[xshift=2cm]
                    \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
                    \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                    \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
                    \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                    \node (C) at (2, 2) {\(C\)};
                    \draw[->] (A) -- (C) node [midway, above] {\(S \circ R\)};
                    \begin{pgfonlayer}{behind}
                        \draw (A4) -- (C2);
                        \draw (A4) -- (C3);
                    \end{pgfonlayer}
                \end{scope}
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    That is,
    \begin{equation}
        S \circ R = \{(d, \beta), (d, \gamma)\} \subseteq A \times C.
    \end{equation}
    This leads to the following definition.
    \begin{dfn}{Relation Composition}{}
        Let \(A\), \(B\), and \(C\) be sets with relations \(R \subseteq A \times B\) and \(S \subseteq B \times C\).
        Then the \define{composite relation}\index{relation composition} \(S \circ R\) is the set
        \begin{equation*}
            S \circ R \coloneqq \{(a, c) \in A \times C \mid \exists b \in B \text{ such that } (a, b) \in R \text{ and } (b, c) \in S\}.
        \end{equation*}
    \end{dfn}
    
    Now that we have composition we just need an identity, and after some playing around with the definition of composition one quickly comes to the identity
    \begin{equation}
        \id_A \coloneq \{(a, a) \in A \times A \mid a \in A\} \subseteq A \times A.
    \end{equation}
    Now we can define a category.
    
    \begin{dfn}{\(\Rel\)}{def:Rel}
        The category \(\Rel\)\index{Rel@\(\Rel\)} has sets as objects.
        A morphism \(R \colon A \to B\) is a relation \(R \subseteq A \times B\).
        Composition of morphisms is composition of relations, defined for \(R \colon A \to B\) and \(g \colon B \to C\) by
        \begin{equation}
            S \circ R = \{(a, c) \mid \exists b \in B : aRb \land bSc\} \subseteq A \times C.
        \end{equation}
        The identity morphism is the identity relation, \(\id_A \colon A \to A\), given by
        \begin{equation}
            \id_A = \{(a, a) \mid a \in A\} \subseteq A \times A.
        \end{equation}
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Rel\) is a category.
        \begin{proof}
            The collection \(\Rel(A, B)\) is simply the power set of \(A \times B\).
            Importantly the empty set is a relation on any pair of sets, including the empty set, and the empty set is the identity relation on the empty set.
            So consider nonempty sets.
            
            Composition of relations is associative.
            Let \(R \colon A \to B\), \(S \colon B \to C\), and \(T \colon C \to D\) be relations.
            We want to show that \(T \circ (S \circ R) = (T \circ S) \circ R\).
            To do so we will prove that each pair \((a, b) \in T \circ (S \circ R)\) is also an element of \((T \circ S) \circ R\).
            The converse, that each pair \((a, b) \in (T \circ S) \circ R\) is an element of \(T \circ (S \circ R)\), follows by the same logic in reverse.
            So take some \((a, b) \in T \circ (S \circ R)\).
            By definition there exists some \(x\) such that \((x, b) \in T\) and \((a, x) \in S \circ R\).
            Thus, there exists some \(y\) such that \((y, x) \in S\) and \((a, y) \in S\).
            Since \((y, x) \in S\) and \((x, b) \in T\) we have that \((y, b) \in T \circ S\).
            Since \((a, y) \in R\) we have \((a, b) \in (T \circ S) \circ R\).
            
            Let \(R \colon A \to B\) be a relation and \(\id_A = \{(a, a) \mid a \in A\}\) the identity relation.
            Then for all \((a, b) \in R\) we have \((a, a) \in \id_A\), meaning that \((a, b) \in R \circ \id_A\).
            Similarly for all \((a, b) \in R \circ \id_A\) we must have \(x\) such that \((x, b) \in R\), and \((x, a) \in \id_A\), which means that \(x = a\) and so \((a, b) \in R\).
            Thus \(R \circ \id_A = R\).
            Similarly \(\id_B \circ R = R\).
            Hence the identity law is satisfied.
        \end{proof}
    \end{lma}
    
    We can represent relations as binary \(\abs{B} \times \abs{A}\) matrices with a \(1\) in the \((i, j)\) slot if \((a, b) \in R\), where \(a\) is the \(j\)th element of \(A\) and \(b\) the \(i\)th element of \(B\) in some arbitrary fixed ordering of \(A\) and \(B\).
    Further this mapping is one-to-one, meaning each binary matrix also defines a representation.
    For example, indexing top to bottom we have
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-representation}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(R\)};
            \begin{pgfonlayer}{behind}
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
        \end{tikzpicture}
        \leftrightsquigarrow
        M(R) = 
        \begin{pmatrix}
            0 & 0 & 0 & 0\\
            0 & 1 & 1 & 1\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    Composition of relations is then given by matrix multiplication taking everything mod 2.
    More formally we have a map
    \begin{equation}
        M \colon \powerset(A \times B) \to \matrices[\abs{B}]{\abs{A}}{\integers_2}
    \end{equation}
    where \(\integers_2 = \{0, 1\}\) has addition and multiplication defined mod 2.
    
    As an example notice that we have
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-representation-2}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
        
            \fill[highlight] (0, 0.25) coordinate (B3) circle [radius=0.075cm];
            \fill[highlight] (0, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.25) coordinate (B1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (B) at (0, 2) {\(B\)};
            \node (C) at (2, 2) {\(C\)};
            \draw[->] (B) -- (C) node [midway, above] {\(S\)};
            \begin{pgfonlayer}{behind}
                \draw (B1) -- (C2);
                \draw (B3) -- (C2);
                \draw (B3) -- (C3);
            \end{pgfonlayer}
        \end{tikzpicture}
        \leftrightsquigarrow
        M(S) = 
        \begin{pmatrix}
            0 & 0 & 0\\
            1 & 0 & 1\\
            0 & 0 & 1
        \end{pmatrix}
    \end{equation}
    and
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-representation-3}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \node (A) at (0, 2) {\(A\)};
            
            \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (C) at (2, 2) {\(C\)};
            \draw[->] (A) -- (C) node [midway, above] {\(S \circ R\)};
            \begin{pgfonlayer}{behind}
                \draw (A4) -- (C2);
                \draw (A4) -- (C3);
            \end{pgfonlayer}
        \end{tikzpicture}
        \leftrightsquigarrow M(S \circ R) = 
        \begin{pmatrix}
            0 & 0 & 0 & 0\\
            0 & 0 & 0 & 1\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    The composite can then be calculated as
    \begin{equation}
        M(S \circ R) = M(S)M(R) = 
        \begin{pmatrix}
            0 & 0 & 0\\
            1 & 0 & 1\\
            0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            0 & 0 & 0 & 0\\
            0 & 1 & 1 & 1\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        =
        \begin{pmatrix}
            0 & 0 & 0 & 0\\
            0 & 0 & 0 & 1\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    
    This map, \(M\), actually defines a functor (\cref{def:functor}) \(M \colon \Rel \to \Mat[\integers_2]\), where \(\Mat[\integers_2]\) is the category of binary matrices, whose objects are natural numbers and a morphism \(n \to m\) is an \(n \times m\) matrix.
    The functor \(M\) then maps sets to their size, \(A \mapsto M(A) = \abs{A}\), and relations to matrices as described above.
    
    This is actually an example of a more general idea of a representation\footnote{see \course{Symmetries of Quantum Mechanics} or \course{Symmetries of Particles and Fields}}, where we replace objects with matrices preserving the structure of the original space.
    These ideas can all be understood as functors \(\cat{C} \to \Mat\) for some appropriate category \(\category{C}\) and ring \(\field\).
    This in turn is more commonly thought of as a map \(\category{C} \to \FVect\) identifying matrices with linear maps.
    This allows for generalisations to infinite dimensional representations, \(\category{C} \to \Vect\).
    We'll define \(\FVect\) and \(\Vect\) shortly (\cref{def:Vect,def:FVect}).
    
    On the surface \(\Rel\) seems quite similar to \(\Set\), after all the objects of both are the same.
    However, it's really the morphisms which are important, and these are quite different.
    The ability to replace relations with matrices means that \(\Rel\) is actually quite similar to another category, \(\Hilb\) (\cref{def:Hilb and FHilb}).
    This makes \(\Rel\) a nice in between for \(\Set\) and \(\Hilb\), which turn out to be the categories in which we think of most classical and quantum computing as occurring in respectively.
    
    We now list some properties of \(\Rel\):
    \begin{itemize}
        \item \(\Rel\) is a concrete category.
        \item \(\Rel\) is a dagger category.
        \item \(\Rel\) has both products and coproducts given by disjoint union.
        \item \(\Rel\) is a monoidal category with the monoidal product given by the Cartesian product.
    \end{itemize}
    
    \subsection{\texorpdfstring{\(\Vect\), \(\FVect\), \(\Hilb\), and \(\FHilb\)}{Vect, FVect, Hilb, and FHilb}}
    \subsubsection{\texorpdfstring{\(\Vect\) and \(\FVect\)}{Vect and FVect}}
    \begin{dfn}{Vector Space}{}
        A \defineindex{vector space}, \((V, \field, +, \cdot)\), is a set, \(V\), a field\footnote{if you don't know what this is just replace it with \(\reals\) or \(\complex\) and don't worry about it}, \(\field\), and two operations, \defineindex{vector addition}, \(+ \colon V \times V \to V\), and \defineindex{scalar multiplication}, \(\cdot \colon \field \times V \to V\), such that
        \begin{itemize}
            \item \((V, +)\) is an Abelian group, that is
            \begin{itemize}
                \item vector addition is associative: \(u + (v + w) = (u + v) + w\) for all \(u, v, w \in V\);
                \item vector addition is commutative: \(u + v = v + u\) for all \(u, v \in V\);
                \item additive identity: there exists \(0 \in V\) such that \(0 + v = v\) for all \(v \in V\);
                \item additive inverse: for every \(v \in V\) there exists \(-v \in V\) such that \(v + (-v) = v - v = 0\);
            \end{itemize}
            \item scalar multiplication distributes over vector addition: \(\alpha \cdot (u + v) = (\alpha \cdot v) + (\alpha \cdot v)\) for all \(\alpha \in \field\) and \(u, v \in V\);
            \item field identity acts as the identity: \(1 \cdot v = v\) for all \(v \in V\) where 1 is the multiplicative identity in \(\field\);
            \item field addition distributes over scalar multiplication: \((\alpha + \beta) \cdot v = (\alpha \cdot v) + (\beta \cdot v)\);
            \item compatibility of field and scalar multiplication: \(\alpha \cdot (b \cdot v) = (a \cdot_{\field} b) \cdot v\) for all \(\alpha, \beta \in \field\) and \(v \in V\) where \(\cdot_{\field}\) is multiplication in the field.
        \end{itemize}
    \end{dfn}
    
    \begin{dfn}{Linear Map}{}
        Let \((V, \field, +_V, \cdot_V)\) and \((W, \field, +_W, \cdot_W)\) be vector spaces over the same field, \(\field\).
        A \defineindex{linear map} between these vector spaces is a function \(T \colon V \to W\) such that
        \begin{equation}
            T(u +_V v) = T(u) +_W T(v), \qqand T(\alpha \cdot_V v) = \alpha \cdot_W T(v)
        \end{equation}
        for all \(\alpha \in \field\) and \(u, v \in V\).
    \end{dfn}
    
    From now on we drop the explicit symbol for scalar multiplication, writing \(\alpha v\) for \(\alpha \cdot v\), as well as dropping labels differentiating which vector space an operation is defined on.
    So linearity is expressed as
    \begin{equation}
        T(u + v) = T(u) + T(v), \qqand T(\alpha v) = \alpha T(v).
    \end{equation}
    We also refer to \(V\) and \(W\) as vector spaces (over \(\field\)) leaving the operations (and potentially the field) implicit.
    
    \begin{dfn}{\(\Vect\)}{def:Vect}
        Fix some field \(\field\).
        The category \(\Vect\)\index{Vect@\(\Vect\)} has vector spaces over \(\field\) as objects and linear maps as morphisms.
        Composition of morphisms is composition of linear maps, which is composition of the underlying functions.
        The identity morphisms are the identity linear maps, which are the underlying identity functions.
    \end{dfn}
    
    \begin{lma}{}{lma:Vect is a category}
        \(\Vect\) is a category.
        \begin{proof}
            Composition of linear maps inherits associativity from the underlying functions and similarly the identity laws follow from the identity laws of the underlying functions.
            We therefore only need to show that the composite of two linear maps is again linear.
            Let \(T \colon U \to V\) and \(S \colon V \to W\) be linear maps between vector spaces \(U\), \(V\), and \(W\) over some field \(\field\).
            Then for all \(u, v \in V\) and \(\alpha \in \field\) we have
            \begin{multline}
                (S \circ T)(u + v) = S(T(u + v)) = S(T(u) + T(v))\\
                = S(T(u)) + S(T(v)) = (S \circ T)(u) + (S \circ T)(v)
            \end{multline}
            using the linearity of \(T\) and then \(S\), and
            \begin{equation}
                (S \circ T)(\alpha v) = S(T(\alpha v)) = S(\alpha T(v)) = \alpha S(T(v)) = \alpha (S \circ T)(v)
            \end{equation}
            again using linearity of \(T\) and then \(S\).
            Hence the composite of two linear maps is again a linear map.
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Basis}{}
        Let \(V\) be a vector space.
        A subset \(\symcal{B} \subset V\) is \defineindex{linearly independent} if for every finite subset of \(\{\ve{1}, \dotsc, \ve{n}\} \subseteq \symcal{B}\) satisfies
        \begin{equation}
            \alpha_1 \ve{1} + \dotsb + \alpha_n \ve{n} = 0
        \end{equation}
        only for the trivial case of \(\alpha_i = 0\).
        
        A subset \(\symcal{B} \subset V\) spans \(V\) if every \(v \in V\) can be written as
        \begin{equation}
            v = \alpha_1\ve{1} + \dotsb + \alpha_n\ve{n}
        \end{equation}
        for some finite subset \(\{\ve{1}, \dotsc, \ve{n}\} \subseteq \symcal{B}\).
        We call \(\alpha_i\) the \define{components}\index{component} of \(v\).
        
        If a subset \(\symcal{B} \subset V\) is linearly independent and spans \(V\) then we call it a \defineindex{basis}.
    \end{dfn}
    
    Every vector space has a basis (assuming the axiom of choice).
    It is a fact that any two bases have the same cardinality, which we call the \defineindex{dimension} of the space.
    A vector space is \defineindex{finite-dimensional} if it's dimension is finite.
    
    \begin{dfn}{\(\FVect\)}{def:FVect}
        The category \(\FVect\) has finite-dimensional vector spaces as objects and linear maps as morphisms.
    \end{dfn}
    
    \begin{lma}{}{}
        \(\FVect\) is a category.
        \begin{proof}
            This follows from \(\Vect\) being a category.
        \end{proof}
    \end{lma}
    
    Linear maps are often thought of as matrices, although this only works in the finite-dimensional case.
    Take two vector spaces \(V\) and \(W\) with bases \(\{v_i\}\) and \(\{w_i\}\) respectively.
    Then a linear map \(T \colon V \to W\) defines a matrix with components \(T_{ij}\) given by \(T(v_i)_j\), where \(T(v_i)_j\) is the \(j\)th component of \(T(v_i)\), which is what we get evaluating the linear map at the \(i\)th basis vector, \(v_i\).
    A matrix, \(T_{ij}\), defines a linear map \(T \colon V \to W\) similarly, by defining \(T(v_i)\) to be formed from components \(T(v_i)_j = T_{ij}\), and then using linearity to extend this definition to any vector in \(V\).
    
    \begin{dfn}{\(\Mat\)}{def:Mat}
        The category \(\Mat\)\index{Mat@\(\Mat\)} has natural numbers as objects with a morphism \(n \to m\) being an \(m \times n\) matrix with entries in \(\field\).
        Composition of morphisms is matrix multiplication and the identity matrix is the identity morphism.
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Mat\) is a category.
        \begin{proof}
            The product of an \(m \times n\) matrix and a \(n \times \ell\) matrix is an \(m \times \ell\) matrix, reflecting the fact we can compose morphisms \(\ell \to n\) and \(n \to m\) to get a morphism \(\ell \to m\).
            Associativity follows from associativity of matrix multiplication and the identity matrix is clearly the identity.
        \end{proof}
    \end{lma}
    
    There is an equivalence (\cref{def:equivalence and others}) \(\Mat \to \FVect\) given by sending \(n \to \field^n\) and sending a matrix to a linear map as described above.
    This formalises the idea that linear maps and matrices are equivalent ways of doing linear algebra in finite dimensions.
    
    \subsubsection{\texorpdfstring{\(\Hilb\) and \(\FHilb\)}{Hilb and FHilb}}
    \begin{dfn}{Inner Product Space}{}
        An \defineindex{inner product space}, \((V, \braket{-}{-})\), is a vector space, \(V\), over the field \(\field = \reals, \complex\) equipped with an \defineindex{inner product} \(\braket{-}{-} \colon V \times V \to \field\) such that the inner product is
        \begin{itemize}
            \item conjugate symmetric: \(\braket{u}{v} = \braket{v}{u}^*\) for all \(u, v \in V\);
            \item linear in the \emph{second} argument: \(\braket{u}{\alpha v} = \alpha\braket{u}{v}\) for all \(\alpha \in \field\) and \(u, v \in V\), this implies antilinearity in the first argument: \(\braket{\alpha u}{v} = \alpha^*\braket{u}{v}\);
            \item positive-definite: \(\braket{v}{v} > 0\) for all \(v \in V\) with \(v \ne 0\) and \(\braket{0}{0} = 0\), note that conjugate symmetry implies \(\braket{v}{v} = \braket{v}{v}^*\) so \(\braket{v}{v}\) is real.
        \end{itemize}
        Note that for the \(\field = \reals\) case we can simply ignore the complex conjugates.
    \end{dfn}
    
    \begin{dfn}{Hilbert Space}{}
        A \defineindex{Hilbert space} is an inner product space \((H, \braket{-}{-})\) such that \(H\) is complete with respect to the norm \(\norm{-} \colon H \to \reals\) defined by \(\norm{v} \coloneqq \sqrt{\braket{v}{v}}\), we say that this is the norm induced by the inner product.
        Being complete means that if the sequence \(\{v_i\} \subseteq H\) is such that
        \begin{equation}
            \sum_{i = 1}^{\infty} \norm{v_i}
        \end{equation}
        converges (as a series in \(\reals\)) then
        \begin{equation}
            \sum_{i = 1}^{\infty} v_i
        \end{equation}
        converges to some \(v \in H\), in the sense that for all \(\varepsilon > 0\) there exists some \(N \in \naturals\) such that for all \(n > N\) we have
        \begin{equation}
            \norm*{v - \sum_{i = 1}^{n} v_i} < \varepsilon,
        \end{equation}
        or equivalently,
        \begin{equation}
            \lim_n \norm*{v - \sum_{i = 1}^{n} v_i} = 0.
        \end{equation}
    \end{dfn}
    
    We will assume that Hilbert spaces are inner product spaces over \(\complex\) unless stated otherwise, although most facts will hold for real Hilbert spaces as well.
    Complex Hilbert spaces are where all quantum mechanics takes place, so we don't lose much by making this assumption.
    
    This requirement of completeness is really just a technical requirement to make things well defined in certain circumstances, and we won't ever have reason to make use of it explicitly.
    For our purposes inner product space and Hilbert space are basically synonyms, but we're slightly safer working in Hilbert spaces due to this extra condition.
    
    \begin{dfn}{Bounded Linear Map}{}
        Let \(H\) and \(K\) be Hilbert spaces with norms \(\norm{-}_H\) and \(\norm{-}_K\) respectively, both induced by the inner product.
        A \defineindex{bounded linear map} is a map \(T \colon H \to K\) such that
        \begin{equation}
            \norm{T(v)}_K \le M \norm{v}_H
        \end{equation}
        for some \(M \in \reals\) and all \(v \in H\).
    \end{dfn}
    
    \begin{dfn}{\(\Hilb\) and \(\FHilb\)}{def:Hilb and FHilb}
        Fix some field \(\field = \reals, \complex\).
        The category \(\Hilb\) has Hilbert spaces as objects and bounded linear maps as morphisms.
        The category \(\FHilb\) has finite-dimensional Hilbert spaces as objects and bounded linear maps as morphisms.
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Hilb\) and \(\FHilb\) are categories.
        \begin{proof}
            Associativity and identities follow from the underlying functions.
            We need only show that the composite of two bounded linear maps is again a bounded linear map.
            Let \(T \colon H \to K\) and \(S \colon K \to J\) be bounded linear maps between the Hilbert spaces \(H\), \(K\), and \(J\).
            Then there exist some \(M, N \in \reals\) such that \(\norm{T(v)}_K \le M\norm{v}_H\) and \(\norm{S(u)}_J \le N\norm{u}\) for all \(v \in H\) and \(u \in K\).
            Then we have \(\norm{(S \circ T)(v)}_J = \norm{S(T(v))}_J \le N\norm{T(v)}_K \le NM\norm{v}_H\) for all \(v \in V\), and \(NM \in \reals\), so the map is bounded again.
            The composite of two linear maps is linear, as shown in \cref{lma:Vect is a category},
            so the composite of two bounded linear maps is a bounded linear map.
        \end{proof}
    \end{lma}
    
    We now make a few basic definitions.
    \begin{dfn}{Orthonormal}{}
        Let \(H\) be a Hilbert space and \(\{\ve{i}\}\) a basis of \(H\), then we say that this basis is \defineindex{orthogonal} if \(\braket{\ve{i}}{\ve{j}} = 0\) for \(i \ne j\).
        If \(\braket{\ve{i}}{\ve{j}} = \delta_{ij}\) we say the basis is \defineindex{orthonormal}.
    \end{dfn}
    
    \begin{dfn}{Adjoint}{}
        If \(H\) and \(K\) are Hilbert spaces and \(T \colon H \to K\) is a linear map then we define the \defineindex{adjoint} to be the linear map \(T^{\hermit} \colon K \to H\) such that \(\braket{T(v)}{w} = \braket{v}{T^\hermit(w)}\).
    \end{dfn}
    
    In terms of matrices representing linear maps the adjoint corresponds to the conjugate transpose matrix.
    
    \begin{dfn}{Bras and Kets}{def:bras and kets}
        Let \(H\) be a Hilbert space.
        Given some \(v \in H\) its \defineindex{ket} is the map \(\ket{v} \colon \complex \to H\) defined by \(z \mapsto zv\) and its bra is the map \(\bra{v} \colon H \to \complex\) defined bg \(w \mapsto \braket{v}{w}\).
    \end{dfn}
    
    This definition is rather formal and doesn't correspond to how we usually think about bras and kets.
    Typically we think about elements of \(H\) as being kets.
    This works since each linear map \(\complex \to H\) is completely defined by where it sends \(1\), so really linear maps of this form just pick out elements of \(H\), and \(\ket{v}\) is such that \(1 \mapsto 1v = v\).
    So we often write \(\ket{v}\) when we might actually mean \(v\) by this definition.
    We then think of bras similarly as being their own vectors in some other space, which we'll define in the next definition, and then we think of the mapping \(w \mapsto \braket{v}{w}\) as simply performing multiplication \(\bra{v} \ket{w} = \braket{v}{w}\).
    
    \begin{dfn}{Dual Space}{}
        Let \(V\) be a vector space.
        Then \(V^* \coloneqq \Vect(V, \field)\) is the \defineindex{dual space}.
    \end{dfn}
    
    This definition means that \(V^*\) is the space of linear maps \(V \to \field\), which in the case of a Hilbert space we can recognise as the space of bras.
    Note that we are equipping \(\Vect(V, \field)\) with the obvious vector space (or Hilbert space) structure given by adding and scaling linear maps pointwise.
    
    
    \subsection{More Examples}
    \begin{exm}{Sets with Structure}{}
        Many categories can be described as having objects formed from sets with structure, and morphisms being structure preserving functions.
        These are all concrete categories, in the sense that we can forget the structure and just think of them as sets and functions.
        Some examples of these sets with structure are
        \begin{itemize}
            
            \item The category \(\Mon\)\index{Mon@\(\Mon\)} has monoids as objects and monoid homomorphisms as morphisms.
            \item The category \(\Grp\)\index{Grp@\(\Grp\)} has groups as objects and group homomorphisms as morphisms.
            \item The category \(\Ring\)\index{Ring@\(\Ring\)} has rings as objects and ring homomorphisms as morphisms.
            \item The category \(\CRing\)\index{CRing@\(\CRing\)} has commutative rings as objects and ring homomorphisms as morphisms.
            \item The category \(\Field\)\index{Field@\(\Field\)} has fields as objects and field homomorphisms as morphisms.
            \item The category \(\RMod\)\index{R-Mod@\(\RMod\)} for a fixed ring \(R\) has left modules over \(R\) as objects and module homomorphisms as morphisms.
            Note that the special case where \(R\) is a field is \(\Vect[R]\).
            \item The category \(\Top\) has topological spaces as objects and continuos maps as morphisms.
            \item The category \(\pointedTop\) has pointed topological spaces, \((X, \bullet)\), as objects, that is topological spaces with some special point, \(\bullet\), and based maps as morphisms, that is continuous maps preserving this special point, that is \(f \colon (X, \bullet) \to (Y, \ast)\) is continuous and \(f(\bullet) = \ast\).
            \item The category \(\Pos\) has posets as objects and monotone functions as morphisms.
        \end{itemize}
        See \cref{chap:maths definitions} for relevant definitions.
    \end{exm}
    
    \begin{exm}{Posets}{}
        Given a poset, \((P, \le)\), we can define a category whose objects are elements of \(P\) and there is a unique morphism \(a \to b\) if \(a \le b\).
        Since this morphism is unique and \(a \le a\) the identity is simply the unique morphism \(a \to a\).
        Since \(a \le b\) and \(b \le c\) implies \(a \le c\) there is a unique morphism \(a \to c\) in this case, which must then be the morphism formed by composing the morphisms \(a \to b\) and \(b \to c\).
    \end{exm}
    
    \begin{exm}{Single Object Categories}{}
        Given a monoid, \(M\), we can interpret it as a category with a single object, which we call \(\bullet\), and then the elements of \(M\) are morphisms \(\bullet \to \bullet\) with composition of morphisms given by the monoid product.
        The identity of the monoid is the identity morphism.
        In a sense categories just generalise monoids to have more objects.
        The process of defining a quantity, then mapping the definition to a particular category with a single object, and then allowing there to be multiple objects is called \defineindex{oidification}, and the resulting object is suffixed with -oid.
        So a monoidoid is a category.
        
        Given a group, \(G\), we can interpret it as a single object category where all morphisms are isomorphisms (\cref{def:isomorphism and others}).
        This extra condition is simply reflecting the condition that a group has all inverses.
        A \defineindex{groupoid} is then a category in which all morphisms are isomorphisms.
        
        Note that we can proceed in the opposite direction, given a category with a single object (with all morphisms being isomorphisms) this can be interpreted as a monoid (group).
    \end{exm}
    
    \section{Diagrams}\index{diagram}
    Composition of morphisms can be quite confusing.
    There is lots of data to specify, which objects are involved, what are the morphisms called, do the morphisms have any other properties we might be interested in and so on.
    It doesn't help that the order in which we write composition of functions is the reverse of the order in which the functions are applied.
    The solution to this is to draw pictures with all of the objects as nodes and the morphisms as arrows between them.
    Composition of morphisms is then given by reading the arrows backwards.
    A simple example is
    \begin{equation}
        A \xrightarrow{f} B \xrightarrow{g} C
    \end{equation}
    which represents two morphisms, \(f \colon A \to B\) and \(g \colon B \to C\), which can be composed to give \((g \circ f) \colon A \to C\).
    We might add this morphism to our diagram,
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f"] \arrow[rr, bend right, "g \circ f"'] & B \arrow[r, "g"] & C
        \end{tikzcd}
        .
    \end{equation}
    although we don't have to since it's existence is ensured by the definition of a category.
    Both paths taken in this diagram give the same result.
    
    In general if all paths in a diagram between two fixed objects give the same result we say that the diagram \define{commutes}\index{commute}, or is a \defineindex{commutative diagram}.
    This can be useful to specify a lot of algebraic relations in a single commutative diagram.
    For example, the diagram
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f"] \arrow[d, "h"'] & B \arrow[r, "g"] \arrow[d, "i"'] & C\\
            D \arrow[r, "k"'] & E \arrow[ur, "j"']
        \end{tikzcd}
    \end{equation}
    commuting is equivalent to the following requirements:
    \begin{equation}
        g \circ f = j \circ k \circ h, \qquad i \circ f = k \circ h, \qqand g = j \circ i,
    \end{equation}
    which are given by requiring that the outer parallelogram commutes, the square commutes, and the triangle commutes respectively.
    
    Since the definition of a category means every object has an identity morphism and that these identity morphisms don't really affect composition we leave the identity morphisms implicit in the diagram.
    We could write them in, for example
    \begin{equation}
        \begin{tikzcd}
           A \arrow[r, "f"] \arrow[loop left, "\id_A"] & B \arrow[loop right, "\id_B"]
        \end{tikzcd}
    \end{equation}
    commuting is simply the identity law, telling us that, among other things, \(f \circ \id_A = f = \id_B \circ f\).
    We can also state the associativity law as the commutativity of the following:
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f"] \arrow[rr, bend left, "g \circ f"] & B \arrow[r, "g"] \arrow[rr, bend right, "h \circ g"'] & C \arrow[r, "h"] & D.
        \end{tikzcd}
    \end{equation}
    
    \section{Terminology}
    We now introduce some terminology which will be helpful.
    \begin{dfn}{}{def:isomorphism and others}
        For a morphisms \(f \colon A \to B\)
        \begin{itemize}
            \item we call \(A\) the \defineindex{domain}, or \define{source}\index{source|see{domain}}, of \(f\);
            \item we call \(B\) the \defineindex{codomain}, or \define{target}\index{target|see{codomain}}, of \(f\);
            \item we call \(f\) an \defineindex{endomorphism} if \(A = B\);
            \item we call \(f\) an \defineindex{isomorphism} if there exists \(f^{-1} \colon B \to A\) such that \(f^{-1} \circ f = \id_A\) and \(f \circ f^{-1} = \id_{B}\);
            \item we call \(f\) an \defineindex{automorphism} if it is an isomorphism and endomorphism;
            \item we call \(A\) \defineindex{isomorphic} to \(B\) if \(f\) is an isomorphism, and write \(A \isomorphic B\);
            \item we call \(f\) an \defineindex{epimorphism}, or \define{epic}\index{epic|see{epimorphism}}, if \(g \circ f = h \circ f\) implies \(g = h\) for all \(g, h \colon B \to C\);
            \item we call \(f\) a \defineindex{monomorphism}, or \define{monic}\index{mono|see{monomorphism}}, if \(f \circ g = f \circ h\) implies \(g = h\) for all \(g, h \colon C \to A\).
        \end{itemize}
    \end{dfn}
    
    The most important definition here is isomorphisms.
    Often equality is too strict a requirement for comparing two objects.
    Instead isomorphism is usually the correct level of similarity.
    In particular in a concrete category objects are isomorphic if there is an invertible structure preserving map between them, giving a one-to-one pairing of elements.
    This allows for trivial differences between objects, like renaming of elements or operations, to be ignored.
    For example, the groups \((\{0, 1\}, +_2)\) and \((\{1, -1\}, \cdot)\) are not equal, but they are isomorphic.
    
    \begin{lma}{}{}
        Let \(\category{C}\) be a category with objects \(A\) and \(B\).
        If \(f \colon A \to B\) is an isomorphism then the inverse is unique.
        \begin{proof}
            Suppose this wasn't the case, so \(f \colon A \to B\) has two inverses, \(g, g' \colon B \to A\), which are both such that \(g \circ f = g' \circ f = \id_A\) and \(f \circ g = f \circ g' = \id_B\).
            Then we have
            \begin{equation}
                g = g \circ \id_B = g \circ (f \circ g') = (g \circ f) \circ g' = \id_A \circ g' = g'.
            \end{equation}
        \end{proof}
    \end{lma}
    
    The condition that \(f\) and \(f^{-1}\) are inverses is equivalent to requiring that
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f", shift left=1] & B \arrow[l, "f^{-1}", shift left=1]
        \end{tikzcd}
    \end{equation}
    commutes.
    
    \section{Graphical Notation}
    In this section we will introduce a graphical notation which can be used to reason about categories.
    This notation treats categories as processes, taking some input and producing some output.
    We read the notation from bottom to top, which we can imagine is the progression of \enquote{time} through the process.
    This notation obeys the rules, and spirit, of category theory.
    In particular, we don't write objects.
    Instead we represent each object through it's identity morphism, so \(A\) is represented by \(\id_A\).
    An identity morphism is then represented by a line, labelled by the object, representing a process in which nothing happens, the input at the bottom of the page, is just passed directly to the output at the top of the page:
    \begin{equation}
        \id_A = 
        \tikzsetnextfilename{categories-graphical-language-identity}
        \begin{tikzpicture}[baseline=(A.base)]
            \draw[wire] (0, 0) -- (0, 1) node [midway, left] (A) {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    A general morphism, \(f \colon A \to B\), is represented in this graphical notation by placing a box on the wire, representing some process occurring, and labelling the wire either side with the appropriate object:
    \begin{equation}
        f = 
        \tikzsetnextfilename{categories-graphical-language-morphism}
        \begin{tikzpicture}[baseline=(f.base)]
            \node[morphism] (f) {\(f\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- ++ (0, 1) node [midway, left] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    Composition of morphisms is represented by doing one process after the other, which in this notation just means writing one box after the other and joining them by a wire of the appropriate type.
    If we introduce a second morphism \(g \colon B \to C\) then we have
    \begin{equation}
        g \circ f = 
        \tikzsetnextfilename{categories-graphical-language-composition}
        \begin{tikzpicture}[baseline=(g.base)]
            \node[morphism] (g) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (g) -- ++ (0, -1) node [midway, left] {\(B\)};
            \draw[wire] (g) -- ++ (0, 1) node [midway, left] {\(C\)};
            \node at (0.5, 0) {\(\circ\)};
            \node[morphism] (f) at (1, 0) {\(f\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- ++ (0, 1) node [midway, left] {\(B\)};
        \end{tikzpicture}
        \ =
        \tikzsetnextfilename{categories-graphical-language-composite}
        \begin{tikzpicture}[baseline=(B.base)]
            \node[morphism] (f) {\(f\)};
            \node[morphism, above= 0.75cm of f] (g) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- (g) node [midway, left] (B) {\(B\)};
            \draw[wire] (g) -- ++ (0, 1) node [midway, left] {\(C\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    This graphical notation makes the axioms of a category implicit.
    For the identity law since identity morphisms are just drawn as wires clearly the following holds:
    \begin{equation}
        \tikzsetnextfilename{categories-graphical-language-identity-law}
        \begin{tikzpicture}[baseline=(A.base),
            morphism/.append style={minimum width=0.6cm}
            ]
            \node[morphism] (idA) at (0, 0) {\(\id_A\)};
            \node[morphism] (f1) at (0, 1.5) {\(f\)};
            \node[morphism] (f2) at (1.5, 0.75) {\(f\)};
            \node[morphism] (f3) at (3, 0) {\(f\)};
            \node[morphism] (idB) at (3, 1.5) {\(\id_B\)};
            \draw[wire] (idA) -- (f1) node [midway, left] (A) {\(A\)};
            \draw[wire] (idA) -- ++ (0, -1) node [midway, left] {\(A\)} coordinate (bottom);
            \draw[wire] (f1) -- ++ (0, 1) node [midway, left] {\(B\)} coordinate (top);
            \draw[wire] (f2) -- (f2 |- bottom) node [midway, left] {\(A\)};
            \draw[wire] (f2) -- (f2 |- top) node [midway, left] {\(B\)};
            \draw[wire] (f3) -- (f3 |- bottom) node [midway, left] {\(A\)};
            \draw[wire] (idB) -- (f3) node [midway, left] {\(B\)};
            \draw[wire] (idB) --(f3 |- top) node [midway, left] {\(B\)};
            \node at (0.75, 0.75) {\(=\)};
            \node at (2.25, 0.75) {\(=\)};
        \end{tikzpicture}
        ,
    \end{equation}
    and this is just the identity law
    \begin{equation}
        f \circ \id_A = f = \id_B \circ f.
    \end{equation}
    Similarly we don't bother drawing brackets around composites, since associativity makes them unnecessary.
    If we did draw brackets then considering the objects and morphisms
    \begin{equation}
        A \xrightarrow{f} B \xrightarrow{g} C \xrightarrow{h} D
    \end{equation}
    we can write the associativity law as
    \begin{equation}
        \tikzsetnextfilename{categories-graphical-language-associativity-law}
        \begin{tikzpicture}[baseline=(g.base)]
            \node[morphism] (f) at (0, 0) {\(f\)};
            \node[morphism] (g) at (0, 1.5) {\(\phantomrlap{g}{f}\)};
            \node[morphism] (h) at (0, 3) {\(\phantomrlap{h}{f}\)};
            \draw[wire] (f) -- (g) node [midway, left] {\(B\)};
            \draw[wire] (g) -- (h) node [midway, left] {\(C\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (h) -- ++ (0, 1) node [midway, left] {\(D\)};
            \node[rotate=90] at (0, 1.1) {\(\Bigg(\)};
            \node[rotate=90] at (0, 3.4) {\(\Bigg)\)};
            
            \node at (1, 1.5) {\(=\)};
            \begin{scope}[xshift=2cm]
                \node[morphism] (f) at (0, 0) {\(f\)};
                \node[morphism] (g) at (0, 1.5) {\(\phantomrlap{g}{f}\)};
                \node[morphism] (h) at (0, 3) {\(\phantomrlap{h}{f}\)};
                \draw[wire] (f) -- (g) node [midway, left] {\(B\)};
                \draw[wire] (g) -- (h) node [midway, left] {\(C\)};
                \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
                \draw[wire] (h) -- ++ (0, 1) node [midway, left] {\(D\)};
                \node[rotate=90] at (0, -0.4) {\(\Bigg(\)};
                \node[rotate=90] at (0, 1.9) {\(\Bigg)\)};
            \end{scope}
        \end{tikzpicture}
        ,
    \end{equation}
    which is just
    \begin{equation}
        (h \circ g) \circ f = h \circ (g \circ f).
    \end{equation}
    
    So by manipulating this graphical notation naturally we apply the category axioms.
    This makes this notation very powerful.
    While this notation looks new if we were to instead draw it horizontally and replace wires with \(\circ\) and \(\id_X\) then it would just be the usual algebraic notation for morphism composition.
    The real power of this notation comes when we introduce new concepts, such as monoidal products, which extend the graphical notation into a two-dimensional notation which works in much the same way, while the usual algebraic notation quickly becomes cluttered and hard to use.
    
    
    \section{Functors}
    The spirit of category theory is that it is not objects that are important but morphisms between them.
    This suggests that whenever we see a mathematical object we should look for maps between them preserving the relevant structure.
    Well, categories are objects, so we should look for maps between them.
    Such maps should preserve the category's structure, which is
    \begin{itemize}
        \item the collection of objects;
        \item the collections of morphisms;
        \item the composition of morphisms;
        \item the identity morphisms.
    \end{itemize}
    For example, if we have a morphism \(f \colon A \to B\) in one category then our map should send this to another morphism in the new category, and the domain and codomain of that morphism should somehow be related to \(A\) and \(B\).
    It shouldn't matter whether we do composition of morphisms and then map, or map and the compose morphisms.
    Identities should map to identities.
    To this end we make the following definition of a map between categories preserving this structure.
    
    \begin{dfn}{Functor}{def:functor}
        Let \(\cat{C}\) and \(\cat{D}\) be categories.
        A \defineindex{functor}, \(F \colon \cat{C} \to \cat{D}\), is composed of two mappings:
        \begin{itemize}
            \item each object \(A \in \Ob(\cat{C})\) is mapped to some object \(F(A) \in \Ob(\cat{D})\);
            \item each morphism \(f \in \cat{C}(A, B)\) is mapped to some morphism \(F(f) \in \cat{D}(F(A), F(B))\);
        \end{itemize}
        such that
        \begin{itemize}
            \item composition is preserved: \(F(g \circ f) = F(g) \circ F(f)\) for \(f \colon A \to B\) and \(g \colon B \to C\) morphisms in \(\cat{C}\);
            \item identities are preserved: \(F(\id_A) = \id_{F(A)}\) for \(A \in \Ob(\cat{C})\).
        \end{itemize}
    \end{dfn}
    
    \begin{ntn}{}{}
        It is common to drop the brackets when applying a functor, a bit like we may apply a linear map, \(T\), to some vector, \(v\), by writing \(Tv\) and thinking of it as matrix multiplication.
        In this case a functor \(F \colon \cat{C} \to \cat{D}\) maps each \(A \in \Ob(\cat{C})\) to some object \(FA \in \Ob(\cat{D})\), each morphism \(f \in \cat{C}(A, B)\) to some morphism \(Ff \in \cat{D}(FA, FB)\), such that \(F(g \circ f) = Fg \circ Ff\) and \(F\id_A = \id_{FA}\).
    \end{ntn}
    
    Note that what we have defined above is a \defineindex{covariant functor}.
    It is also possible to define a \defineindex{contravariant functor} which reverses the direction of morphisms.
    The difference in the definition is that each \(f \in \cat{C}(A, B)\) is assigned to some \(Ff \in \cat{D}(FB, FA)\) and \(F(g \circ f) = Ff \circ Fg\).
    We'll assume that all functors are covariant.
    
    \begin{exm}{}{}
        \begin{itemize}
            \item The constant functor, \(C_X \colon \cat{C} \to \cat{D}\), maps every object of \(\cat{C}\) to \(X \in \Ob(\cat{D})\) and every morphism \(f \colon A \to B\) in \(\cat{C}\) to the identity morphism \(\id_X \colon X \to X\) in \(\cat{D}\).
            \item The identity functor, \(\id_{\cat{C}} \colon \cat{C} \to \cat{C}\) maps every object and morphism in \(\cat{C}\) to itself.
            \item The power set can be regarded as a functor \(\powerset\colon \Set \to \Set\), sending each set to its power set and each function \(f \colon A \to B\) to the map sending \(U \in \powerset(A)\) to its image
            \begin{equation}
                f(U) \coloneqq \{f(u) \mid u \in U\}.
            \end{equation}
            \item The map \(V \mapsto V^*\) defines a functor \(\Vect \to \Vect\).
            \item If \(G\) is a group viewed as a one object category then a functor \(G \to \Set\) defines a group action, sending the single object of \(G\) to the set, \(S\), upon which \(G\) acts, and sending each morphism, \(g\) (recall morphisms are just elements of the group) to the mapping on that set \(s \mapsto g \mathbin{.} s\).
            \item If \(G\) is a group viewed as a one object category then a functor \(G \to \Vect\) defines a group action on a vector space, which is to say this functor defines a representation.
            \item If \(G\) and \(H\) are groups viewed as one object categories then a functor \(G \to H\) is simply a group homomorphism, sending the single object of \(G\) to the single object of \(H\) and preserving composition, which is the group operation.
            \item The map sending each complex Lie group to its Lie algebra is a functor \(\LieGrp \to \LieAlg\)\footnote{See \course{Symmetries of Quantum Mechanics} or \course{Symmetries of Particles and Fields}}.
            \item A \defineindex{forgetful functor}, \(\cat{C} \to \Set\) maps a category to \(\Set\) by \enquote{forgetting} the structure of the objects in \(\cat{C}\).
            A category, \(\cat{C}\), is called \define{concrete}\index{concrete category} if there exists a forgetful functor \(\cat{C} \to \Set\).
            \begin{itemize}
                \item The forgetful functor \(U \colon \Grp \to \Set\) sends each group to its underlying set and each group homomorphism to its underlying function.
                \item The forgetful functor \(U \colon \Vect \to \Set\) sends each vector space to its underlying set and each linear transformation to its underlying function.
                \item The forgetful functor \(U \colon \Top \to \Set\) sends each topological space to its underlying set and each continuous function to its underlying function.
            \end{itemize}
            \item A partially forgetful functor \(\cat{C} \to \cat{D}\) generalises forgetful functors by forgetting some, but not necessarily all, structure of the objects in \(\cat{C}\).
            \begin{itemize}
                \item The partially forgetful functor \(\Grp \to \Mon\) sends each group to itself, but now viewed as a monoid, essentially forgetting that inverses exist.
                \item The partially forgetful functor \(\Hilb \to \Vect\) sends each Hilbert space to itself, but forgets the inner product structure.
                \item The partially forgetful functor \(\CRing \to \Ring\) sends each commutative ring to itself, forgetting that multiplication is commutative.
                \item The partially forgetful functor \(\Ring \to \Ab\) sends each ring to its additive group, forgetting how to multiply.
            \end{itemize}
            \item A \defineindex{free functor} is, in a sense\footnote{This sense can be made formal through the notion of an adjoint, an important concept in category theory but one we won't explore.} the reverse of a forgetful functor, sending a set to the most general object of the appropriate type.
            \begin{itemize}
                \item The free functor \(\Set \to \Grp\) sends each set the free group it generates, which is the set of words generated by concatenating elements of the set and elements declared to be their inverses, with the group operation being concatenation and the identity the empty string.
                No other relations between elements of the set exist.
                \item The free functor \(\Set \to \Vect\) takes a set and declares that all the elements in it are linearly independent and form a basis for a vector space whose elements are formal linear combinations of these elements.
            \end{itemize}
        \end{itemize}
    \end{exm}
    
    The next definition introduces some terminology used to talk about functors.
    
    \begin{dfn}{}{def:equivalence and others}
        A functor \(F \colon \cat{C} \to \cat{D}\) is
        \begin{itemize}
            \item \define{full}\index{full functor} when the mapping \(f \mapsto Ff\) defines a surjection \(\cat{C}(A, B) \to \cat{D}(FA, FB)\);
            \item \define{faithful}\index{faithful functor} when the mapping \(f \mapsto Ff\) defines an injection \(\cat{C}(A, B) \to \cat{D}(FA, FB)\);
            \item \defineindex{essentially surjective on objects} when for each \(B \in \Ob(\cat{D})\) there is some \(A \in \Ob(\cat{C})\) such that \(FA \isomorphic B\), essentially when the mapping \(A \mapsto FA\) is surjective, but replacing equality with isomorphism in the definition of surjectivity;
            \item an \defineindex{equivalence} when it is full, faithful, and essentially surjective on objects;
            \item an \defineindex{endofunctor} if \(\cat{C} = \cat{D}\).
        \end{itemize}
    \end{dfn}
    
    The most important definition here is that of an equivalence.
    Equality of categories is far too strong a condition.
    We can also define a notion of isomorphism between categories, but this is also too strong.
    Relaxing the requirements for isomorphism, by changing surjective on objects to essentially surjective on objects, we get the notion of equivalence, which is the sweet spot where a functor preserves enough structure for the two categories to be the same for all intents and purposes, without being too restrictive.
    
    It should be noted that there are multiple, equivalent, definitions of equivalence.
    The one we've given here is probably the easiest to understand, but not always the easiest to apply.
    
    One example we've already seen of an equivalence is the functor \(\Mat \to \FVect\) sending \(n \mapsto \field^n\) and a matrix to the associated linear map on the vector space with some fixed basis.
    This is an equivalence since all finite dimensional vector spaces of the same dimension are isomorphic, which means that this functor is essentially surjective on objects.
    
    Now that we have categories and maps between them it is natural to ask if this forms a category, and indeed it does!
    
    \begin{dfn}{\(\Cat\)}{}
        The category \(\Cat\)\index{Cat@\(\Cat\)} has (small) categories as its objects and functors as its morphisms.
        Composition of functors is composition of the two mappings \(\Ob(\cat{C}) \to \Ob(\cat{D})\) and \(\cat{C}(A, B) \to \cat{D}(FA, FB)\), and the identity is the identity functor.
    \end{dfn}
    
    To be clear, if \(F \colon \cat{C} \to \cat{D}\) and \(G \colon \cat{D} \to \cat{E}\) are functors then the functor \((G \circ F) \colon \cat{C} \to \cat{E}\) sends \(A \in \Ob(\cat{C})\) to \((G \circ F)(A) = (G \circ F)A = G(F(A)) = GFA\), and sends \(f \in \cat{C}(A, B)\) to \((G \circ F)(f) = (G \circ F)f = G(F(f)) = GFf \in \cat{E}(GFA, GFB)\).
    Associativity is guaranteed by associativity of the underlying mappings and the identity functor is clearly an identity morphism.
    
    \section{Natural Transformations}
    We have now defined functors as mathematical objects.
    Following the spirit of category theory we should look for maps between functors preserving their structure.
    A map between functors should preserve the functor structure.
    This means that it shouldn't matter if we map to a different functor and then apply the functor, or apply a functor and then map to the other functor.
    This leads to the following definition.
    
    \begin{dfn}{Natural Transformation}{}
        Let \(\cat{C}\) and \(\cat{D}\) be categories and \(F, G \colon \cat{C} \to \cat{D}\) functors.
        For every object \(A \in \Ob(\cat{C})\) a \defineindex{natural transformation}, \(\zeta \colon F \naturalTransformation G\), assigns a morphism \(\zeta_A \colon FA \to GA\) in \(\cat{D}\) such that for every morphism \(f \colon A \to B\) in \(\cat{C}\) the following diagram commutes:
        \begin{equation}
            \begin{tikzcd}
                FA \arrow[r, "\zeta_A"] \arrow[d, "Ff"'] & GA \arrow[d, "Gf"]\\
                FB \arrow[r, "\zeta_B"'] & GB\mathrlap{.}
            \end{tikzcd}
        \end{equation}
        
        If every component, \(\zeta_A\), is an isomorphism then \(\zeta\) is called a \defineindex{natural isomorphism} and \(F\) and \(G\) are said to be \defineindex{naturally isomorphic}, written \(F \isomorphic G\).
    \end{dfn}
    
    This leads to an alternative, equivalent, definition of an equivalence.
    A functor \(F \colon \cat{C} \to \cat{D}\) is an equivalence if and only if there is a functor \(G \colon \cat{D} \to \cat{C}\) and natural isomorphisms \(G \circ F \naturalTransformation \id_{\category{C}}\) and \(F \circ G \naturalTransformation \id_{\cat{D}}\).
    Intuitively, an equivalence is a function which is invertible up to natural isomorphism.
    
    Note that it is common to write all of the data needed to define a natural transformation as
    \begin{equation}
        \begin{tikzcd}[column sep=large]
            \cat{C}
            \arrow[r, "F"{name=F}, bend left=40]
            \arrow[r, "G"{name=G, swap}, bend right=40]
            & \cat{D}.
            \arrow[Rightarrow, from=F.south, to=G.north-|F, shorten <=5pt, shorten >=5pt]
        \end{tikzcd}
    \end{equation}
    
    \begin{exm}{Natural Transformations}{}
        These examples are taken from \cite{leinster} and make use of the notation used there representing a natural transformation as a collection of morphisms indexed by objects, \((\zeta_A)_{A \in \Ob(\cat{C})}\).
        \begin{itemize}
            \item A \defineindex{discrete category} is a category in which the only maps are the identity maps.
            If \(\cat{C}\) is a discrete category then a functor \(F \colon \cat{C} \to \cat{D}\) is simply a family of objects \((FA)_{A \in \Ob(\cat{C})}\).
            In this case a natural transformation \(\zeta \colon F \naturalTransformation G\) is just a family of maps \((\zeta_A \colon FA \to GA)_{A \in \Ob(\cat{C})}\).
            The naturality axiom is automatically fulfilled since it holds trivially for identities.
            \item Fix some natural number \(n\).
            For any commutative ring, \(R\), the \(n \times n\) matrices with entries in \(R\) form a monoid, \(M_n(R)\), under matrix multiplication.
            Any ring homomorphism \(R \to S\) induces a monoid homomorphism \(M_n(R) \to M_n(S)\) by acting elementwise on the entries of the matrix with the homomorphism.
            This defines a functor \(M_n \colon \CRing \to \Mon\).
            
            The elements of any ring, \(R\), form a monoid, \(U(R)\), under multiplication, this is an example of a forgetful functor \(U \colon \CRing \to \Mon\).
            
            Every \(n \times n\) matrix, \(X\), over a commutative ring \(R\) has a determinant \(\det_R(X) \in R\).
            The properties
            \begin{equation}
                \det_R(XY) = \det_R(X)\det_R(Y), \qqand \det_R(I) = 1
            \end{equation}
            tell us that for each commutative ring \(R\) the function \(\det_R \colon M_n(R) \to U(R)\) is a monoid homomorphism.
            Thus we have a family of maps
            \begin{equation}
                (\det_R \colon M_n(R) \to U(R))_{R \in \Ob(\CRing)}.
            \end{equation}
            This family of maps defines a natural transformation \(\det \colon M_n \naturalTransformation U\).
            
            \item Consider the category \(\FVect\).
            The map \((-)^* \colon \FVect \to \FVect\) sending each vector space to its dual is a contravariant functor.
            Thus the map \((-)^{**} \colon \FVect \to \FVect\) sending each vector space to its double dual is also a covariant functor.
            For each \(V \in \Ob(\FVect)\) we have a canonical isomorphism \(\zeta_V \colon V \to V^{**}\).
            Given \(v \in V\) the element \(\zeta_V(v) \in V^{**}\) is evaluation at \(v\), that is \(\zeta_V(v) \colon V^* \to \field\) maps \(\varphi \in V^*\) to \(\varphi(v) \in \field\).
            This defines a natural transformation \(\zeta \colon 1_{\FVect} \naturalTransformation (-)^{**}\) meaning that each vector space is naturally isomorphic to its double dual.
            Note that given \(F, G \colon \cat{C} \to \cat{D}\) we say that \(FA \isomorphic GA\) naturally in \(A\) if \(F\) and \(G\) are naturally isomorphic.
            In this case \(V \isomorphic V^{**}\) naturally in \(V\).
        \end{itemize}
    \end{exm}
    
    Functors are objects, and natural transformations are maps between them, so we should try to define a category.
    \begin{dfn}{Functor Category}{}
        Let \(\cat{C}\) and \(\cat{D}\) be categories.
        The \defineindex{functor category} \(\functorCategory{\cat{C}}{\cat{D}}\), also written \(\cat{D}^{\cat{C}}\), has functors \(F, G \colon \cat{C} \to \cat{D}\) as objects and natural transformations \(F \naturalTransformation G\) as morphisms.
        Composition of natural transformations is done component wise and the identity is the identity natural transformation which assigns the to each \(A \in \Ob(\cat{C})\) the identity morphism \(\id_{FA}\) in \(\cat{D}\).
    \end{dfn}
    
    An example of a functor category is the category of \(G\)-sets\footnote{that is sets upon which \(G\) acts} for some group \(G\), which is \(\functorCategory{G}{\Set}\), where \(G\) is viewed as a one object category.
    Similarly \(\functorCategory{G}{\Vect}\) is the category of \(\field\)-linear representations of \(G\).
    In both of these cases morphisms are so called equivariant maps.
    Given sets (vector spaces) \(X\) and \(Y\) upon which we have a \(G\) action (representation) defined an \defineindex{equivariant map} is a function (linear map) \(f \colon X \to Y\) such that \(f(g \mathbin{.} x) = g \mathbin{.} f(x)\), that is a map which commutes with the group action, so
    \begin{equation}
        \begin{tikzcd}
            X \arrow[r, "f"] \arrow[d, "g \mathbin{.} {}"'] & Y \arrow[d, "g \mathbin{.} {}"]\\
            X \arrow[r, "f"'] & Y
        \end{tikzcd}
    \end{equation}
    commutes.
    This diagram is exactly what we get if we specialise the diagram defining a natural transformation to functors \(G \to \Set\) (\(G \to \Vect\)).
    
    \section{Products}
    We want to generalise the Cartesian product of sets, which is defined as
    \begin{equation}
        A \times B \coloneqq \{(a, b) \mid a \in A \text{ and } b \in B\}.
    \end{equation}
    To do so we need to remove reference to elements, since this isn't a concept that we have in an arbitrary category.
    Clearly this definition defines a new set, so we have a new object in \(\Set\).
    In order to do away with reference to elements notice that we can define two functions \(p_A \colon A \times B \to A\) and \(p_B \colon A \times B \to B\) which project out the elements of a pair.
    That is \(p_A(a, b) = a\) and \(p_B(a, b) = b\).
    We can summarise this as
    \begin{equation}
        \begin{tikzcd}
            A & A \times B \arrow[l, "p_A"'] \arrow[r, "p_B"] & B.
        \end{tikzcd}
    \end{equation}
    
    The key insight comes in requiring that \(A \times B\) is, in a sense, the most general\footnote{this concept, both for products and more general (co)limits, is explained well in \cite{milewski}, which is where I'm replicating this argument from.} set satisfying this property, since the Cartesian product doesn't add in any unnecessary restrictions.
    Thus, if we have another set, \(X\), which is a candidate for defining the product then there must be morphisms \(f \colon X \to A\) and \(g \colon X \to B\) which are candidates for \(p_A\) and \(p_B\) respectively.
    We can now make \enquote{most general} precise.
    Given \(A \times B\) with \(X\), \(f\), and \(g\) we should be able to recreate \(f\) and \(g\) by first mapping to \(A \times B\) and then projecting out.
    This map to \(A \times B\) is what we call the product of \(f\) and \(g\), written \(f \times g\), \((f, g)\), or \(\binom{f}{g}\).
    We can summarise this definition by requiring that the following diagram commutes:
    \begin{equation}
        \begin{tikzcd}
            & X \arrow[d, "f \times g"{pos=0.6}, dashed, "!"'{xshift=0.075cm}] \arrow[dl, "f"'] \arrow[dr, "g"] & \\
            A & A \times B \arrow[l, "p_A"] \arrow[r, "p_B"'] & B.
        \end{tikzcd}
    \end{equation}
    Here we use a dashed arrow to denote that this diagram is telling us that this arrow exists, and the \(!\) tells us that this arrow is unique.
    
    This is an example of a more general concept in category theory, called a \defineindex{limit}.
    The general idea is that we can define some prototype, here \(A \times B\) with the maps \(p_A\) and \(p_B\).
    Then we posit a candidate object which factors through the prototype, then we look for an object satisfying this property.
    
    Notice that we now have done away with all references to elements of \(A \times B\), so we can generalise this definition to arbitrary categories.
    
    \begin{dfn}{Product}{def:product}
        Let \(\cat{C}\) be a category with objects \(A\) and \(B\).
        The product of \(A\) and \(B\), if it exists, is the object \(A \times B\) and the morphisms \(p_A \colon A \times B \to A\) and \(p_B \colon A \times B \to B\) such that
        \begin{equation}
            \begin{tikzcd}
                & X \arrow[d, "f \times g"{pos=0.6}, dashed, "!"'{xshift=0.075cm}] \arrow[dl, "f"'] \arrow[dr, "g"] & \\
                A & A \times B \arrow[l, "p_A"] \arrow[r, "p_B"'] & B
            \end{tikzcd}
        \end{equation}
        commutes for all \(X\) with morphisms to \(f \colon X \to A\) and \(g \colon X \to B\).
    \end{dfn}
    
    \begin{exm}{Products}{}
        \begin{itemize}
            \item In \(\Top\) the product of two topological spaces is the topological space formed from the Cartesian product of the two topological spaces equipped with the product topology, which is the coarsest topology for which all projections are continuous.
            \item In \(\RMod\) the product is the Cartesian product of the modules with addition defined componentwise and multiplication defined to be distributive.
            \item In \(\Grp\) the product is the direct product of groups, that is the Cartesian product of the underlying sets and then componentwise multiplication.
            \item In \(\Rel\) products are disjoint unions.
            \item In a poset viewed as a single object category the product of two elements is the greatest lower bound.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Product Category}{}
        Let \(\cat{C}\) and \(\cat{D}\) be categories.
        The \defineindex{product category} \(\cat{C} \times \cat{D}\) has
        \begin{itemize}
            \item as objects pairs of objects \((A, B)\) with \(A \in \Ob(\cat{C})\) and \(B \in \Ob(\cat{D})\);
            \item as morphisms \((A_1, B_1) \to (A_2, B_2)\) pairs of morphisms \((f, g)\) with \(f \colon A_1 \to A_2\) and \(g \colon B_1 \to B_2\) that is, \(f \in \cat{C}(A_1, A_2)\) and \(g \in \cat{D}(B_1, B_2)\);
            \item as composition componentwise composition from the two categories, that is \((f_2, g_2) \circ (f_1, g_1) = (f_2 \circ f_1, g_2 \circ g_1)\);
            \item as identities pairs of identities from the two categories, that is \(\id_{(A, B)} = (\id_A, \id_B)\).
        \end{itemize}
    \end{dfn}
    
    For the case where \(\cat{C}\) and \(\cat{D}\) are small the product category \(\cat{C} \times \cat{D}\) is exactly the categorical product of \(\cat{C}\) and \(\cat{D}\) in \(\Cat\).
    
    The main use of product categories is to define bifunctors, which are the generalisation of functors to two variables.
    A \defineindex{bifunctor} is exactly a functor \(\cat{C} \times \cat{D} \to \cat{E}\), but it's usually easier to think of \(\cat{C}\) and \(\cat{D}\) as being separate, just like how we might treat the horizontal and vertical axes as being independent in a function \(\reals^2 \to \reals\).
    
    \chapter{Monoidal Categories}
    \section{Tensor Products}
    \begin{dfn}{Bilinear}{}
        Let \(U\), \(V\) and \(W\) be vector spaces over \(\field\).
        A \defineindex{bilinear map} is a function \(T \colon U \times V \to W\) which is linear in each variable.
        That is, the maps \(T(-, v) \colon U \to W\) defined by \(u \mapsto T(u, v)\) and \(T(u, -) \colon V \to W\) defined by \(v \mapsto T(u, v)\) are linear.
    \end{dfn}
    
    The tensor product provides a way to combine two vector spaces into a new vector space.
    The simplest definition is that given vector spaces \(U\) and \(V\) over some field \(\field\) the tensor product \(U \otimes V\) consists of all linear combinations of elements of the form \(u \otimes v\) with \(u \in U\) and \(v \in V\).
    This definition makes direct reference to elements of the vector spaces, so isn't compatible with the spirit of category theory.
    The useful thing about the tensor product is it combines two vector spaces into one in such a way that we can define linear maps on the new space in terms of linear maps on the original spaces.
    For example, the linear map sending \(u \in U\) to \(2u\) automatically extends to a linear map on \(U \otimes V\) sending \(u \otimes v\) to \(2u \otimes v = (2u) \otimes v = 2(u \otimes v)\).
    This inspires the following definition.
    
    \begin{dfn}{Tensor Product}{}
        Let \(U\), \(V\), and \(W\) be vector spaces over \(\field\).
        The \defineindex{tensor product} of \(U\) and \(V\) is a vector space, which we call \(U \otimes V\), equipped with a bilinear map \(f \colon U \times V \to U \otimes V\) such that for all bilinear maps \(g \colon U \times V \to W\) there exists a unique linear map \(h \colon U \otimes V \to W\) such that \(g = h \circ f\).
        That is, such that
        \begin{equation}
            \begin{tikzcd}[column sep=large]
                U \times V \arrow[r, "f \text{ (bilinear)}"] \arrow[dr, "g \text{ (bilinear)}"'] & U \otimes V \arrow[d, dashed, "h \text{ (linear)}"]\\
                & W
            \end{tikzcd}
        \end{equation}
        commutes.
    \end{dfn}
    
    The important idea here is that a linear map \(U \otimes V \to W\) is just as good as a bilinear map \(U \times V \to W\), and since linear maps are much nicer to work with, and since \(U \times V\) is not a vector space, we usually prefer to work with \(U \otimes V\).
    
    Note that not all elements of \(U \otimes V\) are of the form \(u \otimes v\).
    For example, there is no way to write \(\ve{1} \otimes \ve{1} + \ve{2} \otimes \ve{2} \in V \otimes V\) in this form.
    This will be important later.
    
    The tensor product also gives us a natural way to combine two linear maps, we simply act on the relevant part of the tensor product with the relevant function.
    
    \begin{dfn}{Tensor Product of Linear Maps}{}
        Let \(U\), \(U'\), \(V\) and \(V'\) be vector spaces over \(\field\).
        Let \(f \colon U \to U'\) and \(g \colon V \to V'\).
        Then the \define{tensor product}\index{tensor product!of linear maps} of \(f\) and \(g\) is defined to be the map \(f \otimes g \colon U \otimes V \to U' \otimes V'\) given by defining \((f \otimes g)(u \otimes v) = f(u) \otimes g(v)\) and extending this to all of \(U \otimes V\) through linearity.
    \end{dfn}
    
    The tensor product of vector spaces can be extended to any inner product space in a straight forward manner, the inner product simply factorises.
    
    \begin{dfn}{Tensor Product of Inner Product Spaces}{}\index{tensor product!of Hilbert spaces}
        Let \((U, \braket{-}{-}_U)\) and \((V, \braket{-}{-}_W)\) be inner product spaces (Hilbert spaces) over \(\field\).
        Then \((U \otimes V, \braket{-}{-}_{U \otimes V})\) is an inner product space (Hilbert space) with the inner product
        \begin{equation}
            \braket{u \otimes v}{u' \otimes v'}_{U \otimes V} = \braket{u}{u'}_U \braket{v}{v'}_V.
        \end{equation}
    \end{dfn}
    
    \section{Monoidal Categories: The Idea}
    Monoidal categories generalise the tensor product to other categories.
    The idea being that tensor products allow us to work with multiple objects at the same time, or in parallel.
    Recall that we've seen four ways to consider categories:
    \begin{itemize}
        \item physical systems and the processes occurring;
        \item data types and the algorithms manipulating them;
        \item algebraic structures and structure preserving functions;
        \item logical propositions and implications between them.
    \end{itemize}
    We can consider the idea of a monoidal category in each of these frameworks:
    \begin{itemize}
        \item independent systems evolving separately;
        \item running algorithms in parallel;
        \item products or sums of geometric structures;
        \item using separate proofs of \(P\) and \(Q\) to prove \(P \land Q\).
    \end{itemize}
    
    \section{Monoidal Categories: The Need for Specificity}
    \label{sec:monoidal categories: the need for specificity}
    Consider processes \(A\), \(B\), and \(C\) with some notion of parallel composition, \(\otimes\).
    So, \(A \otimes B\) is what we get by doing both processes \(A\) and \(B\) at the same time.
    After playing around with this idea for a while one may come upon the question of what the relationship between
    \begin{equation}
        (A \otimes B) \otimes C \qqand A \otimes (B \otimes C)
    \end{equation}
    should be.
    It's not right for them to be equal, since this isn't the case for vector spaces with the tensor product or for sets with the Cartesian product, which is the equivalent in \(\Set\) as we'll see later.
    
    For example, viewed as sets we have \(((a, b), c)\) as an element of \((A \times B) \times C\), but \((a, (b, c))\) as an element of \(A \times (B \times C)\).
    Clearly there is a map \((A \times B) \times C \to A \times (B \times C)\) given by \(((a, b), c) \mapsto (a, (b, c))\).
    We say that this map associates these distinct objects.
    This map is also clearly invertible, the inverse being \((a, (b, c)) \mapsto ((a, b), c)\).
    So, we have \((A \times B) \times C \isomorphic A \times (B \times C)\).
    
    More questions arise when we think about what the processes \(A\), \(B\), and \(C\) are.
    For example there is often a concept of a trivial system where we don't do anything.
    We want it to be such that running this trivial system in parallel with another system is as if we weren't doing the trivial thing at all.
    However this is clearly not the case in, for example, a computer running a trivial algorithm, it still has to compile and run this trivial algorithm.
    So again we want the computation of \(A\) and the trivial system in parallel to be isomorphic to the computation of \(A\) alone.
    
    Another question, which we won't see an answer too until later, is does order matter?
    That is, should we treat \(A \otimes B\) and \(B \otimes A\) as the equal?
    What about isomorphic?
    It is also possible that they aren't even isomorphic, although in many of the cases we're interested in they will be.
    
    To answer these questions we have to be careful when defining a monoidal category, the task of the next section.
    
    \section{Monoidal Categories: The Definition}
    \begin{dfn}{Monoidal Category}{}
        A \defineindex{monoidal category} is formed from the following data:
        \begin{enumerate}
            \item a category, \(\cat{C}\);
            \item a \defineindex{tensor product functor}\index{tensor product}
            \begin{equation}
                {-} \otimes {-} \colon \cat{C} \times \cat{C} \to \cat{C},
            \end{equation}
            also called the \defineindex{monoidal product};
            \item a \defineindex{unit object} \(I \in \Ob(\cat{C})\);
            \item a natural isomorphism
            \begin{equation}
                \alpha \colon ({-} \otimes {-}) \otimes {-} \naturalTransformation {-} \otimes ({-} \otimes {-})
            \end{equation}
            called the \defineindex{associator} which has components
            \begin{equation}
                \alpha_{A,B,C} \colon (A \otimes B) \otimes C \to A \otimes (B \otimes C)
            \end{equation}
            for \(A, B, C \in \Ob(\cat{C})\);
            \item a natural isomorphism
            \begin{equation}
                \lambda \colon I \otimes {-} \naturalTransformation -
            \end{equation}
            called the \defineindex{left unitor}\index{unitor} which has components
            \begin{equation}
                \lambda_A \colon I \otimes A \to A
            \end{equation}
            for \(A \in \Ob(\cat{C})\);
            \item a natural isomorphism
            \begin{equation}
                \rho \colon {-} \otimes I \naturalTransformation -
            \end{equation}
            called the \defineindex{right unitor} which has components
            \begin{equation}
                \rho_A \colon A \otimes A \to A
            \end{equation}
            for \(A \in \Ob(\cat{C})\).
        \end{enumerate}
        This data must be such that the \defineindex{triangle equation} holds, which is that 
        \begin{equation}
            \begin{tikzcd}
                (A \otimes I) \otimes B \arrow[rr, "\alpha_{A, I, B}"] \arrow[dr, "\rho_A \otimes \id_B"'] && A \otimes (I \otimes B) \arrow[dl, "\id_A \otimes \lambda_B"]\\
                & A \otimes B &
            \end{tikzcd}
        \end{equation}
        commutes for all \(A, B \in \Ob(\cat{C})\), and the \defineindex{pentagon equation} holds, which is that
        \begin{equation}
            \begin{tikzcd}[column sep=-1cm]
                & (A \otimes (B \otimes C)) \otimes D \arrow[rr, "\alpha_{A, B \otimes C, D}"] && A \otimes ((B \otimes C) \otimes D) \arrow[dr, "\id_A \otimes \alpha_{B,C,D}", pos=0.6] & \\
                ((A \otimes B) \otimes C) \otimes D \arrow[ur, "\alpha_{A,B,C} \otimes \id_D", pos=0.4] \arrow[drr, "\alpha_{A\otimes B,C,D}"'] &&&& A \otimes (B \otimes (C \otimes D)) \\
                && (A \otimes B) \otimes (C \otimes D) \arrow[urr, "\alpha_{A,B,C\otimes D}"'] && 
            \end{tikzcd}
        \end{equation}
        commutes for all \(A, B, C, D \in \Ob(\cat{C})\).
    \end{dfn}
    
    This is quite a large definition so we'll break it down and hopefully this will make it less daunting.
    First lets look at what the data of a monoidal category tells us:
    \begin{enumerate}
        \item the category tells us the type of objects and morphisms we are considering;
        \item the tensor product functor tells us how to combine two objects to get a new object, and two morphisms to get a new morphism;
        \item the unit object represents a trivial process in which nothing happens;
        \item the associator allows us to move brackets around and only change things by an isomorphism, which means no important change occurs;
        \item the unitors allow us to remove the unit object when it is involved in a product, again only changing things by an isomorphism.
    \end{enumerate}

    Now lets look at the triangle equation.
    It tells us how the unitors and associator combine.
    Starting at the top left, with \((A \otimes I) \otimes B\) there are two ways to get to \(A \otimes B\).
    We can either remove \(I\) by applying \(\rho_A\) to the \(A \otimes I\) bit, which means applying \(\rho_A \otimes \id_B\) to \((A \otimes I) \otimes B\), or we can reassociate, by applying \(\alpha_{A,I,B}\) to get \(A \otimes (I \otimes B)\) then remove the unit by applying \(\lambda_B\) to the \(I \otimes B\) bit, which means applying \(\id_A \otimes \lambda_B\) to \(A \otimes (I \otimes B)\).
    The triangle equality says that both of these are actually the same, that is
    \begin{equation}
        \rho_A \otimes \id_B = (\id_A \otimes \lambda_A) \circ \alpha_{A,I,B}.
    \end{equation}

    Finally, lets look at the pentagon equation.
    This tells us how the associator can be applied to reassociate products of four objects, and it turns out that this is enough to completely specify how the associator reassociates products of any number of elements.
    Starting on the left we have the completely left associated \(((A \otimes B) \otimes C) \otimes D\).
    One path we can take to reassociate is to ignore \(D\) and reassociate \((A \otimes B) \otimes C\) to \((A \otimes (B \otimes C))\), this is done using \(\alpha_{A, B, C} \otimes \id_D\).
    Next we can ignore the fact that \(B \otimes C\) is the product of two objects and think of it as a single object, \(X\), so we have \((A \otimes X) \otimes D\), which we can reassociate using \(\alpha_{A, X, D} = \alpha_{A, B \otimes C, D}\) to get \(A \otimes ((B \otimes C) \otimes D)\).
    Finally we can ignore \(A\) and reassociate \((B \otimes C) \otimes D\) using \(\id_A \otimes \alpha_{B, C, D}\) to get \(A \otimes (B \otimes (C \otimes D))\).
    Alternatively, if we start with \(((A \otimes B) \otimes C) \otimes D\) we can treat \(A \otimes B\) as a single object, \(Y\), and reassociate \((Y \otimes C) \otimes D\) with \(\alpha_{Y, C, D} = \alpha_{A \otimes B, C, D}\) to get \((A \otimes B) \otimes (C \otimes D)\).
    Then we treat \(C \otimes D\) as a single object, \(Z\), and reassociate \((A \otimes B) \otimes Z\) using \(\alpha_{A, B, Z} = \alpha_{A, B, C \otimes D}\) to get \(A \otimes (B \otimes (C \otimes D))\).
    The pentagon equation tells us that both of these ways of reassociating from left to right give are the same, that is
    \begin{equation}
        (\id_A \otimes \, \alpha_{B, C, D}) \circ (\alpha_{A, B \otimes C, D}) \circ (\alpha_{A, B, C} \otimes \id_D) = \alpha_{A, B, C\otimes D} \circ \alpha_{A\otimes B, C, D}.
    \end{equation}
    
    To summarise, the triangle equation says that all ways of removing \(I\) from \((A \otimes I) \otimes B\) to get \(A \otimes B\) are the same, and the pentagon equation says that all ways of reassociating \(((A \otimes B) \otimes C) \otimes D\) to get \(A \otimes (B \otimes (C \otimes D))\) are the same.
    
    Another way of viewing the definition of a monoidal category is as the (vertical\footnote{cf.\@ horizontal categorification, or oidification, in which we realise a particular structure within a single object category and then generalise to multi-object categories, e.g.\@ a group is a one object category in which all morphisms are isomorphisms, and a groupoid is a category in which all morphisms are isomorphisms. Categories are already a generalisation of monoids in this sense.}) \defineindex{categorification} of a monoid.
    By this we mean that we generalise a structure applied to sets, here a monoid, to a structure on categories.
    Typically this is done by replacing elements with objects, functions with functors, and equality with isomorphism.
    Compare the definitions of a monoid, \(M\), and a monoidal category, \(\cat{C}\):
    \begin{multiitem}
        \mitemxx{Elements, \(a, b, c \in M\)}{Objects, \(A, B, C \in \Ob(\cat{C})\)}
        \mitemxx{A binary function \(\cdot \colon M \times M \to M\)}{A bifunctor \(\otimes\colon \cat{C} \times \cat{C} \to \cat{C}\)}
        \mitemxx{\((a \cdot b) \cdot c = a \cdot (b \cdot c)\)}{\((A \otimes B) \otimes C \isomorphic A \otimes (B \otimes C)\)}
        \mitemxx{Identity \(e \in M\) such that \(e \cdot a = a = a \cdot e\)}{Unit \(I \in \Ob(\cat{C})\) such that \(I \otimes A \isomorphic A \isomorphic A \otimes I\)}
    \end{multiitem}
    
    While these requirements seem quite complex they are incredibly powerful, and fairly easy to work with in practice, because they are so restrictive.
    It turns out that the monoidal product works exactly how our intuition says it does, and this is the crux of the next theorem, which we shan't prove.
    
    \begin{thm}{Coherence Theorem for Monoidal Categories}{}
        In a monoidal category and well-typed equation built from associators, unitors, and their inverses holds.
    \end{thm}
    
    By well-typed we simply mean that both sides of the equation must be morphisms with matching domain and codomain, and that any compositions are defined, so morphisms in the composition match domain to codomain.
    This means that we don't have to actually use the triangle and pentagon equations directly, as long as they hold we can write down pretty much anything we like as long as it makes sense and it will be true.
    
    \section{Monoidal Categories: The Examples}
    \subsection{\texorpdfstring{\(\Set\)}{Set}}
    The obvious choice for a monoidal product on \(\Set\) is the Cartesian product.
    We then need to look for a unit, something which we can take a Cartesian product with but not really change anything.
    One way we might come to an answer is to recognise that the unit behaves just like 1 in a product, hang on, 1 is something that we can represent as a set, a singleton, and there's our answer.
    Given some set \(A\) and some singleton\footnote{Since an isomorphism in \(\Set\) is a bijection all sets of the same size are isomorphic, so all singletons are the same for the purpose of category theory.} \(\{\bullet\}\) the Cartesian product \(A \times \{\bullet\}\) consists of elements \((a, \bullet)\) with \(a \in A\), and \(\{\bullet\} \times A\) consists of elements \((\bullet, a)\).
    From both of these we can easily recover \(A\) by just ignoring the \(\bullet\).
    This gives us our unitors.
    We've already seen in \cref{sec:monoidal categories: the need for specificity} how \(((a, b), c)) \mapsto (a, (b, c))\) defines the associator for the Cartesian product.
    Thus we can make the following definition.
    
    \begin{dfn}{\(\Set\) as a Monoidal Category}{}
        The category \(\Set\)\index{Set@\(\Set\)!as a monoidal category} can be promoted to a monoidal category by defining
        \begin{enumerate}
            \item the monoidal product to be the Cartesian product;
            \item the unit to be some singleton, \(I = \{\bullet\}\);
            \item the associator to have components \(\alpha_{A, B, C} \colon (A \times B) \times C \to A \times (B \times C)\) defined by \(((a, b), c) \mapsto (a, (b, c))\);
            \item the left unitor at \(A\) to be the function \(\lambda_A \colon \{\bullet\} \times A \to A\) defined by \((\bullet, a) \mapsto a\);
            \item the right unitor at \(A\) to be the function \(\rho_A \colon A \times \{\bullet\} \to A\) defined by \((a, \bullet) \mapsto a\).
        \end{enumerate}
    \end{dfn}
    
    \begin{wrn}
        This is not the only way we can make \(\Set\) into a monoidal category, but it is the only one that we'll use and it is usually what people are talking about when they say \(\Set\) is a monoidal category.
        Another ways of making \(\Set\) into a monoidal category involve taking \(A \otimes B = A \sqcup B\) to be the disjoint union with \(I = \emptyset\).
        Yet another choice is \(A \otimes B = A \sqcup B \sqcup (A \times B)\) with \(I = \emptyset\).
    \end{wrn}
    
    This common definition of \(\Set\) as a monoidal category is an example of a more general idea.
    In some category \(\cat{C}\) a \defineindex{terminal object}, \(T \in \Ob(\cat{C})\), is an object such that for any object \(A \in \Ob(\cat{C})\) there exists exactly one morphism \(A \to T\).
    Similarly a \defineindex{initial object}, \(I \in \Ob(\cat{C})\), is an object such that for any object \(A \in \Ob(\cat{C})\) there exists exactly one morphism \(I \to A\).
    Any category, \(\cat{C}\), with terminal objects and products can be made into a monoidal category by taking the monoidal product to be the categorical product and the unit to be the terminal object.
    Similarly and category with initial objects and coproducts\footnote{A \defineindex{coproduct} is defined similarly to a categorical product, but with the arrows reversed.} can be made into a monoidal category by taking the tensor product to be the coproduct and the unit to be the initial object.
    An example of a coproduct/initial object monoidal category is \(\Set\) with disjoint union as a monoidal product.
    
    \begin{lma}{}{}
        \(\Set\) equipped with the Cartesian product is a monoidal category.
        
        \begin{proof}
            We first demonstrate that the triangle equation holds.
            To do so we modify the diagram defining the triangle equation to show the elements being mapped, instead of the objects, and commutativity should be clear from this
            \begin{equation}
                \begin{tikzcd}[column sep=-0.5cm]
                    ((a, \bullet), b) \arrow[rr, "\alpha_{A,I,B}", mapsto] \arrow[dr, "\rho_A \times \id_B"', end anchor={[xshift=-1cm]}, pos=0.4, mapsto] && (a, (\bullet, b) \arrow[dl, "\id_A \times \lambda_B", end anchor={[xshift=1cm]}, pos=0.4, mapsto]\\
                    & (\rho_A(a, \bullet), \id_B(b)) = (a, b) = (\id_A(a), \lambda_B(\bullet, b)). &
                \end{tikzcd}
            \end{equation}
            Similarly, we can demonstrate the commutativity of the pentagon:
            \begin{equation}
                \begin{tikzcd}[row sep=large]
                    \parbox{4cm}{\centering\((\alpha_{A,B,C}((a, b), c), \id_D(D))\) \(=((a, (b, c)), d)\)} \arrow[r, mapsto, "\alpha_{A, B\times C, D}"] & \parbox{3.5cm}{\centering\(\alpha_{A, B \times C, D}((a, (b, c)), d)\) \(=(a, ((b, c), d))\)} \arrow[dd, "\id_A \times \alpha_{B,C,D}", mapsto] \\
                    (((a, b), c), d) \arrow[u, "\alpha_{A,B,C} \times \id_D", mapsto] \arrow[d, "\alpha_{A\times B,C,D}"', mapsto]\\
                    \parbox{4cm}{\centering\(\alpha_{A\times B,C,D}(((a, b), c), d)\) \(=((a, b), (c, d))\)} \arrow[r, "\alpha_{A,B,C\times D}"', end anchor={[yshift=-2ex, xshift=1.2em]}, start anchor={[xshift=-1em]}, mapsto, pos=0.8] & \parbox{4cm}{\centering\((\id_A(a), \alpha_{B,C,D}((b, c), d))\) \(= (a, (b, (c, d))) =\) \(\alpha_{A,B,C\times D}((a, b), (c, d))\).}
                \end{tikzcd}
            \end{equation}
        \end{proof}
    \end{lma}
    
    \subsection{\texorpdfstring{\(\Rel\)}{Rel}}
    Having had success with the Cartesian product as the monoidal product we'll try the same in \(\Rel\).
    If \(R \subseteq A \times B\) and \(S \subset C \times D\) are relations then \(R \times S \subseteq (A \times B) \times (C \times D)\) is a relation on \(A \times B\) and \(C \times D\) such that \((a, c)(R \times S)(b, d)\) if and only if \(a R b\) and \(c S d\).
    As before the singleton, \(\{\bullet\}\), acts as the unit, since as with \(\Set\) we can easily recover \(A\) from either \(\{\bullet\} \times A\) or \(A \times \{\bullet\}\).
    The only difference is that in \(\Rel\) our morphisms are relations, so instead of a function \(\{\bullet\} \times A \to A\) giving us \(A\) back we instead define a relation on \((\{\bullet\} \times A) \times A\) where \((\bullet, a) \sim a\), we use \(\sim\) here since this is an isomorphism in \(\Rel\), it is not an equivalence relation, since the two sets on either side of the relation are different.
    Similarly, the associators are given by changing the function in \(\Set\) into a relation.
    
    \begin{dfn}{\(\Rel\) as a Monoidal Category}{}
        The category \(\Rel\)\index{Rel@\(\Rel\)!as a monoidal category} can be promoted to a monoidal category by defining
        \begin{itemize}
            \item the monoidal product to be the Cartesian product;
            \item the unit to be some singleton, \(I = \{\bullet\}\);
            \item the associator to have components given by the relation \(\sim \subseteq [(A \times B) \times C] \times [A \times (B \times C)]\) defined by \(((a, b), c) \sim (a, (b, c))\);
            \item the left unitor to have components given by the relation \(\sim \subseteq [I \times A] \times A\) defined by \((\bullet, a) \sim a\);
            \item the right unitor to have components given by the relation \(\sim \subseteq [A \times I] \times A\) defined by \((a, \bullet) \sim a\).
        \end{itemize}
    \end{dfn}
    
    The proof that this makes \(\Rel\) a monoidal category is almost identical to the proof for \(\Set\), just replacing functions and equality with relations, so we won't repeat it.
    
    This is one of the first examples where we see that \(\Rel\) differs from \(\Set\), despite both having the Cartesian product as the monoidal product.
    The Cartesian product is \emph{not} a categorical product (in the sense of \cref{def:product}) in \(\Rel\), and \(\emptyset\) is the terminal object of \(\Rel\), not \(\{\bullet\}\).
    We'll see shortly that \(\Hilb\) as a monoidal category is also formed using a non-categorical product, making \(\Rel\) more like \(\Hilb\) in this sense.
    
    \subsection{\texorpdfstring{\(\Hilb\)}{Hilb}}
    It should not be surprising that \(\Vect\) and \(\Hilb\), and their finite-dimensional subcategories, can be made into monoidal categories, after all these were the models for the definition of monoidal categories we used at the start of the chapter.
    We'll define \(\Hilb\) as a monoidal category of complex Hilbert spaces, but the definition is exactly the same for \(\Vect\)\index{Vect@\(\Vect\)!as a monoidal category} as a monoidal category, just replace \(\complex\) with \(\field\) and ignore inner products.
    
    \begin{dfn}{\(\Hilb\) as a Monoidal Category}{}
        The category \(\Hilb\)\index{Hilb@\(\Hilb\)!as a monoidal category} can promoted to a monoidal category by defining
        \begin{itemize}
            \item the monoidal product to be the tensor product;
            \item the unit to be the one-dimensional Hilbert space\footnote{\(\complex\) is a vector space over itself, and a Hilbert space over itself with the inner product \(\braket{z}{w} = z^* w\).} \(\complex\);
            \item the associator to have components \(\alpha_{H,J,K} \colon (H \otimes J) \otimes K \to H \otimes (J \otimes K)\) defined on vectors of the form \((u \otimes v) \otimes w\) by \((u \otimes v) \otimes w \mapsto u \otimes (v \otimes w)\) and extended to all other vectors linearly;
            \item the left unitor to have components \(\lambda_A \colon I \otimes A \to A\) defined on vectors of the form \(1 \otimes v\) by \(1 \otimes v \mapsto v\) and extended to all other vectors linearly;
            \item the right unitor to have components \(\rho_A \colon A \otimes I \to A\) defined on vectors of the form \(v \otimes 1\) by \(v \otimes 1 \mapsto v\) and extended to all other vectors linearly.
        \end{itemize}
    \end{dfn}
    
    As with \(\Set\) there are other ways to make \(\Hilb\) into a monoidal category, but this is the one that is most useful and is what we will mean when we say \(\Hilb\) is a monoidal category.
    
    It's worth taking a minute to discus how the associator and unitors are defined.
    Recall that vectors in \(H \otimes J\) are of the form \(\sum_i u_i \otimes v_i\) with \(u_i \in H\) and \(v_i \in J\).
    We can define a linear map \(T \colon H \otimes J \to K\) by defining \(T(u \otimes v)\) for any \(u \in H\) and \(v \in J\), requiring that this map is linear then completely defines \(T\) since we must then have
    \begin{equation}
        T\left( \sum_i u_i \otimes v_i \right) = \sum_i T(u_i \otimes v_i).
    \end{equation}
    In the specific case of the associator we have that
    \begin{equation}
        (u_1 \otimes v_1) \otimes w_1 + (u_2 \otimes v_2) \otimes w_2 + \dotsb \xmapsto{\alpha_{H,J,K}} u_1 \otimes (v_1 \otimes w_1) + u_2 \otimes (v_2 \otimes w_2) + \dotsb,
    \end{equation}
    We further have that if \(T(u \otimes v)\) is defined for some specific \(u \in H\) and any \(v \in J\) then we can extend this linearly to be defined for any vector parallel to \(u\), which we might write as \(\lambda u\), by \(T(\lambda u \otimes v) = \lambda T(u \otimes v)\).
    Using this the left unitor is given by
    \begin{equation}
        \lambda_A(z \otimes u) = \lambda_A(z1 \otimes u) = z\lambda_A(1 \otimes u) = zu
    \end{equation}
    where we first view \(z \in \complex\) as a vector, and then as a scalar scaling the unit vector \(1 \in \complex\).
    The right unitor is defined analogously.
    
    \subsection{More Examples}
    \begin{exm}{}{}
        \begin{itemize}
            \item The category of (small) categories, \(\Cat\), is a monoidal category when equipped with the product of categories as the monoidal product and \(\one\), the category with a single object and only its identity morphism, as the unit.
            This is another example of a categorical product/terminal object monoidal category.
            \item The tensor product generalises to \(\RMod\) and makes \(\RMod\) a monoidal category with the tensor product over \(R\), \(\otimes_R\), serving as the monoidal product and \(R\), viewed as a module over itself, serving as the unit.
            This also extends to algebras over \(R\).
            \item The category of Abelian groups with group homomorphisms, \(\Ab\), is a monoidal category equipped with the monoidal product \(\otimes_{\integers}\), which is the product of Abelian groups viewed as \(\integers\)-modules, where \(na\) for \(n \in \integers\) and \(a \in G\) is \(a + \dotsb + a\) if \(n > 0\), \(e\) if \(n = 0\), and \(-a - \dotsb - a\) if \(n < 0\), with \(G\) viewed as an additive group and where each sum here has \(n\) terms.
            \item Any monoid can be viewed as a monoidal category by taking the set of objects to be the set of elements, the only morphisms to be identities, and the monoid product as the tensor product of objects.
            \item The category, \(\functorCategory{\cat{C}}{\cat{C}}\) of endofunctors on some category, \(\cat{C}\), is a monoidal category with composition of functors as the monoidal product and the identity functor as the unit.
            This example is important in defining monads.
            \item The category of pointed topological spaces, \(\pointedTop\), is a monoidal category with the smash product as the monoidal product and the pointed 0-sphere (that is the two point space \((\{-1, 1\}, \{\emptyset, \{-1\}, \{1\}, \{-1, 1\}\})\) with one point chosen as the base point) serving as the unit.
            The smash product, \(\wedge\), is defined between two pointed spaces, \((X, x_\bullet)\) and \((Y, y_\bullet)\), by forming \(X \times Y\) then identifying \((x, y_\bullet) \sim (x_\bullet, y)\) for all \(x \in X\) and \(y \in Y\), and then equipping the space \((X \times Y)/\sim\) with the quotient topology.
            \item Given any category, \(\cat{C}\), we can define a free monoidal category in an analogous way to defining a free monoid, simply take the objects to be finite sequences, \(A_1 \otimes \dotsb \otimes A_n\), of objects in \(\cat{C}\), then we have a morphism \(A_1\otimes \dotsb \otimes A_n \to B_1 \otimes \dotsb \otimes B_m\) only if \(m = n\), in which case the morphism is a finite sequence of morphisms \(f_1 \otimes \dotsb \otimes f_n\), with \(f_i \colon A_i \to B_i\) a morphism in \(\cat{C}\).
            The monoidal product is then the concatenation of these lists, giving \((A_1 \otimes \dotsb \otimes A_n) \otimes (B_1 \otimes \dotsb \otimes B_m) = A_1 \otimes \dotsb \otimes A_n \otimes B_1 \otimes \dotsb \otimes B_m\), where now \(m\) and \(n\) may differ.
            The unit is the empty sequence of objects.
        \end{itemize}
    \end{exm}
    
    \section{Interchange Law}
    The interchange law allows us to swap composition and the monoidal product, and is automatically satisfied by any monoidal category.
    
    \begin{thm}{Interchange Law}{}
        Let \(\cat{C}\) be a monoidal category with monoidal product \(\otimes\).
        Consider the following objects morphisms in \(\cat{C}\):
        \begin{equation}
            A \xrightarrow{f} B \xrightarrow{g} C, \qqand D \xrightarrow{h} E \xrightarrow{j} F.
        \end{equation}
        We have
        \begin{equation}
            (g \circ f) \otimes (j \circ h) = (g \otimes j) \circ (f \otimes h).
        \end{equation}
        \begin{proof}
            For the proof we use the notation \(\otimes(-, -) = - \otimes -\) to emphasise the functoriality of \(\otimes\).
            This allows us to write
            \begin{equation}
                (g \circ f) \otimes (j \circ h) = \otimes(g \circ f, j \circ h).
            \end{equation}
            Now consider \(\cat{C} \times \cat{C}\), the domain of \(\otimes\).
            We can identify that \((g \circ f, j \circ h) = (g, j) \circ (f, h)\), where composition on the left is in \(\cat{C}\) and composition on the right is in \(\cat{C} \times \cat{C}\).
            Thus in \(\cat{C} \times \cat{C}\) we have
            \begin{equation}
                (A, D) \xrightarrow{(f, h)} (B, E) \xrightarrow{(g, j)} (C, F)
            \end{equation}
            which we can write as
            \begin{equation}
                X \xrightarrow{p} Y \xrightarrow{q} Z.
            \end{equation}
            Then we have \(\otimes(q \circ p) = (\otimes(q)) \circ (\otimes(p))\) by the functoriality of \(\otimes\).
            That is, we have \((\otimes(g, j)) \circ (\otimes(f, h))\), which we can then write as \((g \otimes j) \circ (f \otimes h)\).
            Putting this together we have
            \begin{alignat}{2}
                (g \circ f) \otimes (j \circ h) &= \otimes(g \circ f, j \circ h)\\
                &= \otimes((g, j) \circ (f, h)) \qquad && \text{composition in } \cat{C} \times \cat{C} \notag\\
                &= (\otimes(g, j)) \circ (\otimes(f, h)) \qquad && \text{functoriality of } \otimes \notag\\
                &= (g \otimes j) \circ (f \otimes h). \notag \qedhere
            \end{alignat}
        \end{proof}
    \end{thm}
    
    \section{Graphical Notation}
    We can extend the graphical notation for categories to monoidal categories.
    To do so we make use of the idea that the monoidal product combines processes in parallel, and so we write the monoidal product by simply writing the two processes next to each other.
    For example, if we have objects \(A\) and \(B\) then, representing objects as identity morphisms, we write
    \begin{equation}
        A \otimes B = 
        \tikzsetnextfilename{mon-cat-graphical-mon-prod-of-objects}
        \begin{tikzpicture}[baseline=(A.base)]
            \draw[wire] (0, 0) -- (0, 1) node [left, midway] (A) {\(A\)};
            \draw[wire] (1, 0) -- (1, 1) node [left, midway] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    For morphisms \(f \colon A \to B\) and \(g \colon C \to D\) we draw \((f \otimes g) \colon A \otimes C \to B \otimes D\) as
    \begin{equation}
        f \otimes g = 
        \tikzsetnextfilename{mon-cat-graphical-mon-prod-of-morphisms}
        \begin{tikzpicture}[baseline=(f.base)]
            \node[morphism] (f) {\(f\)};
            \node[morphism, right= 0.5cm of f] (g) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- ++ (0, 1) node [midway, left] {\(B\)};
            \draw[wire] (g) -- ++ (0, -1) node [midway, right] {\(C\)};
            \draw[wire] (g) -- ++ (0, 1) node [midway, right] {\(D\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    The unit, \(I\), is drawn as the empty diagram:
    \begin{equation}
        I = 
        \tikzsetnextfilename{mon-cat-graphical-unit}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw[wire] (0, 0) -- (0, 1) node [midway, left] (I) {\(I\)};
            \node at (0.5, 0.5) (equals) {\(=\)};
            \draw[dashed, thick] (1, 0) rectangle (2, 1);
            \node at (2.5, 0.5) {\(=\)};
            \node at (4, 0) {};
        \end{tikzpicture}
        .
    \end{equation}
    This means we don't have to draw left or right unitors, since their action is to simply remove the unit, and we don't draw the unit any way.
    Similarly, we don't have to draw the associator since we're not drawing any brackets anyway.
    This only works because of the coherence theorem, which means that we can only build a single morphism of a given type, and so the unitors and associators don't give us any information that the domain and codomain don't.
    
    In the graphical notation the interchange law,
    \begin{equation}
        (g \circ f) \otimes (j \circ h) = (g \otimes j) \circ (f \otimes h),
    \end{equation}
    becomes
    \begin{equation}
        \tikzsetnextfilename{mon-cat-graphical-interchange-law}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[morphism] (f) {\(f\)};
            \node[morphism, above=of f] (g) {\(\phantomrlap{g}{f}\)};
            \node[morphism, right=1.25cm of f] (h) {\(\phantomrlap{h}{f}\)};
            \node[morphism, above=of h] (j) {\(\phantomrlap{j}{f}\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- (g) node [midway, left] {\(B\)};
            \draw[wire] (g) -- ++ (0, 1) node [midway, left] {\(C\)};
            \draw[wire] (h) -- ++ (0, -1) node [midway, right] {\(D\)};
            \draw[wire] (h) -- (j) node [midway, right] {\(E\)};
            \draw[wire] (j) -- ++ (0, 1) node [midway, right] {\(F\)};
            \draw[thick, decoration={calligraphic brace}, decorate] (-0.5, -1) -- (-0.5, 2.5);
            \draw[thick, decoration={calligraphic brace, mirror}, decorate] (0.5, -1) -- (0.5, 2.5);
            \draw[thick, decoration={calligraphic brace}, decorate] (1.25, -1) -- (1.25, 2.5);
            \draw[thick, decoration={calligraphic brace, mirror}, decorate] (2.25, -1) -- (2.25, 2.5);
            \node (equals) at (2.75, 0.75) {\(=\)};
            \begin{scope}[xshift=3.5cm]
                \node[morphism] (f) {\(f\)};
                \node[morphism, above=of f] (g) {\(\phantomrlap{g}{f}\)};
                \node[morphism, right=of f] (h) {\(\phantomrlap{h}{f}\)};
                \node[morphism, above=of h] (j) {\(\phantomrlap{j}{f}\)};
                \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
                \draw[wire] (f) -- (g) node [midway, left] {\(B\)};
                \draw[wire] (g) -- ++ (0, 1) node [midway, left] {\(C\)};
                \draw[wire] (h) -- ++ (0, -1) node [midway, right] {\(D\)};
                \draw[wire] (h) -- (j) node [midway, right] {\(E\)};
                \draw[wire] (j) -- ++ (0, 1) node [midway, right] {\(F\)};
                \draw[thick, decoration={calligraphic brace, mirror}, decorate] (-0.5, -0.3) -- (2.25, -0.3);
                \draw[thick, decoration={calligraphic brace}, decorate] (-0.5, 0.3) -- (2.25, 0.3);
                \draw[thick, decoration={calligraphic brace, mirror}, decorate] (-0.5, 1.2) -- (2.25, 1.2);
                \draw[thick, decoration={calligraphic brace}, decorate] (-0.5, 1.8) -- (2.25, 1.8);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    In order to proceed with calculations we need to define what it means for two diagrams to be \enquote{the same}.
    Exact equality is too strict, since it's impossible to actually draw two identical diagrams.
    Besides, we want to be able to move bits of the diagrams around, such as sliding morphisms along the wires, or drawing the wires in different ways.
    This leads to the following definition.
    
    \begin{dfn}{Planar Isotopy}{}
        Two diagrams are \defineindex{planar isotopic} if one can be deformed continuously into the other such that
        \begin{itemize}
            \item the diagrams remain confined to a rectangular region of the plane;
            \item input and output wires terminate at the bottom and top of this region, and the order they reach the edge cannot change;
            \item components never intersect.
        \end{itemize}
    \end{dfn}
    
    This can be made more rigorous through the idea of an isotopy, which is a homotopy between the two diagrams such that at every point the diagrams remain embedded in the rectangular region of the plane in a non-intersecting way and the inputs and outputs are fixed.
    
    For example, consider the following diagrams involving the morphisms \(f \colon I \to A \otimes B\), \(g \colon B \otimes C \to I\), and \(h \colon I \to I\).
    This example is a planar isotopy
    \begin{equation}
        \tikzsetnextfilename{mon-cat-graphical-planar-isotopy}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[morphism] at (-1, 1) {\(h\)};
            \node[morphism, minimum width=1.5cm] (f) at (0, 0) {\(f\)};
            \node[morphism, minimum width=1.5cm] (g) at (1, 1) {\(\phantomrlap{g}{f}\)};
            \draw[wire] ($(f.north) + (0.5, 0)$) -- ($(g.south) + (-0.5, 0)$);
            \draw[wire] ($(g.south) + (0.5, 0)$) -- ++ (0, -1.5);
            \draw[wire] ($(f.north) + (-0.5, 0)$) -- ++ (0, 1.5);
            \draw[lightgray] (-1.5, -0.75) rectangle (2, 1.75);
            \node (equals) at (2.5, 0.5) {\(\stackrel{\mathrm{iso}}{=}\)};
            \begin{scope}[xshift=4.1cm]
                \node[morphism] at (1, 0.5) {\(h\)};
                \node[morphism, minimum width=1.5cm] (f) at (0, 0) {\(f\)};
                \node[morphism, minimum width=1.5cm] (g) at (1, 1) {\(\phantomrlap{g}{f}\)};
                \draw[wire, rounded corners] ($(f.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (-0.3, 0.1) -- ++ (0.3, 0.1) -- ($(g.south) + (-0.5, 0)$);
                \draw[wire, rounded corners] ($(g.south) + (0.5, 0)$) -- ++ (0, -0.3) -- ++ (-0.4, -0.4) -- ++ (0, -0.8);
                \draw[wire, rounded corners] ($(f.north) + (-0.5, 0)$) -- ++ (0, 0.3) -- ++ (-0.4, 0.4) -- ++ (0, 0.4) -- ++ (0.2, 0.2) -- ++ (0, 0.2);
                \draw[lightgray] (-1.1, -0.75) rectangle (2, 1.75);
            \end{scope}
        \end{tikzpicture}
    \end{equation}
    since all that happened is the wires were bent and the \(h\) box moved under the \(f\) box and between the two wires, all of which can happen without ever crossing any wires or morphism boxes (which we treat as point like for the purposes of isotopies).
    Note that the light grey box represents the bounding region to which the diagram is confined.
    
    This example is \emph{not} a planar isotopy, since it is not possible to do this deformation without the \(h\) box either leaving the bounded region or crossing over one of the wires:
    \begin{equation}
        \tikzsetnextfilename{mon-cat-graphical-not-a-planar-isotopy}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[morphism] at (-1, 1) {\(h\)};
            \node[morphism, minimum width=1.5cm] (f) at (0, 0) {\(f\)};
            \node[morphism, minimum width=1.5cm] (g) at (1, 1) {\(\phantomrlap{g}{f}\)};
            \draw[wire] ($(f.north) + (0.5, 0)$) -- ($(g.south) + (-0.5, 0)$);
            \draw[wire] ($(g.south) + (0.5, 0)$) -- ++ (0, -1.5);
            \draw[wire] ($(f.north) + (-0.5, 0)$) -- ++ (0, 1.5);
            \draw[lightgray] (-1.5, -0.75) rectangle (2, 1.75);
            \node (equals) at (2.5, 0.5) {\(\stackrel{\mathrm{iso}}{\ne}\)};
            \begin{scope}[xshift=4cm]
                \node[morphism] at (2.3, 0.25) {\(h\)};
                \node[morphism, minimum width=1.5cm] (f) at (0, 0) {\(f\)};
                \node[morphism, minimum width=1.5cm] (g) at (1, 1) {\(\phantomrlap{g}{f}\)};
                \draw[wire] ($(f.north) + (0.5, 0)$) -- ($(g.south) + (-0.5, 0)$);
                \draw[wire] ($(g.south) + (0.5, 0)$) -- ++ (0, -1.5);
                \draw[wire] ($(f.north) + (-0.5, 0)$) -- ++ (0, 1.5);
                \draw[lightgray] (-1, -0.75) rectangle (2.7, 1.75);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    \subsection{Correctness Theorem}
    In order for the graphical notation to be useful we need the legal moves in the graphical notation, planar isotopies, to correspond to legal moves in monoidal categories, the axioms.
    Fortunately this is the case, leading to the following theorem, which we won't prove.
    
    \begin{thm}{Correctness of the Graphical Notation for Monoidal Categories}{}
        Any well-formed equation of the form \(f = g\) for morphisms \(f\) and \(g\) in a monoidal category follows from the axioms of monoidal categories if and only if it holds in the graphical notation up to planar isotopy.
    \end{thm}
    
    Note that by \enquote{well-formed} we mean that \(f\) and \(g\) have the same (co)domains.
    
    There are two parts to this theorem, known as soundness and completeness.
    For morphisms \(f\) and \(g\) in some monoidal category define
    \begin{itemize}
        \item \(P(f, g)\) to be the proposition \enquote{under the axioms of a monoidal category \(f = g\)};
        \item \(Q(f, g)\) to be the proposition \enquote{\(f\) and \(g\) are planar isotopic when expressed in the graphical notation}.
    \end{itemize}
    Then \defineindex{soundness} is the assertion that \(P(f, g) \implies Q(f, g)\) for all such \(f\) and \(g\).
    This is relatively easy to check, we simply check that every axiom of a monoidal category when translated into the graphical notation becomes a planar isotopy, and this is pretty trivial since most aspects of a monoidal category, units, unitors, and associators, simply aren't drawn in the graphical notation.
    The converse is \defineindex{completeness} which is the assertion that \(Q(f, g) \implies P(f, g)\) for all such \(f\) and \(g\).
    This is harder to prove, to do so we must show that any planar isotopy can be generated by a finite set of moves, each of which obeys the axioms of a monoidal category, and that combining these moves also obeys the axioms of a monoidal category.
    
    \section{States}
    \subsection{States}
    To follow the spirit of category theory we shouldn't talk about elements of sets, or particular vectors in some vector space.
    However, often we will have reason to want to refer to these things.
    Fortunately it is possible to do so using morphisms.
    
    Consider some set, \(A\), from which we want to pick an element, \(a\).
    We can identify this selection of a single element with a function \(f\colon\{\bullet\} \to A\) such that \(f(\bullet) = a\).
    Similarly, consider some vector space, \(V\), over \(\field\) from which we want to pick a vector, \(v\).
    We can identify this selection of a single element with a linear map \(T \colon \field \to V\) such that \(T(1) = v\), and then \(T(k) = T(k1) = kT(v) = kv\) for all \(k \in \field\).
    This way of getting around talking about the substructure of objects leads to the following definition.
    
    \begin{dfn}{State}{}
        Consider a monoidal category with unit \(I\) and object \(A\).
        A \defineindex{state} of \(A\) is a morphism \(I \to A\).
    \end{dfn}
    
    So the function \(f\) and the linear map \(T\) are states of \(A\) and \(V\) respectively.
    Another example is from \(\Rel\), where a state is a relation \(R \subseteq \{\bullet\} \times A\).
    Since this will be of the form \(R = \{(\bullet, a), (\bullet, b), \dotsc\}\) for \(a, b, \dotsc, \in A\) we see that a state picks out a subset, \(\{a, b, \dotsc\} \subseteq A\).
    
    Now consider a Hilbert space, \(H\).
    A state is a linear map \(\complex \to H\).
    We can define a state, \(T_v \colon \complex \to H\) for each \(v \in H\) through \(T_v(1) = v\), and so \(T_v(z) = T_v(z1) = zT_v(1) = zv\), but this is exactly the definition of a ket (\cref{def:bras and kets}), \(T_v = \ket{v}\), so a state of a Hilbert space is exactly a ket.
    This is, after all, why we chose the word state, since we ultimately want to consider states of some quantum system, which are usually considered to be kets in some Hilbert space.
    
    Graphically a state is a morphism \(I \to A\), so since we don't draw \(I\) it looks like the morphism takes no input.
    For this reason we change the shape of the box to be a triangle.
    For example, the state \(a \colon I \to A\) is represented as
    \begin{equation}
        a = 
        \tikzsetnextfilename{mon-cat-graphical-state}
        \begin{tikzpicture}[baseline=(a.base)]
            \node[state] (a) {\(a\)};
            \draw[wire] (a) -- ++ (0, 1) node [midway, left] {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    \subsection{Effects}
    Often in category theory after making a definition we should ask what happens if we reverse the arrows in the definition.
    In the case of states this leads to the following definition.
    \begin{dfn}{Effect}{}
        Consider a monoidal category with unit \(I\) and object \(A\).
        An \defineindex{effect} on \(A\) is a morphism \(A \to I\).
    \end{dfn}
    
    Consider a Hilbert space, \(H\).
    Then an effect is a linear map \(H \to \complex\).
    We can define one such map for each \(v \in H\), \(T_v \colon H \to \complex\) and \(T_v(w) = \braket{v}{w}\), but this is exactly the definition of the bra (\cref{def:bras and kets}), so \(T_v = \bra{v}\).
    Thus we can interpret an effect as an observation of a quantum system.
    
    An effect is drawn similarly to a state, the effect \(a \colon  A \to I\) is drawn as
    \begin{equation}
        a = 
        \tikzsetnextfilename{mon-cat-graphical-effect}
        \begin{tikzpicture}[baseline=(a.base)]
            \node[effect] (a) {\(a\)};
            \draw[wire] (a) -- ++ (0, -1) node [midway, left] {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    \subsection{Joint States}
    Next we have to ask how the notion of a state combines with the monoidal product.
    This leads to the following definition.
    
    \begin{dfn}{Joint State}{}
        Consider a monoidal category with unit \(I\) and objects \(A\) and \(B\).
        A \defineindex{joint state} of \(A\) and \(B\) is a morphism \(I \to A \otimes B\).
    \end{dfn}
    
    This definition generalises to any number of objects.
    
    We might draw a joint effect \(c \colon I \to A \times B\) as
    \begin{equation}
        c = 
        \tikzsetnextfilename{mon-cat-graphical-joint-state}
        \begin{tikzpicture}[baseline=(c.base)]
            \node[state] (c) {\(c\)};
            \draw[wire] ($(c.north) + (-0.25, 0)$) -- ++ (0, 0.75) node [midway, left] {\(A\)};
            \draw[wire] ($(c.north) + (0.25, 0)$) -- ++ (0, 0.75) node [midway, right] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    After playing around with this definition for a while one might recognise two types of joint effect, those which factor and those which don't.
    
    \begin{dfn}{Product and Entangled State}{}
        Consider a monoidal category with unit \(I\) and objects \(A\) and \(B\).
        A joint state, \(c \colon I \to A \otimes B\), is a \defineindex{product state} if it is of the form
        \begin{equation}
            I \xrightarrow{\lambda_I^{-1}} I \otimes I \xrightarrow{a \otimes b} A \otimes B.
        \end{equation}
        That is, we can write \(c = (a \otimes b) \circ \lambda_I^{-1}\) as a product of two states, \(a \colon I \to A\) and \(b \colon I \to B\).
        
        A joint state which cannot be written in this form is called an \defineindex{entangled state}.
    \end{dfn}
    
    \begin{rmk}
        Note that \(\lambda_I = \rho_I\) in any monoidal category, so we could equally well have defined a product state to be of the form
        \begin{equation}
            I \xrightarrow{\rho_I^{-1}} I \otimes I \xrightarrow{a \otimes b} A \otimes B
        \end{equation}
    \end{rmk}
    
    Graphically if \(c\) is a product state then
    \begin{equation} 
        \tikzsetnextfilename{mon-cat-graphical-product-state}
        \begin{tikzpicture}[baseline=(c.base)]
            \node[state] (c) {\(\phantomrlap{c}{b}\)};
            \draw[wire] ($(c.north) + (-0.25, 0)$) -- ++ (0, 0.75) node [midway, left] {\(A\)} coordinate (top);
            \draw[wire] ($(c.north) + (0.25, 0)$) -- ++ (0, 0.75) node [midway, right] {\(B\)};
            \node[right=0.5cm of c] {\(=\)};
            \node[state, right=1.5cm of c] (a) {\(\phantomrlap{a}{b}\)};
            \node[state, right=0.5cm of a] (b) {\(b\)};
            \draw[wire] (a) -- (a |- top) node [midway, left] {\(A\)};
            \draw[wire] (b) -- (b |- top) node [midway, right] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    \begin{exm}{}{}
        In \(\Set\) states are elements, so
        \begin{itemize}
            \item joint states of \(A\) and \(B\) are elements of \(A \times B\);
            \item product states are elements \((a, b) \in A \times B\);
            \item entangled states don't exist.
        \end{itemize}
        In \(\Rel\) states are subsets, so
        \begin{itemize}
            \item joint states of \(A\) and \(B\) are subsets of \(A \times B\);
            \item product states are \enquote{square} subsets of the form \(U \times V \subseteq A \times B\) where \(U \subseteq A\) and \(V \subseteq B\), for more details see \cref{fig:square subset};
            \item entangled states are any subsets of \(A \times B\) not of this form.
        \end{itemize}
        In \(\Hilb\) states are vectors, so
        \begin{itemize}
            \item joint states of \(H\) and \(K\) are vectors in \(H \otimes K\);
            \item product states are factorisable states, i.e.\@ states which can be written as \(u \otimes v\) for \(u \in H\) and \(v \in K\);
            \item entangled states are states which aren't factorisable, which are exactly the entangled states of quantum mechanics, such as\footnote{here we identify a vector with its ket, and we write implicit tensor products, meaning \(\ket{ab} = \ket{a} \otimes \ket{b} \leftrightarrow a \otimes b\), the states \(\ket{0}\) and \(\ket{1}\) can be taken as two orthonormal basis vectors in \(\complex^2\).} \(\ket{01} + \ket{10}\).
        \end{itemize}
    \end{exm}
    
    \begin{figure}
        \tikzsetnextfilename{mon-cat-square-subset-is-product-state-in-rel}
        \begin{tikzpicture}
            \fill[highlight!10] (0, 0) rectangle (4, 4);
            \draw[->] (0, 0) -- (4, 0) node [below, highlight] {\(A\)};
            \draw[->] (0, 0) -- (0, 4) node [left, highlight] {\(B\)};
            \draw[ultra thick, highlight] (1, 0) -- (3, 0) node [below, midway] {\(U\)};
            \draw[ultra thick, highlight] (0, 2) -- (0, 3) node [left, midway] {\(V\)};
            \draw[thick, highlight, fill=highlight!40] (1, 2) rectangle (3, 3) node [midway] {\(U \times V\)};
            \node[below left, highlight] at (4, 4) {\(A \times B\)};
        \end{tikzpicture}
        \caption{An example of a \enquote{square} subset of \(\reals \times \reals\), identified with the plane, given \(U, V \subseteq \reals\) we get a rectangle \(U \times V\) in the plane.}
        \label{fig:square subset}
    \end{figure}
    
    This demonstrates another way in which \(\Rel\) is more like \(\Hilb\) than \(\Set\), both \(\Rel\) and \(\Hilb\) have entangled states.
    
    %   Appdendix
    \appendixpage
    \begin{appendices}
        \include{parts/maths-defs}
    \end{appendices}
    
    \backmatter
    \printbibliography
    %    \renewcommand{\glossaryname}{Acronyms}
    %    \printglossary[acronym]
    \printindex
\end{document}
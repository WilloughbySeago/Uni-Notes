% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

%% Packages
\usepackage{xfrac}
\usepackage{csquotes}
\usepackage{multienum}
\newenvironment{multiitem}{%
    \multienumerate\renewcommand{\labelname}{\textbullet}%
}{%
    \endmultienumerate%
}

% Tikz stuff
\usepackage{tikz}
%\tikzset{>=latex}
% External
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
% Other libraries
\undef{\eth}
\undef{\digamma}
\undef{\backepsilon}
\usetikzlibrary{positioning, decorations.pathreplacing, calligraphy, calc, shapes.geometric, decorations.markings}
\usepackage{tikz-cd}

\BeforeBeginEnvironment{tikzcd}{\tikzexternaldisable}
\AfterEndEnvironment{tikzcd}{\tikzexternalenable}

% ZX calculus
\tikzset{Z spider/.style={fill=Green!50, draw=Green, text=black, circle, minimum width=1mm, inner sep=2pt, thick}}
\tikzset{X spider/.style={fill=Red!50, draw=Red, text=black, circle, minimum width=1mm, inner sep=2pt, thick}}

% Graphical calculus
\tikzset{wire/.style={thick}}
\tikzset{morphism/.style={thick, draw, inner sep=2pt}}
\tikzset{triangle/.style={regular polygon, regular polygon sides=3}}
\tikzset{state/.style={morphism, triangle, shape border rotate=180}}
\tikzset{effect/.style={morphism, triangle}}
\tikzset{scalar/.style={draw, shape=circle, inner sep=2pt}}
\tikzset{over wire/.style={thick, preaction={draw, background color, line width=#1}}}
\tikzset{over wire/.default={1.5mm}}
\tikzset{daggerable morphism/.style={morphism, trapezium, trapezium left angle=90, trapezium right angle=70}}
\tikzset{daggered morphism/.style={morphism, trapezium, trapezium left angle=70, trapezium right angle=90, shape border rotate=180}}
\tikzset{left dual/.style={wire, postaction={decoration={markings, mark=at position #1 with {\arrow{>}}}, decorate}}}
\tikzset{right dual/.style={wire, postaction={decoration={markings, mark=at position #1 with {\arrow{<}}}, decorate}}}
\tikzset{left dual/.default={0.5}}
\tikzset{right dual/.default={0.5}}
\tikzset{dual morphism/.style={daggerable morphism, trapezium, trapezium left angle=90, trapezium right angle=70, shape border rotate=180}}
\tikzset{monoid dot/.style={circle, fill, inner sep=0pt, minimum width=2mm}}
\tikzset{comonoid dot/.style={circle, draw, fill=background color, inner sep=0pt, minimum width=2mm}}

\newcommand{\monoidProduct}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[monoid product/.style={circle, fill, inner sep=0pt, minimum width=1.25mm}]
        \node[monoid product] {};
        \draw[wire] (0, 0) -- ++ (0, 0.15);
        \draw[wire] (-0.15, -0.15) arc (180:0:0.15);
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\monoidIdentity}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[monoid product/.style={circle, fill, inner sep=0pt, minimum width=1.25mm}]
        \node[monoid product] {};
        \draw[wire] (0, 0) -- ++ (0, 0.25);
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\monoidProductAdjoint}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[monoid product/.style={circle, fill, inner sep=0pt, minimum width=1.25mm}]
        \draw[wire] (0, 0) -- ++ (0, -0.15);
        \draw[wire] (-0.15, 0.15) arc (180:360:0.15);
        \node[monoid product] {};
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\monoidIdentityAdjoint}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[monoid product/.style={circle, fill, inner sep=0pt, minimum width=1.25mm}]
        \draw[wire] (0, 0) -- ++ (0, -0.25);
        \node[monoid product] {};
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\comonoidProduct}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[monoid product/.style={circle, draw, fill=background color, inner sep=0pt, minimum width=1.25mm}]
        \draw[wire] (0, 0) -- ++ (0, -0.15);
        \draw[wire] (-0.15, 0.15) arc (180:360:0.15);
        \node[monoid product] {};
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\comonoidProductAdjoint}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[monoid product/.style={circle, draw, fill=background color, inner sep=0pt, minimum width=1.25mm}]
        \draw[wire] (0, 0) -- ++ (0, 0.15);
        \draw[wire] (-0.15, -0.15) arc (180:0:0.15);
        \node[monoid product] {};
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\comonoidIdentity}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[monoid product/.style={circle, draw, fill=background color, inner sep=0pt, minimum width=1.25mm}]
        \draw[wire] (0, 0) -- ++ (0, -0.25);
        \node[monoid product] {};
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\comonoidIdentityAdjoint}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[monoid product/.style={circle, draw, fill=background color, inner sep=0pt, minimum width=1.25mm}]
        \node[monoid product] {};
        \draw[wire] (0, 0) -- ++ (0, 0.25);
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\pairofpantsProduct}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[scale=0.5, >={Classical TikZ Rightarrow[scale=0.5, line width=0.15mm]}, every line/.style]
        \draw[left dual, rounded corners] (0, 0) -- ++ (0, 0.2) -- ++ (0.5, 0.2) -- ++ (0, 0.2);
        \draw[right dual, rounded corners] (1.5, 0) -- ++ (0, 0.2) -- ++ (-0.5, 0.2) -- ++ (0, 0.2);
        \draw[left dual=0.55] (0.5, 0) arc (180:0:0.25);
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\pairofpantsIdentity}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[scale=0.5, >={Classical TikZ Rightarrow[scale=0.5, line width=0.15mm]}, every line/.style]
        \draw[left dual=0.55, rounded corners] (-0.25, 0.25) arc (180:360:0.25);
    \end{tikzpicture}%
    \tikzexternalenable%
}
\newcommand{\swapMorphism}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[scale=0.25]
        \draw[wire] (0, 0) arc (180:90:0.5) arc (270:360:0.5);
        \draw[wire] (1, 0) arc (0:90:0.5) arc (270:180:0.5);
    \end{tikzpicture}
    \tikzexternalenable%
}
\newcommand{\braidMorphism}{%
    \tikzexternaldisable%
    \begin{tikzpicture}[scale=0.25]
        \draw[wire] (0, 0) arc (180:90:0.5) arc (270:360:0.5);
        \draw[over wire=1mm] (1, 0) arc (0:90:0.5) arc (270:180:0.5);
    \end{tikzpicture}
    \tikzexternalenable%
}

\RequirePackage[%
sorting=none,  % Don't sort the references, they will appear in the order they are first cited
style=numeric-comp,  % citations like [1] and citing 1, 2, and 3 gives [1-3]
giveninits=true,  % style author names as J. Doe
language=british  % Make dates dd/mm/yyyy
]{biblatex}
\addbibresource{references.bib}
\usepackage{xurl}

% References, should be last things loaded
\usepackage[pdfauthor={Willoughby Seago},pdftitle={Categories and Quantum Information},pdfkeywords={category theory, quantum information, quantum computing, monoidal category},pdfsubject={Gauge Theories}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}

% Highlight colour
\definecolor{Red}{HTML}{D00000}
\definecolor{Yellow}{HTML}{FFBA08}
\definecolor{Blue}{HTML}{3F88C5}
\definecolor{Navy}{HTML}{032B43}
\definecolor{Green}{HTML}{136F63}

\colorlet{highlight}{Green}

\colorlet{codeTextColor}{Navy}

% Title page info
\title{Categories and Quantum Informatics}
\author{Willoughby Seago}
\date{January 17, 2023}
% \subtitle{}
% \subsubtitle{}

% Commands
\lstdefinestyle{haskell}{
    language=haskell,
    literate=
        {->}{$\to$}{1}
        {=>}{$\Rightarrow$}{1}
}

% Change tikzcd background colour in environments
\colorlet{background color}{white}  % normal background is white
% background in lemma
\colorlet{lemma background}{black!5}
% background in definitions
\colorlet{definition background}{beige!35}
% background in theorems
\colorlet{theorem background}{black!5}
\AtBeginEnvironment{lma}{\colorlet{background color}{lemma background}}
\AtEndEnvironment{lma}{\colorlet{background color}{white}}
\colorlet{example background}{azure(web)(azuremist)!45}
\AtBeginEnvironment{exm}{\colorlet{background color}{example background}}
\AtEndEnvironment{exm}{\colorlet{background color}{white}}
\tikzcdset{background color=background color}
\AtBeginEnvironment{dfn}{\colorlet{background color}{definition background}}
\AtEndEnvironment{dfn}{\colorlet{background color}{white}}
\AtBeginEnvironment{thm}{\colorlet{background color}{theorem background}}
\AtEndEnvironment{thm}{\colorlet{background color}{white}}

% Text
\newcommand*{\course}[1]{\textit{#1}}
\newcommand{\Haskell}{\textit{Haskell}}

% Maths
\newcommand{\e}{\symrm{e}}
\DeclarePairedDelimiter{\denotes}{\lBrack}{\rBrack}
\newcommand{\cat}[1]{\symbfsfup{#1}}
\makeatletter
\newcommand{\c@egory}[1]{\symbfsfup{#1}}
\newcommand{\Set}{\c@egory{Set}}
\newcommand{\Rel}{\c@egory{Rel}}
\renewcommand{\field}{\symbb{k}}
\newcommand{\Mat}[1][\field]{{\c@egory{Mat}_{#1}}}
\newcommand{\Vect}[1][\field]{{\c@egory{Vect}_{#1}}}
\newcommand{\FVect}[1][\field]{{\c@egory{FVect}_{#1}}}
\newcommand{\Hilb}{\c@egory{Hilb}}
\newcommand{\FHilb}{\c@egory{FHilb}}
\newcommand{\Mon}{\c@egory{Mon}}
\newcommand{\Grp}{\c@egory{Grp}}
\newcommand{\Ring}{\c@egory{Ring}}
\newcommand{\CRing}{\c@egory{CRing}}
\newcommand{\Field}{\c@egory{Field}}
\newcommand{\RMod}[1][R]{#1{-}\c@egory{Mod}}
\newcommand{\Top}{\c@egory{Top}}
\newcommand{\pointedTop}{\c@egory{Top}_{\bullet}}
\newcommand{\Cat}{\c@egory{Cat}}
\newcommand{\Ab}{\c@egory{Ab}}
\newcommand{\Pos}{\c@egory{Pos}}
\newcommand{\LieGrp}{\c@egory{LieGrp}}
\newcommand{\LieAlg}{\c@egory{LieAlg}}
\newcommand{\one}{\c@egory{1}}
\newcommand{\Hask}{\c@egory{Hask}}
\newcommand{\Graph}{\c@egory{Graph}}
\makeatother
\DeclareMathOperator{\Ob}{Ob}
\DeclareMathOperator{\id}{id}
\newcommand{\isomorphic}{\cong}
\newcommand{\op}{\symrm{op}}
\renewcommand{\ve}[1]{e_{#1}}
\newcommand{\topology}{\symcal{T}}
\newcommand{\powerset}{\symcal{P}}
\newcommand{\phantomrlap}[2]{\mathrlap{#1}\phantom{#2}}
\newcommand{\naturalTransformation}{\Rightarrow}
\DeclarePairedDelimiterX{\functorCategory}[2]{[}{]}{#1, #2}
\newcommand{\hermit}{\dagger}
\undef\det
\DeclareMathOperator{\det}{det}
\AtBeginDocument{\renewcommand{\mapstochar}{\rule[0.5pt]{0.5pt}{4.3pt}\mkern-1mu}}
\newcommand{\true}{\symtt{true}}
\newcommand{\false}{\symtt{false}}
\DeclarePairedDelimiter{\cardinality}{\lvert}{\rvert}
\newcommand{\leftdual}{\dashv}
\newcommand{\name}[1]{\ulcorner #1 \urcorner}
\newcommand{\coname}[1]{\llcorner #1 \lrcorner}
\newcommand{\equaliso}{\stackrel{\mathrm{iso}}{=}}
\newcommand{\notequaliso}{\stackrel{\mathrm{iso}}{\ne}}
\DeclareMathOperator{\tr}{tr}
\newcommand{\const}{\mathrm{const}}
\newcommand{\leftadjoint}{\vdash}

\includeonly{parts/maths-defs}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{}
    \tableofcontents
    \listoffigures
    \mainmatter
    
    \chapter{Introduction}
    The material delivered in the lectures is included in these notes, but so is extra material pulled from the textbook \cite{heunen}, as well as other sources, such as \cite{leinster}.
    I include various additional examples, some which require some mathematical background to understand.
    Some definitions are included in the appendices, but those unfamiliar with the material in these examples should feel free to just skip them.
    I've also included pointers to notes from other courses where appropriate, particularly the courses \course{Symmetries of Quantum Mechanics} and \course{Symmetries of Particles and Fields}, which both cover the areas of representation theory and Lie theory.
    The notes from these courses and others can be found at \url{https://github.com/WilloughbySeago/Uni-Notes}.
    Again, any unfamiliar material from these courses can be skipped.
    A fair few of the examples simply come from relevant Wikipedia pages, and I haven't performed detailed checks of all the facts in these cases.
    
    There are a few notational things which don't align with the course.
    A big one is leaving out brackets for functors, writing \(FA\) and \(Ff\) instead of \(F(A)\) and \(F(f)\).
    The use of \(-\) as a blank to be filled in with some object is also not used in the course.
    
    \part{Introduction}
    
    \chapter{ZX Calculus}
    In this chapter we will introduce \defineindex{ZX calculus}.
    This is a diagramatic notation for performing calculations.
    ZX calculus is mathematically rigorous, and developing the maths explaining this is a large part of this course.
    ZX calculus provides a higher level of abstraction that a quantum circuit, focusing less on implementation and more on what the circuit is doing.
    ZX calculus is built from a relatively small number of building blocks.
    It is the freedom we have in combining these that makes ZX calculus so powerful.
    
    We'll introduce ZX calculus in a seemingly backwards manner, first introducing which sorts of diagrams we can have, then how to manipulate the diagrams then what the diagrams mean.
    
    \section{Types of Diagrams}
    A diagram in ZX calculus is somewhat like a flowchart.
    The playing field is the two-dimensional page.
    We imagine that time goes upwards and space extends to the left and right.
    This means that a process described by a ZX calculus starts by entering the bottom of the diagram and ends when we leave the top of the diagram.
    Qubits are represented by wires, which are just lines.
    Processes are represented by boxes, for now we won't focus on what the process might be.
    The following diagram represents a process which takes in three qubits and produces two qubits:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-example-process}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, 0) -- ++ (1.5, 0) -- ++ (-0.25, 0.75) -- ++ (-1.25, 0) -- cycle;
            \draw (0.3, 0) -- ++ (0, -1);
            \draw (0.75, 0) -- ++ (0, -1);
            \draw (1.2, 0) -- ++ (0, -1);
            \draw (0.4, 0.75) -- ++ (0, 1);
            \draw (0.85, 0.75) -- ++ (0, 1);
        \end{tikzpicture}
        .
    \end{equation}
    
    In diagrams it isn't important exactly how we draw the wires, so long as they are connected in the same way, so in the same order both on the box and along the top and bottom, the diagram corresponds to the same equation.
    For example, the following is equivalent to the previous diagram
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-example-process-with-weird-wires}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, 0) -- ++ (1.5, 0) -- ++ (-0.25, 0.75) -- ++ (-1.25, 0) -- cycle;
            \draw (0.3, 0) -- ++ (0, -0.25) arc (0:-180:0.3) arc (0:180:0.3) -- ++ (0, -0.75);
            \draw[rounded corners] (0.75, 0) -- ++ (0, -0.15) -- ++ (0.5, -0.2) -- ++ (0, -0.35) -- ++ (-0.5, -0.2) -- ++ (0, -0.15);
            \draw[rounded corners] (1.2, 0) -- ++ (0, -0.15) -- ++ (-0.5, -0.2) -- ++ (0, -0.35) -- ++ (0.5, -0.2) -- ++ (0, -0.15);
            \draw[rounded corners] (0.4, 0.75) -- ++ (0, 0.2) -- ++ (-1, 0.6) -- ++ (0, 0.2);
            \draw[rounded corners] (0.85, 0.75) -- ++ (0, 0.1) -- ++ (0.2, 0.2) -- ++ (0, 0.1) coordinate (A);
            \draw (A) arc (0:330:0.3) coordinate (B);
            \draw[rounded corners] (B) -- ++ (0.15, 0.25) -- ++ (0, 0.5);
        \end{tikzpicture}
        .
    \end{equation}
    
    We are also free to change the orientation of the box, so long as the the connectivity stays the same.
    This is why we draw the box as a trapezium without rotational symmetry.
    For example, the following diagram is equivalent to both of the previous diagrams.
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-example-process-with-rotated-box}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, 0) -- ++ (-0.25, 0.75) -- ++ (1.5, 0) -- ++ (0, -0.75) -- cycle;
            \draw (0.05, 0.75) -- ++ (0, 0.3) arc (0:180:0.3) -- ++ (0, -1.15) arc (180:270:0.3) -- ++ (1.5, 0) arc (90:0:0.3) -- ++ (0, -0.3);
            \draw (0.5, 0.75) -- ++ (0, 0.3) arc (180:90:0.3) -- ++ (1, 0) arc (90:0:0.3) -- ++ (0, -1.4) arc (0:-90:0.3) -- ++ (-0.6, 0) arc (90:180:0.3) -- ++ (0, -0.05);
            \draw (0.95, 0.75) -- ++ (0, 0.1) arc (180:0:0.3) -- ++ (0, -1.1) arc (0:-90:0.3) -- ++ (-0.5, 0) arc (90:180:0.3) -- ++ (0, -0.15);
            \draw (0.4, 0) -- ++ (0, -0.1) arc (0:-90:0.2) -- ++ (-0.3, 0) arc (270:180:0.2) -- ++ (0, 1.45) arc (180:90:0.2) -- ++ (0.7, 0) arc (-90:0:0.2);
            \draw (0.85, 0) -- ++ (0, -0.1) arc (180:270:0.2) -- ++ (0.5, 0) arc (-90:0:0.2) -- ++ (0, 1.45) arc (0:90:0.2) -- ++ (-0.7, 0) arc (270:180:0.2);
        \end{tikzpicture}
        .
    \end{equation}

    A sensible question to ask now is what process does this box represent.
    We'll get to this.
    For now we'll just say that the process can be built up of fundamental process.
    There are four processes which we use to build any diagram in ZX calculus.
    They are
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-building-blocks}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[Z spider] (A) at (0, 0) {\(\alpha\)};
            \draw (A) -- ++ (0, 1);
            \node[Z spider] (B) at (1.5, 0) {\(\alpha\)};
            \draw (B) -- ++ (0, 1);
            \draw[rounded corners=7pt] (B) -- ++ (-0.5, 0) -- ++ (0, -0.75);
            \draw[rounded corners=7pt] (B) -- ++ (0.5, 0) -- ++ (0, -0.75);
            \node[X spider] (C) at (3, 0) {\(\alpha\)};
            \draw (C) -- ++ (0, 1);
            \node[X spider] (D) at (4.5, 0) {\(\alpha\)};
            \draw (D) -- ++ (0, 1);
            \draw[rounded corners=7pt] (D) -- ++ (-0.5, 0) -- ++ (0, -0.75);
            \draw[rounded corners=7pt] (D) -- ++ (0.5, 0) -- ++ (0, -0.75);
        \end{tikzpicture}
        .
    \end{equation}
    Actually, \(\alpha\) can take any value in \([0, 2\pi)\), so there are really an uncountable number of these building blocks.
    For short if the phase is zero then we omit the label:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-zero-phase-1}
        \begin{tikzpicture}[baseline=(A.base)]
            \node[X spider] (A) at (0, 0) {\(0\)};
        \end{tikzpicture}
        =
        \tikzsetnextfilename{ZXcalculus-zero-phase-2}
        \begin{tikzpicture}
            \node[X spider] at (0, 0) {};
        \end{tikzpicture}
        , \qqand
        \tikzsetnextfilename{ZXcalculus-zero-phase-3}
        \begin{tikzpicture}[baseline=(A.base)]
            \node[Z spider] (A) at (0, 0) {\(0\)};
        \end{tikzpicture}
        =
        \tikzsetnextfilename{ZXcalculus-zero-phase-4}
        \begin{tikzpicture}
            \node[Z spider] at (0, 0) {};
        \end{tikzpicture}
    \end{equation}
    
    We call these \define{spiders}\index{spider}.
    In particular, the green is a \(Z\) spider and the red is an \(X\) spider.
    
    Combining these pieces we can quickly build up fairly complex diagrams.
    For example, the diagram in \cref{fig:example ZX diagram} is a process which takes in two qubits and outputs two qubits.
    
    \begin{figure}
        \tikzsetnextfilename{ZXcalculus-example-using-building-blocks}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[X spider] (A) at (0, 0) {};
            \node[X spider, above=of A] (B) {\(\tfrac{\pi}{2}\)};
            \node[Z spider, below right=of A] (C) {\(\tfrac{\pi}{3}\)};
            \node[Z spider, below=of C] (D) {\(\pi\)};
            \node[Z spider, above right=of C] (E) {\(\pi\)};
            \node[Z spider, above=of E] (F) {\(\tfrac{\pi}{3}\)};
            \node[X spider, below right=of E] (G) {};
            \node[Z spider, below=of G] (H) {};
            \node[X spider, above right=of G] (I) {\(\tfrac{\pi}{4}\)};
            \node[Z spider, above right=of I] (J) {\(\tfrac{\pi}{2}\)};
            \node[Z spider, below right=of H] (K) {\(\tfrac{\pi}{4}\)};
            \node[Z spider, below=of K] (L) {\(\tfrac{\pi}{2}\)};
            
            \draw (A) -- (B);
            \draw (A) -- (C);
            \draw (C) -- (D);
            \draw (C) -- (E);
            \draw (E) -- (F);
            \draw (E) -- (G);
            \draw (G) -- (H);
            \draw (G) -- (I);
            \draw (I) -- (J);
            \draw (H) -- (K);
            \draw (K) -- (L);
            \draw[rounded corners] (K) -- ++ (2.5, 0) -- ++ (0, 5.55) -- (J);
            \draw[rounded corners] (I) -- ++ (-0.75, 0.75) -- ++ (0, 2) coordinate (M);
            \draw (J) -- (J |- M);
            \draw[rounded corners] (A) -- ++ (-1, -1) -- ++ (0, -2) -- ++ (2.5, -2.5) -- ++ (0, -1) coordinate (N);
            \draw[rounded corners] (G) -- ++ (-1, -1) -- ++ (0, -2) -- ++ (-3.5, 0) coordinate (O) -- (O |- N);
        \end{tikzpicture}
        \caption{A diagram in ZX calculus taking two qubits to two qubits.}
        \label{fig:example ZX diagram}
    \end{figure}
    
    \section{Simplifying Diagrams}
    There are two types of rules by which we might manipulate diagrams.
    The first, which we've already seen, are \define{graphical rules}\index{graphical rule} which allow us to move different pieces around so long as we don't change the connectivity.
    More formally two diagrams are equivalent if the are isotopic as graphs, a concept we'll make precise later, but for now two diagrams are isotopic if fixing all of the inputs and outputs as well as the points at which they connect it is possible to continuously morph one into the other.
    We allow the wires to pass through each other in this process.
    
    The second type of rule corresponds to specific properties of the basic building blocks.
    There are quite a few of these, and for now we'll just list them without much explanation.
    First we have the \define{monoid rules}\index{monoid rule} which are
    \begin{equation*}
        \tikzsetnextfilename{ZXcalculus-monoid-rules-X}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
            \node[Z spider] (A) {};
            \draw (A) -- ++ (0, 0.5);
            \draw (B) -- (-0.5, -1);
            \draw (0.2, -0.8) coordinate (E) arc (180:0:0.3) coordinate (F);
            \node[Z spider] (D) at (C) {};
            \draw (E) -- (E |- 0, -1);
            \draw (F) -- (F |- 0, -1);
            \node at (1.1, 0) {\(=\)};
            \begin{scope}[xshift=2.2cm, xscale=-1]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[Z spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \draw (B) -- (-0.5, -1);
                \draw (0.2, -0.8) coordinate (E) arc (180:0:0.3) coordinate (F);
                \node[Z spider] (D) at (C) {};
                \draw (E) -- (E |- 0, -1);
                \draw (F) -- (F |- 0, -1);
            \end{scope}
            
            \begin{scope}[xshift=3.89cm]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[Z spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \draw (B) -- ++ (0, -0.5);
                \draw (C) -- ++ (0, -0.5);
                \node at (0.8, 0) {\(=\)};
                \begin{scope}[xshift=1.6cm]
                    \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                    \node[Z spider] (A) {};
                    \draw (A) -- ++ (0, 0.5);
                    \draw[rounded corners] (B) -- ++ (0, -0.15) -- ++ (1, -0.2) -- ++ (0, -0.15);
                    \draw[rounded corners] (C) -- ++ (0, -0.15) -- ++ (-1, -0.2) -- ++ (0, -0.15);
                \end{scope}
            \end{scope}
            
            \begin{scope}[xshift=7.4cm]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[Z spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \node[Z spider] at (B) {};
                \draw (C) -- ++ (0, -0.5);
                \node at (0.75, 0) {\(=\)};
                \draw (1.125, -1) -- (1.125, 0.5);
                \node at (1.5, 0) {\(=\)};
                \node[Z spider] (C) at (2.1, -0.6) {};
                \node[Z spider] (D) at (2.1, 0.1) {};
                \draw (D) -- ++ (0, 0.4);
                \draw (C) -- ++ (0, -0.4);
                \draw (D) to[out=-45, in=45] (C);
                \draw (D) to[out=-135, in=135] (C);
            \end{scope}
        \end{tikzpicture}
        ,
    \end{equation*}
    and the same for the other colour:
    \begin{equation*}
        \tikzsetnextfilename{ZXcalculus-monoid-rules-Z}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
            \node[X spider] (A) {};
            \draw (A) -- ++ (0, 0.5);
            \draw (B) -- (-0.5, -1);
            \draw (0.2, -0.8) coordinate (E) arc (180:0:0.3) coordinate (F);
            \node[X spider] (D) at (C) {};
            \draw (E) -- (E |- 0, -1);
            \draw (F) -- (F |- 0, -1);
            \node at (1.1, 0) {\(=\)};
            \begin{scope}[xshift=2.2cm, xscale=-1]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[X spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \draw (B) -- (-0.5, -1);
                \draw (0.2, -0.8) coordinate (E) arc (180:0:0.3) coordinate (F);
                \node[X spider] (D) at (C) {};
                \draw (E) -- (E |- 0, -1);
                \draw (F) -- (F |- 0, -1);
            \end{scope}
            
            \begin{scope}[xshift=3.89cm]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[X spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \draw (B) -- ++ (0, -0.5);
                \draw (C) -- ++ (0, -0.5);
                \node at (0.8, 0) {\(=\)};
                \begin{scope}[xshift=1.6cm]
                    \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                    \node[X spider] (A) {};
                    \draw (A) -- ++ (0, 0.5);
                    \draw[rounded corners] (B) -- ++ (0, -0.15) -- ++ (1, -0.2) -- ++ (0, -0.15);
                    \draw[rounded corners] (C) -- ++ (0, -0.15) -- ++ (-1, -0.2) -- ++ (0, -0.15);
                \end{scope}
            \end{scope}
            
            \begin{scope}[xshift=7.4cm]
                \draw (-0.5, -0.5) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[X spider] (A) {};
                \draw (A) -- ++ (0, 0.5);
                \node[X spider] at (B) {};
                \draw (C) -- ++ (0, -0.5);
                \node at (0.75, 0) {\(=\)};
                \draw (1.125, -1) -- (1.125, 0.5);
                \node at (1.5, 0) {\(=\)};
                \node[X spider] (C) at (2.1, -0.6) {};
                \node[X spider] (D) at (2.1, 0.1) {};
                \draw (D) -- ++ (0, 0.4);
                \draw (C) -- ++ (0, -0.4);
                \draw (D) to[out=-45, in=45] (C);
                \draw (D) to[out=-135, in=135] (C);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation*}
    
    The next set of rules are called the \define{Frobenius rules}\index{Frobenius rule}, they are
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-Frobenius-rules}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, -0.2) -- ++ (0, 0.7) arc (180:0:0.3) arc (-180:0:0.3) -- ++ (0, 0.7);
            \node[Z spider] (A) at (0.3, 0.8) {};
            \node[Z spider] (B) at (0.9, 0.2) {};
            \draw (A) -- ++ (0, 0.4);
            \draw (B) -- ++ (0, -0.4);
            \node at (1.5, 0.5) {\(=\)};
            \draw (1.8, 1.2) -- ++ (0, -0.7) arc (-180:0:0.3) arc (180:0:0.3) -- ++ (0, -0.7);
            \node[Z spider] (C) at (2.1, 0.2) {};
            \node[Z spider] (D) at (2.7, 0.8) {};
            \draw (D) -- ++ (0, 0.4);
            \draw (C) -- ++ (0, -0.4);
            
            \begin{scope}[xshift=4.5cm]
                \draw (0, -0.2) -- ++ (0, 0.7) arc (180:0:0.3) arc (-180:0:0.3) -- ++ (0, 0.7);
                \node[X spider] (A) at (0.3, 0.8) {};
                \node[X spider] (B) at (0.9, 0.2) {};
                \draw (A) -- ++ (0, 0.4);
                \draw (B) -- ++ (0, -0.4);
                \node at (1.5, 0.5) {\(=\)};
                \draw (1.8, 1.2) -- ++ (0, -0.7) arc (-180:0:0.3) arc (180:0:0.3) -- ++ (0, -0.7);
                \node[X spider] (C) at (2.1, 0.2) {};
                \node[X spider] (D) at (2.7, 0.8) {};
                \draw (D) -- ++ (0, 0.4);
                \draw (C) -- ++ (0, -0.4);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    The \define{fusion rules}\index{fusion rule} allows us to combine multiple nodes of the same colour in some circumstances:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-fusion-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, -0.5) coordinate (A) arc (180:0:0.5) coordinate (B);
            \node[Z spider] (C) at (-0.5, -0.9) {\(\smash{\mathrlap{\alpha}}\phantom{\beta}\)};
            \node[Z spider] (D) at (0.5, -0.9) {\(\beta\)};
            \node[Z spider] (E) at (0, 0) {};
            \draw (C) -- (A);
            \draw (D) -- (B);
            \draw (E) -- ++ (0, 0.5);
            \node (equals) at (1, -0.2) {\(=\)};
            \node[Z spider] (F) at (2, -0.75) {\(\alpha + \beta\)};
            \draw (F.north) -- (F |- 0, 0.5);
            \begin{scope}[xshift=4.5cm]
                \draw (-0.5, -0.5) coordinate (A) arc (180:0:0.5) coordinate (B);
                \node[X spider] (C) at (-0.5, -0.9) {\(\smash{\mathrlap{\alpha}}\phantom{\beta}\)};
                \node[X spider] (D) at (0.5, -0.9) {\(\beta\)};
                \node[X spider] (E) at (0, 0) {};
                \draw (C) -- (A);
                \draw (D) -- (B);
                \draw (E) -- ++ (0, 0.5);
                \node at (1, -0.2) {\(=\)};
                \node[X spider] (F) at (2, -0.75) {\(\alpha + \beta\)};
                \draw (F.north) -- (F |- 0, 0.5);
            \end{scope}
        \end{tikzpicture}
        ,
    \end{equation}
    where addition is taken modulo \(2\pi\).
    
    Before introducing the next set of rules we introduce some shorthand notation:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-2-parity-spiders}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[Z spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.75);
            \draw (A) -- ++ (0, -0.75);
            \node (equals) at (0.5, 0) {\(\coloneqq\)};
            \draw (1, -0.2) coordinate (B) arc (180:0:0.5) coordinate (C);
            \node[Z spider] (D) at (1.5, 0.3) {};
            \node[Z spider] at (B) {\(\alpha\)};
            \draw (C) -- ++ (0, -0.5);
            \draw (D) -- ++ (0, 0.45);
            \begin{scope}[xshift=3.25cm]
                \node[X spider] (A) {\(\alpha\)};
                \draw (A) -- ++ (0, 0.75);
                \draw (A) -- ++ (0, -0.75);
                \node (equals) at (0.5, 0) {\(\coloneqq\)};
                \draw (1, -0.2) coordinate (B) arc (180:0:0.5) coordinate (C);
                \node[X spider] (D) at (1.5, 0.3) {};
                \node[X spider] at (B) {\(\alpha\)};
                \draw (C) -- ++ (0, -0.5);
                \draw (D) -- ++ (0, 0.45);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    We also define the \defineindex{Hadamard}:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-hadamard}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[draw] (H) {\(H\)};
            \node (equals) at (0.7, 0) {\(\coloneqq\)};
            \node[Z spider] (A) at (1.5, 1) {\(\scriptstyle\frac{\pi}{2}\)};
            \node[X spider] (B) at (1.5, 0) {\(\scriptstyle\frac{\pi}{2}\)};
            \node[Z spider] (C) at (1.5, -1) {\(\scriptstyle\frac{\pi}{2}\)};
            \draw (A) -- (B);
            \draw (B) -- (C);
            \draw (A) -- ++ (0, 0.65) coordinate (D);
            \draw (C) -- ++ (0, -0.65) coordinate (E);
            \draw (H) -- (H |- D);
            \draw (H) -- (H |- E);
        \end{tikzpicture}
        .
    \end{equation}
    It's safe to give this a square symbol, with rotational symmetry, since we can see from the definition that it is rotationally symmetric.
    
    We then have two \define{identity rules}\index{identity rule}, the first is just a repeat of one of the monoid rules, but now in this new shorthand, the second is new:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-identity-rules}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[Z spider] (A) {};
            \node[X spider] (B) at (1.5, 0) {};
            \draw (A) -- ++ (0, 0.5);
            \draw (A) -- ++ (0, -0.5);
            \draw (B) -- ++ (0, 0.5);
            \draw (B) -- ++ (0, -0.5);
            \draw (0.75, -0.5) -- (0.75, 0.5);
            \node (equals) at (0.4, 0) {\(=\)};
            \node at (1.1, 0) {\(=\)};
            
            \begin{scope}[xshift=3cm]
                \draw (-0.5, -0.5) arc (180:0:0.5);
                \node[Z spider] (A) at (0, 0) {};
                \node[Z spider] (B) at (0, 0.5) {};
                \draw (A) -- (B);
                \begin{scope}[xshift=1.4cm]
                    \draw (-0.5, -0.5) arc (180:0:0.5);
                    \node[X spider] (A) at (0, 0) {};
                    \node[X spider] (B) at (0, 0.5) {};
                    \draw (A) -- (B);
                \end{scope}
                \node at (0.7, 0) {\(=\)};
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    The next rule allows us to change the colour of a node, at the cost of some Hadamards, it is appropriately called the \defineindex{colour change rule}:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-colour-change-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, -0.5) coordinate (A) arc (180:0:0.5) coordinate (B);
            \node[X spider] (C) {};
            \node (equals) at (1, 0) {\(=\)};
            \draw (1.6, -0.5) coordinate (D) arc (180:0:0.5) coordinate (E);
            \node[Z spider] (F) at (2.1, 0) {};
            \node[above = 0.35cm of F, draw] (H1) {\(H\)};
            \node[below, draw] (H2) at (D) {\(H\)};
            \node[below, draw] (H3) at (E) {\(H\)};
            \draw (F) -- (H1);
            \draw (H1) -- ++ (0, 0.7) coordinate (G);
            \draw (H2) -- ++ (0, -0.5);
            \draw (H3) -- ++ (0, -0.5) coordinate (I);
            \draw (C) -- (C |- G);
            \draw (A) -- (A |- I);
            \draw (B) -- (B |- I);
        \end{tikzpicture}
        .
    \end{equation}
    The next rule is called the \defineindex{copy rule}, since it allows us to make two diagrams out of one:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-copy-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, -0.5) arc (180:0:0.5);
            \node[Z spider] (A) at (0, 0) {};
            \node[X spider] (B) at (0, 0.75) {};
            \draw (A) -- (B);
            \node (equals) at (1, 0) {\(=\)};
            \node[X spider] (C) at (1.5, 0.75) {};
            \node[X spider] (D) at (2, 0.75) {};
            \draw (C) -- ++ (0, -1.25);
            \draw (D) -- ++ (0, -1.25);
        \end{tikzpicture}
        .
    \end{equation}
    Our next rule allows for an \(X\) spider with a phase of \(\pi\) to be copied pulling it through a \(Z\) spider.
    It is called the \define{\(\symbf{\pi}\)-copy rule}\index{\(\pi\)-copy rule}:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-pi-copy-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, -0.5) coordinate (C) arc (180:0:0.5) coordinate (D);
            \node[Z spider] (A) at (0, 0) {\(\alpha\)};
            \node[X spider] (B) at (0, 0.75) {\(\pi\)};
            \draw (A) -- (B);
            \draw (B) -- ++ (0, 0.75);
            \node (equals) at (1, 0) {\(=\)};
            \draw (C) -- ++ (0, -0.5);
            \draw (D) -- ++ (0, -0.5);
            \draw (1.7, -0.3) coordinate (E) -- ++ (0, 0.3) arc (180:0:0.5) -- ++ (0, -0.3) coordinate (F);
            \node[Z spider] (G) at (2.2, 0.5) {\(\alpha\)};
            \node[X spider] (H) at (E) {\(\pi\)};
            \node[X spider] (I) at (F) {\(\pi\)};
            \draw (G) -- (G |- 0, 1.5);
            \draw (H) -- (H |- 0, -1);
            \draw (I) -- (I |- 0, -1);
        \end{tikzpicture}
        .
    \end{equation}
    The next rule is called the \defineindex{bialgebra rule}:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-bialgebra-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw (-0.5, 1) arc (-180:0:0.5);
            \draw (-0.5, -1) arc (180:0:0.5);
            \node[Z spider] (A) at (0, 0.5) {};
            \node[X spider] (B) at (0, -0.5) {};
            \draw (A) -- (B);
            \node (equals) at (1, 0) {\(=\)};
            \draw (2, 0.5) arc (90:270:0.5);
            \draw (3, 0.5) arc (90:-90:0.5);
            \node[X spider] (C) at (2, 0.5) {};
            \node[X spider] (D) at (3, 0.5) {};
            \node[Z spider] (E) at (2, -0.5) {};
            \node[Z spider] (F) at (3, -0.5) {};
            \draw (C) -- ++ (0, 0.5);
            \draw (D) -- ++ (0, 0.5);
            \draw (E) -- ++ (0, -0.5);
            \draw (F) -- ++ (0, -0.5);
            \draw[rounded corners] (C) -- ++ (0.25, 0) -- ++ (0.5, -1) -- (F);
            \draw[rounded corners] (E) -- ++ (0.25, 0) -- ++ (0.5, 1) -- (D);
        \end{tikzpicture}
        .
    \end{equation}
    The final rule is rather simple, it's simply that we can ignore overall factors, called the \defineindex{scalar rule}, it corresponds to the following:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-scalar-rule}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[Z spider] (A) {\(\phantom{\beta}\smash{\mathllap{\alpha}}\)};
            \node[X spider, above=1cm of A] (B) {\(\beta\)};
            \draw (A) -- (B);
            \node (equals) at (0.7, 0.7) {\(=\)};
            \draw[dashed, thick, Navy] (1.2, 0) rectangle (2.5, 1.6);
            \node at (3, 0.7) {\(=\)};
            \node at (4.3, 0) {};
        \end{tikzpicture}
        .
    \end{equation}
    Here the dashed box as well as the empty space both represent the empty diagram, which is simply the trivial identity process taking in no qubits, doing nothing, and outputting no qubits.
    
    \section{Interpretation}
    We'll see in more detail what these rules mean, where they come from, and why they have the names they do.
    For now it is enough to know that combined the monoid rules, Frobenius rules, fusion rules, and identity rules tell us that it doesn't matter how the dots of the same colour are connected, so long as the phases in the dots add to the same value modulo \(2\pi\).
    
    We can represent a qubit as an element of \(\complex^2\).
    Then the rules about \(Z\) spiders tell us how to multiply matrices which are diagonal in the computational basis, \(\{\ket{0}, \ket{1}\}\), with eigenvalues \(\e^{i\alpha}\), and the rules about \(X\) spiders tell us how to multiply matrices which are diagonal in the Hadamard transformed basis formed from
    \begin{equation}
        \ket{+} = H\ket{0} = \frac{1}{\sqrt{2}}(\ket{0} + \ket{1}), \qqand \ket{-} = H\ket{1} = \frac{1}{\sqrt{2}}(\ket{0} - \ket{1}).
    \end{equation}
    The colour change rule tells us how to convert one basis to the other.
    The bialgebra rule tells us that these bases are complimentary, that they are at the maximal angle to each other.
    The copy and \(\pi\)-copy rules are just artefacts of nicely chosen bases.
    
    Using this we can develop the \defineindex{standard model} of ZX calculus, which represents each diagram as a matrix acting on the input qubits.
    More formally we can define a map
    \begin{equation}
        \denotes{-} \colon (n\text{-to-}m \text{ qubit ZX diagram}) \to (2^n \times 2^m \text{ complex matrices}).
    \end{equation}
    Then a process which takes a single qubit, does nothing to it, and immediately outputs it is represented as
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-identity}
        \begin{tikzpicture}[baseline=0.4cm]
            \draw (0, 0) -- (0, 1);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1 & 0\\
            0 & 1
        \end{pmatrix}
        .
    \end{equation}
    Unsurprisingly doing nothing to a qubit gives the identity.
    
    The following diagram takes in two qubits, swaps them, and then returns them:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-swap}
        \begin{tikzpicture}[baseline=0.4cm]
            \draw[rounded corners] (0, 0) -- ++ (0, 0.2) -- ++ (0.5, 0.6) -- ++ (0, 0.2);
            \draw[rounded corners] (0.5, 0) -- ++ (0, 0.2) -- ++ (-0.5, 0.6) -- ++ (0, 0.2);
        \end{tikzpicture}
        \longmapsto 
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    In terms of matrices this acts as follows:
    \begin{equation}
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        \left[
            \begin{pmatrix}
                a\\ b
            \end{pmatrix}
            \otimes
            \begin{pmatrix}
                c\\ d
            \end{pmatrix}
        \right]
        =
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            ac\\ ad\\ bc\\ bd
        \end{pmatrix}
        =
        \begin{pmatrix}
            ac\\ bc\\ ad\\ bd
        \end{pmatrix}
        =
        \begin{pmatrix}
            c\\ d
        \end{pmatrix}
        \otimes
        \begin{pmatrix}
            a\\ b
        \end{pmatrix}
        .
    \end{equation}
    
    It is possible to have diagrams which create qubits from nothing.
    In this case we should take the input to simply be 1.
    The following diagram creates a pair of qubits:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-create-qubits}
        \begin{tikzpicture}[baseline=-0.35cm]
            \draw (0, 0) arc (-180:0:0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1\\ 0\\ 0\\ 1
        \end{pmatrix}
        .
    \end{equation}
    Similarly we can destroy qubits:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-destroy-qubits}
        \begin{tikzpicture}[baseline=0.15cm]
            \draw (0, 0) arc (180:0:0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    The Hadamard gives the \define{Hadamard matrix}\index{Hadamard!matrix}, note that we're ignoring an overall scalar, there's usually a factor of \(1/\sqrt{2}\):
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-hadamard}
        \begin{tikzpicture}[baseline=(H.base)]
            \node[draw] (H) {\(H\)};
            \draw (H) -- ++ (0, 0.5);
            \draw (H) -- ++ (0, -0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1 & 1\\
            1 & -1
        \end{pmatrix}
        .
    \end{equation}
    We can create a single qubit:
    \begin{equation}
         \tikzsetnextfilename{ZXcalculus-denotates-identity-one-arity-Z}
        \begin{tikzpicture}[baseline=(A.base)]
            \node[Z spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1\\ \e^{i\alpha}
        \end{pmatrix}
        , \qqand
        \tikzsetnextfilename{ZXcalculus-denotes-one-arity-X}
        \begin{tikzpicture}[baseline=(A.base)]
            \node[X spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.5);
        \end{tikzpicture}
        \longmapsto
        \begin{pmatrix}
            1 + \e^{i\alpha}\\ 1 - \e^{i\alpha}
        \end{pmatrix}
    \end{equation}
    The three-arity spiders give the following matrices
    \begin{align}
        \tikzsetnextfilename{ZXcalculus-denotates-3-arity-Z}
        \begin{tikzpicture}[baseline=(A.base)]
            \draw (-0.5, -0.5) arc (180:0:0.5);
            \node[Z spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.5);
        \end{tikzpicture}
        &\longmapsto
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 0 & 0 & \e^{i\alpha}
        \end{pmatrix}
        ,\\ 
        \tikzsetnextfilename{ZXcalculus-denotes-three-arity-X}
        \begin{tikzpicture}[baseline=(A.base)]
            \draw (-0.5, -0.5) arc (180:0:0.5);
            \node[X spider] (A) {\(\alpha\)};
            \draw (A) -- ++ (0, 0.5);
        \end{tikzpicture}
        &\longmapsto
        \begin{pmatrix}
            1 + \e^{i\alpha} & 1 - \e^{i\alpha} & 1 - \e^{i\alpha} & 1 + \e^{i\alpha}\\
            1 - \e^{i\alpha} & 1 + \e^{i\alpha} & 1 + \e^{i\alpha} & 1 - \e^{i\alpha}
        \end{pmatrix}
    \end{align}
    
    Writing two processes next to each other gives their tensor product:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-tensor-product}
        \begin{tikzpicture}[baseline=(f.base)]
            \draw (0, 0) -- ++ (1, 0) -- ++ (-0.25, 0.75) -- ++ (-0.75, 0) -- cycle;
            \node (f) at (0.4, 0.375) {\(f\)};
            \draw (0.375, 0) -- ++ (0, -0.5);
            \draw (0.375, 0.75) -- ++ (0, 0.5);
            \begin{scope}[xshift=1.5cm]
                \draw (0, 0) -- ++ (1, 0) -- ++ (-0.25, 0.75) -- ++ (-0.75, 0) -- cycle;
                \node at (0.4, 0.375) {\(g\)};
                \draw (0.375, 0) -- ++ (0, -0.5);
                \draw (0.375, 0.75) -- ++ (0, 0.5);
            \end{scope}
        \end{tikzpicture}
        \longmapsto f \otimes g
    \end{equation}
    Writing two processes one after the other connected up represents doing them in the order they are connected from bottom to top, which is composition:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-denotates-composition}
        \begin{tikzpicture}[baseline=1cm]
            \draw (0, 0) -- ++ (1, 0) -- ++ (-0.25, 0.75) -- ++ (-0.75, 0) -- cycle;
            \node (f) at (0.4, 0.375) {\(f\)};
            \draw (0.375, 0) -- ++ (0, -0.5);
            \draw (0.375, 0.75) -- ++ (0, 0.75);
            \begin{scope}[yshift=1.5cm]
                \draw (0, 0) -- ++ (1, 0) -- ++ (-0.25, 0.75) -- ++ (-0.75, 0) -- cycle;
                \node at (0.4, 0.375) {\(g\)};
                \draw (0.375, 0.75) -- ++ (0, 0.5);
            \end{scope}
        \end{tikzpicture}
        \longmapsto g \circ f
    \end{equation}
    Note that for processes represented by matrices composition is just matrix multiplication.
    
    This mapping makes precise what any one ZX diagram represents.
    What is important is that this isn't changed when we apply the rules of ZX calculus.
    This is the crux of the following theorem.
    
    \begin{thm}{ZX Calculus is Sound}{}
        Let \(D_1\) and \(D_2\) be diagrams in ZX calculus.
        If \(D_1 = D_2\) according to the rules of ZX calculus then \(\denotes{D_1} = \denotes{D_2}\).
    \end{thm}
    
    As well as being a rigorous way to manipulate objects ZX calculus can also approximate any process from \(m\) qubits to \(n\) qubits to arbitrary precision.
    \begin{thm}{ZX Calculus is Approximately Universal}{}
        For any \(2^m \times 2^n\) matrix, \(f\), and any error margin, \(\varepsilon > 0\), there exists a diagram, \(D\), in ZX calculus built only from terms with phases an integer multiple of \(\pi/4\) such that \(\norm{\denotes{D} - f} < \varepsilon\) for some appropriate matrix norm \(\norm{-}\).
    \end{thm}
    This is one of the reasons that ZX calculus is so powerful.
    
    Another desirable quality for a notation like ZX calculus is that it be complete.
    By this we mean that if two matrices are equal and both given by some ZX diagram then there should be a graphical proof of this using only the rules of ZX calculus.
    This is the case if we assume the following two axioms, which are sound under the standard interpretation:
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-extra-completeness-rule-1-lhs}
        \begin{tikzpicture}[font=\scriptsize, baseline=2cm]
            \draw (-1, 1) arc (-180:0:1);
            \node[Z spider] (A) at (0, 0) {};
            \node[X spider] (B) at (-1, 1) {};
            \node[X spider] (C) at (1, 1) {};
            \node[Z spider] (D) at (-2, 1) {\(\varphi\)};
            \node[Z spider] (E) at (-1, 2) {\(\varphi\)};
            \node[X spider] (F) at (0, 2) {\(\vartheta\)};
            \node[Z spider] (G) at (2, 1) {\(\psi\)};
            \node[Z spider] (H) at (1, 2) {\(\psi\)};
            \node[X spider] (I) at (-1, 3) {\(-\vartheta\)};
            \node[Z spider] (J) at (0, 3) {};
            
            \draw (0, -0.5) -- (A);
            \draw (B) -- (D);
            \draw (B) -- (E);
            \draw (C) -- (G);
            \draw (C) -- (H);
            \draw (E) -- (F);
            \draw (F) -- (H);
            \draw (E) -- (I);
            \draw (I) -- (J);
            \draw (J) -- (F);
            \draw (H) -- (1, 4);
            \draw (I) -- (-1, 4);
            \draw (J) -- (0, 4);
        \end{tikzpicture}
        =
        \tikzsetnextfilename{ZXcalculus-extra-completeness-rule-1-rhs}
        \begin{tikzpicture}[font=\scriptsize, baseline=2cm]
            \draw (-1, 1) arc (-180:0:1);
            \node[Z spider] (A) at (0, 0) {};
            \node[X spider] (B) at (-1, 1) {};
            \node[X spider] (C) at (1, 1) {};
            \node[Z spider] (D) at (-2, 1) {\(\varphi\)};
            \node[Z spider] (E) at (-1, 2) {\(\varphi\)};
            \node[X spider] (F) at (0, 2) {\(-\vartheta\)};
            \node[Z spider] (G) at (2, 1) {\(\psi\)};
            \node[Z spider] (H) at (1, 2) {\(\psi\)};
            \node[X spider] (I) at (1, 3) {\(\vartheta\)};
            \node[Z spider] (J) at (0, 3) {};
            
            \draw (0, -0.5) -- (A);
            \draw (B) -- (D);
            \draw (B) -- (E);
            \draw (C) -- (G);
            \draw (C) -- (H);
            \draw (E) -- (F);
            \draw (F) -- (H);
            \draw (H) -- (I);
            \draw (F) -- (J);
            \draw (J) -- (I);
            \draw (E) -- (-1, 4);
            \draw (J) -- (0, 4);
            \draw (I) -- (1, 4);
        \end{tikzpicture}
    \end{equation}
    for any phases \(\varphi\), \(\psi\), and \(\vartheta\) which are integer multiples of \(\pi/4\), and the second axiom
    \begin{equation}
        \tikzsetnextfilename{ZXcalculus-extra-completeness-rule-2-lhs}
        \begin{tikzpicture}[baseline=3cm]
            \draw (0, -0.5) -- (0, 6.5);
            \draw (2, 6) arc (90:270:1);
            \draw (2, 2) arc (90:270:1);
            \node[Z spider] (E) at (0, 1) {};
            \node[X spider] (A) at (0, 2) {};
            \node[Z spider] at (0, 3) {\(-\sfrac{\pi}{2}\)};
            \node[X spider] (B) at (0, 4) {};
            \node[Z spider] (F) at (0, 5) {};
            \node[Z spider] (C) at (-1, 2) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] (D) at (-1, 4) {\(\tfrac{\pi}{4}\)};
            \node[X spider] (G) at (1, 1) {};
            \node[X spider] (H) at (1, 5) {};
            \node[Z spider] at (2, 2) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] at (2, 0) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] at (2, 4) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] at (2, 6) {\(\sfrac{\pi}{4}\)};
            
            \draw (A) -- (C);
            \draw (B) -- (D);
            \draw (E) -- (G);
            \draw (F) -- (H);
        \end{tikzpicture}
        \quad = \quad
        \tikzsetnextfilename{ZXcalculus-extra-completeness-rule-2-rhs}
        \begin{tikzpicture}[baseline=3cm]
            \draw (0, -0.5) -- (0, 6.5);
            \draw (2, 5) arc (90:270:1);
            \node[X spider] at (0, 1) {\(\sfrac{\pi}{2}\)};
            \node[Z spider] at (0, 2) {\(\sfrac{\pi}{4}\)};
            \node[X spider] (A) at (0, 3) {\(\pi\)};
            \node[Z spider] (C) at (0, 4) {};
            \node[Z spider] (B) at (-1, 3) {\(\sfrac{\pi}{4}\)};
            \node[X spider] (D) at (1, 4) {\(\pi\)};
            \node[Z spider] at (2, 5) {\(\sfrac{\pi}{4}\)};
            \node[Z spider] at (2, 3) {\(\sfrac{\pi}{4}\)};
            
            \draw (A) -- (B);
            \draw (C) -- (D);
        \end{tikzpicture}
        .
    \end{equation}
    Call ZX calculus with these rules added \define{\(\symbf{\pi/4}\)-ZX calculus}\index{\(\pi/4\)-ZX calculus}.
    
    \begin{thm}{\(\pi/4\)-ZX Calculus is Complete}{}
        Let \(D_1\) and \(D_2\) be diagrams in \(\pi/4\)-ZX calculus.
        If \(\denotes{D_1} = \denotes{D_2}\) then \(D_1 = D_2\) under the axioms of \(\pi/4\)-ZX calculus.
    \end{thm}
    
    The third thing making ZX calculus powerful is how well it can be automated.
    All a ZX calculation is is a finite labelled graph.
    Once you've implemented a way of applying the rules this can then be done very efficiently on a computer.
    
    A common use of ZX calculus is in quantum circuit optimisation.
    Given some quantum algorithm as a quantum circuit it is often possible to optimise the circuit.
    For example, the \(T\) gate,
    \begin{equation}
        T = 
        \begin{pmatrix}
            1 & 0\\
            0 & \e^{-i\pi/4}
        \end{pmatrix}
        ,
    \end{equation}
    is typically expensive to implement, so reducing the number of \(T\) gates in a circuit is usually desirable.
    Given some circuit making use of \(T\) gates we can use the universality of ZX calculus to convert the circuit into a ZX diagram, then manipulate the ZX diagram and then convert it back to a, hopefully, more optimised circuit with fewer \(T\) gates.
    
    \chapter{Semantics}
    \section{Types of Semantics}
    Consider the two following pseudocode fragments:
    \begin{align}
        P &= \lstinline|if 1 = 1 then F else G|,\\
        Q &= \lstinline|if 1 = 0 then F else F|.
    \end{align}
    Are \(P\) and \(Q\) the same program?
    There are two schools of thought:
    \begin{itemize}
        \item No. Clearly looking at them both programs are implemented differently, \(P\) makes reference to \lstinline|G|, \(Q\) makes reference to 0.
        \item Yes. Both programs take no input and output \lstinline|F|.
    \end{itemize}
    Whether or not we count these as the same program depends on what we are interested in.
    
    To aid in our analysis we assign the code fragments their meanings, encoded in some appropriate mathematical object.
    This gives a mapping
    \begin{equation}
        \denotes{-} \colon \text{Programs} \to \text{Mathematical Objects}.
    \end{equation}
    For now we'll leave the details of exactly what mathematical objects alone.
    If we are interested in implementation details then we assign \(P\) and \(Q\) to objects encoding these details.
    This is called \defineindex{operational semantics}.
    If we aren't interested in implementation details then we assign \(P\) and \(Q\) to objects which treat them as black boxes with inputs and outputs.
    This is called \defineindex{denotational semantics}.
    If this is what we do then we find that \(\denotes{P} = \denotes{Q} = \denotes{\lstinline|F|}\).
    
    We want this mapping to preserve the structure of our programs.
    For example, suppose that we have two processes, \lstinline|F| and \lstinline|G|, which can be composed by running them one after another.
    We might write this as \(\lstinline|F; G|\) in a language using semicolons to terminate a line.
    In order to reason about our program, regardless of which type of semantics we are interested in, we want the result to be the same if we compose the programs and then look at the semantics or look at the semantics and then compose the programs.
    That is we want
    \begin{equation}
        \denotes{\lstinline|F; G|} = \denotes{\lstinline|F|} \circ \denotes{\lstinline|G|}.
    \end{equation}
    Here \(\circ\) is some method of composing the semantics of two programs.
    Another structure which we may want to preserve is the ability to compute things in parallel, for example
    \begin{equation}
        \denotes{\lstinline|paralell(F, G)|} = \denotes{\lstinline|F|} \otimes \denotes{\lstinline|G|}.
    \end{equation}
    
    \section{Motivation}
    Why might we care about this sort of analysis?
    This reasoning can be used to ground assumptions and correct erroneous assumptions.
    We can also use semantics to justify transformations of programs, for example, to demonstrate that a compiled program does the same thing as the original program.
    It is also often the case that it is easier to reason about the mathematical objects encoding the programs, rather than the programs themselves, in fact sometimes it isn't possible to reason directly about the programs, such as in quantum computing where many operations are like black boxes, even if we can't look at the operational semantics we can still consider the denotational semantics and the flow of information through the program to compare programs.
    
    Choosing different semantics also allows us to focus on different details.
    Operational semantics focus on implementation details, such as memory usage and running time, which is useful if we want to improve the efficiency of our programs.
    Denotational semantics focus on results, which is useful if we want to check that our program does what we want.
    
    \section{Mathematical Objects}
    There are many possible mathematical objects to encode programs.
    Simple programs can be modelled as set theoretical functions from some set of possible inputs to some set of possible outputs.
    More specifically we can use something like \(\lambda\)-calculus to represent programs.
    In this way we can represent both operational semantics, by writing our functions as expressions in the input variables, and denotational semantics, by focussing on the input and output values.
    
    In quantum computing operational semantics are often not an option.
    So we will mostly stick to denotational semantics.
    The objects we choose to represent programs are categories.
    Category theory supports combing programs as we saw in the examples above and gives us a powerful graphical language for computations.
    
    \part{Categories}
    \chapter{Categories}
    \section{Motivation}
    We want an abstract mathematical formalism in which the meaning of a program lives, allowing us to abstract away implementation details and reason about computations.
    Such a formalism provides a map
    \begin{equation}
        \denotes{-} \colon \text{programs} \to \text{mathematical objects}.
    \end{equation}
    There are several features which are desirable of such a formalism, including but not limited to,
    \begin{itemize}
        \item a notion of composition, if \lstinline|F| and \lstinline|G| are programs and \lstinline|F; G| is running \lstinline|F| then \lstinline|G| then we should have \(\denotes{\lstinline|F; G|} = \denotes{\lstinline|G|} \circ \denotes{\lstinline|F|}\) where \(\circ\) represents composition in this mathematical formalism;
        \item a notion of concurrency, if \lstinline|F par G| corresponds to running \lstinline|F| and \lstinline|G| at the same time then we should have \(\denotes{\lstinline|F par G|} = \denotes{\lstinline|F|} \otimes \denotes{\lstinline|G|}\);
        \item a notion of calling programs recursively, if \lstinline|X| is some code calling \lstinline|F| then we should be able to compute \lstinline|F(X)|, and this requires our mathematical formalism to have structures of the form \(\denotes{\lstinline|F(X)|} = \denotes{\lstinline|F|}(\denotes{\lstinline|X|})\).
    \end{itemize}
    More concisely, our formalism should preserve composition, concurrency, and function application.
    
    The question we have to ask is what sort of mathematical objects we're considering.
    There are multiple options, each with their own advantages and disadvantages.
    Some common options are
    \begin{itemize}
        \item \(\lambda\)-calculus is an algebraic notation for functions. It is similar in syntax to \Haskell{}.
        It works with anonymous functions, or lambda functions, such as the function \lstinline[literate={lambda}{$\lambda$}{1}, mathescape]|lambda x. * 2 x| which takes an argument, \lstinline|x|, and multiplies it by 2, using prefix notation.
        Compare this to the \Haskell{} function
        \begin{lstlisting}[language=haskell, gobble=12]
            timesThree x = (*) 2 x
        \end{lstlisting}
        This choice is good for analysing implementation details and is simply a precise notation for applying functions, a common mathematical operation.
        \item Partially ordered sets, or posets, where the elements of the poset are partially completed calculations, any two elements are comparable if they are the same calculation at, potentially, different levels of completion, and the more complete calculation is greater.
        This choice is good for analysing computations step by step without worrying about how each step is implemented.
        \item Categories, which is what we'll use.
        This option subsumes both \(\lambda\)-calculus, as a method of defining functions, and posets, which can be regarded as a special case of a category.
    \end{itemize}
    
    Our goal will be to work in a general category to develop theory, imposing only the required restrictions for things to work out.
    Then we can pick a particular category to analyse our work in, with the interpretation depending on the category we pick.
    Often we can work in a generic category and then specialise the result to a category to perform either classical or quantum computations.
    
    \section{Categories: The Idea}
    A category consists of two pieces of data:
    \begin{itemize}
        \item Objects, \(A, B, C, \dotsc\);
        \item Morphisms, \(f \colon A \to B\), between objects.
    \end{itemize}

    Given some specific category there are various ways to think of computations occurring in this category.
    Some examples are given here:
    \begin{itemize}
        \item We can think of the objects as physical systems and the morphisms as processes.
        For example,
        \begin{itemize}
            \item Two objects may be a full cup and an empty cup and a process may be drinking the drink or making a new drink.
            \item Two objects might be a plate and pieces of broken pottery and a process may be dropping the plate on the floor.
        \end{itemize}
        \item We can think of objects as data types, and morphisms as functions between these types.
        Borrowing \Haskell{} notation some examples are
        \begin{itemize}
            \item One object might be \lstinline|Int|, and a morphism \lstinline[style=haskell]|f :: Int -> Int| defined by \lstinline|f n = 2 * n|.
            \item Another object might be \lstinline|String|, and a morphism \lstinline|len :: String -> Int| defined by
            \begin{lstlisting}[style=haskell, gobble=16]
                len [] = 0
                len (x : xs) = 1 + len xs
            \end{lstlisting}
            \item Another object might be \lstinline[style=haskell]|Num a => [a]|, and a morphism
            \begin{lstlisting}[style=haskell, gobble=16]
                mySum :: (Num a) => [a] -> a
                mySum [] = 0
                mySum (x : xs) = x + mySum xs
            \end{lstlisting}
        \end{itemize}
        \item Objects are algebraic structures and morphisms are structure preserving maps.
        For example,
        \begin{itemize}
            \item Objects are sets and morphisms are functions.
            \item Objects are groups and morphisms are homomorphisms.
            \item Objects are topological spaces and morphisms are continuous functions.
            \item Objects are vector spaces and morphisms are linear maps.
        \end{itemize}
        \item Objects are logical propositions and morphisms are implications between them.
        For example, we could take the objects \enquote{it rains} and \enquote{I get wet} and then we might have a morphism \(\text{\enquote{it rains}} \implies \text{\enquote{I get wet}}\).
        But what if we're indoors?
        We might introduce another object, \enquote{I am outside}, and then we may have a morphism \(\text{\enquote{it rains}} \land \text{\enquote{I am outside}} \implies \text{\enquote{I get wet}}\).
        Here we've implicitly defined another object \(\text{\enquote{it rains}} \land \text{\enquote{I am outside}}\) using logical conjunction, \(\land\).
        This is actually an example of a product in this category, we'll see what this means later.
    \end{itemize}
    
    It turns out that the second and fourth examples, programs/algorithms and propositions/implications are actually the same!
    This is known as the Curry--Howard isomorphism, and allows us to write proofs as programs and vice versa.
    
    The mindset that one should have when doing category theory is
    \begin{important}
        Morphisms are more important than objects.
    \end{important}
    \noindent This might seem backwards at first, for example we spend a lot of time thinking about groups, and homomorphisms are only one aspect that we consider, but it turns out that we can learn a lot about groups by studying how they relate to other groups, and this is done through homomorphisms.
    This mindset is particularly useful for cases such as quantum computing where we \emph{can't} look at internal structure, and can only look at how systems relate to each other.
    
    \section{Categories: The Definition}
    Categories are objects and morphisms.
    The definition of a category simply states what we mean by this and the properties that morphisms are expected to have.
    
    \begin{dfn}{Category}{}
        A \defineindex{category}, \(\cat{C}\), consists of the following data
        \begin{itemize}
            \item a collection of \define{objects}\index{object}, \(\Ob(\cat{C})\) (often denoted \(\mathop{\operatorname{Obj}}(\cat{C})\) or simply \(\cat{C}\));
            \item for every pair of objects, \(A, B \in \Ob(\cat{C})\) a collection of \define{morphisms}\index{morphism} (also known as \define{maps}\index{map|see{morphism}} or \define{arrows}\index{arrow|see{morphism}}), \(\cat{C}(A, B)\) (often denoted \(\hom_{\cat{C}}(A, B)\), \(\mathop{\operatorname{Mor}}_{\cat{C}}(A, B)\), possibly without the subscript \(\cat{C}\) when the category is clear, this collection is often called a \defineindex{hom set}), where for \(f \in \cat{C}(A, B)\) we write \(f \colon A \to B\) or \(A \xrightarrow{f} B\);
            \item a map \(\circ \colon \cat{C}(B, C) \times \cat{C}(A, B) \to \cat{C}(A, C)\) which assigns to each \(f \colon A \to B\) and \(g \colon B \to C\) some \define{composite}\index{composition} \((g \circ f) \colon A \to C\);
            \item for every object \(A \in \Ob(\cat{C})\) a morphism \(\id_A \colon A \to A\), that is \(\id_A \in \cat{C}(A, A)\), called the \defineindex{identity morphism}.
        \end{itemize}
        This data is subject to the following conditions:
        \begin{itemize}
            \item \defineindex{associativity} of \(\circ\): for all objects \(A, B, C, D \in \Ob(\cat{C})\) and for all morphisms \(f \colon A \to B\), \(g \colon B \to C\), and \(h \colon C \to D\) we have
            \begin{equation}
                h \circ (g \circ f) = (h \circ g) \circ f,
            \end{equation}
            so we can unambiguously write \(h \circ g \circ f\) for both of these;
            \item \defineindex{identity} law: for all objects \(A, B \in \Ob(\cat{C})\) and for all morphisms \(f \colon A \to B\) we have
            \begin{equation}
                f \circ \id_A = f = \id_B \circ f.
            \end{equation}
        \end{itemize}
    \end{dfn}
    
    \subsection{Technicality}
    Notice that in the definition we use the word \enquote{collection}.
    It is tempting to replace this with \enquote{set}, but this can cause issues.
    For example, the set of all sets is not a set, due to Russell's paradox.
    However, we will shortly see that we have categories where the objects are all sets, and so \(\Ob(\cat{C})\) cannot be a set in this case.
    We aren't going to worry too much about these types of issues, and may erroneously refer to these collections as sets.
    A category is \define{small}\index{small category} if both \(\Ob(\cat{C})\) and the collection of all morphisms between any two objects are sets.
    A category is \define{locally small}\index{locally small category} if for all objects \(A\) and \(B\) \(\cat{C}(A, B)\) is a set.
    Many statements we make throughout will apply only to small, or more likely locally small categories.
    
    \section{Categories: The Examples}
    \subsection{\texorpdfstring{\(\Set\)}{Set}}
    \begin{dfn}{\normalsize\(\Set\)}{}
        The category \(\Set\)\index{Set@\(\Set\)} has sets as objects.
        A morphism \(A \to B\) is simply a function from \(A\) to \(B\).
        Composition of morphisms is composition of functions, defined for \(f \colon A \to B\) and \(g \colon B \to C\) by \((g \circ f) \colon A \to C\) given by \((g \circ f)(a) = g(f(a))\) for all \(a \in A\).
        The identity morphism is the identity function, \(\id_A \colon A \to A\), given by \(\id_A(a) = a\) for all \(a \in A\).
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Set\) is a category.
        \begin{proof}
            The collection \(\Set(A, B)\) of functions \(A \to B\) exists for all sets \(A\) and \(B\).
            It will be empty if \(B = \emptyset\) and \(A \ne \emptyset\) which is allowed, if \(A = \emptyset\) and \(B = \emptyset\) then there is a unique function \(f \colon \emptyset \to \emptyset\) which is also the identity on the empty set.
            Function composition is associative.
            Take sets \(A\), \(B\), \(C\), and \(D\), and morphisms \(f \colon A \to B\), \(g \colon B \to C\), and \(h \colon C \to D\).
            Then
            \begin{equation}
                (h \circ (g \circ f))(x) = h((g \circ f)(x)) = h(g(f(x))) = (h \circ g)(f(x)) = ((h \circ g) \circ f)(x)
            \end{equation}
            for all \(x \in A\), so \(h \circ (g \circ f) = (h \circ g) \circ f\).
            The identity function is then such that
            \begin{equation}
                (f \circ \id_A)(x) = f(\id_A(x)) = f(x) = \id_B(f(x)) = (\id_B \circ f)(x)
            \end{equation}
            and so \(f \circ \id_A = f = \id_B \circ f\).
        \end{proof}
    \end{lma}
    
    We can think of a function \(f \colon A \to B\) as dynamically indicating how elements of \(A\) transform into elements of \(B\).
    Pictorially, we can represent \(f \colon A \to B\) and \(g \colon B \to C\) as
    \begin{equation}
        \tikzsetnextfilename{categories-set-function-example}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B3) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(f\)};
            \begin{pgfonlayer}{behind}
                \draw (A1) -- (B1);
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
            
            \begin{scope}[xshift=4cm]
                \fill[highlight] (0, 0.25) coordinate (B1) circle [radius=0.075cm];
                \fill[highlight] (0, 0.75) coordinate (B2) circle [radius=0.075cm];
                \fill[highlight] (0, 1.25) coordinate (B3) circle [radius=0.075cm];
                \fill[highlight] (2, 0.25) coordinate (C1) circle [radius=0.075cm];
                \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                \fill[highlight] (2, 1.25) coordinate (C3) circle [radius=0.075cm];
                \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \node (B) at (0, 2) {\(B\)};
                \node (C) at (2, 2) {\(C\)};
                \draw[->] (B) -- (C) node [midway, above] {\(g\)};
                \begin{pgfonlayer}{behind}
                    \draw (B1) -- (C1);
                    \draw (B2) -- (C3);
                    \draw (B3) -- (C3);
                \end{pgfonlayer}
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    Then composition is just following the lines between sets:
    \begin{equation}
        \tikzsetnextfilename{categories-set-function-composition}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B3) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(f\)};
            \begin{pgfonlayer}{behind}
                \draw (A1) -- (B1);
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
            
            \begin{scope}[xshift=2cm]
                \fill[highlight] (2, 0.25) coordinate (C1) circle [radius=0.075cm];
                \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                \fill[highlight] (2, 1.25) coordinate (C3) circle [radius=0.075cm];
                \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \node (C) at (2, 2) {\(C\)};
                \draw[->] (B) -- (C) node [midway, above] {\(g\)};
                \begin{pgfonlayer}{behind}
                    \draw (B1) -- (C1);
                    \draw (B2) -- (C3);
                    \draw (B3) -- (C3);
                \end{pgfonlayer}
            \end{scope}
            
            \begin{scope}[yshift=-3cm]
                \fill[highlight] (0, 0) coordinate (A1) circle [radius=0.075cm];
                \fill[highlight] (0, 0.5) coordinate (A2) circle [radius=0.075cm];
                \fill[highlight] (0, 1) coordinate (A3) circle [radius=0.075cm];
                \fill[highlight] (0, 1.5) coordinate (A4) circle [radius=0.075cm];
                \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
                \node (A) at (0, 2) {\(A\)};
                
                \begin{scope}[xshift=2cm]
                    \fill[highlight] (2, 0.25) coordinate (C1) circle [radius=0.075cm];
                    \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                    \fill[highlight] (2, 1.25) coordinate (C3) circle [radius=0.075cm];
                    \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                    \node (C) at (2, 2) {\(C\)};
                    \draw[->] (A) -- (C) node [midway, above] {\(g \circ f\)};
                    \begin{pgfonlayer}{behind}
                        \draw (A1) -- (C1);
                        \draw (A2) -- (C1);
                        \draw (A3) -- (C1);
                        \draw (A4) -- (C3);
                    \end{pgfonlayer}
                \end{scope}
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    As a process composition in set is just doing one thing and then the other.
    
    \(\Set\) is the prototypical category.
    It is often tempting to think of other examples of categories, which we'll meet shortly, as simply sets with extra structure, and indeed this is often, but not always, the case.
    A \defineindex{concrete category} is a category in which all objects can be mapped to sets and morphisms can be mapped to functions between these sets.
    This mapping is done with something called a functor, which we'll define later (\cref{def:functor}).
    
    We'll list some properties of \(\Set\) here, some of which won't make sense until later.
    \begin{itemize}
        \item \(\Set\) is a concrete category.
        \item \(\Set\) has the empty set as an initial object, and the singleton as a terminal object.
        \item \(\Set\) is complete and co-complete, in particular the product is the Cartesian product and the coproduct is the disjoint union.
        \item \(\Set\) is a monoidal category with the monoidal product given by the Cartesian product.
    \end{itemize}
    
    \subsection{\texorpdfstring{\(\Rel\)}{Rel}}
    \begin{dfn}{Relation}{}
        Let \(A\) and \(B\) be sets.
        A \defineindex{relation}, \(R\), is a subset of \(A \times B\).
        If \((a, b) \in R\) we write \(a R b\).
    \end{dfn}
    Relations are morphisms in a category we will defined shortly (\cref{def:Rel}), so for \(R \subset A \times B\) we write \(R \colon A \to B\).
    In a similar manner to functions between sets being dynamic transformations we can think of relations as being non-deterministic transformations, where each element can transform into multiple objects, or possibly don't map across at all.
    For example, if we take \(A = \{a, b, c, d\}\), \(B = \{1, 2, 3\}\), and \(C = \{\alpha, \beta, \gamma\}\) then the relations
    \begin{align}
        R &= \{(b, 2), (c, 2), (d, 2), (d, 3)\} \subseteq A \times B, \qquad \text{and}\\
        S &= \{(1, \beta), (3, \beta), (3, \gamma)\} \subseteq B \times C
    \end{align}
    can be represented pictorially as
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-example}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(R\)};
            \begin{pgfonlayer}{behind}
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
            
            \begin{scope}[xshift=4cm]
                \fill[highlight] (0, 0.25) coordinate (B3) circle [radius=0.075cm];
                \fill[highlight] (0, 0.75) coordinate (B2) circle [radius=0.075cm];
                \fill[highlight] (0, 1.25) coordinate (B1) circle [radius=0.075cm];
                \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
                \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
                \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \node (B) at (0, 2) {\(B\)};
                \node (C) at (2, 2) {\(C\)};
                \draw[->] (B) -- (C) node [midway, above] {\(S\)};
                \begin{pgfonlayer}{behind}
                    \draw (B1) -- (C2);
                    \draw (B3) -- (C2);
                    \draw (B3) -- (C3);
                \end{pgfonlayer}
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    As with \(\Set\) we then compose relations by joining up these lines:
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-composition}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(R\)};
            \begin{pgfonlayer}{behind}
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
            
            \begin{scope}[xshift=2cm]
                \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
                \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
                \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                \node (C) at (2, 2) {\(C\)};
                \draw[->] (B) -- (C) node [midway, above] {\(S\)};
                \begin{pgfonlayer}{behind}
                    \draw (B1) -- (C2);
                    \draw (B3) -- (C2);
                    \draw (B3) -- (C3);
                \end{pgfonlayer}
            \end{scope}
            
            \begin{scope}[yshift=-3cm]
                \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
                \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
                \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
                \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
                \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
                \node (A) at (0, 2) {\(A\)};
                
                \begin{scope}[xshift=2cm]
                    \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
                    \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
                    \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
                    \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
                    \node (C) at (2, 2) {\(C\)};
                    \draw[->] (A) -- (C) node [midway, above] {\(S \circ R\)};
                    \begin{pgfonlayer}{behind}
                        \draw (A4) -- (C2);
                        \draw (A4) -- (C3);
                    \end{pgfonlayer}
                \end{scope}
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    That is,
    \begin{equation}
        S \circ R = \{(d, \beta), (d, \gamma)\} \subseteq A \times C.
    \end{equation}
    This leads to the following definition.
    \begin{dfn}{Relation Composition}{}
        Let \(A\), \(B\), and \(C\) be sets with relations \(R \subseteq A \times B\) and \(S \subseteq B \times C\).
        Then the \define{composite relation}\index{relation composition} \(S \circ R\) is the set
        \begin{equation*}
            S \circ R \coloneqq \{(a, c) \in A \times C \mid \exists b \in B \text{ such that } (a, b) \in R \text{ and } (b, c) \in S\}.
        \end{equation*}
    \end{dfn}
    
    Now that we have composition we just need an identity, and after some playing around with the definition of composition one quickly comes to the identity
    \begin{equation}
        \id_A \coloneq \{(a, a) \in A \times A \mid a \in A\} \subseteq A \times A.
    \end{equation}
    Now we can define a category.
    
    \begin{dfn}{\normalsize\(\Rel\)}{def:Rel}
        The category \(\Rel\)\index{Rel@\(\Rel\)} has sets as objects.
        A morphism \(R \colon A \to B\) is a relation \(R \subseteq A \times B\).
        Composition of morphisms is composition of relations, defined for \(R \colon A \to B\) and \(g \colon B \to C\) by
        \begin{equation}
            S \circ R = \{(a, c) \mid \exists b \in B : aRb \land bSc\} \subseteq A \times C.
        \end{equation}
        The identity morphism is the identity relation, \(\id_A \colon A \to A\), given by
        \begin{equation}
            \id_A = \{(a, a) \mid a \in A\} \subseteq A \times A.
        \end{equation}
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Rel\) is a category.
        \begin{proof}
            The collection \(\Rel(A, B)\) is simply the power set of \(A \times B\).
            Importantly the empty set is a relation on any pair of sets, including the empty set, and the empty set is the identity relation on the empty set.
            So consider nonempty sets.
            
            Composition of relations is associative.
            Let \(R \colon A \to B\), \(S \colon B \to C\), and \(T \colon C \to D\) be relations.
            We want to show that \(T \circ (S \circ R) = (T \circ S) \circ R\).
            To do so we will prove that each pair \((a, b) \in T \circ (S \circ R)\) is also an element of \((T \circ S) \circ R\).
            The converse, that each pair \((a, b) \in (T \circ S) \circ R\) is an element of \(T \circ (S \circ R)\), follows by the same logic in reverse.
            So take some \((a, b) \in T \circ (S \circ R)\).
            By definition there exists some \(x\) such that \((x, b) \in T\) and \((a, x) \in S \circ R\).
            Thus, there exists some \(y\) such that \((y, x) \in S\) and \((a, y) \in S\).
            Since \((y, x) \in S\) and \((x, b) \in T\) we have that \((y, b) \in T \circ S\).
            Since \((a, y) \in R\) we have \((a, b) \in (T \circ S) \circ R\).
            
            Let \(R \colon A \to B\) be a relation and \(\id_A = \{(a, a) \mid a \in A\}\) the identity relation.
            Then for all \((a, b) \in R\) we have \((a, a) \in \id_A\), meaning that \((a, b) \in R \circ \id_A\).
            Similarly for all \((a, b) \in R \circ \id_A\) we must have \(x\) such that \((x, b) \in R\), and \((x, a) \in \id_A\), which means that \(x = a\) and so \((a, b) \in R\).
            Thus \(R \circ \id_A = R\).
            Similarly \(\id_B \circ R = R\).
            Hence the identity law is satisfied.
        \end{proof}
    \end{lma}
    
    We can represent relations as binary \(\abs{B} \times \abs{A}\) matrices with a \(1\) in the \((i, j)\) slot if \((a, b) \in R\), where \(a\) is the \(j\)th element of \(A\) and \(b\) the \(i\)th element of \(B\) in some arbitrary fixed ordering of \(A\) and \(B\).
    Further this mapping is one-to-one, meaning each binary matrix also defines a representation.
    For example, indexing top to bottom we have
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-representation}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (B3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (B1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (A) at (0, 2) {\(A\)};
            \node (B) at (2, 2) {\(B\)};
            \draw[->] (A) -- (B) node [midway, above] {\(R\)};
            \begin{pgfonlayer}{behind}
                \draw (A2) -- (B2);
                \draw (A3) -- (B2);
                \draw (A4) -- (B2);
                \draw (A4) -- (B3);
            \end{pgfonlayer}
        \end{tikzpicture}
        \leftrightsquigarrow
        M(R) = 
        \begin{pmatrix}
            0 & 0 & 0 & 0\\
            0 & 1 & 1 & 1\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    Composition of relations is then given by matrix multiplication taking everything mod 2.
    More formally we have a map
    \begin{equation}
        M \colon \powerset(A \times B) \to \matrices[\abs{B}]{\abs{A}}{\integers_2}
    \end{equation}
    where \(\integers_2 = \{0, 1\}\) has addition and multiplication defined mod 2.
    
    As an example notice that we have
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-representation-2}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
        
            \fill[highlight] (0, 0.25) coordinate (B3) circle [radius=0.075cm];
            \fill[highlight] (0, 0.75) coordinate (B2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.25) coordinate (B1) circle [radius=0.075cm];
            \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (B) at (0, 2) {\(B\)};
            \node (C) at (2, 2) {\(C\)};
            \draw[->] (B) -- (C) node [midway, above] {\(S\)};
            \begin{pgfonlayer}{behind}
                \draw (B1) -- (C2);
                \draw (B3) -- (C2);
                \draw (B3) -- (C3);
            \end{pgfonlayer}
        \end{tikzpicture}
        \leftrightsquigarrow
        M(S) = 
        \begin{pmatrix}
            0 & 0 & 0\\
            1 & 0 & 1\\
            0 & 0 & 1
        \end{pmatrix}
    \end{equation}
    and
    \begin{equation}
        \tikzsetnextfilename{categories-rel-relation-representation-3}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \pgfdeclarelayer{behind}
            \pgfsetlayers{behind, main}
            \fill[highlight] (0, 0) coordinate (A4) circle [radius=0.075cm];
            \fill[highlight] (0, 0.5) coordinate (A3) circle [radius=0.075cm];
            \fill[highlight] (0, 1) coordinate (A2) circle [radius=0.075cm];
            \fill[highlight] (0, 1.5) coordinate (A1) circle [radius=0.075cm];
            \draw[thick] (0, 0.75) circle [x radius=0.25cm, y radius=1cm];
            \node (A) at (0, 2) {\(A\)};
            
            \fill[highlight] (2, 0.25) coordinate (C3) circle [radius=0.075cm];
            \fill[highlight] (2, 0.75) coordinate (C2) circle [radius=0.075cm];
            \fill[highlight] (2, 1.25) coordinate (C1) circle [radius=0.075cm];
            \draw[thick] (2, 0.75) circle [x radius=0.25cm, y radius=0.75cm];
            \node (C) at (2, 2) {\(C\)};
            \draw[->] (A) -- (C) node [midway, above] {\(S \circ R\)};
            \begin{pgfonlayer}{behind}
                \draw (A4) -- (C2);
                \draw (A4) -- (C3);
            \end{pgfonlayer}
        \end{tikzpicture}
        \leftrightsquigarrow M(S \circ R) = 
        \begin{pmatrix}
            0 & 0 & 0 & 0\\
            0 & 0 & 0 & 1\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    The composite can then be calculated as
    \begin{equation}
        M(S \circ R) = M(S)M(R) = 
        \begin{pmatrix}
            0 & 0 & 0\\
            1 & 0 & 1\\
            0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            0 & 0 & 0 & 0\\
            0 & 1 & 1 & 1\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        =
        \begin{pmatrix}
            0 & 0 & 0 & 0\\
            0 & 0 & 0 & 1\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    
    This map, \(M\), actually defines a functor (\cref{def:functor}) \(M \colon \Rel \to \Mat[\integers_2]\), where \(\Mat[\integers_2]\) is the category of binary matrices, whose objects are natural numbers and a morphism \(n \to m\) is an \(n \times m\) matrix.
    The functor \(M\) then maps sets to their size, \(A \mapsto M(A) = \abs{A}\), and relations to matrices as described above.
    
    This is actually an example of a more general idea of a representation\footnote{see \course{Symmetries of Quantum Mechanics} or \course{Symmetries of Particles and Fields}}, where we replace objects with matrices preserving the structure of the original space.
    These ideas can all be understood as functors \(\cat{C} \to \Mat\) for some appropriate category \(\cat{C}\) and ring \(\field\).
    This in turn is more commonly thought of as a map \(\cat{C} \to \FVect\) identifying matrices with linear maps.
    This allows for generalisations to infinite dimensional representations, \(\cat{C} \to \Vect\).
    We'll define \(\FVect\) and \(\Vect\) shortly (\cref{def:Vect,def:FVect}).
    
    On the surface \(\Rel\) seems quite similar to \(\Set\), after all the objects of both are the same.
    However, it's really the morphisms which are important, and these are quite different.
    The ability to replace relations with matrices means that \(\Rel\) is actually quite similar to another category, \(\Hilb\) (\cref{def:Hilb and FHilb}).
    This makes \(\Rel\) a nice in between for \(\Set\) and \(\Hilb\), which turn out to be the categories in which we think of most classical and quantum computing as occurring in respectively.
    
    We now list some properties of \(\Rel\):
    \begin{itemize}
        \item \(\Rel\) is a concrete category.
        \item \(\Rel\) is a dagger category.
        \item \(\Rel\) has both products and coproducts given by disjoint union.
        \item \(\Rel\) is a monoidal category with the monoidal product given by the Cartesian product.
    \end{itemize}
    
    \subsection{\texorpdfstring{\(\Vect\), \(\FVect\), \(\Hilb\), and \(\FHilb\)}{Vect, FVect, Hilb, and FHilb}}
    \subsubsection{\texorpdfstring{\(\Vect\) and \(\FVect\)}{Vect and FVect}}
    \begin{dfn}{Vector Space}{}
        A \defineindex{vector space}, \((V, \field, +, \cdot)\), is a set, \(V\), a field\footnote{if you don't know what this is just replace it with \(\reals\) or \(\complex\) and don't worry about it}, \(\field\), and two operations, \defineindex{vector addition}, \(+ \colon V \times V \to V\), and \defineindex{scalar multiplication}, \(\cdot \colon \field \times V \to V\), such that
        \begin{itemize}
            \item \((V, +)\) is an Abelian group, that is
            \begin{itemize}
                \item vector addition is associative: \(u + (v + w) = (u + v) + w\) for all \(u, v, w \in V\);
                \item vector addition is commutative: \(u + v = v + u\) for all \(u, v \in V\);
                \item additive identity: there exists \(0 \in V\) such that \(0 + v = v\) for all \(v \in V\);
                \item additive inverse: for every \(v \in V\) there exists \(-v \in V\) such that \(v + (-v) = v - v = 0\);
            \end{itemize}
            \item scalar multiplication distributes over vector addition: \(\alpha \cdot (u + v) = (\alpha \cdot v) + (\alpha \cdot v)\) for all \(\alpha \in \field\) and \(u, v \in V\);
            \item field identity acts as the identity: \(1 \cdot v = v\) for all \(v \in V\) where 1 is the multiplicative identity in \(\field\);
            \item field addition distributes over scalar multiplication: \((\alpha + \beta) \cdot v = (\alpha \cdot v) + (\beta \cdot v)\);
            \item compatibility of field and scalar multiplication: \(\alpha \cdot (b \cdot v) = (a \cdot_{\field} b) \cdot v\) for all \(\alpha, \beta \in \field\) and \(v \in V\) where \(\cdot_{\field}\) is multiplication in the field.
        \end{itemize}
    \end{dfn}
    
    \begin{dfn}{Linear Map}{}
        Let \((V, \field, +_V, \cdot_V)\) and \((W, \field, +_W, \cdot_W)\) be vector spaces over the same field, \(\field\).
        A \defineindex{linear map} between these vector spaces is a function \(T \colon V \to W\) such that
        \begin{equation}
            T(u +_V v) = T(u) +_W T(v), \qqand T(\alpha \cdot_V v) = \alpha \cdot_W T(v)
        \end{equation}
        for all \(\alpha \in \field\) and \(u, v \in V\).
    \end{dfn}
    
    From now on we drop the explicit symbol for scalar multiplication, writing \(\alpha v\) for \(\alpha \cdot v\), as well as dropping labels differentiating which vector space an operation is defined on.
    So linearity is expressed as
    \begin{equation}
        T(u + v) = T(u) + T(v), \qqand T(\alpha v) = \alpha T(v).
    \end{equation}
    We also refer to \(V\) and \(W\) as vector spaces (over \(\field\)) leaving the operations (and potentially the field) implicit.
    
    \begin{dfn}{\normalsize\(\Vect\)}{def:Vect}
        Fix some field \(\field\).
        The category \(\Vect\)\index{Vect@\(\Vect\)} has vector spaces over \(\field\) as objects and linear maps as morphisms.
        Composition of morphisms is composition of linear maps, which is composition of the underlying functions.
        The identity morphisms are the identity linear maps, which are the underlying identity functions.
    \end{dfn}
    
    \begin{lma}{}{lma:Vect is a category}
        \(\Vect\) is a category.
        \begin{proof}
            Composition of linear maps inherits associativity from the underlying functions and similarly the identity laws follow from the identity laws of the underlying functions.
            We therefore only need to show that the composite of two linear maps is again linear.
            Let \(T \colon U \to V\) and \(S \colon V \to W\) be linear maps between vector spaces \(U\), \(V\), and \(W\) over some field \(\field\).
            Then for all \(u, v \in V\) and \(\alpha \in \field\) we have
            \begin{multline}
                (S \circ T)(u + v) = S(T(u + v)) = S(T(u) + T(v))\\
                = S(T(u)) + S(T(v)) = (S \circ T)(u) + (S \circ T)(v)
            \end{multline}
            using the linearity of \(T\) and then \(S\), and
            \begin{equation}
                (S \circ T)(\alpha v) = S(T(\alpha v)) = S(\alpha T(v)) = \alpha S(T(v)) = \alpha (S \circ T)(v)
            \end{equation}
            again using linearity of \(T\) and then \(S\).
            Hence the composite of two linear maps is again a linear map.
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Basis}{}
        Let \(V\) be a vector space.
        A subset \(\symcal{B} \subset V\) is \defineindex{linearly independent} if for every finite subset of \(\{\ve{1}, \dotsc, \ve{n}\} \subseteq \symcal{B}\) satisfies
        \begin{equation}
            \alpha_1 \ve{1} + \dotsb + \alpha_n \ve{n} = 0
        \end{equation}
        only for the trivial case of \(\alpha_i = 0\).
        
        A subset \(\symcal{B} \subset V\) spans \(V\) if every \(v \in V\) can be written as
        \begin{equation}
            v = \alpha_1\ve{1} + \dotsb + \alpha_n\ve{n}
        \end{equation}
        for some finite subset \(\{\ve{1}, \dotsc, \ve{n}\} \subseteq \symcal{B}\).
        We call \(\alpha_i\) the \define{components}\index{component} of \(v\).
        
        If a subset \(\symcal{B} \subset V\) is linearly independent and spans \(V\) then we call it a \defineindex{basis}.
    \end{dfn}
    
    Every vector space has a basis (assuming the axiom of choice).
    It is a fact that any two bases have the same cardinality, which we call the \defineindex{dimension} of the space.
    A vector space is \defineindex{finite-dimensional} if it's dimension is finite.
    
    \begin{dfn}{\normalsize\(\FVect\)}{def:FVect}
        The category \(\FVect\) has finite-dimensional vector spaces as objects and linear maps as morphisms.
    \end{dfn}
    
    \begin{lma}{}{}
        \(\FVect\) is a category.
        \begin{proof}
            This follows from \(\Vect\) being a category.
        \end{proof}
    \end{lma}
    
    Linear maps are often thought of as matrices, although this only works in the finite-dimensional case.
    Take two vector spaces \(V\) and \(W\) with bases \(\{v_i\}\) and \(\{w_i\}\) respectively.
    Then a linear map \(T \colon V \to W\) defines a matrix with components \(T_{ij}\) given by \(T(v_i)_j\), where \(T(v_i)_j\) is the \(j\)th component of \(T(v_i)\), which is what we get evaluating the linear map at the \(i\)th basis vector, \(v_i\).
    A matrix, \(T_{ij}\), defines a linear map \(T \colon V \to W\) similarly, by defining \(T(v_i)\) to be formed from components \(T(v_i)_j = T_{ij}\), and then using linearity to extend this definition to any vector in \(V\).
    
    \begin{dfn}{\normalsize\(\Mat\)}{def:Mat}
        The category \(\Mat\)\index{Mat@\(\Mat\)} has natural numbers as objects with a morphism \(n \to m\) being an \(m \times n\) matrix with entries in \(\field\).
        Composition of morphisms is matrix multiplication and the identity matrix is the identity morphism.
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Mat\) is a category.
        \begin{proof}
            The product of an \(m \times n\) matrix and a \(n \times \ell\) matrix is an \(m \times \ell\) matrix, reflecting the fact we can compose morphisms \(\ell \to n\) and \(n \to m\) to get a morphism \(\ell \to m\).
            Associativity follows from associativity of matrix multiplication and the identity matrix is clearly the identity.
        \end{proof}
    \end{lma}
    
    There is an equivalence (\cref{def:equivalence and others}) \(\Mat \to \FVect\) given by sending \(n \to \field^n\) and sending a matrix to a linear map as described above.
    This formalises the idea that linear maps and matrices are equivalent ways of doing linear algebra in finite dimensions.
    
    \subsubsection{\texorpdfstring{\(\Hilb\) and \(\FHilb\)}{Hilb and FHilb}}
    \begin{dfn}{Inner Product Space}{}
        An \defineindex{inner product space}, \((V, \braket{-}{-})\), is a vector space, \(V\), over the field \(\field = \reals, \complex\) equipped with an \defineindex{inner product} \(\braket{-}{-} \colon V \times V \to \field\) such that the inner product is
        \begin{itemize}
            \item conjugate symmetric: \(\braket{u}{v} = \braket{v}{u}^*\) for all \(u, v \in V\);
            \item linear in the \emph{second} argument: \(\braket{u}{\alpha v} = \alpha\braket{u}{v}\) for all \(\alpha \in \field\) and \(u, v \in V\), this implies antilinearity in the first argument: \(\braket{\alpha u}{v} = \alpha^*\braket{u}{v}\);
            \item positive-definite: \(\braket{v}{v} > 0\) for all \(v \in V\) with \(v \ne 0\) and \(\braket{0}{0} = 0\), note that conjugate symmetry implies \(\braket{v}{v} = \braket{v}{v}^*\) so \(\braket{v}{v}\) is real.
        \end{itemize}
        Note that for the \(\field = \reals\) case we can simply ignore the complex conjugates.
    \end{dfn}
    
    \begin{dfn}{Hilbert Space}{}
        A \defineindex{Hilbert space} is an inner product space \((H, \braket{-}{-})\) such that \(H\) is complete with respect to the norm \(\norm{-} \colon H \to \reals\) defined by \(\norm{v} \coloneqq \sqrt{\braket{v}{v}}\), we say that this is the norm induced by the inner product.
        Being complete means that if the sequence \(\{v_i\} \subseteq H\) is such that
        \begin{equation}
            \sum_{i = 1}^{\infty} \norm{v_i}
        \end{equation}
        converges (as a series in \(\reals\)) then
        \begin{equation}
            \sum_{i = 1}^{\infty} v_i
        \end{equation}
        converges to some \(v \in H\), in the sense that for all \(\varepsilon > 0\) there exists some \(N \in \naturals\) such that for all \(n > N\) we have
        \begin{equation}
            \norm*{v - \sum_{i = 1}^{n} v_i} < \varepsilon,
        \end{equation}
        or equivalently,
        \begin{equation}
            \lim_n \norm*{v - \sum_{i = 1}^{n} v_i} = 0.
        \end{equation}
    \end{dfn}
    
    We will assume that Hilbert spaces are inner product spaces over \(\complex\) unless stated otherwise, although most facts will hold for real Hilbert spaces as well.
    Complex Hilbert spaces are where all quantum mechanics takes place, so we don't lose much by making this assumption.
    
    This requirement of completeness is really just a technical requirement to make things well defined in certain circumstances, and we won't ever have reason to make use of it explicitly.
    For our purposes inner product space and Hilbert space are basically synonyms, but we're slightly safer working in Hilbert spaces due to this extra condition.
    
    \begin{dfn}{Bounded Linear Map}{}
        Let \(H\) and \(K\) be Hilbert spaces with norms \(\norm{-}_H\) and \(\norm{-}_K\) respectively, both induced by the inner product.
        A \defineindex{bounded linear map} is a map \(T \colon H \to K\) such that
        \begin{equation}
            \norm{T(v)}_K \le M \norm{v}_H
        \end{equation}
        for some \(M \in \reals\) and all \(v \in H\).
    \end{dfn}
    
    \begin{dfn}{{\normalsize\(\Hilb\)} and {\normalsize\(\FHilb\)}}{def:Hilb and FHilb}
        Fix some field \(\field = \reals, \complex\).
        The category \(\Hilb\) has Hilbert spaces as objects and bounded linear maps as morphisms.
        The category \(\FHilb\) has finite-dimensional Hilbert spaces as objects and bounded linear maps as morphisms.
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Hilb\) and \(\FHilb\) are categories.
        \begin{proof}
            Associativity and identities follow from the underlying functions.
            We need only show that the composite of two bounded linear maps is again a bounded linear map.
            Let \(T \colon H \to K\) and \(S \colon K \to J\) be bounded linear maps between the Hilbert spaces \(H\), \(K\), and \(J\).
            Then there exist some \(M, N \in \reals\) such that \(\norm{T(v)}_K \le M\norm{v}_H\) and \(\norm{S(u)}_J \le N\norm{u}\) for all \(v \in H\) and \(u \in K\).
            Then we have \(\norm{(S \circ T)(v)}_J = \norm{S(T(v))}_J \le N\norm{T(v)}_K \le NM\norm{v}_H\) for all \(v \in V\), and \(NM \in \reals\), so the map is bounded again.
            The composite of two linear maps is linear, as shown in \cref{lma:Vect is a category},
            so the composite of two bounded linear maps is a bounded linear map.
        \end{proof}
    \end{lma}
    
    We now make a few basic definitions.
    \begin{dfn}{Orthonormal}{}
        Let \(H\) be a Hilbert space and \(\{\ve{i}\}\) a basis of \(H\), then we say that this basis is \defineindex{orthogonal} if \(\braket{\ve{i}}{\ve{j}} = 0\) for \(i \ne j\).
        If \(\braket{\ve{i}}{\ve{j}} = \delta_{ij}\) we say the basis is \defineindex{orthonormal}.
    \end{dfn}
    
    \begin{dfn}{Adjoint}{}
        If \(H\) and \(K\) are Hilbert spaces and \(T \colon H \to K\) is a linear map then we define the \defineindex{adjoint} to be the linear map \(T^{\hermit} \colon K \to H\) such that \(\braket{T(v)}{w} = \braket{v}{T^\hermit(w)}\).
    \end{dfn}
    
    In terms of matrices representing linear maps the adjoint corresponds to the conjugate transpose matrix.
    
    \begin{dfn}{Bras and Kets}{def:bras and kets}
        Let \(H\) be a Hilbert space.
        Given some \(v \in H\) its \defineindex{ket} is the map \(\ket{v} \colon \complex \to H\) defined by \(z \mapsto zv\) and its bra is the map \(\bra{v} \colon H \to \complex\) defined by \(w \mapsto \braket{v}{w}\).
    \end{dfn}
    
    This definition is rather formal and doesn't correspond to how we usually think about bras and kets.
    Typically we think about elements of \(H\) as being kets.
    This works since each linear map \(\complex \to H\) is completely defined by where it sends \(1\), so really linear maps of this form just pick out elements of \(H\), and \(\ket{v}\) is such that \(1 \mapsto 1v = v\).
    So we often write \(\ket{v}\) when we might actually mean \(v\) by this definition.
    We then think of bras similarly as being their own vectors in some other space, which we'll define in the next definition, and then we think of the mapping \(w \mapsto \braket{v}{w}\) as simply performing multiplication \(\bra{v} \ket{w} = \braket{v}{w}\).
    
    \begin{dfn}{Dual Space}{}
        Let \(V\) be a vector space.
        Then \(V^* \coloneqq \Vect(V, \field)\) is the \defineindex{dual space}.
    \end{dfn}
    
    This definition means that \(V^*\) is the space of linear maps \(V \to \field\), which in the case of a Hilbert space we can recognise as the space of bras.
    Note that we are equipping \(\Vect(V, \field)\) with the obvious vector space (or Hilbert space) structure given by adding and scaling linear maps pointwise.
    
    
    \subsection{More Examples}
    \begin{exm}{Sets with Structure}{}
        Many categories can be described as having objects formed from sets with structure, and morphisms being structure preserving functions.
        These are all concrete categories, in the sense that we can forget the structure and just think of them as sets and functions.
        Some examples of these sets with structure are
        \begin{itemize}
            
            \item The category \(\Mon\)\index{Mon@\(\Mon\)} has monoids as objects and monoid homomorphisms as morphisms.
            \item The category \(\Grp\)\index{Grp@\(\Grp\)} has groups as objects and group homomorphisms as morphisms.
            \item The category \(\Ring\)\index{Ring@\(\Ring\)} has rings as objects and ring homomorphisms as morphisms.
            \item The category \(\CRing\)\index{CRing@\(\CRing\)} has commutative rings as objects and ring homomorphisms as morphisms.
            \item The category \(\Field\)\index{Field@\(\Field\)} has fields as objects and field homomorphisms as morphisms.
            \item The category \(\RMod\)\index{R-Mod@\(\RMod\)}\index{Mod@\makeatletter\(\c@egory{Mod}\)\makeatother} for a fixed ring \(R\) has left modules over \(R\) as objects and module homomorphisms as morphisms.
            Note that the special case where \(R\) is a field is \(\Vect[R]\).
            \item The category \(\Top\) has topological spaces as objects and continuos maps as morphisms.
            \item The category \(\pointedTop\) has pointed topological spaces, \((X, \bullet)\), as objects, that is topological spaces with some special point, \(\bullet\), and based maps as morphisms, that is continuous maps preserving this special point, that is \(f \colon (X, \bullet) \to (Y, \ast)\) is continuous and \(f(\bullet) = \ast\).
            \item The category \(\Pos\) has posets as objects and monotone functions as morphisms.
        \end{itemize}
        See \cref{chap:maths definitions} for relevant definitions.
    \end{exm}
    
    \begin{exm}{Posets}{}
        Given a poset, \((P, \le)\), we can define a category whose objects are elements of \(P\) and there is a unique morphism \(a \to b\) if \(a \le b\).
        Since this morphism is unique and \(a \le a\) the identity is simply the unique morphism \(a \to a\).
        Since \(a \le b\) and \(b \le c\) implies \(a \le c\) there is a unique morphism \(a \to c\) in this case, which must then be the morphism formed by composing the morphisms \(a \to b\) and \(b \to c\).
    \end{exm}
    
    \begin{exm}{Single Object Categories}{}
        Given a monoid, \(M\), we can interpret it as a category with a single object, which we call \(\bullet\), and then the elements of \(M\) are morphisms \(\bullet \to \bullet\) with composition of morphisms given by the monoid product.
        The identity of the monoid is the identity morphism.
        In a sense categories just generalise monoids to have more objects.
        The process of defining a quantity, then mapping the definition to a particular category with a single object, and then allowing there to be multiple objects is called \defineindex{oidification}, and the resulting object is suffixed with -oid.
        So a monoidoid is a category.
        
        Given a group, \(G\), we can interpret it as a single object category where all morphisms are isomorphisms (\cref{def:isomorphism and others}).
        This extra condition simply reflects the condition that a group has all inverses.
        A \defineindex{groupoid} is then a category in which all morphisms are isomorphisms.
        
        Note that we can proceed in the opposite direction, given a category with a single object (with all morphisms being isomorphisms) this can be interpreted as a monoid (group).
    \end{exm}
    
    \section{Diagrams}\index{diagram}
    Composition of morphisms can be quite confusing.
    There is lots of data to specify, which objects are involved, what are the morphisms called, do the morphisms have any other properties we might be interested in and so on.
    It doesn't help that the order in which we write composition of functions is the reverse of the order in which the functions are applied.
    The solution to this is to draw pictures with all of the objects as nodes and the morphisms as arrows between them.
    Composition of morphisms is then given by reading the arrows backwards.
    A simple example is
    \begin{equation}
        A \xrightarrow{f} B \xrightarrow{g} C
    \end{equation}
    which represents two morphisms, \(f \colon A \to B\) and \(g \colon B \to C\), which can be composed to give \((g \circ f) \colon A \to C\).
    We might add this morphism to our diagram,
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f"] \arrow[rr, bend right, "g \circ f"'] & B \arrow[r, "g"] & C
        \end{tikzcd}
        .
    \end{equation}
    although we don't have to since it's existence is ensured by the definition of a category.
    Both paths taken in this diagram give the same result.
    
    In general if all paths in a diagram between two fixed objects give the same result we say that the diagram \define{commutes}\index{commute}, or is a \defineindex{commutative diagram}.
    This can be useful to specify a lot of algebraic relations in a single commutative diagram.
    For example, the diagram
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f"] \arrow[d, "h"'] & B \arrow[r, "g"] \arrow[d, "i"'] & C\\
            D \arrow[r, "k"'] & E \arrow[ur, "j"']
        \end{tikzcd}
    \end{equation}
    commuting is equivalent to the following requirements:
    \begin{equation}
        g \circ f = j \circ k \circ h, \qquad i \circ f = k \circ h, \qqand g = j \circ i,
    \end{equation}
    which are given by requiring that the outer parallelogram commutes, the square commutes, and the triangle commutes respectively.
    
    Since the definition of a category means every object has an identity morphism and that these identity morphisms don't really affect composition we leave the identity morphisms implicit in the diagram.
    We could write them in, for example
    \begin{equation}
        \begin{tikzcd}
           A \arrow[r, "f"] \arrow[loop left, "\id_A"] & B \arrow[loop right, "\id_B"]
        \end{tikzcd}
    \end{equation}
    commuting is simply the identity law, telling us that, among other things, \(f \circ \id_A = f = \id_B \circ f\).
    We can also state the associativity law as the commutativity of the following:
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f"] \arrow[rr, bend left, "g \circ f"] & B \arrow[r, "g"] \arrow[rr, bend right, "h \circ g"'] & C \arrow[r, "h"] & D.
        \end{tikzcd}
    \end{equation}
    
    \section{Terminology}
    We now introduce some terminology which will be helpful.
    \begin{dfn}{}{def:isomorphism and others}
        For a morphisms \(f \colon A \to B\)
        \begin{itemize}
            \item we call \(A\) the \defineindex{domain}, or \define{source}\index{source|see{domain}}, of \(f\);
            \item we call \(B\) the \defineindex{codomain}, or \define{target}\index{target|see{codomain}}, of \(f\);
            \item we call \(f\) an \defineindex{endomorphism} if \(A = B\);
            \item we call \(f\) an \defineindex{isomorphism} if there exists \(f^{-1} \colon B \to A\) such that \(f^{-1} \circ f = \id_A\) and \(f \circ f^{-1} = \id_{B}\);
            \item we call \(f\) an \defineindex{automorphism} if it is an isomorphism and endomorphism;
            \item we call \(A\) \defineindex{isomorphic} to \(B\) if \(f\) is an isomorphism, and write \(A \isomorphic B\);
            \item we call \(f\) an \defineindex{epimorphism}, or \define{epic}\index{epic|see{epimorphism}}, if \(g \circ f = h \circ f\) implies \(g = h\) for all \(g, h \colon B \to C\);
            \item we call \(f\) a \defineindex{monomorphism}, or \define{monic}\index{mono|see{monomorphism}}, if \(f \circ g = f \circ h\) implies \(g = h\) for all \(g, h \colon C \to A\).
        \end{itemize}
    \end{dfn}
    
    The most important definition here is isomorphisms.
    Often equality is too strict a requirement for comparing two objects.
    Instead isomorphism is usually the correct level of similarity.
    In particular in a concrete category objects are isomorphic if there is an invertible structure preserving map between them, giving a one-to-one pairing of elements.
    This allows for trivial differences between objects, like renaming of elements or operations, to be ignored.
    For example, the groups \((\{0, 1\}, +_2)\) and \((\{1, -1\}, \cdot)\) are not equal, but they are isomorphic.
    
    \begin{lma}{}{}
        Let \(\cat{C}\) be a category with objects \(A\) and \(B\).
        If \(f \colon A \to B\) is an isomorphism then the inverse is unique.
        \begin{proof}
            Suppose this wasn't the case, so \(f \colon A \to B\) has two inverses, \(g, g' \colon B \to A\), which are both such that \(g \circ f = g' \circ f = \id_A\) and \(f \circ g = f \circ g' = \id_B\).
            Then we have
            \begin{equation}
                g = g \circ \id_B = g \circ (f \circ g') = (g \circ f) \circ g' = \id_A \circ g' = g'.
            \end{equation}
        \end{proof}
    \end{lma}
    
    The condition that \(f\) and \(f^{-1}\) are inverses is equivalent to requiring that
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f", shift left=1] & B \arrow[l, "f^{-1}", shift left=1]
        \end{tikzcd}
    \end{equation}
    commutes.
    
    \section{Graphical Notation}
    In this section we will introduce a graphical notation which can be used to reason about categories.
    This notation treats categories as processes, taking some input and producing some output.
    We read the notation from bottom to top, which we can imagine is the progression of \enquote{time} through the process.
    This notation obeys the rules, and spirit, of category theory.
    In particular, we don't write objects.
    Instead we represent each object through it's identity morphism, so \(A\) is represented by \(\id_A\).
    An identity morphism is then represented by a line, labelled by the object, representing a process in which nothing happens, the input at the bottom of the page, is just passed directly to the output at the top of the page:
    \begin{equation}
        \id_A = 
        \tikzsetnextfilename{categories-graphical-language-identity}
        \begin{tikzpicture}[baseline=(A.base)]
            \draw[wire] (0, 0) -- (0, 1) node [midway, left] (A) {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    A general morphism, \(f \colon A \to B\), is represented in this graphical notation by placing a box on the wire, representing some process occurring, and labelling the wire either side with the appropriate object:
    \begin{equation}
        f = 
        \tikzsetnextfilename{categories-graphical-language-morphism}
        \begin{tikzpicture}[baseline=(f.base)]
            \node[morphism] (f) {\(f\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- ++ (0, 1) node [midway, left] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    Composition of morphisms is represented by doing one process after the other, which in this notation just means writing one box after the other and joining them by a wire of the appropriate type.
    If we introduce a second morphism \(g \colon B \to C\) then we have
    \begin{equation}
        g \circ f = 
        \tikzsetnextfilename{categories-graphical-language-composition}
        \begin{tikzpicture}[baseline=(g.base)]
            \node[morphism] (g) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (g) -- ++ (0, -1) node [midway, left] {\(B\)};
            \draw[wire] (g) -- ++ (0, 1) node [midway, left] {\(C\)};
            \node at (0.5, 0) {\(\circ\)};
            \node[morphism] (f) at (1, 0) {\(f\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- ++ (0, 1) node [midway, left] {\(B\)};
        \end{tikzpicture}
        \ =
        \tikzsetnextfilename{categories-graphical-language-composite}
        \begin{tikzpicture}[baseline=(B.base)]
            \node[morphism] (f) {\(f\)};
            \node[morphism, above= 0.75cm of f] (g) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- (g) node [midway, left] (B) {\(B\)};
            \draw[wire] (g) -- ++ (0, 1) node [midway, left] {\(C\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    This graphical notation makes the axioms of a category implicit.
    For the identity law since identity morphisms are just drawn as wires clearly the following holds:
    \begin{equation}
        \tikzsetnextfilename{categories-graphical-language-identity-law}
        \begin{tikzpicture}[baseline=(A.base),
            morphism/.append style={minimum width=0.6cm}
            ]
            \node[morphism] (idA) at (0, 0) {\(\id_A\)};
            \node[morphism] (f1) at (0, 1.5) {\(f\)};
            \node[morphism] (f2) at (1.5, 0.75) {\(f\)};
            \node[morphism] (f3) at (3, 0) {\(f\)};
            \node[morphism] (idB) at (3, 1.5) {\(\id_B\)};
            \draw[wire] (idA) -- (f1) node [midway, left] (A) {\(A\)};
            \draw[wire] (idA) -- ++ (0, -1) node [midway, left] {\(A\)} coordinate (bottom);
            \draw[wire] (f1) -- ++ (0, 1) node [midway, left] {\(B\)} coordinate (top);
            \draw[wire] (f2) -- (f2 |- bottom) node [midway, left] {\(A\)};
            \draw[wire] (f2) -- (f2 |- top) node [midway, left] {\(B\)};
            \draw[wire] (f3) -- (f3 |- bottom) node [midway, left] {\(A\)};
            \draw[wire] (idB) -- (f3) node [midway, left] {\(B\)};
            \draw[wire] (idB) --(f3 |- top) node [midway, left] {\(B\)};
            \node at (0.75, 0.75) {\(=\)};
            \node at (2.25, 0.75) {\(=\)};
        \end{tikzpicture}
        ,
    \end{equation}
    and this is just the identity law
    \begin{equation}
        f \circ \id_A = f = \id_B \circ f.
    \end{equation}
    Similarly we don't bother drawing brackets around composites, since associativity makes them unnecessary.
    If we did draw brackets then considering the objects and morphisms
    \begin{equation}
        A \xrightarrow{f} B \xrightarrow{g} C \xrightarrow{h} D
    \end{equation}
    we can write the associativity law as
    \begin{equation}
        \tikzsetnextfilename{categories-graphical-language-associativity-law}
        \begin{tikzpicture}[baseline=(g.base)]
            \node[morphism] (f) at (0, 0) {\(f\)};
            \node[morphism] (g) at (0, 1.5) {\(\phantomrlap{g}{f}\)};
            \node[morphism] (h) at (0, 3) {\(\phantomrlap{h}{f}\)};
            \draw[wire] (f) -- (g) node [midway, left] {\(B\)};
            \draw[wire] (g) -- (h) node [midway, left] {\(C\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (h) -- ++ (0, 1) node [midway, left] {\(D\)};
            \node[rotate=90] at (0, 1.1) {\(\Bigg(\)};
            \node[rotate=90] at (0, 3.4) {\(\Bigg)\)};
            
            \node at (1, 1.5) {\(=\)};
            \begin{scope}[xshift=2cm]
                \node[morphism] (f) at (0, 0) {\(f\)};
                \node[morphism] (g) at (0, 1.5) {\(\phantomrlap{g}{f}\)};
                \node[morphism] (h) at (0, 3) {\(\phantomrlap{h}{f}\)};
                \draw[wire] (f) -- (g) node [midway, left] {\(B\)};
                \draw[wire] (g) -- (h) node [midway, left] {\(C\)};
                \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
                \draw[wire] (h) -- ++ (0, 1) node [midway, left] {\(D\)};
                \node[rotate=90] at (0, -0.4) {\(\Bigg(\)};
                \node[rotate=90] at (0, 1.9) {\(\Bigg)\)};
            \end{scope}
        \end{tikzpicture}
        ,
    \end{equation}
    which is just
    \begin{equation}
        (h \circ g) \circ f = h \circ (g \circ f).
    \end{equation}
    
    So by manipulating this graphical notation naturally we apply the category axioms.
    This makes this notation very powerful.
    While this notation looks new if we were to instead draw it horizontally and replace wires with \(\circ\) and \(\id_X\) then it would just be the usual algebraic notation for morphism composition.
    The real power of this notation comes when we introduce new concepts, such as monoidal products, which extend the graphical notation into a two-dimensional notation which works in much the same way, while the usual algebraic notation quickly becomes cluttered and hard to use.
    
    Note that while wires don't need to be vertical, sometimes it's helpful to have them not be to fit more things in, wires are not, in general, allowed to be horizontal, this is so that we can't turn a morphism \enquote{upside down}, although we'll see later that this is sometimes possible.
    
    
    \section{Functors}
    The spirit of category theory is that it is not objects that are important but morphisms between them.
    This suggests that whenever we see a mathematical object we should look for maps between them preserving the relevant structure.
    Well, categories are objects, so we should look for maps between them.
    Such maps should preserve the category's structure, which is
    \begin{itemize}
        \item the collection of objects;
        \item the collections of morphisms;
        \item the composition of morphisms;
        \item the identity morphisms.
    \end{itemize}
    For example, if we have a morphism \(f \colon A \to B\) in one category then our map should send this to another morphism in the new category, and the domain and codomain of that morphism should somehow be related to \(A\) and \(B\).
    It shouldn't matter whether we do composition of morphisms and then map, or map and the compose morphisms.
    Identities should map to identities.
    To this end we make the following definition of a map between categories preserving this structure.
    
    \begin{dfn}{Functor}{def:functor}
        Let \(\cat{C}\) and \(\cat{D}\) be categories.
        A \defineindex{functor}, \(F \colon \cat{C} \to \cat{D}\), is composed of two mappings:
        \begin{itemize}
            \item each object \(A \in \Ob(\cat{C})\) is mapped to some object \(F(A) \in \Ob(\cat{D})\);
            \item each morphism \(f \in \cat{C}(A, B)\) is mapped to some morphism \(F(f) \in \cat{D}(F(A), F(B))\);
        \end{itemize}
        such that
        \begin{itemize}
            \item composition is preserved: \(F(g \circ f) = F(g) \circ F(f)\) for \(f \colon A \to B\) and \(g \colon B \to C\) morphisms in \(\cat{C}\);
            \item identities are preserved: \(F(\id_A) = \id_{F(A)}\) for \(A \in \Ob(\cat{C})\).
        \end{itemize}
    \end{dfn}
    
    \begin{ntn}{}{}
        It is common to drop the brackets when applying a functor, a bit like we may apply a linear map, \(T\), to some vector, \(v\), by writing \(Tv\) and thinking of it as matrix multiplication.
        In this case a functor \(F \colon \cat{C} \to \cat{D}\) maps each \(A \in \Ob(\cat{C})\) to some object \(FA \in \Ob(\cat{D})\), each morphism \(f \in \cat{C}(A, B)\) to some morphism \(Ff \in \cat{D}(FA, FB)\), such that \(F(g \circ f) = Fg \circ Ff\) and \(F\id_A = \id_{FA}\).
    \end{ntn}
    
    Note that what we have defined above is a \defineindex{covariant functor}.
    It is also possible to define a \defineindex{contravariant functor} which reverses the direction of morphisms.
    The difference in the definition is that each \(f \in \cat{C}(A, B)\) is assigned to some \(Ff \in \cat{D}(FB, FA)\) and \(F(g \circ f) = Ff \circ Fg\).
    We'll assume that all functors are covariant unless stated otherwise.
    
    \begin{exm}{Functors}{}
        \begin{itemize}
            \item The \defineindex{constant functor}, \(\const_X \colon \cat{C} \to \cat{D}\), maps every object of \(\cat{C}\) to \(X \in \Ob(\cat{D})\) and every morphism \(f \colon A \to B\) in \(\cat{C}\) to the identity morphism \(\id_X \colon X \to X\) in \(\cat{D}\).
            \item The \defineindex{identity functor}, \(\id_{\cat{C}} \colon \cat{C} \to \cat{C}\) maps every object and morphism in \(\cat{C}\) to itself.
            \item If \(\cat{C}\) has products we can define the \defineindex{diagonal functor}, \(\Delta \colon \cat{C} \to \cat{C} \times \cat{C}\), such that \(\Delta(A) = A \times A\) for all objects \(A\) and \(\Delta(f) = f \times f\) for all morphisms \(f\).
            \item The power set can be regarded as a functor \(\powerset\colon \Set \to \Set\), sending each set to its power set and each function \(f \colon A \to B\) to the map sending \(U \in \powerset(A)\) to its image
            \begin{equation}
                f(U) \coloneqq \{f(u) \mid u \in U\}.
            \end{equation}
            \item The map \(V \mapsto V^*\) defines a functor \(\Vect \to \Vect\).
            \item If \(G\) is a group viewed as a one object category then a functor \(G \to \Set\) defines a group action, sending the single object of \(G\) to the set, \(S\), upon which \(G\) acts, and sending each morphism, \(g\) (recall morphisms are just elements of the group) to the mapping on that set \(s \mapsto g \mathbin{.} s\).
            \item If \(G\) is a group viewed as a one object category then a functor \(G \to \Vect\) defines a group action on a vector space, which is to say this functor defines a representation.
            \item If \(G\) and \(H\) are groups viewed as one object categories then a functor \(G \to H\) is simply a group homomorphism, sending the single object of \(G\) to the single object of \(H\) and preserving composition, which is the group operation.
            \item The map sending each complex Lie group to its Lie algebra is a functor \(\LieGrp \to \LieAlg\)\footnote{See \course{Symmetries of Quantum Mechanics} or \course{Symmetries of Particles and Fields}}.
            \item A \defineindex{forgetful functor}, \(\cat{C} \to \Set\) maps a category to \(\Set\) by \enquote{forgetting} the structure of the objects in \(\cat{C}\).
            A category, \(\cat{C}\), is called \define{concrete}\index{concrete category} if there exists a forgetful functor \(\cat{C} \to \Set\).
            \begin{itemize}
                \item The forgetful functor \(U \colon \Grp \to \Set\) sends each group to its underlying set and each group homomorphism to its underlying function.
                \item The forgetful functor \(U \colon \Vect \to \Set\) sends each vector space to its underlying set and each linear transformation to its underlying function.
                \item The forgetful functor \(U \colon \Top \to \Set\) sends each topological space to its underlying set and each continuous function to its underlying function.
            \end{itemize}
            \item A partially forgetful functor \(\cat{C} \to \cat{D}\) generalises forgetful functors by forgetting some, but not necessarily all, structure of the objects in \(\cat{C}\).
            \begin{itemize}
                \item The partially forgetful functor \(\Grp \to \Mon\) sends each group to itself, but now viewed as a monoid, essentially forgetting that inverses exist.
                \item The partially forgetful functor \(\Hilb \to \Vect\) sends each Hilbert space to itself, but forgets the inner product structure.
                \item The partially forgetful functor \(\CRing \to \Ring\) sends each commutative ring to itself, forgetting that multiplication is commutative.
                \item The partially forgetful functor \(\Ring \to \Ab\) sends each ring to its additive group, forgetting how to multiply.
            \end{itemize}
            \item A \defineindex{free functor} is, in a sense\footnote{This sense can be made formal through the notion of an adjoint, an important concept in category theory but one we won't explore.} the reverse of a forgetful functor, sending a set to the most general object of the appropriate type.
            \begin{itemize}
                \item The free functor \(\Set \to \Grp\) sends each set the free group it generates, which is the set of words generated by concatenating elements of the set and elements declared to be their inverses, with the group operation being concatenation and the identity the empty string.
                No other relations between elements of the set exist.
                \item The free functor \(\Set \to \Vect\) takes a set and declares that all the elements in it are linearly independent and form a basis for a vector space whose elements are formal linear combinations of these elements.
            \end{itemize}
        \end{itemize}
    \end{exm}
    
    The next definition introduces some terminology used to talk about functors.
    
    \begin{dfn}{}{def:equivalence and others}
        A functor \(F \colon \cat{C} \to \cat{D}\) is
        \begin{itemize}
            \item \define{full}\index{full functor} when the mapping \(f \mapsto Ff\) defines a surjection \(\cat{C}(A, B) \to \cat{D}(FA, FB)\);
            \item \define{faithful}\index{faithful functor} when the mapping \(f \mapsto Ff\) defines an injection \(\cat{C}(A, B) \to \cat{D}(FA, FB)\);
            \item \defineindex{essentially surjective on objects} when for each \(B \in \Ob(\cat{D})\) there is some \(A \in \Ob(\cat{C})\) such that \(FA \isomorphic B\), essentially when the mapping \(A \mapsto FA\) is surjective, but replacing equality with isomorphism in the definition of surjectivity;
            \item an \defineindex{equivalence} when it is full, faithful, and essentially surjective on objects;
            \item an \defineindex{endofunctor} if \(\cat{C} = \cat{D}\).
        \end{itemize}
    \end{dfn}
    
    The most important definition here is that of an equivalence.
    Equality of categories is far too strong a condition.
    We can also define a notion of isomorphism between categories, but this is also too strong.
    Relaxing the requirements for isomorphism, by changing surjective on objects to essentially surjective on objects, we get the notion of equivalence, which is the sweet spot where a functor preserves enough structure for the two categories to be the same for all intents and purposes, without being too restrictive.
    
    It should be noted that there are multiple, equivalent, definitions of equivalence.
    The one we've given here is probably the easiest to understand, but not always the easiest to apply.
    
    One example we've already seen of an equivalence is the functor \(\Mat \to \FVect\) sending \(n \mapsto \field^n\) and a matrix to the associated linear map on the vector space with some fixed basis.
    This is an equivalence since all finite dimensional vector spaces of the same dimension are isomorphic, which means that this functor is essentially surjective on objects.
    
    Now that we have categories and maps between them it is natural to ask if this forms a category, and indeed it does!
    
    \begin{dfn}{\normalsize\(\Cat\)}{}
        The category \(\Cat\)\index{Cat@\(\Cat\)} has (small) categories as its objects and functors as its morphisms.
        Composition of functors is composition of the two mappings \(\Ob(\cat{C}) \to \Ob(\cat{D})\) and \(\cat{C}(A, B) \to \cat{D}(FA, FB)\), and the identity is the identity functor.
    \end{dfn}
    
    To be clear, if \(F \colon \cat{C} \to \cat{D}\) and \(G \colon \cat{D} \to \cat{E}\) are functors then the functor \((G \circ F) \colon \cat{C} \to \cat{E}\) sends \(A \in \Ob(\cat{C})\) to \((G \circ F)(A) = (G \circ F)A = G(F(A)) = GFA\), and sends \(f \in \cat{C}(A, B)\) to \((G \circ F)(f) = (G \circ F)f = G(F(f)) = GFf \in \cat{E}(GFA, GFB)\).
    Associativity is guaranteed by associativity of the underlying mappings and the identity functor is clearly an identity morphism.
    
    \section{Natural Transformations}
    We have now defined functors as mathematical objects.
    Following the spirit of category theory we should look for maps between functors preserving their structure.
    A map between functors should preserve the functor structure.
    This means that it shouldn't matter if we map to a different functor and then apply the functor, or apply a functor and then map to the other functor.
    This leads to the following definition.
    
    \begin{dfn}{Natural Transformation}{}
        Let \(\cat{C}\) and \(\cat{D}\) be categories and \(F, G \colon \cat{C} \to \cat{D}\) functors.
        For every object \(A \in \Ob(\cat{C})\) a \defineindex{natural transformation}, \(\zeta \colon F \naturalTransformation G\), assigns a morphism \(\zeta_A \colon FA \to GA\) in \(\cat{D}\) such that for every morphism \(f \colon A \to B\) in \(\cat{C}\) the following diagram commutes:
        \begin{equation}
            \begin{tikzcd}
                FA \arrow[r, "\zeta_A"] \arrow[d, "Ff"'] & GA \arrow[d, "Gf"]\\
                FB \arrow[r, "\zeta_B"'] & GB\mathrlap{.}
            \end{tikzcd}
        \end{equation}
        
        If every component, \(\zeta_A\), is an isomorphism then \(\zeta\) is called a \defineindex{natural isomorphism} and \(F\) and \(G\) are said to be \defineindex{naturally isomorphic}, written \(F \isomorphic G\).
    \end{dfn}
    
    This leads to an alternative, equivalent, definition of an equivalence.
    A functor \(F \colon \cat{C} \to \cat{D}\) is an equivalence if and only if there is a functor \(G \colon \cat{D} \to \cat{C}\) and natural isomorphisms \(G \circ F \naturalTransformation \id_{\cat{C}}\) and \(F \circ G \naturalTransformation \id_{\cat{D}}\).
    Intuitively, an equivalence is a function which is invertible up to natural isomorphism.
    
    Note that it is common to write all of the data needed to define a natural transformation as
    \begin{equation}
        \begin{tikzcd}[column sep=large]
            \cat{C}
            \arrow[r, "F"{name=F}, bend left=40]
            \arrow[r, "G"{name=G, swap}, bend right=40]
            & \cat{D}.
            \arrow[Rightarrow, from=F.south, to=G.north-|F, shorten <=5pt, shorten >=5pt]
        \end{tikzcd}
    \end{equation}
    
    \begin{exm}{Natural Transformations}{}
        These examples are taken from \cite{leinster} and make use of the notation used there representing a natural transformation as a collection of morphisms indexed by objects, \((\zeta_A)_{A \in \Ob(\cat{C})}\).
        \begin{itemize}
            \item A \defineindex{discrete category} is a category in which the only maps are the identity maps.
            If \(\cat{C}\) is a discrete category then a functor \(F \colon \cat{C} \to \cat{D}\) is simply a family of objects \((FA)_{A \in \Ob(\cat{C})}\).
            In this case a natural transformation \(\zeta \colon F \naturalTransformation G\) is just a family of maps \((\zeta_A \colon FA \to GA)_{A \in \Ob(\cat{C})}\).
            The naturality axiom is automatically fulfilled since it holds trivially for identities.
            \item Fix some natural number \(n\).
            For any commutative ring, \(R\), the \(n \times n\) matrices with entries in \(R\) form a monoid, \(M_n(R)\), under matrix multiplication.
            Any ring homomorphism \(R \to S\) induces a monoid homomorphism \(M_n(R) \to M_n(S)\) by acting elementwise on the entries of the matrix with the homomorphism.
            This defines a functor \(M_n \colon \CRing \to \Mon\).
            
            The elements of any ring, \(R\), form a monoid, \(U(R)\), under multiplication, this is an example of a forgetful functor \(U \colon \CRing \to \Mon\).
            
            Every \(n \times n\) matrix, \(X\), over a commutative ring \(R\) has a determinant \(\det_R(X) \in R\).
            The properties
            \begin{equation}
                \det_R(XY) = \det_R(X)\det_R(Y), \qqand \det_R(I) = 1
            \end{equation}
            tell us that for each commutative ring \(R\) the function \(\det_R \colon M_n(R) \to U(R)\) is a monoid homomorphism.
            Thus we have a family of maps
            \begin{equation}
                (\det_R \colon M_n(R) \to U(R))_{R \in \Ob(\CRing)}.
            \end{equation}
            This family of maps defines a natural transformation \(\det \colon M_n \naturalTransformation U\).
            
            \item Consider the category \(\FVect\).
            The map \((-)^* \colon \FVect \to \FVect\) sending each vector space to its dual is a contravariant functor.
            Thus the map \((-)^{**} \colon \FVect \to \FVect\) sending each vector space to its double dual is also a covariant functor.
            For each \(V \in \Ob(\FVect)\) we have a canonical isomorphism \(\zeta_V \colon V \to V^{**}\).
            Given \(v \in V\) the element \(\zeta_V(v) \in V^{**}\) is evaluation at \(v\), that is \(\zeta_V(v) \colon V^* \to \field\) maps \(\varphi \in V^*\) to \(\varphi(v) \in \field\).
            This defines a natural transformation \(\zeta \colon 1_{\FVect} \naturalTransformation (-)^{**}\) meaning that each vector space is naturally isomorphic to its double dual.
            Note that given \(F, G \colon \cat{C} \to \cat{D}\) we say that \(FA \isomorphic GA\) naturally in \(A\) if \(F\) and \(G\) are naturally isomorphic.
            In this case \(V \isomorphic V^{**}\) naturally in \(V\).
        \end{itemize}
    \end{exm}
    
    Functors are objects, and natural transformations are maps between them, so we should try to define a category.
    \begin{dfn}{Functor Category}{}
        Let \(\cat{C}\) and \(\cat{D}\) be categories.
        The \defineindex{functor category} \(\functorCategory{\cat{C}}{\cat{D}}\), also written \(\cat{D}^{\cat{C}}\), has functors \(F, G \colon \cat{C} \to \cat{D}\) as objects and natural transformations \(F \naturalTransformation G\) as morphisms.
        Composition of natural transformations is done component wise and the identity is the identity natural transformation which assigns the to each \(A \in \Ob(\cat{C})\) the identity morphism \(\id_{FA}\) in \(\cat{D}\).
    \end{dfn}
    
    An example of a functor category is the category of \(G\)-sets\footnote{that is sets upon which \(G\) acts} for some group \(G\), which is \(\functorCategory{G}{\Set}\), where \(G\) is viewed as a one object category.
    Similarly \(\functorCategory{G}{\Vect}\) is the category of \(\field\)-linear representations of \(G\).
    In both of these cases morphisms are so called equivariant maps.
    Given sets (vector spaces) \(X\) and \(Y\) upon which we have a \(G\) action (representation) defined an \defineindex{equivariant map} is a function (linear map) \(f \colon X \to Y\) such that \(f(g \mathbin{.} x) = g \mathbin{.} f(x)\), that is a map which commutes with the group action, so
    \begin{equation}
        \begin{tikzcd}
            X \arrow[r, "f"] \arrow[d, "g \mathbin{.} {}"'] & Y \arrow[d, "g \mathbin{.} {}"]\\
            X \arrow[r, "f"'] & Y
        \end{tikzcd}
    \end{equation}
    commutes.
    This diagram is exactly what we get if we specialise the diagram defining a natural transformation to functors \(G \to \Set\) (\(G \to \Vect\)).
    
    \section{Initial and Terminal Objects}
    \epigraph{A comathematician is a comachine for turning cotheorems into ffee.}{}
    \begin{dfn}{Initial and Terminal Objects}{}
        Let \(\cat{C}\) be a category.
        An \defineindex{initial object}, \(I \in \Ob(\cat{C})\) is an object which has a unique morphism into every other object.
        That is for all \(A \in \Ob(\cat{C})\) there is a unique morphism \(I \to A\).
        
        A \defineindex{terminal object}, \(T \in \Ob(\cat{C})\) is an object which has a unique morphism out of every other object.
        That is for all \(A \in \Ob(\cat{C})\) there is a unique morphism \(A \to T\).
    \end{dfn}
    
    Notice that these two concepts are related by turning the arrows around.
    We say that the two concepts are opposite or dual.
    This is common in category theory, that we have two such definitions, and as such it motivates the following definition.
    
    \begin{dfn}{Opposite Category}{}
        Given a category, \(\cat{C}\), the \defineindex{opposite category}, \(\cat{C}^{\op}\), has the same objects, that is \(\Ob(\cat{C}) = \Ob(\cat{C}^{\op})\), but all morphisms are reversed, that is if we have a morphism \(f\colon A \to B\) in \(\cat{C}\) then there is a morphism \(f^{\op}\colon B \to A\) in \(\cat{C}^{\op}\), so \(\cat{C}(A, B) = \cat{C}^{\op}(B, A)\).
    \end{dfn}
    
    With this definition we can reformulate the definition of initial and terminal objects into an initial object is an object with a unique morphism into every other object, and a terminal object is an initial object in the opposite category.
    
    Another concept that we can reformulate with opposite categories is contravariant functors.
    A contravariant functor, \(F \colon \cat{C} \to \cat{D}\), which is such that \(F(g \circ f) = Ff \circ Fg\), is exactly a covariant functor from the opposite category, \(F\colon \cat{C}^{\op} \to \cat{D}\), or equivalently, into the opposite category, \(F\colon \cat{C} \to \cat{D}^{\op}\).
    
    In general if we define some property, \(x\), in a category we can often find the same property in the opposite category, and we prefix this property, viewed in the original category, with \enquote{co}.
    For example, in the next section we will define products, and a product in the opposite category is a coproduct.
    Given a property the coproperty is defined by reversing all of the arrows, so an initial object is a coterminal object, and a terminal object is a coinitial object, and so on.
    
    \section{Products}
    We want to generalise the Cartesian product of sets, which is defined as
    \begin{equation}
        A \times B \coloneqq \{(a, b) \mid a \in A \text{ and } b \in B\}.
    \end{equation}
    To do so we need to remove reference to elements, since this isn't a concept that we have in an arbitrary category.
    Clearly this definition defines a new set, so we have a new object in \(\Set\).
    In order to do away with reference to elements notice that we can define two functions \(p_A \colon A \times B \to A\) and \(p_B \colon A \times B \to B\) which project out the elements of a pair.
    That is \(p_A(a, b) = a\) and \(p_B(a, b) = b\).
    We can summarise this as
    \begin{equation}
        \begin{tikzcd}
            A & A \times B \arrow[l, "p_A"'] \arrow[r, "p_B"] & B.
        \end{tikzcd}
    \end{equation}
    
    The key insight comes in requiring that \(A \times B\) is, in a sense, the most general\footnote{this concept, both for products and more general (co)limits, is explained well in \cite{milewski}, which is where I'm replicating this argument from.} set satisfying this property, since the Cartesian product doesn't add in any unnecessary restrictions.
    Thus, if we have another set, \(X\), which is a candidate for defining the product then there must be morphisms \(f \colon X \to A\) and \(g \colon X \to B\) which are candidates for \(p_A\) and \(p_B\) respectively.
    We can now make \enquote{most general} precise.
    Given \(A \times B\) with \(X\), \(f\), and \(g\) we should be able to recreate \(f\) and \(g\) by first mapping to \(A \times B\) and then projecting out.
    This map to \(A \times B\) is what we call the product of \(f\) and \(g\), written \(f \times g\), \((f, g)\), or \(\binom{f}{g}\).
    We can summarise this definition by requiring that the following diagram commutes:
    \begin{equation}
        \begin{tikzcd}
            & X \arrow[d, "f \times g"{pos=0.6}, dashed, "!"'{xshift=0.075cm}] \arrow[dl, "f"'] \arrow[dr, "g"] & \\
            A & A \times B \arrow[l, "p_A"] \arrow[r, "p_B"'] & B.
        \end{tikzcd}
    \end{equation}
    Here we use a dashed arrow to denote that this diagram is telling us that this arrow exists, and the \(!\) tells us that this arrow is unique.
    
    This is an example of a more general concept in category theory, called a \defineindex{limit}.
    The general idea is that we can define some prototype, here \(A \times B\) with the maps \(p_A\) and \(p_B\).
    Then we posit a candidate object which factors through the prototype, then we look for an object satisfying this property.
    
    Notice that we now have done away with all references to elements of \(A \times B\), so we can generalise this definition to arbitrary categories.
    
    \begin{dfn}{Product}{def:product}
        Let \(\cat{C}\) be a category with objects \(A\) and \(B\).
        The product of \(A\) and \(B\), if it exists, is the object \(A \times B\) and the morphisms \(p_A \colon A \times B \to A\) and \(p_B \colon A \times B \to B\) such that
        \begin{equation}
            \begin{tikzcd}
                & X \arrow[d, "f \times g"{pos=0.6}, dashed, "!"'{xshift=0.075cm}] \arrow[dl, "f"'] \arrow[dr, "g"] & \\
                A & A \times B \arrow[l, "p_A"] \arrow[r, "p_B"'] & B
            \end{tikzcd}
        \end{equation}
        commutes for all \(X\) with morphisms to \(f \colon X \to A\) and \(g \colon X \to B\).
        
        The \defineindex{coproduct} is the product in the opposite category.
    \end{dfn}
    
    \begin{exm}{Products}{}
        \begin{itemize}
            \item In \(\Set\) the product is the Cartesian product, and the coproduct is the disjoint union.
            \item In \(\Top\) the product of two topological spaces is the topological space formed from the Cartesian product of the two topological spaces equipped with the product topology, which is the coarsest topology for which all projections are continuous.
            \item In \(\RMod\) the product is the Cartesian product of the modules with addition defined componentwise and multiplication defined to be distributive.
            \item In \(\Grp\) the product is the direct product of groups, that is the Cartesian product of the underlying sets and then componentwise multiplication.
            \item In \(\Rel\) products are disjoint unions.
            \item In a poset viewed as a single object category the product of two elements is the greatest lower bound.
            \item In \(\Hask\)\footnote{\(\Hask\) can be thought of as a subcategory of \(\Set\), where the only objects are the \Haskell{} types, viewed as sets with all values of that type as their elements. There are some tricky things about \Haskell{}, such as the existence of bottom, \lstinline[mathescape]|$\bot$|, which represents an error. This also causes problems when trying to take products and coproducts, but typically we just ignore these issues.}, the category of \Haskell{} types with \Haskell{} functions as morphisms, a product type is one defined as follows:
            \begin{lstlisting}[gobble=16, style=haskell]
                data Product a b = Product a b
            \end{lstlisting}
            This can be thought of as the Cartesian product, so here we have pairs, or tuples, of type \lstinline|(a, b)|.
            On the other hand a sum type, or coproduct type, is defined as:
            \begin{lstlisting}[gobble=16, style=haskell]
                data Sum a b = A a | B b
            \end{lstlisting}
            This can be thought of as the disjoint union, which is can defined for sets by \enquote{tagging} each element with the set it comes from, then taking the usual union, here we \enquote{tag} the element by using one of the constructors, \lstinline|A| or \lstinline|B|, and then our final type allows for either of these constructors to be used, taking their union.
            
            Notice that we need something of both type \lstinline|a| and \lstinline|b| to define a product, corresponding to the \enquote{and} nature of the product, and only something of type \lstinline|a| or type \lstinline|b| to define the sum type, corresponding to the \enquote{or} nature of the sum.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Product Category}{}
        Let \(\cat{C}\) and \(\cat{D}\) be categories.
        The \defineindex{product category} \(\cat{C} \times \cat{D}\) has
        \begin{itemize}
            \item as objects pairs of objects \((A, B)\) with \(A \in \Ob(\cat{C})\) and \(B \in \Ob(\cat{D})\);
            \item as morphisms \((A_1, B_1) \to (A_2, B_2)\) pairs of morphisms \((f, g)\) with \(f \colon A_1 \to A_2\) and \(g \colon B_1 \to B_2\) that is, \(f \in \cat{C}(A_1, A_2)\) and \(g \in \cat{D}(B_1, B_2)\);
            \item as composition componentwise composition from the two categories, that is \((f_2, g_2) \circ (f_1, g_1) = (f_2 \circ f_1, g_2 \circ g_1)\);
            \item as identities pairs of identities from the two categories, that is \(\id_{(A, B)} = (\id_A, \id_B)\).
        \end{itemize}
    \end{dfn}
    
    For the case where \(\cat{C}\) and \(\cat{D}\) are small the product category \(\cat{C} \times \cat{D}\) is exactly the categorical product of \(\cat{C}\) and \(\cat{D}\) in \(\Cat\).
    
    The main use of product categories is to define bifunctors, which are the generalisation of functors to two variables.
    A \defineindex{bifunctor} is exactly a functor \(\cat{C} \times \cat{D} \to \cat{E}\), but it's usually easier to think of \(\cat{C}\) and \(\cat{D}\) as being separate, just like how we might treat the horizontal and vertical axes as being independent in a function \(\reals^2 \to \reals\).
    
    \chapter{Monoidal Categories}
    \section{Tensor Products}
    \begin{dfn}{Bilinear}{}
        Let \(U\), \(V\) and \(W\) be vector spaces over \(\field\).
        A \defineindex{bilinear map} is a function \(T \colon U \times V \to W\) which is linear in each variable.
        That is, the maps \(T(-, v) \colon U \to W\) defined by \(u \mapsto T(u, v)\) and \(T(u, -) \colon V \to W\) defined by \(v \mapsto T(u, v)\) are linear.
    \end{dfn}
    
    The tensor product provides a way to combine two vector spaces into a new vector space.
    The simplest definition is that given vector spaces \(U\) and \(V\) over some field \(\field\) the tensor product \(U \otimes V\) consists of all linear combinations of elements of the form \(u \otimes v\) with \(u \in U\) and \(v \in V\).
    This definition makes direct reference to elements of the vector spaces, so isn't compatible with the spirit of category theory.
    The useful thing about the tensor product is it combines two vector spaces into one in such a way that we can define linear maps on the new space in terms of linear maps on the original spaces.
    For example, the linear map sending \(u \in U\) to \(2u\) automatically extends to a linear map on \(U \otimes V\) sending \(u \otimes v\) to \(2u \otimes v = (2u) \otimes v = 2(u \otimes v)\).
    This inspires the following definition.
    
    \begin{dfn}{Tensor Product}{}
        Let \(U\), \(V\), and \(W\) be vector spaces\footnote{this definition also holds for \(R\)-modules replacing linear with \(R\)-linear.} over \(\field\).
        The \defineindex{tensor product} of \(U\) and \(V\) is a vector space, which we call \(U \otimes V\), equipped with a bilinear map \(f \colon U \times V \to U \otimes V\) such that for all bilinear maps \(g \colon U \times V \to W\) there exists a unique linear map \(h \colon U \otimes V \to W\) such that \(g = h \circ f\).
        That is, such that
        \begin{equation}
            \begin{tikzcd}[column sep=large]
                U \times V \arrow[r, "f \text{ (bilinear)}"] \arrow[dr, "g \text{ (bilinear)}"'] & U \otimes V \arrow[d, dashed, "h \text{ (linear)}"]\\
                & W
            \end{tikzcd}
        \end{equation}
        commutes.
    \end{dfn}
    
    The important idea here is that a linear map \(U \otimes V \to W\) is just as good as a bilinear map \(U \times V \to W\), and since linear maps are much nicer to work with, and since \(U \times V\) is not a vector space, we usually prefer to work with \(U \otimes V\).
    
    Note that not all elements of \(U \otimes V\) are of the form \(u \otimes v\).
    For example, there is no way to write \(\ve{1} \otimes \ve{1} + \ve{2} \otimes \ve{2} \in V \otimes V\) in this form.
    This will be important later.
    
    The tensor product also gives us a natural way to combine two linear maps, we simply act on the relevant part of the tensor product with the relevant function.
    
    \begin{dfn}{Tensor Product of Linear Maps}{}
        Let \(U\), \(U'\), \(V\) and \(V'\) be vector spaces over \(\field\).
        Let \(f \colon U \to U'\) and \(g \colon V \to V'\).
        Then the \define{tensor product}\index{tensor product!of linear maps} of \(f\) and \(g\) is defined to be the map \(f \otimes g \colon U \otimes V \to U' \otimes V'\) given by defining \((f \otimes g)(u \otimes v) = f(u) \otimes g(v)\) and extending this to all of \(U \otimes V\) through linearity.
    \end{dfn}
    
    The tensor product of vector spaces can be extended to any inner product space in a straight forward manner, the inner product simply factorises.
    
    \begin{dfn}{Tensor Product of Inner Product Spaces}{}\index{tensor product!of Hilbert spaces}
        Let \((U, \braket{-}{-}_U)\) and \((V, \braket{-}{-}_W)\) be inner product spaces (Hilbert spaces) over \(\field\).
        Then \((U \otimes V, \braket{-}{-}_{U \otimes V})\) is an inner product space (Hilbert space) with the inner product
        \begin{equation}
            \braket{u \otimes v}{u' \otimes v'}_{U \otimes V} = \braket{u}{u'}_U \braket{v}{v'}_V.
        \end{equation}
    \end{dfn}
    
    \section{Monoidal Categories: The Idea}
    Monoidal categories generalise the tensor product to other categories.
    The idea being that tensor products allow us to work with multiple objects at the same time, or in parallel.
    Recall that we've seen four ways to consider categories:
    \begin{itemize}
        \item physical systems and the processes occurring;
        \item data types and the algorithms manipulating them;
        \item algebraic structures and structure preserving functions;
        \item logical propositions and implications between them.
    \end{itemize}
    We can consider the idea of a monoidal category in each of these frameworks:
    \begin{itemize}
        \item independent systems evolving separately;
        \item running algorithms in parallel;
        \item products or sums of geometric structures;
        \item using separate proofs of \(P\) and \(Q\) to prove \(P \land Q\).
    \end{itemize}
    
    \section{Monoidal Categories: The Need for Specificity}
    \label{sec:monoidal categories: the need for specificity}
    Consider processes \(A\), \(B\), and \(C\) with some notion of parallel composition, \(\otimes\).
    So, \(A \otimes B\) is what we get by doing both processes \(A\) and \(B\) at the same time.
    After playing around with this idea for a while one may come upon the question of what the relationship between
    \begin{equation}
        (A \otimes B) \otimes C \qqand A \otimes (B \otimes C)
    \end{equation}
    should be.
    It's not right for them to be equal, since this isn't the case for vector spaces with the tensor product or for sets with the Cartesian product, which is the equivalent in \(\Set\) as we'll see later.
    
    For example, viewed as sets we have \(((a, b), c)\) as an element of \((A \times B) \times C\), but \((a, (b, c))\) as an element of \(A \times (B \times C)\).
    Clearly there is a map \((A \times B) \times C \to A \times (B \times C)\) given by \(((a, b), c) \mapsto (a, (b, c))\).
    We say that this map associates these distinct objects.
    This map is also clearly invertible, the inverse being \((a, (b, c)) \mapsto ((a, b), c)\).
    So, we have \((A \times B) \times C \isomorphic A \times (B \times C)\).
    
    More questions arise when we think about what the processes \(A\), \(B\), and \(C\) are.
    For example there is often a concept of a trivial system where we don't do anything.
    We want it to be such that running this trivial system in parallel with another system is as if we weren't doing the trivial thing at all.
    However this is clearly not the case in, for example, a computer running a trivial algorithm, it still has to compile and run this trivial algorithm.
    So again we want the computation of \(A\) and the trivial system in parallel to be isomorphic to the computation of \(A\) alone.
    
    Another question, which we won't see an answer too until later, is does order matter?
    That is, should we treat \(A \otimes B\) and \(B \otimes A\) as the equal?
    What about isomorphic?
    It is also possible that they aren't even isomorphic, although in many of the cases we're interested in they will be.
    
    To answer these questions we have to be careful when defining a monoidal category, the task of the next section.
    
    \section{Monoidal Categories: The Definition}
    \begin{dfn}{Monoidal Category}{}
        A \defineindex{monoidal category} is formed from the following data:
        \begin{enumerate}
            \item a category, \(\cat{C}\);
            \item a \defineindex{tensor product functor}\index{tensor product}
            \begin{equation}
                {-} \otimes {-} \colon \cat{C} \times \cat{C} \to \cat{C},
            \end{equation}
            also called the \defineindex{monoidal product};
            \item a \defineindex{unit object} \(I \in \Ob(\cat{C})\);
            \item a natural isomorphism
            \begin{equation}
                \alpha \colon ({-} \otimes {-}) \otimes {-} \naturalTransformation {-} \otimes ({-} \otimes {-})
            \end{equation}
            called the \defineindex{associator} which has components
            \begin{equation}
                \alpha_{A,B,C} \colon (A \otimes B) \otimes C \to A \otimes (B \otimes C)
            \end{equation}
            for \(A, B, C \in \Ob(\cat{C})\);
            \item a natural isomorphism
            \begin{equation}
                \lambda \colon I \otimes {-} \naturalTransformation -
            \end{equation}
            called the \defineindex{left unitor}\index{unitor} which has components
            \begin{equation}
                \lambda_A \colon I \otimes A \to A
            \end{equation}
            for \(A \in \Ob(\cat{C})\);
            \item a natural isomorphism
            \begin{equation}
                \rho \colon {-} \otimes I \naturalTransformation -
            \end{equation}
            called the \defineindex{right unitor} which has components
            \begin{equation}
                \rho_A \colon A \otimes A \to A
            \end{equation}
            for \(A \in \Ob(\cat{C})\).
        \end{enumerate}
        This data must be such that the \defineindex{triangle equation} holds, which is that 
        \begin{equation}
            \begin{tikzcd}
                (A \otimes I) \otimes B \arrow[rr, "\alpha_{A, I, B}"] \arrow[dr, "\rho_A \otimes \id_B"'] && A \otimes (I \otimes B) \arrow[dl, "{\id_A} \otimes \lambda_B"]\\
                & A \otimes B &
            \end{tikzcd}
        \end{equation}
        commutes for all \(A, B \in \Ob(\cat{C})\), and the \defineindex{pentagon equation} holds, which is that
        \begin{equation}
            \begin{tikzcd}[column sep=-1cm]
                & (A \otimes (B \otimes C)) \otimes D \arrow[rr, "\alpha_{A, B \otimes C, D}"] && A \otimes ((B \otimes C) \otimes D) \arrow[dr, "{\id_A} \otimes \alpha_{B,C,D}", pos=0.6] & \\
                ((A \otimes B) \otimes C) \otimes D \arrow[ur, "\alpha_{A,B,C} \otimes \id_D", pos=0.4] \arrow[drr, "\alpha_{A\otimes B,C,D}"'] &&&& A \otimes (B \otimes (C \otimes D)) \\
                && (A \otimes B) \otimes (C \otimes D) \arrow[urr, "\alpha_{A,B,C\otimes D}"'] && 
            \end{tikzcd}
        \end{equation}
        commutes for all \(A, B, C, D \in \Ob(\cat{C})\).
    \end{dfn}
    
    This is quite a large definition so we'll break it down and hopefully this will make it less daunting.
    First lets look at what the data of a monoidal category tells us:
    \begin{enumerate}
        \item the category tells us the type of objects and morphisms we are considering;
        \item the tensor product functor tells us how to combine two objects to get a new object, and two morphisms to get a new morphism;
        \item the unit object represents a trivial process in which nothing happens;
        \item the associator allows us to move brackets around and only change things by an isomorphism, which means no important change occurs;
        \item the unitors allow us to remove the unit object when it is involved in a product, again only changing things by an isomorphism.
    \end{enumerate}
    
    The naturality condition for \(\alpha\) gives the commuting diagram
    \begin{equation}
        \begin{tikzcd}
            (A \otimes B) \otimes C \arrow[r, "\alpha_{A,B,C}"] \arrow[d, "(f \otimes g) \otimes h"'] & A \otimes (B \otimes C) \arrow[d, "f \otimes (g \otimes h)"]\\
            (A' \otimes B') \otimes C' \arrow[r, "\alpha_{A',B',C'}"'] & A' \otimes (B' \otimes C')
        \end{tikzcd}
    \end{equation}
    for all \(f \colon A \to A'\), \(g \colon B \to B'\), and \(h \colon C \to C'\).
    In other words, it doesn't matter if we apply maps and then move brackets, or move brackets and then apply maps.
    The naturality conditions for \(\lambda\) and \(\rho\) can be combined into the following commuting diagram
    \begin{equation}
        \begin{tikzcd}
            I \otimes A \arrow[r, "\lambda_A"] \arrow[d, "{\id_I} \otimes f"'] & A \arrow[d, "f"'] & A \otimes I \arrow[l, "\rho_A"'] \arrow[d, "f \otimes \id_{I}"]\\
            I \otimes B \arrow[r, "\lambda_B"] & B & B \otimes I \arrow[l, "\rho_B"]
        \end{tikzcd}
    \end{equation}
    for all \(f \colon A \to B\).
    In other words, we can remove the identity and then apply \(f\), or we can do nothing to the identity while applying \(f\) and then remove the identity.
    Note that it is common to write \(I\) for \(\id_I\), there should be no confusion as \(I \otimes f\) can only mean \({\id_I} \otimes f\), there is no way to take the tensor product of an object and a morphism.
    Thus, we could write the naturality condition for \(\lambda\), the left square, as either \(f \circ \lambda_A = \lambda_B \circ ({\id_I} \otimes f)\) or the slightly more succinct \(f \circ \lambda_A = \lambda_B \circ (I \otimes f)\).
    In fact, this is sometimes done for all objects, not just the unit.

    Now lets look at the triangle equation.
    It tells us how the unitors and associator combine.
    Starting at the top left, with \((A \otimes I) \otimes B\) there are two ways to get to \(A \otimes B\).
    We can either remove \(I\) by applying \(\rho_A\) to the \(A \otimes I\) bit, which means applying \(\rho_A \otimes \id_B\) to \((A \otimes I) \otimes B\), or we can reassociate, by applying \(\alpha_{A,I,B}\) to get \(A \otimes (I \otimes B)\) then remove the unit by applying \(\lambda_B\) to the \(I \otimes B\) bit, which means applying \({\id_A} \otimes \lambda_B\) to \(A \otimes (I \otimes B)\).
    The triangle equality says that both of these are actually the same, that is
    \begin{equation}
        \rho_A \otimes \id_B = ({\id_A} \otimes \lambda_A) \circ \alpha_{A,I,B}.
    \end{equation}

    Finally, lets look at the pentagon equation.
    This tells us how the associator can be applied to reassociate products of four objects, and it turns out that this is enough to completely specify how the associator reassociates products of any number of elements.
    Starting on the left we have the completely left associated \(((A \otimes B) \otimes C) \otimes D\).
    One path we can take to reassociate is to ignore \(D\) and reassociate \((A \otimes B) \otimes C\) to \((A \otimes (B \otimes C))\), this is done using \(\alpha_{A, B, C} \otimes \id_D\).
    Next we can ignore the fact that \(B \otimes C\) is the product of two objects and think of it as a single object, \(X\), so we have \((A \otimes X) \otimes D\), which we can reassociate using \(\alpha_{A, X, D} = \alpha_{A, B \otimes C, D}\) to get \(A \otimes ((B \otimes C) \otimes D)\).
    Finally we can ignore \(A\) and reassociate \((B \otimes C) \otimes D\) using \({\id_A} \otimes \alpha_{B, C, D}\) to get \(A \otimes (B \otimes (C \otimes D))\).
    Alternatively, if we start with \(((A \otimes B) \otimes C) \otimes D\) we can treat \(A \otimes B\) as a single object, \(Y\), and reassociate \((Y \otimes C) \otimes D\) with \(\alpha_{Y, C, D} = \alpha_{A \otimes B, C, D}\) to get \((A \otimes B) \otimes (C \otimes D)\).
    Then we treat \(C \otimes D\) as a single object, \(Z\), and reassociate \((A \otimes B) \otimes Z\) using \(\alpha_{A, B, Z} = \alpha_{A, B, C \otimes D}\) to get \(A \otimes (B \otimes (C \otimes D))\).
    The pentagon equation tells us that both of these ways of reassociating from left to right give are the same, that is
    \begin{equation}
        ({\id_A} \otimes \, \alpha_{B, C, D}) \circ (\alpha_{A, B \otimes C, D}) \circ (\alpha_{A, B, C} \otimes \id_D) = \alpha_{A, B, C\otimes D} \circ \alpha_{A\otimes B, C, D}.
    \end{equation}
    
    To summarise, the triangle equation says that all ways of removing \(I\) from \((A \otimes I) \otimes B\) to get \(A \otimes B\) are the same, and the pentagon equation says that all ways of reassociating \(((A \otimes B) \otimes C) \otimes D\) to get \(A \otimes (B \otimes (C \otimes D))\) are the same.
    
    Another way of viewing the definition of a monoidal category is as the (vertical\footnote{cf.\@ horizontal categorification, or oidification, in which we realise a particular structure within a single object category and then generalise to multi-object categories, e.g.\@ a group is a one object category in which all morphisms are isomorphisms, and a groupoid is a category in which all morphisms are isomorphisms. Categories are already a generalisation of monoids in this sense.}) \defineindex{categorification} of a monoid.
    By this we mean that we generalise a structure applied to sets, here a monoid, to a structure on categories.
    Typically this is done by replacing elements with objects, functions with functors, and equality with isomorphism.
    Compare the definitions of a monoid, \(M\), and a monoidal category, \(\cat{C}\):
    \begin{multiitem}
        \mitemxx{Elements, \(a, b, c \in M\)}{Objects, \(A, B, C \in \Ob(\cat{C})\)}
        \mitemxx{A binary function \(\cdot \colon M \times M \to M\)}{A bifunctor \(\otimes\colon \cat{C} \times \cat{C} \to \cat{C}\)}
        \mitemxx{\((a \cdot b) \cdot c = a \cdot (b \cdot c)\)}{\((A \otimes B) \otimes C \isomorphic A \otimes (B \otimes C)\)}
        \mitemxx{Identity \(e \in M\) such that \(e \cdot a = a = a \cdot e\)}{Unit \(I \in \Ob(\cat{C})\) such that \(I \otimes A \isomorphic A \isomorphic A \otimes I\)}
    \end{multiitem}
    
    While these requirements seem quite complex they are incredibly powerful, and fairly easy to work with in practice, because they are so restrictive.
    It turns out that the monoidal product works exactly how our intuition says it does, and this is the crux of the next theorem, which we shan't prove.
    
    \begin{thm}{Coherence Theorem for Monoidal Categories}{}
        In a monoidal category any well-typed equation built solely from associators, unitors, and their inverses holds.
    \end{thm}
    
    By well-typed we simply mean that both sides of the equation must be morphisms with matching domain and codomain, and that any compositions are defined, so morphisms in the composition match domain to codomain.
    This means that we don't have to actually use the triangle and pentagon equations directly, as long as they hold we can write down pretty much anything we like as long as it makes sense and it will be true.
    
    \section{Monoidal Categories: The Examples}
    \subsection{\texorpdfstring{\(\Set\)}{Set}}
    The obvious choice for a monoidal product on \(\Set\) is the Cartesian product.
    We then need to look for a unit, something which we can take a Cartesian product with but not really change anything.
    One way we might come to an answer is to recognise that the unit behaves just like 1 in a product, hang on, 1 is something that we can represent as a set, a singleton, and there's our answer.
    Given some set \(A\) and some singleton\footnote{Since an isomorphism in \(\Set\) is a bijection all sets of the same size are isomorphic, so all singletons are the same for the purpose of category theory.} \(\{\bullet\}\) the Cartesian product \(A \times \{\bullet\}\) consists of elements \((a, \bullet)\) with \(a \in A\), and \(\{\bullet\} \times A\) consists of elements \((\bullet, a)\).
    From both of these we can easily recover \(A\) by just ignoring the \(\bullet\).
    This gives us our unitors.
    We've already seen in \cref{sec:monoidal categories: the need for specificity} how \(((a, b), c)) \mapsto (a, (b, c))\) defines the associator for the Cartesian product.
    Thus we can make the following definition.
    
    \begin{dfn}{{\normalsize\(\Set\)} as a Monoidal Category}{}
        The category \(\Set\)\index{Set@\(\Set\)!as a monoidal category} can be promoted to a monoidal category by defining
        \begin{enumerate}
            \item the monoidal product to be the Cartesian product;
            \item the unit to be some singleton, \(I = \{\bullet\}\);
            \item the associator to have components \(\alpha_{A, B, C} \colon (A \times B) \times C \to A \times (B \times C)\) defined by \(((a, b), c) \mapsto (a, (b, c))\);
            \item the left unitor at \(A\) to be the function \(\lambda_A \colon \{\bullet\} \times A \to A\) defined by \((\bullet, a) \mapsto a\);
            \item the right unitor at \(A\) to be the function \(\rho_A \colon A \times \{\bullet\} \to A\) defined by \((a, \bullet) \mapsto a\).
        \end{enumerate}
    \end{dfn}
    
    \begin{wrn}
        This is not the only way we can make \(\Set\) into a monoidal category, but it is the only one that we'll use and it is usually what people are talking about when they say \(\Set\) is a monoidal category.
        Another ways of making \(\Set\) into a monoidal category involve taking \(A \otimes B = A \sqcup B\) to be the disjoint union with \(I = \emptyset\).
        Yet another choice is \(A \otimes B = A \sqcup B \sqcup (A \times B)\) with \(I = \emptyset\).
    \end{wrn}
    
    This common definition of \(\Set\) as a monoidal category is an example of a more general idea.
    Recall that in a category \(\cat{C}\) a terminal object, \(T \in \Ob(\cat{C})\), is an object such that for any object \(A \in \Ob(\cat{C})\) there exists exactly one morphism \(A \to T\).
    Similarly an initial object, \(I \in \Ob(\cat{C})\), is an object such that for any object \(A \in \Ob(\cat{C})\) there exists exactly one morphism \(I \to A\).
    Any category, \(\cat{C}\), with terminal objects and products can be made into a monoidal category by taking the monoidal product to be the categorical product and the unit to be the terminal object.
    Similarly any category with initial objects and coproducts\footnote{A \defineindex{coproduct} is defined similarly to a categorical product, but with the arrows reversed.} can be made into a monoidal category by taking the tensor product to be the coproduct and the unit to be the initial object.
    An example of a coproduct/initial object monoidal category is \(\Set\) with disjoint union as a monoidal product.
    
    \begin{lma}{}{}
        \(\Set\) equipped with the Cartesian product is a monoidal category.
        
        \begin{proof}
            We first demonstrate that the triangle equation holds.
            To do so we modify the diagram defining the triangle equation to show the elements being mapped, instead of the objects, and commutativity should be clear from this
            \begin{equation}
                \begin{tikzcd}[column sep=-0.5cm]
                    ((a, \bullet), b) \arrow[rr, "\alpha_{A,I,B}", mapsto] \arrow[dr, "\rho_A \times \id_B"', end anchor={[xshift=-1cm]}, pos=0.4, mapsto] && (a, (\bullet, b) \arrow[dl, "\id_A \times \lambda_B", end anchor={[xshift=1cm]}, pos=0.4, mapsto]\\
                    & (\rho_A(a, \bullet), \id_B(b)) = (a, b) = (\id_A(a), \lambda_B(\bullet, b)). &
                \end{tikzcd}
            \end{equation}
            Similarly, we can demonstrate the commutativity of the pentagon:
            \begin{equation}
                \begin{tikzcd}[row sep=large]
                    \parbox{4cm}{\centering\((\alpha_{A,B,C}((a, b), c), \id_D(D))\) \(=((a, (b, c)), d)\)} \arrow[r, mapsto, "\alpha_{A, B\times C, D}"] & \parbox{3.5cm}{\centering\(\alpha_{A, B \times C, D}((a, (b, c)), d)\) \(=(a, ((b, c), d))\)} \arrow[dd, "\id_A \times \alpha_{B,C,D}", mapsto] \\
                    (((a, b), c), d) \arrow[u, "\alpha_{A,B,C} \times \id_D", mapsto] \arrow[d, "\alpha_{A\times B,C,D}"', mapsto]\\
                    \parbox{4cm}{\centering\(\alpha_{A\times B,C,D}(((a, b), c), d)\) \(=((a, b), (c, d))\)} \arrow[r, "\alpha_{A,B,C\times D}"', end anchor={[yshift=-2ex, xshift=1.2em]}, start anchor={[xshift=-3em, yshift=-1ex]}, mapsto, pos=0.8] & \parbox{4cm}{\centering\((\id_A(a), \alpha_{B,C,D}((b, c), d))\) \(= (a, (b, (c, d))) =\) \(\alpha_{A,B,C\times D}((a, b), (c, d))\).}
                \end{tikzcd}
            \end{equation}
            Finally, we need to show that the unitors and associator satisfy the naturality conditions.
            First consider the left unitor \(\lambda\).
            We can see from the following diagram that the relevant naturality square commutes for all \(f \colon A \to B\):
            \begin{equation}
                \begin{tikzcd}
                    (\bullet, a) \arrow[r, mapsto, "\lambda_A"] \arrow[d, "\id_I \times f"'] & \lambda_A(\bullet, a) = a \arrow[d, mapsto, "f", shift left=0.8cm]\\
                    (\id_I(\bullet), f(a)) = (\bullet, f(a)) \arrow[r, mapsto, "\lambda_B"'] & \lambda_B(\bullet, f(a)) = f(a).
                \end{tikzcd}
            \end{equation}
            Naturality of \(\rho\) is demonstrated in the same way.
            Naturality of \(\alpha\) follows from
            \begin{equation}
                \begin{tikzcd}
                    ((a, b), c) \arrow[r, mapsto, "\alpha_{A,B,C}"] \arrow[d, mapsto, "(f\times g)\times h"'] & (a, (b, c)) \arrow[d, mapsto, "f\times(g\times h)"]\\
                    ((f(a), g(b)), h(c)) \arrow[r, mapsto, "\alpha_{A',B',C'}"'] & (f(a), (g(b), h(c)))
                \end{tikzcd}
            \end{equation}
            commuting for all \(f \colon A \to A'\), \(g \colon B \to B'\), and \(h \colon C \to C'\).
        \end{proof}
    \end{lma}
    
    \subsection{\texorpdfstring{\(\Rel\)}{Rel}}
    Having had success with the Cartesian product as the monoidal product we'll try the same in \(\Rel\).
    If \(R \subseteq A \times B\) and \(S \subset C \times D\) are relations then \(R \times S \subseteq (A \times B) \times (C \times D)\) is a relation on \(A \times B\) and \(C \times D\) such that \((a, c)(R \times S)(b, d)\) if and only if \(a R b\) and \(c S d\).
    As before the singleton, \(\{\bullet\}\), acts as the unit, since as with \(\Set\) we can easily recover \(A\) from either \(\{\bullet\} \times A\) or \(A \times \{\bullet\}\).
    The only difference is that in \(\Rel\) our morphisms are relations, so instead of a function \(\{\bullet\} \times A \to A\) giving us \(A\) back we instead define a relation on \((\{\bullet\} \times A) \times A\) where \((\bullet, a) \sim a\), we use \(\sim\) here since this is an isomorphism in \(\Rel\), it is not an equivalence relation, since the two sets on either side of the relation are different.
    Similarly, the associators are given by changing the function in \(\Set\) into a relation.
    
    \begin{dfn}{{\normalsize\(\Rel\)} as a Monoidal Category}{}
        The category \(\Rel\)\index{Rel@\(\Rel\)!as a monoidal category} can be promoted to a monoidal category by defining
        \begin{itemize}
            \item the monoidal product to be the Cartesian product;
            \item the unit to be some singleton, \(I = \{\bullet\}\);
            \item the associator to have components given by the relation \(\sim \subseteq [(A \times B) \times C] \times [A \times (B \times C)]\) defined by \(((a, b), c) \sim (a, (b, c))\);
            \item the left unitor to have components given by the relation \(\sim \subseteq [I \times A] \times A\) defined by \((\bullet, a) \sim a\);
            \item the right unitor to have components given by the relation \(\sim \subseteq [A \times I] \times A\) defined by \((a, \bullet) \sim a\).
        \end{itemize}
    \end{dfn}
    
    The proof that this makes \(\Rel\) a monoidal category is almost identical to the proof for \(\Set\), just replacing functions and equality with relations, so we won't repeat it.
    
    This is one of the first examples where we see that \(\Rel\) differs from \(\Set\), despite both having the Cartesian product as the monoidal product.
    The Cartesian product is \emph{not} a categorical product (in the sense of \cref{def:product}) in \(\Rel\), and \(\emptyset\) is the terminal object of \(\Rel\), not \(\{\bullet\}\).
    We'll see shortly that \(\Hilb\) as a monoidal category is also formed using a non-categorical product, making \(\Rel\) more like \(\Hilb\) in this sense.
    
    \subsection{\texorpdfstring{\(\Hilb\)}{Hilb}}
    It should not be surprising that \(\Vect\) and \(\Hilb\), and their finite-dimensional subcategories, can be made into monoidal categories, after all these were the models for the definition of monoidal categories we used at the start of the chapter.
    We'll define \(\Hilb\) as a monoidal category of complex Hilbert spaces, but the definition is exactly the same for \(\Vect\)\index{Vect@\(\Vect\)!as a monoidal category} as a monoidal category, just replace \(\complex\) with \(\field\) and ignore inner products.
    
    \begin{dfn}{{\normalsize\(\Hilb\)} as a Monoidal Category}{}
        The category \(\Hilb\)\index{Hilb@\(\Hilb\)!as a monoidal category} can promoted to a monoidal category by defining
        \begin{itemize}
            \item the monoidal product to be the tensor product;
            \item the unit to be the one-dimensional Hilbert space\footnote{\(\complex\) is a vector space over itself, and a Hilbert space over itself with the inner product \(\braket{z}{w} = z^* w\).} \(\complex\);
            \item the associator to have components \(\alpha_{H,J,K} \colon (H \otimes J) \otimes K \to H \otimes (J \otimes K)\) defined on vectors of the form \((u \otimes v) \otimes w\) by \((u \otimes v) \otimes w \mapsto u \otimes (v \otimes w)\) and extended to all other vectors linearly;
            \item the left unitor to have components \(\lambda_A \colon I \otimes A \to A\) defined on vectors of the form \(1 \otimes v\) by \(1 \otimes v \mapsto v\) and extended to all other vectors linearly;
            \item the right unitor to have components \(\rho_A \colon A \otimes I \to A\) defined on vectors of the form \(v \otimes 1\) by \(v \otimes 1 \mapsto v\) and extended to all other vectors linearly.
        \end{itemize}
    \end{dfn}
    
    As with \(\Set\) there are other ways to make \(\Hilb\) into a monoidal category, but this is the one that is most useful and is what we will mean when we say \(\Hilb\) is a monoidal category.
    
    It's worth taking a minute to discus how the associator and unitors are defined.
    Recall that vectors in \(H \otimes J\) are of the form \(\sum_i u_i \otimes v_i\) with \(u_i \in H\) and \(v_i \in J\).
    We can define a linear map \(T \colon H \otimes J \to K\) by defining \(T(u \otimes v)\) for any \(u \in H\) and \(v \in J\), requiring that this map is linear then completely defines \(T\) since we must then have
    \begin{equation}
        T\left( \sum_i u_i \otimes v_i \right) = \sum_i T(u_i \otimes v_i).
    \end{equation}
    In the specific case of the associator we have that
    \begin{equation}
        (u_1 \otimes v_1) \otimes w_1 + (u_2 \otimes v_2) \otimes w_2 + \dotsb \xmapsto{\alpha_{H,J,K}} u_1 \otimes (v_1 \otimes w_1) + u_2 \otimes (v_2 \otimes w_2) + \dotsb,
    \end{equation}
    We further have that if \(T(u \otimes v)\) is defined for some specific \(u \in H\) and any \(v \in J\) then we can extend this linearly to be defined for any vector parallel to \(u\), which we might write as \(\lambda u\), by \(T(\lambda u \otimes v) = \lambda T(u \otimes v)\).
    Using this the left unitor is given by
    \begin{equation}
        \lambda_A(z \otimes u) = \lambda_A(z1 \otimes u) = z\lambda_A(1 \otimes u) = zu
    \end{equation}
    where we first view \(z \in \complex\) as a vector, and then as a scalar scaling the unit vector \(1 \in \complex\).
    The right unitor is defined analogously.
    
    \subsection{More Examples}
    \begin{exm}{Monoidal Categories}{}
        \begin{itemize}
            \item The category of (small) categories, \(\Cat\), is a monoidal category when equipped with the product of categories as the monoidal product and \(\one\), the category with a single object and only its identity morphism, as the unit.
            This is another example of a categorical product/terminal object monoidal category.
            \item The tensor product generalises to \(\RMod\) and makes \(\RMod\) a monoidal category with the tensor product over \(R\), \(\otimes_R\), serving as the monoidal product and \(R\), viewed as a module over itself, serving as the unit.
            This also extends to algebras over \(R\).
            \item The category of Abelian groups with group homomorphisms, \(\Ab\), is a monoidal category equipped with the monoidal product \(\otimes_{\integers}\), which is the product of Abelian groups viewed as \(\integers\)-modules, where \(na\) for \(n \in \integers\) and \(a \in G\) is \(a + \dotsb + a\) if \(n > 0\), \(e\) if \(n = 0\), and \(-a - \dotsb - a\) if \(n < 0\), with \(G\) viewed as an additive group and where each sum here has \(n\) terms.
            \item Any monoid can be viewed as a monoidal category by taking the set of objects to be the set of elements, the only morphisms to be identities, and the monoid product as the tensor product of objects.
            \item The category, \(\functorCategory{\cat{C}}{\cat{C}}\) of endofunctors on some category, \(\cat{C}\), is a monoidal category with composition of functors as the monoidal product and the identity functor as the unit.
            This example is important in defining monads.
            \item The category of pointed topological spaces, \(\pointedTop\), is a monoidal category with the smash product as the monoidal product and the pointed 0-sphere (that is the two point space \((\{-1, 1\}, \{\emptyset, \{-1\}, \{1\}, \{-1, 1\}\})\) with one point chosen as the base point) serving as the unit.
            The smash product, \(\wedge\), is defined between two pointed spaces, \((X, x_\bullet)\) and \((Y, y_\bullet)\), by forming \(X \times Y\) then identifying \((x, y_\bullet) \sim (x_\bullet, y)\) for all \(x \in X\) and \(y \in Y\), and then equipping the space \((X \times Y)/\sim\) with the quotient topology.
            \item Given any category, \(\cat{C}\), we can define a free monoidal category in an analogous way to defining a free monoid, simply take the objects to be finite sequences, \(A_1 \otimes \dotsb \otimes A_n\), of objects in \(\cat{C}\), then we have a morphism \(A_1\otimes \dotsb \otimes A_n \to B_1 \otimes \dotsb \otimes B_m\) only if \(m = n\), in which case the morphism is a finite sequence of morphisms \(f_1 \otimes \dotsb \otimes f_n\), with \(f_i \colon A_i \to B_i\) a morphism in \(\cat{C}\).
            The monoidal product is then the concatenation of these lists, giving \((A_1 \otimes \dotsb \otimes A_n) \otimes (B_1 \otimes \dotsb \otimes B_m) = A_1 \otimes \dotsb \otimes A_n \otimes B_1 \otimes \dotsb \otimes B_m\), where now \(m\) and \(n\) may differ.
            The unit is the empty sequence of objects.
        \end{itemize}
    \end{exm}
    
    \section{Interchange Law}
    The interchange law allows us to swap composition and the monoidal product, and is automatically satisfied by any monoidal category.
    
    \begin{thm}{Interchange Law}{thm:interchange law}
        Let \(\cat{C}\) be a monoidal category with monoidal product \(\otimes\).
        Consider the following objects morphisms in \(\cat{C}\):
        \begin{equation}
            A \xrightarrow{f} B \xrightarrow{g} C, \qqand D \xrightarrow{h} E \xrightarrow{j} F.
        \end{equation}
        We have
        \begin{equation}
            (g \circ f) \otimes (j \circ h) = (g \otimes j) \circ (f \otimes h).
        \end{equation}
        \begin{proof}
            For the proof we use the notation \(\otimes(-, -) = - \otimes -\) to emphasise the functoriality of \(\otimes\).
            This allows us to write
            \begin{equation}
                (g \circ f) \otimes (j \circ h) = \otimes(g \circ f, j \circ h).
            \end{equation}
            Now consider \(\cat{C} \times \cat{C}\), the domain of \(\otimes\).
            We can identify that \((g \circ f, j \circ h) = (g, j) \circ (f, h)\), where composition on the left is in \(\cat{C}\) and composition on the right is in \(\cat{C} \times \cat{C}\).
            Thus in \(\cat{C} \times \cat{C}\) we have
            \begin{equation}
                (A, D) \xrightarrow{(f, h)} (B, E) \xrightarrow{(g, j)} (C, F)
            \end{equation}
            which we can write as
            \begin{equation}
                X \xrightarrow{p} Y \xrightarrow{q} Z.
            \end{equation}
            Then we have \(\otimes(q \circ p) = (\otimes(q)) \circ (\otimes(p))\) by the functoriality of \(\otimes\).
            That is, we have \((\otimes(g, j)) \circ (\otimes(f, h))\), which we can then write as \((g \otimes j) \circ (f \otimes h)\).
            Putting this together we have
            \begin{alignat}{2}
                (g \circ f) \otimes (j \circ h) &= \otimes(g \circ f, j \circ h)\\
                &= \otimes((g, j) \circ (f, h)) \qquad && \text{composition in } \cat{C} \times \cat{C} \notag\\
                &= (\otimes(g, j)) \circ (\otimes(f, h)) \qquad && \text{functoriality of } \otimes \notag\\
                &= (g \otimes j) \circ (f \otimes h). \notag \qedhere
            \end{alignat}
        \end{proof}
    \end{thm}
    
    \section{Graphical Notation}
    We can extend the graphical notation for categories to monoidal categories.
    To do so we make use of the idea that the monoidal product combines processes in parallel, and so we write the monoidal product by simply writing the two processes next to each other.
    For example, if we have objects \(A\) and \(B\) then, representing objects as identity morphisms, we write
    \begin{equation}
        A \otimes B = 
        \tikzsetnextfilename{mon-cat-graphical-mon-prod-of-objects}
        \begin{tikzpicture}[baseline=(A.base)]
            \draw[wire] (0, 0) -- (0, 1) node [left, midway] (A) {\(A\)};
            \draw[wire] (1, 0) -- (1, 1) node [left, midway] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    For morphisms \(f \colon A \to B\) and \(g \colon C \to D\) we draw \((f \otimes g) \colon A \otimes C \to B \otimes D\) as
    \begin{equation}
        f \otimes g = 
        \tikzsetnextfilename{mon-cat-graphical-mon-prod-of-morphisms}
        \begin{tikzpicture}[baseline=(f.base)]
            \node[morphism] (f) {\(f\)};
            \node[morphism, right= 0.5cm of f] (g) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- ++ (0, 1) node [midway, left] {\(B\)};
            \draw[wire] (g) -- ++ (0, -1) node [midway, right] {\(C\)};
            \draw[wire] (g) -- ++ (0, 1) node [midway, right] {\(D\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    The unit, \(I\), is drawn as the empty diagram, or as a dotted line if we want to remember that it's there:
    \begin{equation}
        I = 
        \tikzsetnextfilename{mon-cat-graphical-unit}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw[wire] (0, 0) -- (0, 1) node [midway, left] {\(I\)};
            \draw[wire, dashed] (1, 0) -- (1, 1) node [midway, left] {\(I\)};
            \node at (0.4, 0.5) (equals) {\(=\)};
            \draw[dashed, thick] (1.8, 0) rectangle (2.8, 1);
            \node at (1.4, 0.5) {\(=\)};
            \node at (4, 0) {};
            \node at (3.2, 0.5) {\(=\)};
        \end{tikzpicture}
        .
    \end{equation}
    This means we don't have to draw left or right unitors, since their action is to simply remove the unit, and we don't draw the unit any way.
    If we are drawing units as dashed lines then the left and right unitors can be draw as
    \begin{equation}
        \tikzsetnextfilename{mon-cat-graphical-left-unitor}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[morphism, minimum width=1cm] (lambda) {\(\lambda\)};
            \draw[wire, dashed] ($(lambda.south west) + (0.3, 0)$) -- ++ (0, -0.5) coordinate (bottom);
            \draw[wire] ($(lambda.south east) - (0.3, 0)$) -- ++ (0, -0.5);
            \draw[wire] (lambda.north) -- ++ (0, 0.5) coordinate (top);
            \draw[wire] (2, 0 |- bottom) coordinate (A) -- (2, 0 |- top) coordinate (B);
            \draw[wire, dashed, rounded corners] (1.5, 0 |- bottom) -- ++ (0, 0.2)  -- ($(A)!0.5!(B)$);
            \node (equals) at (1, 0) {\(=\)};
            \node at (2.5, 0) {\(=\)};
            \draw[wire] (3, 0 |- bottom) -- (3, 0 |- top);
        \end{tikzpicture}
        ,\qand
        \tikzsetnextfilename{mon-cat-graphical-right-unitor}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[morphism, minimum width=1cm] (rho) {\(\rho\)};
            \draw[wire] ($(rho.south west) + (0.3, 0)$) -- ++ (0, -0.5) coordinate (bottom);
            \draw[wire, dashed] ($(rho.south east) - (0.3, 0)$) -- ++ (0, -0.5);
            \draw[wire] (rho.north) -- ++ (0, 0.5) coordinate (top);
            \draw[wire] (1.5, 0 |- bottom) coordinate (A) -- (1.5, 0 |- top) coordinate (B);
            \draw[wire, dashed, rounded corners] (2, 0 |- bottom) -- ++ (0, 0.2)  -- ($(A)!0.5!(B)$);
            \node (equals) at (1, 0) {\(=\)};
            \node at (2.5, 0) {\(=\)};
            \draw[wire] (3, 0 |- bottom) -- (3, 0 |- top);
        \end{tikzpicture}
        .
    \end{equation}
    respectively.
    Here the wires joining is a morphism, \(\lambda\) or \(\rho\), we're just not labelling it.
    
    Similarly, we don't have to draw the associator since we're not drawing any brackets anyway.
    This only works because of the coherence theorem, which means that we can only build a single morphism of a given type, and so the unitors and associators don't give us any information that the domain and codomain don't.
    
    In the graphical notation the interchange law,
    \begin{equation}
        (g \circ f) \otimes (j \circ h) = (g \otimes j) \circ (f \otimes h),
    \end{equation}
    becomes
    \begin{equation}
        \tikzsetnextfilename{mon-cat-graphical-interchange-law}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[morphism] (f) {\(f\)};
            \node[morphism, above=of f] (g) {\(\phantomrlap{g}{f}\)};
            \node[morphism, right=1.25cm of f] (h) {\(\phantomrlap{h}{f}\)};
            \node[morphism, above=of h] (j) {\(\phantomrlap{j}{f}\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
            \draw[wire] (f) -- (g) node [midway, left] {\(B\)};
            \draw[wire] (g) -- ++ (0, 1) node [midway, left] {\(C\)};
            \draw[wire] (h) -- ++ (0, -1) node [midway, right] {\(D\)};
            \draw[wire] (h) -- (j) node [midway, right] {\(E\)};
            \draw[wire] (j) -- ++ (0, 1) node [midway, right] {\(F\)};
            \draw[thick, decoration={calligraphic brace}, decorate] (-0.5, -1) -- (-0.5, 2.5);
            \draw[thick, decoration={calligraphic brace, mirror}, decorate] (0.5, -1) -- (0.5, 2.5);
            \draw[thick, decoration={calligraphic brace}, decorate] (1.25, -1) -- (1.25, 2.5);
            \draw[thick, decoration={calligraphic brace, mirror}, decorate] (2.25, -1) -- (2.25, 2.5);
            \node (equals) at (2.75, 0.75) {\(=\)};
            \begin{scope}[xshift=3.5cm]
                \node[morphism] (f) {\(f\)};
                \node[morphism, above=of f] (g) {\(\phantomrlap{g}{f}\)};
                \node[morphism, right=of f] (h) {\(\phantomrlap{h}{f}\)};
                \node[morphism, above=of h] (j) {\(\phantomrlap{j}{f}\)};
                \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
                \draw[wire] (f) -- (g) node [midway, left] {\(B\)};
                \draw[wire] (g) -- ++ (0, 1) node [midway, left] {\(C\)};
                \draw[wire] (h) -- ++ (0, -1) node [midway, right] {\(D\)};
                \draw[wire] (h) -- (j) node [midway, right] {\(E\)};
                \draw[wire] (j) -- ++ (0, 1) node [midway, right] {\(F\)};
                \draw[thick, decoration={calligraphic brace, mirror}, decorate] (-0.5, -0.3) -- (2.25, -0.3);
                \draw[thick, decoration={calligraphic brace}, decorate] (-0.5, 0.3) -- (2.25, 0.3);
                \draw[thick, decoration={calligraphic brace, mirror}, decorate] (-0.5, 1.2) -- (2.25, 1.2);
                \draw[thick, decoration={calligraphic brace}, decorate] (-0.5, 1.8) -- (2.25, 1.8);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    In order to proceed with calculations we need to define what it means for two diagrams to be \enquote{the same}.
    Exact equality is too strict, since it's impossible to actually draw two identical diagrams.
    Besides, we want to be able to move bits of the diagrams around, such as sliding morphisms along the wires, or drawing the wires in different ways.
    This leads to the following definition.
    
    \begin{dfn}{Planar Isotopy}{}
        Two diagrams are \defineindex{planar isotopic} if one can be deformed continuously into the other such that
        \begin{itemize}
            \item the diagrams remain confined to a rectangular region of the plane;
            \item input and output wires terminate at the bottom and top of this region, and the order they reach the edge cannot change;
            \item components never intersect.
        \end{itemize}
    \end{dfn}
    
    This can be made more rigorous through the idea of an isotopy, which is a homotopy between the two diagrams such that at every point the diagrams remain embedded in the rectangular region of the plane in a non-intersecting way and the inputs and outputs are fixed.
    
    For example, consider the following diagrams involving the morphisms \(f \colon I \to A \otimes B\), \(g \colon B \otimes C \to I\), and \(h \colon I \to I\).
    This example is a planar isotopy
    \begin{equation}
        \tikzsetnextfilename{mon-cat-graphical-planar-isotopy}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[morphism] at (-1, 1) {\(h\)};
            \node[morphism, minimum width=1.5cm] (f) at (0, 0) {\(f\)};
            \node[morphism, minimum width=1.5cm] (g) at (1, 1) {\(\phantomrlap{g}{f}\)};
            \draw[wire] ($(f.north) + (0.5, 0)$) -- ($(g.south) + (-0.5, 0)$);
            \draw[wire] ($(g.south) + (0.5, 0)$) -- ++ (0, -1.5);
            \draw[wire] ($(f.north) + (-0.5, 0)$) -- ++ (0, 1.5);
            \draw[lightgray] (-1.5, -0.75) rectangle (2, 1.75);
            \node (equals) at (2.5, 0.5) {\(\equaliso\)};
            \begin{scope}[xshift=4.1cm]
                \node[morphism] at (1, 0.5) {\(h\)};
                \node[morphism, minimum width=1.5cm] (f) at (0, 0) {\(f\)};
                \node[morphism, minimum width=1.5cm] (g) at (1, 1) {\(\phantomrlap{g}{f}\)};
                \draw[wire, rounded corners] ($(f.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (-0.3, 0.1) -- ++ (0.3, 0.1) -- ($(g.south) + (-0.5, 0)$);
                \draw[wire, rounded corners] ($(g.south) + (0.5, 0)$) -- ++ (0, -0.3) -- ++ (-0.4, -0.4) -- ++ (0, -0.8);
                \draw[wire, rounded corners] ($(f.north) + (-0.5, 0)$) -- ++ (0, 0.3) -- ++ (-0.4, 0.4) -- ++ (0, 0.4) -- ++ (0.2, 0.2) -- ++ (0, 0.2);
                \draw[lightgray] (-1.1, -0.75) rectangle (2, 1.75);
            \end{scope}
        \end{tikzpicture}
    \end{equation}
    since all that happened is the wires were bent and the \(h\) box moved under the \(f\) box and between the two wires, all of which can happen without ever crossing any wires or morphism boxes (which we treat as point like for the purposes of isotopies).
    Note that the light grey box represents the bounding region to which the diagram is confined.
    
    This example is \emph{not} a planar isotopy, since it is not possible to do this deformation without the \(h\) box either leaving the bounded region or crossing over one of the wires:
    \begin{equation}
        \tikzsetnextfilename{mon-cat-graphical-not-a-planar-isotopy}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[morphism] at (-1, 1) {\(h\)};
            \node[morphism, minimum width=1.5cm] (f) at (0, 0) {\(f\)};
            \node[morphism, minimum width=1.5cm] (g) at (1, 1) {\(\phantomrlap{g}{f}\)};
            \draw[wire] ($(f.north) + (0.5, 0)$) -- ($(g.south) + (-0.5, 0)$);
            \draw[wire] ($(g.south) + (0.5, 0)$) -- ++ (0, -1.5);
            \draw[wire] ($(f.north) + (-0.5, 0)$) -- ++ (0, 1.5);
            \draw[lightgray] (-1.5, -0.75) rectangle (2, 1.75);
            \node (equals) at (2.5, 0.5) {\(\notequaliso\)};
            \begin{scope}[xshift=4cm]
                \node[morphism] at (2.3, 0.25) {\(h\)};
                \node[morphism, minimum width=1.5cm] (f) at (0, 0) {\(f\)};
                \node[morphism, minimum width=1.5cm] (g) at (1, 1) {\(\phantomrlap{g}{f}\)};
                \draw[wire] ($(f.north) + (0.5, 0)$) -- ($(g.south) + (-0.5, 0)$);
                \draw[wire] ($(g.south) + (0.5, 0)$) -- ++ (0, -1.5);
                \draw[wire] ($(f.north) + (-0.5, 0)$) -- ++ (0, 1.5);
                \draw[lightgray] (-1, -0.75) rectangle (2.7, 1.75);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    \subsection{Correctness Theorem}
    In order for the graphical notation to be useful we need the legal moves in the graphical notation, planar isotopies, to correspond to legal moves in monoidal categories, the axioms.
    Fortunately this is the case, leading to the following theorem, which we won't prove.
    
    \begin{thm}{Correctness of the Graphical Notation for Monoidal Categories}{}
        Any well-formed equation of the form \(f = g\) for morphisms \(f\) and \(g\) in a monoidal category follows from the axioms of monoidal categories if and only if it holds in the graphical notation up to planar isotopy.
    \end{thm}
    
    Note that by \enquote{well-formed} we mean that \(f\) and \(g\) have the same (co)domains.
    
    There are two parts to this theorem, known as soundness and completeness.
    For morphisms \(f\) and \(g\) in some monoidal category define
    \begin{itemize}
        \item \(P(f, g)\) to be the proposition \enquote{under the axioms of a monoidal category \(f = g\)};
        \item \(Q(f, g)\) to be the proposition \enquote{\(f\) and \(g\) are planar isotopic when expressed in the graphical notation}.
    \end{itemize}
    Then \defineindex{soundness} is the assertion that \(P(f, g) \implies Q(f, g)\) for all such \(f\) and \(g\).
    This is relatively easy to check, we simply check that every axiom of a monoidal category when translated into the graphical notation becomes a planar isotopy, and this is pretty trivial since most aspects of a monoidal category, units, unitors, and associators, simply aren't drawn in the graphical notation.
    The converse is \defineindex{completeness} which is the assertion that \(Q(f, g) \implies P(f, g)\) for all such \(f\) and \(g\).
    This is harder to prove, to do so we must show that any planar isotopy can be generated by a finite set of moves, each of which obeys the axioms of a monoidal category, and that combining these moves also obeys the axioms of a monoidal category.
    
    \section{States}
    \subsection{States}
    To follow the spirit of category theory we shouldn't talk about elements of sets, or particular vectors in some vector space.
    However, often we will have reason to want to refer to these things.
    Fortunately it is possible to do so using morphisms.
    
    Consider some set, \(A\), from which we want to pick an element, \(a\).
    We can identify this selection of a single element with a function \(f\colon\{\bullet\} \to A\) such that \(f(\bullet) = a\).
    Similarly, consider some vector space, \(V\), over \(\field\) from which we want to pick a vector, \(v\).
    We can identify this selection of a single element with a linear map \(T \colon \field \to V\) such that \(T(1) = v\), and then \(T(k) = T(k1) = kT(v) = kv\) for all \(k \in \field\).
    This way of getting around talking about the substructure of objects leads to the following definition.
    
    \begin{dfn}{State}{}
        Consider a monoidal category with unit \(I\) and object \(A\).
        A \defineindex{state} of \(A\) is a morphism \(I \to A\).
    \end{dfn}
    
    So the function \(f\) and the linear map \(T\) are states of \(A\) and \(V\) respectively.
    Another example is from \(\Rel\), where a state is a relation \(R \subseteq \{\bullet\} \times A\).
    Since this will be of the form \(R = \{(\bullet, a), (\bullet, b), \dotsc\}\) for \(a, b, \dotsc, \in A\) we see that a state picks out a subset, \(\{a, b, \dotsc\} \subseteq A\).
    
    Now consider a Hilbert space, \(H\).
    A state is a linear map \(\complex \to H\).
    We can define a state, \(T_v \colon \complex \to H\) for each \(v \in H\) through \(T_v(1) = v\), and so \(T_v(z) = T_v(z1) = zT_v(1) = zv\), but this is exactly the definition of a ket (\cref{def:bras and kets}), \(T_v = \ket{v}\), so a state of a Hilbert space is exactly a ket.
    This is, after all, why we chose the word state, since we ultimately want to consider states of some quantum system, which are usually considered to be kets in some Hilbert space.
    
    Graphically a state is a morphism \(I \to A\), so since we don't draw \(I\) it looks like the morphism takes no input.
    For this reason we change the shape of the box to be a triangle.
    For example, the state \(a \colon I \to A\) is represented as
    \begin{equation}
        a = 
        \tikzsetnextfilename{mon-cat-graphical-state}
        \begin{tikzpicture}[baseline=(a.base)]
            \node[state] (a) {\(a\)};
            \draw[wire] (a) -- ++ (0, 1) node [midway, left] {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    \subsection{Effects}
    Often in category theory after making a definition we should ask what happens if we reverse the arrows in the definition.
    In the case of states this leads to the following definition.
    \begin{dfn}{Effect}{}
        Consider a monoidal category with unit \(I\) and object \(A\).
        An \defineindex{effect} on \(A\) is a morphism \(A \to I\).
    \end{dfn}
    
    Consider a Hilbert space, \(H\).
    Then an effect is a linear map \(H \to \complex\).
    We can define one such map for each \(v \in H\), \(T_v \colon H \to \complex\) and \(T_v(w) = \braket{v}{w}\), but this is exactly the definition of the bra (\cref{def:bras and kets}), so \(T_v = \bra{v}\).
    Thus we can interpret an effect as an observation of a quantum system.
    
    An effect is drawn similarly to a state, the effect \(a \colon  A \to I\) is drawn as
    \begin{equation}
        a = 
        \tikzsetnextfilename{mon-cat-graphical-effect}
        \begin{tikzpicture}[baseline=(a.base)]
            \node[effect] (a) {\(a\)};
            \draw[wire] (a) -- ++ (0, -1) node [midway, left] {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    \subsection{Joint States}
    Next we have to ask how the notion of a state combines with the monoidal product.
    This leads to the following definition.
    
    \begin{dfn}{Joint State}{}
        Consider a monoidal category with unit \(I\) and objects \(A\) and \(B\).
        A \defineindex{joint state} of \(A\) and \(B\) is a morphism \(I \to A \otimes B\).
    \end{dfn}
    
    This definition generalises to any number of objects.
    
    We might draw a joint effect \(c \colon I \to A \times B\) as
    \begin{equation}
        c = 
        \tikzsetnextfilename{mon-cat-graphical-joint-state}
        \begin{tikzpicture}[baseline=(c.base)]
            \node[state] (c) {\(c\)};
            \draw[wire] ($(c.north) + (-0.25, 0)$) -- ++ (0, 0.75) node [midway, left] {\(A\)};
            \draw[wire] ($(c.north) + (0.25, 0)$) -- ++ (0, 0.75) node [midway, right] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    After playing around with this definition for a while one might recognise two types of joint effect, those which factor and those which don't.
    
    \begin{dfn}{Product and Entangled State}{}
        Consider a monoidal category with unit \(I\) and objects \(A\) and \(B\).
        A joint state, \(c \colon I \to A \otimes B\), is a \defineindex{product state} if it is of the form
        \begin{equation}
            I \xrightarrow{\lambda_I^{-1}} I \otimes I \xrightarrow{a \otimes b} A \otimes B.
        \end{equation}
        That is, we can write \(c = (a \otimes b) \circ \lambda_I^{-1}\) as a product of two states, \(a \colon I \to A\) and \(b \colon I \to B\).
        
        A joint state which cannot be written in this form is called an \defineindex{entangled state}.
    \end{dfn}
    
    \begin{rmk}
        Note that \(\lambda_I = \rho_I\) in any monoidal category, so we could equally well have defined a product state to be of the form
        \begin{equation}
            I \xrightarrow{\rho_I^{-1}} I \otimes I \xrightarrow{a \otimes b} A \otimes B
        \end{equation}
    \end{rmk}
    
    Graphically if \(c\) is a product state then
    \begin{equation} 
        \tikzsetnextfilename{mon-cat-graphical-product-state}
        \begin{tikzpicture}[baseline=(c.base)]
            \node[state] (c) {\(\phantomrlap{c}{b}\)};
            \draw[wire] ($(c.north) + (-0.25, 0)$) -- ++ (0, 0.75) node [midway, left] {\(A\)} coordinate (top);
            \draw[wire] ($(c.north) + (0.25, 0)$) -- ++ (0, 0.75) node [midway, right] {\(B\)};
            \node[right=0.5cm of c] {\(=\)};
            \node[state, right=1.5cm of c] (a) {\(\phantomrlap{a}{b}\)};
            \node[state, right=0.5cm of a] (b) {\(b\)};
            \draw[wire] (a) -- (a |- top) node [midway, left] {\(A\)};
            \draw[wire] (b) -- (b |- top) node [midway, right] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    \begin{exm}{}{}
        In \(\Set\) states are elements, so
        \begin{itemize}
            \item joint states of \(A\) and \(B\) are elements of \(A \times B\);
            \item product states are elements \((a, b) \in A \times B\);
            \item entangled states don't exist.
        \end{itemize}
        In \(\Rel\) states are subsets, so
        \begin{itemize}
            \item joint states of \(A\) and \(B\) are subsets of \(A \times B\);
            \item product states are \enquote{square} subsets of the form \(U \times V \subseteq A \times B\) where \(U \subseteq A\) and \(V \subseteq B\), for more details see \cref{fig:square subset};
            \item entangled states are any subsets of \(A \times B\) not of this form.
        \end{itemize}
        In \(\Hilb\) states are vectors, so
        \begin{itemize}
            \item joint states of \(H\) and \(K\) are vectors in \(H \otimes K\);
            \item product states are factorisable states, i.e.\@ states which can be written as \(u \otimes v\) for \(u \in H\) and \(v \in K\);
            \item entangled states are states which aren't factorisable, which are exactly the entangled states of quantum mechanics, such as\footnote{here we identify a vector with its ket, and we write implicit tensor products, meaning \(\ket{ab} = \ket{a} \otimes \ket{b} \leftrightarrow a \otimes b\), the states \(\ket{0}\) and \(\ket{1}\) can be taken as two orthonormal basis vectors in \(\complex^2\).} \(\ket{01} + \ket{10}\).
        \end{itemize}
    \end{exm}
    
    \begin{figure}
        \tikzsetnextfilename{mon-cat-square-subset-is-product-state-in-rel}
        \begin{tikzpicture}
            \fill[highlight!10] (0, 0) rectangle (4, 4);
            \draw[->] (0, 0) -- (4, 0) node [below, highlight] {\(A\)};
            \draw[->] (0, 0) -- (0, 4) node [left, highlight] {\(B\)};
            \draw[ultra thick, highlight] (1, 0) -- (3, 0) node [below, midway] {\(U\)};
            \draw[ultra thick, highlight] (0, 2) -- (0, 3) node [left, midway] {\(V\)};
            \draw[thick, highlight, fill=highlight!40] (1, 2) rectangle (3, 3) node [midway] {\(U \times V\)};
            \node[below left, highlight] at (4, 4) {\(A \times B\)};
        \end{tikzpicture}
        \caption{An example of a \enquote{square} subset of \(\reals \times \reals\), identified with the plane, given \(U, V \subseteq \reals\) we get a rectangle \(U \times V\) in the plane.}
        \label{fig:square subset}
    \end{figure}
    
    This demonstrates another way in which \(\Rel\) is more like \(\Hilb\) than \(\Set\), both \(\Rel\) and \(\Hilb\) have entangled states.
    
    \chapter{Scalars}
    \section{Motivation}
    Each vector space comes equipped, by definition, with a field of scalars.
    Most of the time this field is either \(\reals\) or \(\complex\), and we'll consider the complex case here.
    The scalars and the vector space come with a map, called scalar multiplication, which takes a scalar and a vector and produces a vector, \(\cdot \colon \field \times V \to V\), \((a, v) \mapsto a v\).
    We can similarly define a map between (bounded) linear maps, given by\footnote{We use \(\hom\) here to avoid specifying exactly which category of vector spaces we are working in, it could be \(\Vect\), \(\FVect\), \(\Hilb\), or \(\FHilb\).} \(\cdot \colon \field \times \hom(V, W) \to \hom(V, W)\), \((a, f) \mapsto \lambda \cdot f\) where if \(f \colon V \to W\) is a (bounded) linear map then we define \(a \cdot f\) to be the map \((a \cdot f ) \colon V \to W\) defined by \((a \cdot f)(v) = a f(v)\) where on the right hand side multiplication is just scalar multiplication.
    
    This structure is very important, and indeed is part of the definition of a vector space.
    However, as stated here this requires us to talk of individual vectors, which is not in the spirit of category theory.
    In the next section we'll see how the field of scalars can be talked about in a categorical way, and in the section after that we'll generalise our observation to any monoidal category.
    
    \section{Scalars in \texorpdfstring{\(\Hilb\)}{Hilb}}
    Lets work in \(\Hilb\) and try to discus scalars, which we'll take as elements of \(\complex\), in a categorical way.
    We don't want to talk of individual elements of the field, or of individual vectors.
    Instead notice that given some scalar, \(a \in \complex\), can be identified with a linear map \(\complex \to \complex\) defined by \(1 \mapsto a\), which full defines the map through linearity.
    Let's call this map \(f_a\).
    
    Now consider composing these two maps of this type, if \(a, b \in \complex\), we have \((f_b \circ f_a)(1) = f_b(f_a(1)) = f_b(a1) = af_b(1) = ab\), so composition is multiplication in \(\complex\).
    
    What about multiplication of bounded linear maps?
    Consider the bounded linear map \(T \colon H \to K\).
    Given some \(a \in \complex\) we can consider
    \begin{equation}
        \lambda_K \circ (f_a \otimes T) \circ \lambda_H^{-1} \colon H \to K, \qquad H \xrightarrow{\lambda_H^{-1}} \complex \otimes H \xrightarrow{f_a \otimes T} \complex \otimes K \xrightarrow{\lambda_K} K,
    \end{equation}
    this takes in some object in \(H\), maps it to an object in \(\complex \otimes H\), applies \(f_a \otimes T\), mapping it to an object in \(\complex \otimes K\), then maps this to an object in \(K\).
    Let's consider an example, we have
    \begin{multline}
        v \xmapsto{\lambda_{H}^{-1}} 1 \otimes v \xmapsto{f_a \otimes T} f_a(1) \otimes T(v) = a\otimes T(v)\\
        = a(1 \otimes T(v)) \xmapsto{\lambda_K} \lambda_K(a(1 \otimes T(v))) = a\lambda_K(1 \otimes T(v)) = aT(v).
    \end{multline}
    This shows that \(\lambda_K \circ (f_a \otimes T) \circ \lambda_H^{-1}\) is \(a \cdot T\) where \(a \cdot T\) is the product of a scalar and a linear map in the normal sense.
    Putting this all together in a diagram we can define \(a \cdot T\) to be the unique map such that the following diagram commutes
    \begin{equation}
        \begin{tikzcd}
            H \arrow[r, "a \cdot T"] \arrow[d, "\lambda_A^{-1}"'] & K\\
            \complex \otimes H \arrow[r, "f_a \otimes T"'] & \complex \otimes K. \arrow[u, "\lambda_B"']
        \end{tikzcd}
    \end{equation}
    
    We now drop the notation \(f_a\) for the map \(f_a \colon \complex \to \complex\) defined by \(f_a(1) = a\) and instead just write \(a \colon \complex \to \complex\) so \(a(1) = a\), which seems like a very natural notation.
    Then the product of a scalar and a map, \(a \cdot T\), is the unique map such that the following diagram commutes
    \begin{equation}
        \begin{tikzcd}
            H \arrow[r, "a \cdot T"] \arrow[d, "\lambda_A^{-1}"'] & K\\
            \complex \otimes H \arrow[r, "a \otimes T"'] & \complex \otimes K. \arrow[u, "\lambda_B"']
        \end{tikzcd}
    \end{equation}
    From this we can see that the tensor product with an endomorphism on \(\complex\) is pretty much the same as multiplying a scalar and a map.
    The only difference is that for domains to match up we need to insert an extra object, and the easiest way to do this is with the unitors.
    
    In the next section we'll take our work here in \(\Hilb\) and generalise it to an arbitrary monoidal category.
    
    \section{Scalars}
    \begin{dfn}{Scalar}{}
        Let \(\cat{C}\) be a monoidal category with monoidal product \(\otimes\) and unit \(I\).
        Then a \defineindex{scalar} is an endomorphism of the unit.
        That is, a scalar is a morphism in \(\cat{C}(I, I)\), that is a morphism \(I \to I\).
    \end{dfn}
    
    We know that \(wz = zw\) for \(w, z \in \complex\), and it would be good to check if this generalises to our definition of scalars, and it does.
    
    \begin{lma}{Scalars Commute}{}
        Let \(\cat{C}\) be a monoidal category with unit \(I\).
        Then given any two scalars \(a, b \colon I \to I\) we have \(a \circ b = b \circ a\), that is scalars commute.
        \begin{proof}
            We want to show that the following diagram commutes
            \begin{equation}
                \begin{tikzcd}
                    I \arrow[r, "a"] \arrow[d, "b"'] & I \arrow[d, "b"]\\
                    I \arrow[r, "a"'] & I.
                \end{tikzcd}
            \end{equation}
            The naturality square for the left unitor, \(\lambda\), is
            \begin{equation}
                \begin{tikzcd}
                    I \otimes A \arrow[r, "\lambda_A"] \arrow[d, "{\id_I} \otimes f"'] & A \arrow[d, "f"]\\
                    I \otimes B \arrow[r, "\lambda_B"'] & B.
                \end{tikzcd}
            \end{equation}
            This commutes for all \(f \in \cat{C}(A, B)\) by the definition of naturality.
            Consider this square specialised to the case where \(A = B = I\):
            \begin{equation}
                \begin{tikzcd}
                    I \otimes I \arrow[r, "\lambda_I"] \arrow[d, "{\id_I} \otimes f"'] & I \arrow[d, "f"]\\
                    I \otimes I \arrow[r, "\lambda_I"'] & I.
                \end{tikzcd}
            \end{equation}
            Now we have \(f \in \cat{C}(I, I)\), so \(f\) is some scalar.
            The definition of a monoidal category is that \(\lambda\) is a natural isomorphism, so \(\lambda_I^{-1}\) exists.
            We can the write this diagram as
            \begin{equation}
                \begin{tikzcd}
                    I \otimes I \arrow[d, "{\id_I} \otimes f"'] & I \arrow[l, "\lambda_I^{-1}"'] \arrow[d, "f"]\\
                    I \otimes I \arrow[r, "\lambda_I"'] & I.
                \end{tikzcd}
            \end{equation}
            For later use it is useful to have the rotated version of this diagram:
            \begin{equation}
                \begin{tikzcd}
                    I \arrow[r, "f"] \arrow[d, "\lambda_I^{-1}"'] & I\\
                    I \otimes I \arrow[r, "{\id_I} \otimes f"'] & I \otimes I. \arrow[u, "\lambda_I"']
                \end{tikzcd}
            \end{equation}
            Also note that by the coherence theorem we have \(\lambda_I = \rho_I\).
            The naturality square for \(\rho_I\), rotated, gives
            \begin{equation}
                \begin{tikzcd}
                    I \arrow[r, "f"] & I \\
                    I\otimes I \arrow[r, "f \otimes \id_I"'] \arrow[u, "\rho_I"] & I\otimes I, \arrow[u, "\rho_I"']
                \end{tikzcd}
            \end{equation}
            and the same logic as we applied to the left unitor case gives
            \begin{equation}
                \begin{tikzcd}
                    I \arrow[r, "f"] \arrow[d, "\rho_I^{-1}"'] & I \arrow[d, "\rho_I^{-1}"]\\
                    I \otimes I \arrow[r, "f \otimes \id_I"'] & I \otimes I.
                \end{tikzcd}
            \end{equation}
            
            Consider the following objects and morphisms in \(\cat{C}\):
            \begin{equation}
                A \xrightarrow{f} B \xrightarrow{g} C, \qqand D \xrightarrow{h} E \xrightarrow{j} F.
            \end{equation}
            Now consider the interchange law (\cref{thm:interchange law}),
            \begin{equation}
                (g \circ f) \otimes (j \circ h) = (g \otimes j) \circ (f \otimes h).
            \end{equation}
            We have here
            \begin{gather}
                A \xrightarrow{g \circ f} C, \quad D \xrightarrow{j \circ h} F,\\
                A \otimes D \xrightarrow{(j \circ h) \otimes (g \circ f)} C \otimes F,\\
                A \otimes D \xrightarrow{f \otimes h} B \otimes E \xrightarrow{g \otimes j} C \otimes F.\\
            \end{gather}
            This implies commutativity of the following diagram
            \begin{equation}
                \begin{tikzcd}
                    A \otimes D \arrow[r, "f \otimes h"] \arrow[d, "{\id_A} \otimes (j \circ h)"'] & B \otimes E \arrow[d, "g \otimes j"]\\
                    A \otimes F \arrow[r, "(g \circ f) \otimes \id_F"'] & C \otimes F.
                \end{tikzcd}
            \end{equation}
            Now specialise this to the case where \(A = B = C = D = E = F = I\), \(h = g = \id_I\), then we have
            \begin{equation}
                \begin{tikzcd}
                    I \otimes I \arrow[r, "f \otimes \id_I"] \arrow[d, "{\id_I} \otimes j"'] & I \otimes I \arrow[d, "{\id_I} \otimes j"]\\
                    I \otimes I \arrow[r, "f \otimes \id_I"'] & I \otimes I.
                \end{tikzcd}
            \end{equation}
            Now \(f, j \colon I \to I\) are scalars.
            
            We can now take the commutative diagrams we have constructed and paste them together setting \(f = a\) and \(j = b\) to get the following cube:
            \begin{equation}\label{eqn:scalar multiplication cube}
                \begin{tikzcd}
                    I \arrow[rr, "a"] \arrow[dd, "\lambda_I^{-1}"', "\rho_I^{-1}"] \arrow[dr, "b"] & & I \arrow[dd, "\lambda_I^{-1}"', "\rho_I^{-1}", pos=0.75] \arrow[dr, "b"] &\\
                    & I \arrow[rr, "a", pos=0.3, crossing over] & & I \arrow[dd, "\lambda_I"', "\rho_I", pos=0.4]\\
                    I \otimes I \arrow[dr, "{\id_I} \otimes b"'] \arrow[rr, "a \otimes \id_I", pos=0.2] & & I \otimes I \arrow[dr, "{\id_I} \otimes b", pos=0.3] & \\
                    & I \otimes I \arrow[uu, "\lambda_I", "\rho_I"', crossing over, pos=0.3] \arrow[rr, "a \otimes \id_I"'] & & I \otimes I.
                \end{tikzcd}
            \end{equation}
            The sides of this cube all commute, since they are one of the modified naturality squares for either \(\lambda_I\) or \(\rho_I\) from the start of the proof.
            The bottom of the cube commutes as it is the diagram we derived from the interchange law.
            Thus, the top of the cube must commute, since any path along the top can be replaced with a path through the rest of the cube, which must necessarily all be the same.
            The top of the cube is exactly the diagram which states
            \begin{equation}
                a \circ b = b \circ a
            \end{equation}
            for all \(a, b \in \cat{C}(I, I)\).
        \end{proof}
    \end{lma}
    
    Since a scalar is a map \(I \to I\) graphically it is represented by a box with no wires.
    Because of this we'll represent scalars by a circle:
    \begin{equation}
        a = 
        \tikzsetnextfilename{scalars-scalar}
        \begin{tikzpicture}[baseline=(a.base)]
            \node[scalar] (a) {\(a\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    Commutativity of scalar multiplication becomes
    \begin{equation}
        \tikzsetnextfilename{scalars-commutativity-of-scalars}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node[scalar] (a1) {\(\phantomrlap{a}{b}\)};
            \node[scalar, above= 0.5 cm of a1] (b1) {\(b\)};
            \node[scalar, right=of a1] (b2) {\(b\)};
            \node[scalar, above= 0.5 cm of b2] {\(\phantomrlap{a}{b}\)};
            \node (equals) at ($(b1)!0.5!(b2)$) {\(=\)};
        \end{tikzpicture}
        .
    \end{equation}
    Clearly these two diagrams are isotopic, so the correctness of the graphical notation proves commutativity of scalars, and does so much quicker than the proof above.
    
    \section{Scalar Multiplication}
    \begin{dfn}{Scalar Multiplication}{}
        Let \(\cat{C}\) be a monoidal category with unit \(I\), left unitor \(\lambda\), right unitor \(\rho\), and monoidal product \(\otimes\).
        Given a morphism \(f \colon A \to B\), and a scalar \(a \colon I \to I\) both in \(\cat{C}\) then we define the \defineindex{left scalar multiplication}\index{scalar multiplication} of \(f\) by \(a\), denoted \(a \cdot f \colon A \to B\), to be the morphism
        \begin{equation}
            \lambda_B \circ (a \otimes f) \circ \lambda_A^{-1} \colon A \to B.
        \end{equation}
    \end{dfn}
    
    Put another way, the left scalar multiplication of \(f\) by \(a\) is the unique morphism \(a \cdot f\) such that the following commutes
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "a \cdot f"] \arrow[d, "\lambda_A^{-1}"'] & B\\
            I \otimes A \arrow[r, "f \otimes a"'] & I \otimes B. \arrow[u, "\lambda_B"']
        \end{tikzcd}
    \end{equation}
    
    Graphically, we have
    \begin{equation}
        a \cdot f = 
        \tikzsetnextfilename{scalars-left-scalar-multiplication}
        \begin{tikzpicture}[baseline=(a.base)]
            \node[scalar] (a) {\(a\)};
            \node[morphism, right=0.5 cm of a] (f) {\(f\)};
            \draw[wire] (f) -- ++ (0, 1) node [midway, right] {\(B\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, right] {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    We can similarly define right scalar multiplication, \(f \cdot a\), as
    \begin{equation}
        f \cdot a = \rho_B \circ (f \otimes a) \circ \rho_{A}^{-1},
    \end{equation}
    so that the following diagram commutes
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "f \cdot a"] \arrow[d, "\rho_A^{-1}"'] & B\\
            A \otimes I \arrow[r, "a \otimes f"'] & B \otimes I. \arrow[u, "\rho_B"']
        \end{tikzcd}
    \end{equation}
    This is then drawn as
    \begin{equation}
        \tikzsetnextfilename{scalars-right-scalar-multiplication}
        \begin{tikzpicture}[baseline=(a.base)]
            \node[scalar] (a) {\(a\)};
            \node[morphism, left=0.5 cm of a] (f) {\(f\)};
            \draw[wire] (f) -- ++ (0, 1) node [midway, left] {\(B\)};
            \draw[wire] (f) -- ++ (0, -1) node [midway, left] {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    Note that, in general, left and right scalar multiplication are not the same.
    In particular, the two diagrams are not isotopic as to move \(a\) to the other side of \(f\) we either have to cross a wire or leave the bounding box which the wires are attached to.
    We will consider only left scalar multiplication, which we'll simply call scalar multiplication.
    The statements we make will all transfer to right scalar multiplication in an obvious way.
    
    What properties should scalar multiplication have?
    We look to \(\Hilb\) as an example.
    In \(\Hilb\) we have \(1 \in \complex\), corresponding to the map \(1 \mapsto 1\), which is just the identity \(\id_{\complex}\).
    So the properties of 1 in a field, i.e.\@ being a multiplicative identity, should extend to \(\id_I\) in some arbitrary monoidal category.
    
    \begin{lma}{}{lma:id is scalar identity}
        Let \(\cat{C}\) be a monoidal category with unit \(I\) and \(f \colon A \to B\) a morphism of \(\cat{C}\).
        Then \(\id_I \cdot f = f\).
        \begin{proof}
            The definition of scalar multiplication, \(\id_I \cdot f\), is that it makes
            \begin{equation}
                \begin{tikzcd}
                    A \arrow[r, "\id_I \cdot f"] \arrow[d, "\lambda_A^{-1}"'] & B\\
                    I \otimes A \arrow[r, "f \otimes a"'] & I \otimes B. \arrow[u, "\lambda_B"']
                \end{tikzcd}
            \end{equation}
            commute.
            The hypothesis, that \(\id_I \cdot f = f\) is then that the 
            \begin{equation}
                \begin{tikzcd}
                    A \arrow[r, "f"] \arrow[d, "\lambda_A^{-1}"'] & B\\
                    I \otimes A \arrow[r, "f \otimes a"'] & I \otimes B. \arrow[u, "\lambda_B"']
                \end{tikzcd}
            \end{equation}
            commutes.
            Replacing \(\lambda_A^{-1}\) with \(\lambda_A\) and reversing the arrow doesn't change the commutativity of the diagram.
            Hence, the diagram above commutes exactly when
            \begin{equation}
                \begin{tikzcd}
                    A \arrow[r, "a \cdot f"] & B\\
                    I \otimes A \arrow[u, "\lambda_A"] \arrow[r, "f \otimes a"'] & I \otimes B. \arrow[u, "\lambda_B"']
                \end{tikzcd}
            \end{equation}
            commutes.
            This is just the naturality square of \(\lambda\), so commutes by definition, so the fist diagram commutes, so \(\id_I \cdot f = f\).
        \end{proof}
    \end{lma}
    
    A proof of this same fact using the graphical notation is simply
    \begin{equation}
        \tikzsetnextfilename{scalars-idI-is-1-proof}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node (equals) at (0.75, 0) {\(=\)};
            \node at (-1.75, 0) {\(=\)};
            \node[morphism] (f0) at (-2.5, 0) {\(f\)};
            \draw[wire] (f0) -- ++ (0, 0.75);
            \draw[wire] (f0) -- ++ (0, -0.75);
            \node[scalar, inner sep=1pt, left= 0.5cm of f0] {\(\id_I\)};
            \node[morphism] (f) {\(f\)};
            \draw[wire] (f) -- ++ (0, 0.75) coordinate (top);
            \draw[wire] (f) -- ++ (0, -0.75) coordinate (bottom);
            \draw[wire] (-1, 0 |- bottom) -- (-1, 0 |- top) node [midway, left] {\(I\)};
            \node[morphism] (f2) at (1.5, 0) {\(f\)};
            \draw[wire] (f2) -- ++ (0, 0.75);
            \draw[wire] (f2) -- ++ (0, -0.75);
        \end{tikzpicture}
        ,
    \end{equation}
    since we draw the unit object, which corresponds to \(\id_I\) in the graphical notation, as the empty diagram.
    
    An obvious question we can ask is how do we multiply two scalars, since we should have some notion of multiplication in the field of scalars.
    Well it turns out that this is quite simple.
    
    \begin{lma}{}{}
        Let \(\cat{C}\) be a monoidal category with scalars \(a\) and \(b\).
        Then \(a \cdot b = a \circ b\).
        \begin{proof}
            This follows by commutativity of \cref{eqn:scalar multiplication cube}, which starting at the top back left, moving forward then right gives \(a \circ b\), but going down, forward, right, and up gives
            \begin{equation}
                \lambda_I \circ (a \otimes \id_I) \circ ({\id_I} \otimes b) \circ \lambda_I^{-1}.
            \end{equation}
            Applying the interchange law to the middle of this morphism we have
            \begin{equation}
                (a \otimes \id_I) \circ ({\id_I} \otimes b) = (a \circ \id_I) \otimes (\id_I \circ b) = a \otimes b.
            \end{equation}
            Hence, we have
            \begin{equation}
                a \circ b = \lambda_I \circ (a \otimes b) \circ \lambda_I^{-1} = a \cdot b.
            \end{equation}
        \end{proof}
    \end{lma}
    
    A proof of this same fact using the graphical notation is simply
    \begin{equation}
        \tikzsetnextfilename{scalars-adotb-is-acircb-proof}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node (equals) at (0.5, 0) {\(=\)};
            \node[scalar] at (-0.05, 0) {\(\phantomrlap{a}{b}\)};
            \node[scalar] at (-0.8, 0) {\(b\)};
            \node[scalar] at (1, 0.5) {\(\phantomrlap{a}{b}\)};
            \node[scalar] at (1, -0.5) {\(b\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    An important property of scalar multiplication is a compatibility between scalar multiplication and field multiplication.
    By this we mean if we want to multiply a map by two scalars we can either multiply the map individually by both scalars, or we can compute the product of the two scalars and then multiply this by the map.
    
    \begin{lma}{}{lma:scalar multiplication associative}
        Let \(\cat{C}\) be a monoidal category with scalars \(a\) and \(b\) and a morphism \(f\).
        Then \(a \cdot (b \cdot f) = (a \cdot b) \cdot f = (a \circ b) \cdot f\).
        \begin{proof}
            Consider \(a \cdot (b \cdot f)\).
            Expanding the definition of \(\cdot\) we have
            \begin{align}
                b \cdot f &= \lambda_B \circ (b \otimes f) \circ \lambda_A^{-1},\\
                a \cdot (b \cdot f) &= \lambda_B \circ (a \otimes (b \cdot f)) \circ \lambda_A^{-1}.
            \end{align}
            Hence,
            \begin{equation}
                a \cdot (b \cdot f) = \lambda_B \circ (a \otimes (\lambda_B \circ (b \otimes f) \circ \lambda_A^{-1})) \circ \lambda_A^{-1}.
            \end{equation}
            Similarly, we have
            \begin{align}
                a \cdot b &= \lambda_I \circ (a \otimes b) \circ \lambda_I^{-1},\\
                (a \cdot b) \cdot f &= \lambda_B \circ ((\lambda_I \circ (a \otimes b) \circ \lambda_I^{-1}) \otimes f) \circ \lambda_A^{-1}.
            \end{align}
            Now, consider the following diagram, which commutes by the coherence theorem:
            \begin{equation}
                \begin{tikzcd}[sep=large]
                    A \arrow[r, "\id_A", color=Green, text=black] \arrow[d, "\lambda_A^{-1}"', color=Blue, text=black] & A \arrow[r, "a \cdot (b \cdot f)"] \arrow[d, "\lambda_A^{-1}", color=Green, text=black] & B \arrow[r, "\id_B", color=Green, text=black] & B\\
                    I \otimes A \arrow[r, "\id_{I \otimes A}"] \arrow[ddr, "\lambda_I^{-1} \otimes \id_A"', color=Blue, text=black] & I \otimes A \arrow[r, "a \otimes (b \cdot f)"] \arrow[d, "{\id_I} \otimes \lambda_A^{-1}", color=Green, text=black] & I \otimes B \arrow[r, "\id_{I \otimes B}"] \arrow[u, "\lambda_B", color=Green, text=black] & I \otimes B \arrow[u, "\lambda_B"', color=Blue, text=black]\\
                    & I \otimes (I \otimes A) \arrow[r, "a \otimes (b \otimes f)", color=Green, text=black] \arrow[d, "\alpha_{I,I,A}^{-1}"] & I \otimes (I \otimes B) \arrow[u, "{\id_I} \otimes \lambda_B", color=Green, text=black] & \\
                    & (I \otimes I) \otimes A \arrow[r, "(a \otimes b) \otimes f"', color=Blue, text=black] & (I \otimes I) \otimes B \arrow[u, "\alpha_{I,I,B}"] \arrow[uur, "\lambda_I \otimes \id_B"', color=Blue, text=black]
                \end{tikzcd}
            \end{equation}
            The expanded form of \(a \cdot (b \cdot f)\) corresponds to starting at the top left, then applying \(\id_A\) immediately, which does nothing, going right we then apply \(\lambda_A^{-1}\), next we want to apply \(a \otimes (b \cdot f)\), which can be done be going down, right, then up using the definition of scalar multiplication, finally going up again is the last step and takes us to \(B\), to which we can apply \(\id_B\) for free to move to the far right \(B\).
            
            On the other hand, the expanded form of \((a \cdot b) \cdot f\) corresponds to starting at the top left, going down applying \(\lambda_A^{-1}\), then we want to apply \((\lambda_I \circ (a \otimes b) \circ \lambda_I^{-1}) \otimes f\).
            This can be done by first applying \(\lambda_I^{-1} \otimes \id_A\) to apply the \(\lambda_{I}^{-1}\) part, then we can apply the \(a \otimes b\)  and \(f\) parts by applying \((a \otimes b) \otimes f\), next we can apply the \(\lambda_I\) part by applying \(\lambda_I \otimes \id_B\).
            Finally we apply \(\lambda_B\) to get to the top right \(B\).
            
            Since both of these paths start and end at the same point they must correspond to the same morphism.
        \end{proof}
    \end{lma}
    
    The following is a proof in the graphical calculus:
    \begin{equation}
        \tikzsetnextfilename{scalars-abf-associativity}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node (equals) at (2.75, 0) {\(=\)};
            \node[scalar] {\(\phantomrlap{a}{b}\)};
            \node[scalar] at (1, 0) {\(b\)};
            \node[morphism] (f) at (2, 0) {\(f\)};
            \draw[wire] (f) -- ++ (0, -0.5);
            \draw[wire] (f) -- ++ (0, 0.5);
            \draw[thick, decoration={calligraphic brace}, decorate] (0.65, -0.5) -- (0.65, 0.5);
            \draw[thick, decoration={calligraphic brace, mirror}, decorate] (2.3, -0.5) -- (2.3, 0.5);
            \begin{scope}[xshift=3.5cm]
                \node[scalar] {\(\phantomrlap{a}{b}\)};
                \node[scalar] at (1, 0) {\(b\)};
                \node[morphism] (f) at (2, 0) {\(f\)};
                \draw[wire] (f) -- ++ (0, -0.5) coordinate (bottom);
                \draw[wire] (f) -- ++ (0, 0.5) coordinate (top);
                \draw[thick, decoration={calligraphic brace}, decorate] (-0.35, -0.5) -- (-0.35, 0.5);
                \draw[thick, decoration={calligraphic brace, mirror}, decorate] (1.35, -0.5) -- (1.35, 0.5);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}

    \begin{lma}{}{}
        Let \(\cat{C}\) be a (locally small) monoidal category with unit \(I\).
        Then \(\cat{C}(I, I)\) forms a monoid under scalar multiplication.
        \begin{proof}
            The product of two scalars is again a scalar.
            The monoid identity is \(\id_I \in \cat{C}(I, I)\), since \(\id_I \cdot a = a\) for all \(a \in \cat{C}(I, I)\) by \cref{lma:id is scalar identity}.
            \Cref{lma:scalar multiplication associative} proves that \(a \cdot (b \cdot c) = (a \cdot b) \cdot c\) for all scalars \(a, b, c \in \cat{C}(I, I)\), so scalar multiplication is associative.
        \end{proof}
    \end{lma}
    
    The final property we'll prove for now is a version of the interchange law.
    
    \begin{lma}{}{}
        Let \(\cat{C}\) be a monoidal category with scalars \(a\) and \(b\) and morphisms \(f\colon A \to B\) and \(g \colon B \to C\).
        Then \((b \cdot g) \circ (a \cdot f) = (b \circ a) \cdot (g \circ f)\).
        \begin{proof}
            The left hand side is
            \begin{equation}
                (\lambda_C \circ (b \otimes g) \circ \lambda_B^{-1}) \circ (\lambda_B \circ (a \otimes f) \circ \lambda_A^{-1}).
            \end{equation}
            Using associativity of composition to change around the brackets and \(\lambda_B^{-1} \circ \lambda_B = \id_B\), which we don't need to include in a chain of compositions, this becomes
            \begin{equation}
                \lambda_C \circ (b \otimes g) \circ (a \otimes f) \circ \lambda_A^{-1}.
            \end{equation}
            
            The right hand side is
            \begin{equation}
                \lambda_C \circ ((b \circ a) \otimes (g \circ f)) \circ \lambda_{A}^{-1}.
            \end{equation}
            Applying the interchange law to this we get
            \begin{equation}
                \lambda_C \circ (b \otimes g) \circ (a \otimes f) \circ \lambda_{A}^{-1}.
            \end{equation}
            Clearly this is the same result as we got from the left hand side, so the two are equal.
        \end{proof}
    \end{lma}
    
    The following is a proof in the graphical calculus:
    \begin{equation}
        \tikzsetnextfilename{scalars-interchange-law}
        \begin{tikzpicture}[baseline=(equals.base)]
            \node (equals) at (2.5, 0.75) {\(=\)};
            \node[scalar] (a1) {\(\phantomrlap{a}{b}\)};
            \node[scalar, above=of a1] (b1) {\(b\)};
            \node[morphism, right=of a1] (f1) {\(f\)};
            \node[morphism, right=of b1] (g1) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f1) -- (g1);
            \draw[wire] (f1) --  ++ (0, -1);
            \draw[wire] (g1) -- ++ (0, 1);
            \draw[thick, decoration={calligraphic brace, mirror}, decorate] (-0.5, -0.3) -- (2, -0.3);
            \draw[thick, decoration={calligraphic brace}, decorate] (-0.5, 0.3) -- (2, 0.3);
            \draw[thick, decoration={calligraphic brace, mirror}, decorate] (-0.5, 1.2) -- (2, 1.2);
            \draw[thick, decoration={calligraphic brace}, decorate] (-0.5, 1.8) -- (2, 1.8);
            \begin{scope}[xshift=3.5cm]
                \node (equals) at (0, 0) {};
                \node[scalar] (a1) {\(\phantomrlap{a}{b}\)};
                \node[scalar, above=of a1] (b1) {\(b\)};
                \node[morphism, right=of a1] (f1) {\(f\)};
                \node[morphism, right=of b1] (g1) {\(\phantomrlap{g}{f}\)};
                \draw[wire] (f1) -- (g1);
                \draw[wire] (f1) --  ++ (0, -1);
                \draw[wire] (g1) -- ++ (0, 1);
                \draw[thick, decoration={calligraphic brace}, decorate] (-0.3, -0.5) -- (-0.3, 2);
                \draw[thick, decoration={calligraphic brace, mirror}, decorate] (0.3, -0.5) -- (0.3, 2);
                \draw[thick, decoration={calligraphic brace}, decorate] (1.2, -0.5) -- (1.2, 2);
                \draw[thick, decoration={calligraphic brace, mirror}, decorate] (1.7, -0.5) -- (1.7, 2);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    
    \section{Examples}
    In \(\Set\) scalars are functions \(\{\bullet\} \to \{\bullet\}\).
    There is only one such function, \(\bullet \mapsto \bullet\), which is simply \(\id_{\{\bullet\}}\).
    Thus there is a single scalar in \(\Set\), and it is the trivial \(\id_{\{\bullet\}}\) which is such that \(\id_{\{\bullet\}} \cdot f = f\) for all \(f \in \Set(A, B)\).
    This makes sense since sets don't come equipped with a scalar structure.
    
    In \(\Rel\) scalars are relations \(R \subseteq \{\bullet\} \times \{\bullet\}\).
    There are two scalars, \(\true = \{(\bullet, \bullet)\} = \id_{\{\bullet\}}\) and \(\false = \emptyset\).
    Computing all possible products of these two scalars we see that clearly any product involving \(\false\) must necessarily just be \(\false\), since if one of the sets in the definition of relation composition is empty then the result is also empty.
    We also have \(\{(\bullet, \bullet)\} \circ \{(\bullet, \bullet)\} = \{(\bullet, \bullet)\}\).
    Hence, we have
    \begin{equation}
        \begin{array}{c|cc}
            \cdot & \true & \false\\ \hline
            \true & \true & \false\\
            \false & \false & \false
        \end{array}
        .
    \end{equation}
    This is exactly the result we would get if \(\true\) and \(\false\) were booleans and \(\cdot\) was logical conjunction (and).
    This is why we called the \(\true\) and \(\false\) in the first place.
    Further, for any relation \(R  \subseteq A \times B\) we have \(\true \cdot R = R\) and \(\false \cdot R = \emptyset = \false\), so \(\true\) and \(\false\) behave a bit like 1 and 0 for scalar multiplication.
    
    In \(\Hilb\) scalars are linear maps \(\complex \to \complex\).
    These are uniquely determined by where they send 1, so we can identify \(z \in \complex\) with the map \(1 \mapsto z\), and then scalar multiplication behaves exactly as expected, after all we did base scalars on \(\Hilb\) to start with.
    
    \chapter{Braided and Symmetric Monoidal Categories}
    \section{Braided Monoidal Categories: The Idea}
    Recall that we've seen four ways to consider monoidal categories categories:
    \begin{itemize}
        \item physical systems and the processes occurring with independent systems evolving separately;
        \item data types and the algorithms with algorithms running in parallel;
        \item algebraic structures and structure preserving functions with products or sums;
        \item logical propositions and implications between them with separate proofs of \(P\) and \(Q\) proving \(P \land Q\).
    \end{itemize}
    In all of these there is a sense in which order doesn't matter, that \(A \otimes B\) and \(B \otimes A\) are, if not the same, at least similar:
    \begin{itemize}
        \item it doesn't matter what order we place the independent systems, they will evolve to the states so the only difference is the order we place them;
        \item it doesn't matter if we run algorithm 1 on core 1 and algorithm 2 on core 2 or algorithm 1 on core 2 and algorithm 2 on core 1, the only difference is in how the algorithms run, the results are the same;
        \item it often doesn't matter what order we take a product, for example, if \(V\) and \(W\) are vector spaces then \(V \otimes W\) and \(W \otimes V\) are not the same, but we can turn one into the other through the mapping \(v \otimes w \mapsto w \otimes v\);
        \item if we prove \(P\) and \(Q\) separately then we can prove \(P \land Q\) or \(Q \land P\).
    \end{itemize}
    
    The most important example here is that of the vector spaces, while, as is often the case, equality between \(A \otimes B\) and \(B \otimes A\) is too strict it is often enough to just have a map between them.
    The only question then is what properties should this map poses?
    Well, as usual a map replacing equality should be an isomorphism.
    We also want the isomorphism relating \(A \otimes B\) and \(B \otimes A\) to be related to the isomorphism relating \(C \otimes D\) and \(D \otimes C\).
    We want a family of such isomorphisms all related in some way, hang on, we have families of related isomorphisms, they're called natural isomorphisms.
    
    \section{Braided Monoidal Category: The Definition}
    \begin{dfn}{Braided Monoidal Category}{}
        A \defineindex{braided monoidal category} is a monoidal category, \(\cat{C}\) equipped with a natural isomorphism, called the \defineindex{braiding},
        \begin{equation}
            \sigma \colon -\otimes- \naturalTransformation -\otimes-
        \end{equation}
        which for objects \(A\) and \(B\) has components
        \begin{equation}
            \sigma_{A,B} \colon A \otimes B \to B \otimes A.
        \end{equation}
        These must satisfy the \defineindex{hexagon equations}, that is the braiding should make the following diagrams commute:
        \begin{equation}
            \begin{tikzcd}[column sep=1cm]
                & A \otimes (B \otimes C) \arrow[r, "\sigma_{A,B\otimes C}"] \arrow[dl, "\alpha_{A,B,C}^{-1}"'] & (B \otimes C) \otimes A & \\
                (A \otimes B) \otimes C \arrow[dr, "\sigma_{A,B} \otimes \id_C"'] \hspace{-1cm} &&& \hspace{-1cm} B \otimes (C \otimes A) \arrow[ul, "\alpha_{B,C,A}^{-1}"']\\
                & (B \otimes A) \otimes C \arrow[r, "\alpha_{B,A,C}"'] & B \otimes (A \otimes C), \arrow[ur, "{\id_B} \otimes \sigma_{A,C}"']
            \end{tikzcd}
        \end{equation}
        and
        \begin{equation}
            \begin{tikzcd}[column sep=1cm]
                & (A \otimes B) \otimes C \arrow[r, "\sigma_{A\otimes B,C}"] \arrow[dl, "\alpha_{A,B,C}"'] & B \otimes (C \otimes A) & \\
                A \otimes (B \otimes C) \arrow[dr, "{\id_A} \otimes \sigma_{B,C}"'] \hspace{-1cm} &&& \hspace{-1cm} (C \otimes A) \otimes B \arrow[ul, "\alpha_{C,A,B}"']\\
                & A \otimes (C \otimes B) \arrow[r, "\alpha_{A,C,B}^{-1}"'] & (A \otimes C) \otimes B. \arrow[ur, "\sigma_{A,C} \otimes \id_B"']
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    The naturality condition for \(\sigma\) simply means that the following diagram commutes:
    \begin{equation}
        \begin{tikzcd}
            A \otimes B \arrow[r, "\sigma_{A,B}"] \arrow[d, "f \otimes g"'] & B \otimes A \arrow[d, "g \otimes f"]\\
            A' \otimes B' \arrow[r, "\sigma_{A',B'}"'] & B' \otimes A'
        \end{tikzcd}
    \end{equation}
    for all \(f \colon A \to A'\) and \(g \colon B \to B'\).
    In other words, it doesn't matter if we swap then apply a pair of maps or apply a pair of maps and then swap.
    
    Now consider the first hexagon equation.
    Starting at the top left, with \(A \otimes (B \otimes C)\) we can treat \(B \otimes C\) as a single object and swap it with \(A\) to get \((B \otimes C) \otimes A\).
    The first hexagon equation tells us that this is the same as moving the brackets, swapping \(A\) and \(B\) while doing nothing to \(C\), moving the brackets, swapping \(A\) and \(C\) while doing nothing to \(B\), and then moving the brackets.
    More succinctly swapping \(B \otimes C\) with \(A\) is the same as swapping \(A\) with \(B\) and \(C\) individually.
    
    \section{Braided Monoidal Category: The Examples}
    \subsection{\texorpdfstring{\(\Set\)}{Set}}
    The braiding in \(\Set\)\index{Set@\(\Set\)!as a symmetric monoidal category} (as a monoidal category with the Cartesian product) is pretty obvious.
    It's simply swapping the elements in a pair.
    \begin{dfn}{{\normalsize\(\Set\)} as a Braided Monoidal Category}{}
        The monoidal category \(\Set\), with the Cartesian product as the monoidal product, can be promoted to a braided monoidal category by defining \(\sigma_{A,B}\) to be the map \((a, b) \mapsto (b, a)\).
    \end{dfn}
    
    \begin{thm}{}{}
        \(\Set\) is a braided monoidal category.
        \begin{proof}
            First we should show that \(\sigma\) is natural, this follows from commutativity of the following for all \(f \colon A \to A'\) and \(g \colon B \to B'\):
            \begin{equation}
                \begin{tikzcd}
                    (a, b) \arrow[r, mapsto, "\sigma_{A,B}"] \arrow[d, mapsto, "f \times g"] & (b, a) \arrow[d, mapsto, "g \times f"]\\
                    (f(a), g(b)) \arrow[r, mapsto, "\sigma_{A', B'}"] & (g(b), f(a)).
                \end{tikzcd}
            \end{equation}
            Next we need to prove the hexagon equations.
            We'll prove only the first, which follows from commutativity of the following diagram:
            \begin{equation*}
                \begin{tikzcd}
                    & (a, (b, c)) \arrow[r, mapsto, "\sigma_{A,B\otimes C}"] \arrow[dl, mapsto, "\alpha_{A,B,C}^{-1}"'] & ((b, c), a) &\\
                    ((a, b), c) \hspace{-0.8cm} \arrow[dr, mapsto, "\sigma_{A,B} \times \id_C"'] &&& \hspace{-1cm} \parbox{2.8cm}{\centering\((b, (c, a)) = \) \((\id_B(b), \sigma_{A,C}(a, c))\)} \arrow[ul, mapsto, "\alpha_{B,C,A}^{-1}"']\\
                    & \parbox{2.8cm}{\centering\((\sigma_{A,B}(a, b),\id_C(c))\) \(= ((b, a), c)\)} \arrow[r, mapsto, "\alpha_{B,A,C}"'] & (b, (a, c)). \arrow[ur, mapsto, "\id_B \times \sigma_{A,C}"']
                \end{tikzcd}
            \end{equation*}
        \end{proof}
    \end{thm}
    
    \subsection{\texorpdfstring{\(\Rel\)}{Rel}}
    The braiding in \(\Rel\) is similar to that in \(\Set\), we just turn a function into a relation.
    \begin{dfn}{{\normalsize\(\Rel\)} as a Braided Monoidal Category}{}
        The monoidal category \(\Rel\)\index{Rel@\(\Rel\)!as a symmetric monoidal category} can be promoted to a braided monoidal category by defining \(\sigma_{A,B}\) to be the relation \((a, b) \sim (b, a)\).
    \end{dfn}
    
    A proof that this is a valid braiding is very similar to the proof for \(\Set\).
    
    \subsection{\texorpdfstring{\(\Hilb\)}{Hilb}}
    \begin{dfn}{{\normalsize\(\Hilb\)} as a Braided Monoidal Category}{}
        The monoidal category \(\Hilb\)\index{Hilb@\(\Hilb\)!as a symmetric monoidal category} can be promoted to a braided monoidal category by defining \(\sigma_{H,K}\) to be the map \(v \otimes w \mapsto w \otimes v\), and extended linearly to all of \(H \otimes K\).
    \end{dfn}
    
    Again, the proof that this is a valid braiding is very similar to the proof for \(\Set\).
    
    \subsection{More Examples}
    \begin{exm}{}{}
        \begin{itemize}
            \item The category of bimodules with bimodule homomorphisms with the usual tensor product of modules is braided symmetric.
        \end{itemize}
    \end{exm}
    
    \section{Graphical Notation}
    We can extend the graphical notation for monoidal categories to braided monoidal categories.
    To do so we make use of the idea that the braiding allows us to swap elements.
    For objects \(A\) and \(B\) in a braided monoidal category we denote the braiding \(\sigma_{A, B} \colon A \otimes B \to B \otimes A\) as follows
    \begin{equation}
        \sigma_{A,B} = 
        \tikzsetnextfilename{braid-sym-mon-cat-braiding}
        \begin{tikzpicture}[font=\small, baseline=0.4cm]
            \draw[wire, rounded corners] (1, 0) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.3);
            \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.3);
            \node[left] at (0, 0.15) {\(A\)};
            \node[right] at (1, 0.15) {\(B\)};
            \node[left] at (0, 0.85) {\(B\)};
            \node[right] at (1, 0.85) {\(A\)};
        \end{tikzpicture}
        .
    \end{equation}
    Note that the order in which the wires cross, the starting on the bottom left wire going over the top, is important.
    Since \(\sigma\) is a natural isomorphism \(\sigma_{A,B}^{-1} \colon B \otimes A \to A \otimes B\) exists, and we denote it by
    \begin{equation}
        \sigma_{A,B}^{-1} = 
        \tikzsetnextfilename{braid-sym-mon-cat-inverse-braiding}
        \begin{tikzpicture}[font=\small, baseline=0.4cm]
            \draw[wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.3);
            \draw[over wire, rounded corners] (1, 0) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.3);
            \node[left] at (0, 0.15) {\(B\)};
            \node[right] at (1, 0.15) {\(A\)};
            \node[left] at (0, 0.85) {\(A\)};
            \node[right] at (1, 0.85) {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    Notice that now the bottom left wire goes under.
    
    Now that we have wires passing over each other we have clearly entered the third dimension.
    This changes what it means for two diagrams to be the same.
    \begin{dfn}{Spatial Isotopy}{}
        Two diagrams are \defineindex{spatial isotopic} if one can be deformed continuously into the other such that
        \begin{itemize}
            \item the diagrams remain confined in a cuboidal volume of three-dimensional space;
            \item input and output wires terminate at the bottom and top of this region;
            \item components never intersect.
        \end{itemize}
    \end{dfn}
    Note that we now drop the requirement on wires entering in the same order, since we can always cross the wires over, it doesn't really make sense as a definition anyway as we don't have an order defined on the plane, and wires now enter in the rectangle at the bottom of the cuboidal region.
    
    That \(\sigma_{A,B}\) and \(\sigma_{A,B}^{-1}\) are inverses, tells us that \(\sigma_{A,B}^{-1} \circ \sigma_{A,B} = \id_{A \otimes B}\), graphically\footnote{note that the right hand side here can be read as \({\id_A} \otimes \id_B\), but functoriality of \(\otimes\) means that \({\id_A} \otimes \id_B = \id_{A\otimes B}\), which is perhaps clearer with function notation: \(\otimes(\id_A, \id_B) = \id_{\otimes(A,B)}\).},
    \begin{equation}
        \tikzsetnextfilename{braid-sym-mon-cat-braid-braid-inverse}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[wire, rounded corners] (1, 0) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.6) -- ++ (1, 0.4) -- ++ (0, 0.3);
            \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.6) -- ++ (-1, 0.4) -- ++ (0, 0.3);
            \draw[wire] (2, 0) -- (2, 2);
            \draw[wire] (3, 0) -- (3, 2);
            \node at (1.5, 1) {\(=\)};
        \end{tikzpicture}
        .
    \end{equation}
    Similarly, \(\sigma_{A,B} \circ \sigma_{A,B}^{-1} = \id_{B\otimes A}\), or graphically,
    \begin{equation}\label{eqn:graphical braid inverse}
        \tikzsetnextfilename{braid-sym-mon-cat-braid-inverse-braid}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.6) -- ++ (-1, 0.4) -- ++ (0, 0.3);
            \draw[over wire, rounded corners] (1, 0) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.6) -- ++ (1, 0.4) -- ++ (0, 0.3);
            \draw[wire] (2, 0) -- (2, 2);
            \draw[wire] (3, 0) -- (3, 2);
            \node at (1.5, 1) {\(=\)};
        \end{tikzpicture}
        .
    \end{equation}
    In both of these it looks like if the wires were strings we could just pull them taught, this is the type of thing that is allowed in a spatial isotopy.
    
    We can define \(\sigma^{-1}\colon - \otimes - \naturalTransformation - \otimes -\) to be the natural transformation whose components \((\sigma^{-1})_{A,B}\) are just the inverse components of \(\sigma\), so \((\sigma^{-1})_{A,B} = (\sigma_{A,B})^{-1} = \sigma_{A,B}^{-1}\), which is necessarily also natural.
    The naturality conditions
    \begin{align}
        \sigma_{A',B'} \circ (f \otimes g) &= (g \otimes f) \circ \sigma_{A,B},\\
        \sigma_{A',B'}^{-1} \circ (f \otimes g) &= (g \otimes f) \circ \sigma_{A, B}^{-1},
    \end{align}
    for all \(f \colon A \to A'\) and \(g \colon B \to B'\), can be expressed in the graphical notation as
    \begin{equation}
        \tikzsetnextfilename{braid-sym-mon-cat-braid-naturality}
        \begin{tikzpicture}[baseline=(equals)]
            \node[morphism] (f1) {\(f\)};
            \node[morphism, right=0.75cm of f1] (g1) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f1) -- ++ (0, -0.6) coordinate (bottom);
            \draw[wire] (g1) -- ++ (0, -0.6);
            \draw[wire, rounded corners] (g1.north) -- ++ (0, 0.3) -- (0, 0.8 -| f1) -- ++ (0, 0.4);
            \draw[over wire, rounded corners] (f1.north) -- ++ (0, 0.3) -- (0, 0.8 -| g1) -- ++ (0, 0.4) coordinate (top);
            \node (equals) at (1.8, 0.3) {\(=\)};
            \node[morphism] (f2) at (2.5, 0.6) {\(f\)};
            \node[morphism, right=0.75cm of f2] (g2) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f2) -- (f2 |- top);
            \draw[wire] (g2) -- (g2 |- top);
            \draw[wire, rounded corners] (f2.south) -- ++ (0, -0.3) -- (0, -0.2 -| g2) -- (g2 |- bottom);
            \draw[over wire, rounded corners] (g2.south) -- ++ (0, -0.3) -- (0, -0.2 -| f2) -- (f2 |- bottom);
        \end{tikzpicture}
        , \qand
        \tikzsetnextfilename{braid-sym-mon-cat-inverse-braid-naturality}
        \begin{tikzpicture}[baseline=(equals)]
            \node[morphism] (f1) {\(f\)};
            \node[morphism, right=0.75cm of f1] (g1) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f1) -- ++ (0, -0.6) coordinate (bottom);
            \draw[wire] (g1) -- ++ (0, -0.6);
            \draw[wire, rounded corners] (f1.north) -- ++ (0, 0.3) -- (0, 0.8 -| g1) -- ++ (0, 0.4) coordinate (top);
            \draw[over wire, rounded corners] (g1.north) -- ++ (0, 0.3) -- (0, 0.8 -| f1) -- ++ (0, 0.4);
            \node (equals) at (1.8, 0.3) {\(=\)};
            \node[morphism] (f2) at (2.5, 0.6) {\(f\)};
            \node[morphism, right=0.75cm of f2] (g2) {\(\phantomrlap{g}{f}\)};
            \draw[wire] (f2) -- (f2 |- top);
            \draw[wire] (g2) -- (g2 |- top);
            \draw[wire, rounded corners] (g2.south) -- ++ (0, -0.3) -- (0, -0.2 -| f2) -- (f2 |- bottom);
            \draw[over wire, rounded corners] (f2.south) -- ++ (0, -0.3) -- (0, -0.2 -| g2) -- (g2 |- bottom);
        \end{tikzpicture}
        .
    \end{equation}
    So the naturality of \(\sigma\) and \(\sigma^{-1}\) means that graphically we can slide morphisms through crossings.
    
    The hexagon equations become trivial in the graphical notation, they just tell us that crossing two objects at once or crossing them one at a time is the same:
    \begin{equation}
        \tikzsetnextfilename{braid-sym-mon-cat-hexagon-1}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw[wire, rounded corners] (0, 2) -- ++ (0, -0.5) -- ++ (1.3, -1) -- ++ (0, -0.5);
            \draw[wire, rounded corners, xshift=0.2cm] (0, 2) -- ++ (0, -0.5) -- ++ (1.3, -1) -- ++ (0, -0.5);
            \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.5) -- ++ (1.5, 1) -- ++ (0, 0.5);
            \node (equals) at (2, 1) {\(=\)};
            \begin{scope}[xshift=2.5cm]
                \draw[wire, rounded corners] (0, 2) -- ++ (0, -0.8) -- ++ (1.3, -1) -- ++ (0, -0.2);
                \draw[wire, rounded corners, xshift=0.2cm] (0, 2) -- ++ (0, -0.2) -- ++ (1.3, -1) -- ++ (0, -0.8);
                \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.5) -- ++ (1.5, 1) -- ++ (0, 0.5);
            \end{scope}
        \end{tikzpicture}
        , \qand
        \tikzsetnextfilename{braid-sym-mon-cat-hexagon-2}
        \begin{tikzpicture}[baseline=(equals.base)]
            \draw[wire, rounded corners] (0, 2) -- ++ (0, -0.5) -- ++ (1.5, -1) -- ++ (0, -0.5);
            \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.5) -- ++ (1.3, 1) -- ++ (0, 0.5);
            \draw[over wire, rounded corners, xshift=0.2cm] (0, 0) -- ++ (0, 0.5) -- ++ (1.3, 1) -- ++ (0, 0.5);
            \node (equals) at (2, 1) {\(=\)};
            \begin{scope}[xshift=2.5cm]
                \draw[wire, rounded corners] (0, 2) -- ++ (0, -0.5) -- ++ (1.5, -1) -- ++ (0, -0.5);
                \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.8) -- ++ (1.3, 1) -- ++ (0, 0.2);
                \draw[over wire, rounded corners, xshift=0.2cm] (0, 0) -- ++ (0, 0.2) -- ++ (1.3, 1) -- ++ (0, 0.8);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    \subsection{Correctness Theorem}
    The graphical calculus for braided monoidal categories comes with a correctness theorem analogous to the correctness theorem for monoidal categories.
    
   \begin{thm}{Correctness of the Graphical Notation for Braided Monoidal Categories}{}
       Any well-formed equation of the form \(f = g\) for morphisms \(f\) and \(g\) in a braided monoidal category follows from the axioms of braided monoidal categories if and only if it holds in the graphical notation up to spatial isotopy.
   \end{thm}
    
    \begin{exm}{}{}
        Consider the following equation of morphisms \(B \to A \otimes B \otimes C\):
        \begin{equation}
            (\sigma_{A,B} \otimes \id_B) \circ ({\id_A} \otimes f)  = ({\id_A} \otimes \sigma_{B,C}^{-1}) \circ (f \otimes \id_B)
        \end{equation}
        where \(f \colon I \to A \otimes C\).
        This holds in a genera braided monoidal category as we can express this equation as the following equation of diagrams
        \begin{equation}
            \tikzsetnextfilename{braid-sym-mon-cat-graphical-example-1}
            \begin{tikzpicture}[baseline=(equals.base)]
                \node[morphism, minimum width=1cm] (f1) {\(f\)};
                \draw[wire, rounded corners] ($(f1.north west) + (0.2, 0)$) coordinate (A) -- ++ (0, 0.2) -- ++ (-0.5, 0.3) -- ++ (0, 0.4) coordinate (top);
                \draw[wire] ($(f1.north east) - (0.2, 0)$) coordinate (here) -- (here |- top) coordinate (top2);
                \draw[over wire, rounded corners] ($(top)!0.5!(top2)$) -- ++ (0, -0.375) -- ++ (-0.7, -0.3) -- ++ (0, -1.3);
                \begin{scope}[xshift=2cm]
                    \node[morphism, minimum width=1cm] (f2) {\(f\)};
                    \draw[wire, rounded corners] ($(f2.north east) - (0.2, 0)$) coordinate (A) -- ++ (0, 0.2) -- ++ (0.5, 0.3) -- ++ (0, 0.4) coordinate (top);
                    \draw[wire] ($(f2.north west) + (0.2, 0)$) coordinate (here) -- (here |- top) coordinate (top2);
                    \draw[over wire, rounded corners] ($(top)!0.5!(top2)$) -- ++ (0, -0.375) -- ++ (0.7, -0.3) -- ++ (0, -1.3);
                \end{scope}
                \node (equals) at ($(f1)!0.5!(f2)$) {\(=\)};
            \end{tikzpicture}
        \end{equation}
        and we can see that these are spatially isotopic, we can just slide the \(B\) wire over the \(f\) morphism to the other side.
    \end{exm}
    
    \begin{exm}{}{}
        Consider the following spatial isotopy of diagrams
        \begin{equation}
            \tikzsetnextfilename{braid-sym-mon-cat-graphical-example-2}
            \begin{tikzpicture}[baseline=(equals.base)]
                \draw[wire, rounded corners] (1, 0) -- ++ (0, 1.1) -- ++ (-0.5, 0.5) -- ++ (0, 0.3) -- ++ (-0.5, 0.5) -- ++ (0, 0.3);
                \draw[over wire, rounded corners] (0.5, 0) -- ++ (0, 0.3) -- ++ (-0.5, 0.5) -- ++ (0, 1.1) -- ++ (0.5, 0.5) -- ++ (0, 0.3);
                \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (0.5, 0.5) -- ++ (0, 0.3) -- ++ (0.5, 0.5) -- ++ (0, 1.1);
                \node (equals) at (1.5, 1.35) {\(=\)};
                \draw[wire, rounded corners] (3, 0) -- ++ (0, 0.3) -- ++ (-0.5, 0.5) -- ++ (0, 0.3) -- ++ (-0.5, 0.5) -- ++ (0, 1.1);
                \draw[over wire, rounded corners] (2.5, 0) -- ++ (0, 0.3) -- ++ (0.5, 0.5) -- ++ (0, 1.1) -- ++ (-0.5, 0.5) -- ++ (0, 0.3);
                \draw[over wire, rounded corners] (2, 0) -- ++ (0, 1.1) -- ++ (0.5, 0.5) -- ++ (0, 0.3) -- ++ (0.5, 0.5) -- ++ (0, 0.3);
            \end{tikzpicture}
            .
        \end{equation}
        We can convert this into an algebraic equation between morphisms.
        The first step is to work out what the source and target of these morphisms is.
        To do this we label the wires \(A\), \(B\), and \(C\) along the bottom, so the source is \(A \otimes B \otimes C\).
        The wires must have the same labels all the way along, so if we chase these labels up the wires we see that the target is \(C \otimes B \otimes A\).
        Next split the diagrams horizontally into sections so that only one thing is occurring in each section, this gives
        \begin{equation}
            \tikzsetnextfilename{braid-sym-mon-cat-graphical-example-2-2}
            \begin{tikzpicture}[baseline=(equals.base)]
                \draw[wire, rounded corners] (1, 0) -- ++ (0, 1.1) -- ++ (-0.5, 0.5) -- ++ (0, 0.3) -- ++ (-0.5, 0.5) -- ++ (0, 0.3);
                \draw[over wire, rounded corners] (0.5, 0) -- ++ (0, 0.3) -- ++ (-0.5, 0.5) -- ++ (0, 1.1) -- ++ (0.5, 0.5) -- ++ (0, 0.3);
                \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (0.5, 0.5) -- ++ (0, 0.3) -- ++ (0.5, 0.5) -- ++ (0, 1.1);
                \node (equals) at (1.5, 1.35) {\(=\)};
                \draw[wire, rounded corners] (3, 0) -- ++ (0, 0.3) -- ++ (-0.5, 0.5) -- ++ (0, 0.3) -- ++ (-0.5, 0.5) -- ++ (0, 1.1);
                \draw[over wire, rounded corners] (2.5, 0) -- ++ (0, 0.3) -- ++ (0.5, 0.5) -- ++ (0, 1.1) -- ++ (-0.5, 0.5) -- ++ (0, 0.3);
                \draw[over wire, rounded corners] (2, 0) -- ++ (0, 1.1) -- ++ (0.5, 0.5) -- ++ (0, 0.3) -- ++ (0.5, 0.5) -- ++ (0, 0.3);
                \foreach \y in {0.15, 0.95, 1.8, 2.6} {
                    \foreach \xone/\xtwo in {-0.3/1.3, 1.7/3.3} {
                        \draw[thick, Red!50] (\xone, \y) -- (\xtwo, \y);
                    }
                }
                \draw[pen colour=Red!50, thick, decoration={calligraphic brace, mirror}, decorate] (3.5, 0.15) -- (3.5, 0.95) node [midway, right, font=\scriptsize\color{Red!50}] {1};
                \draw[pen colour=Red!50, thick,  decoration={calligraphic brace, mirror}, decorate] (3.5, 0.95) -- (3.5, 1.8) node [midway, right, font=\scriptsize\color{Red!50}] {2};
                \draw[pen colour=Red!50, thick,  decoration={calligraphic brace, mirror}, decorate] (3.5, 1.8) -- (3.5, 2.6) node [midway, right, font=\scriptsize\color{Red!50}] {3};
            \end{tikzpicture}
            .
        \end{equation}
        We can then just read from the bottom up.
        Starting on the left region 1 is swapping \(A\) and \(B\) and leaving \(C\) alone, so it corresponds to \(\sigma_{A,B} \otimes \id_C\), region 2 then leaves \(B\) alone (which is now the left most wire) and swaps \(A\) and \(C\), so it is \({\id_B} \otimes \sigma_{A,C}\), and region 3 swaps \(B\) and \(C\) leaving \(A\) alone, so it is \(\sigma_{B,C} \otimes \id_A\).
        Doing the same for the diagram on the right we get the equation
        \begin{multline}
            (\sigma_{B,C} \otimes \id_A) \circ ({\id_B} \otimes \sigma_{A,C}) \circ (\sigma_{A,B} \otimes \id_C)\\
            = ({\id_C} \otimes \sigma_{A,C}) \circ (\sigma_{A,C} \otimes \id_B) \circ ({\id_A} \otimes \sigma_{B,C}).
        \end{multline}
    \end{exm}
    
    \section{Symmetric Monoidal Category}
    It is tempting to think that swapping two objects and swapping them back should do nothing.
    However, this is not always the case, in general \(A \otimes B\) and the result of applying \(\sigma_{A,B}\) then \(\sigma_{B,A}\) will only be isomorphic.
    Often though this isomorphism turns out to be the identity, and this leads to the following definition.
    \begin{dfn}{Symmetric Monoidal Category}{}
        A \defineindex{symmetric monoidal category} is a braided monoidal category with braiding \(\sigma\) such that
        \begin{equation}
            \sigma_{B,A} \circ \sigma_{A,B} = \id_{A \otimes B}
        \end{equation}
        for all objects \(A\) and \(B\).
    \end{dfn}
    
    That is, a symmetric monoidal category is exactly a braided monoidal category in which we can swap and swap back without changing anything.
    So, in a symmetric monoidal category we have \(\sigma_{A,B}^{-1} = \sigma_{B,A}\).
    
    In the graphical notation \(\sigma_{B,A} \circ \sigma_{A,B} = \id_{A\otimes B}\) is
    \begin{equation}
        \tikzsetnextfilename{braid-sym-mon-cat-sym-sym-inverse}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[wire, rounded corners] (1, 0) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.3) coordinate (A);
            \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.3) coordinate (B);
            \draw[wire, rounded corners] (B) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.3);
            \draw[over wire, rounded corners] (A) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.3);
            \draw[wire] (2, 0) -- (2, 2);
            \draw[wire] (3, 0) -- (3, 2);
            \node at (1.5, 1) {\(=\)};
        \end{tikzpicture}
        .
    \end{equation}
    Note the subtle difference to the identity \(\sigma_{A,B}^{-1} \circ \sigma_{A,B}\) in \cref{eqn:graphical braid inverse}, here the wires swap which one is on top.
    One can imagine the wires being strings, pulling on them, and them passing through each other to become taught.
    This is the type of thing that is allowed in a four-dimensional isotopy.
    
    \begin{dfn}{Four-Dimensional Isotopy}{}
        Two diagrams are related by a four-dimensional isotopy if one can be deformed continuously into the other such that
        \begin{itemize}
            \item the diagrams remain confined in a cuboidal volume of three-dimensional space;
            \item input and output wires terminate at the bottom and top of this region.
        \end{itemize}
    \end{dfn}
    
    The extra dimension gives us room to move wires which seem linked in three dimensions past each other in the fourth dimension, unlinking them in three dimensions.
    This is part of a deep mathematical fact that there can be no (nontrivial) knots in four dimensions.
    
    Since wires can pass through each other for a symmetric monoidal category we don't need to keep track of which wire is on top, so we write
    \begin{equation}
        \tikzsetnextfilename{braid-sym-mon-cat-symmetric-braiding-1}
        \begin{tikzpicture}[baseline=0.4cm]
            \draw[wire, rounded corners] (1, 0) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.3);
            \draw[wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.3);
        \end{tikzpicture}
        =
        \tikzsetnextfilename{braid-sym-mon-cat-symmetric-braiding-2}
        \begin{tikzpicture}[baseline=0.4cm]
            \draw[wire, rounded corners] (1, 0) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.3);
            \draw[over wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.3);
        \end{tikzpicture}
        =
        \tikzsetnextfilename{braid-sym-mon-cat-symmetric-braiding-3}
        \begin{tikzpicture}[baseline=0.4cm]
            \draw[wire, rounded corners] (0, 0) -- ++ (0, 0.3) -- ++ (1, 0.4) -- ++ (0, 0.3);
            \draw[over wire, rounded corners] (1, 0) -- ++ (0, 0.3) -- ++ (-1, 0.4) -- ++ (0, 0.3);
        \end{tikzpicture}
        .
    \end{equation}
    
    Like (braided) monoidal categories there is a correctness theorem to go with the graphical notation.
    
    \begin{thm}{Correctness of the Graphical Notation for Symmetric Monoidal Categories}{}
        Any well-formed equation of the form \(f = g\) in a symmetric monoidal category follows from the axioms of symmetric monoidal categories if and only if it holds in the graphical notation up to graphical equivalence.
    \end{thm}
    
    Note that unlike the monoidal and braided monoidal cases we don't have isotopy here, this is because although graphical equivalence (a deliberately vague term essentially defined by being the thing required to make this theorem true) is almost certainly four-dimensional isotopy it has not yet been proven.
    
    \begin{exm}{}{}
        \begin{itemize}
            \item \(\Set\) equipped with the Cartesian product is a symmetric monoidal category;
            \item \(\Rel\) equipped with the Cartesian product is a symmetric monoidal category;
            \item \(\Hilb\) equipped with the tensor product is a symmetric monoidal category;
            \item \(\Grp\) equipped with the Cartesian product with the trivial group as unit is a symmetric monoidal category;
            \item the category of representations equipped with the tensor product of representations, which is a tensor product of the representation spaces and factor-wise group action on the product, so \(g \mathbin{.} (v \otimes u) = (g \mathbin{.} v) \otimes (g \mathbin{.} u)\), is a symmetric monoidal category;
            \item \(\Cat\) equipped with the product of categories is a symmetric monoidal category;
            \item every Cartesian category equipped with its categorical product as a monoidal product with the terminal object as the unit is a symmetric monoidal category by uniqueness of the categorical product;
            \item A \(\integers_2\)-graded vector space\footnote{See also my notes from the \course{Quantum Field Theory} course, where a \(\integers_2\)-graded vector space equipped with a product is called a Grassmann algebra.} is a vector space, \(V\), along with a decomposition \(V = V_0 \oplus V_1\).
            A \(\integers_2\)-graded map is a map \(f\colon V \to W\) with \(V = V_0 \oplus V_1\) and \(W = W_0 \oplus W_1\) such that if \(v \in V_0\) then \(f(v) \in W_0\) and if \(v \in V_1\) then \(f(v) \in W_1\).
            The tensor product of vector spaces makes the category of \(\integers_2\)-graded vector spaces with grading preserving maps into a monoidal category.
            There are two ways to make this into a symmetric monoidal category.
            The first is to forget the grading, in which case the braiding is \(u \otimes v \mapsto v \otimes u\), the second is that if \(u, v \in V_1\) then we define the braiding to be \(u \otimes v \mapsto -v \otimes u\) and if either \(u \in V_0\) or \(v \in V_0\) (or both) then the braiding is \(u \otimes v \mapsto v \otimes u\).
        \end{itemize}
    \end{exm}
    
    \chapter{Dagger Categories}
    \epigraph{O happy dagger!}{Juliet in Romeo and Juliet}
    
    \section{Dagger Categories: The Idea}
    In the definition of \(\Hilb\), the category of Hilbert spaces, we didn't make use of the inner product, except for requiring its existence as part of the definition of a Hilbert space.
    This leaves a hole in our categorical approach to quantum computing, since the inner product is incredibly important in quantum mechanics.
    So, in this chapter we will demonstrate how we can introduce the important parts of an inner product to our categories.
    
    Rather than the inner product itself we use the fact that the inner product can define an adjoint.
    \begin{dfn}{Adjoint}{}
        Let \(H\) be a Hilbert spaces and consider a bounded linear map \(f \colon H \to H\).
        We define the \defineindex{adjoint}\footnote{Not to be confused with adjoint functors.} of this map, \(f^{\dagger} \colon H \to H\), to be the unique map such that \(\braket{u}{f(v)} = \braket{f^{\hermit}(u)}{v}\) for all \(u, v \in H\).
    \end{dfn}
    
    The adjoint has the following properties:
    \begin{equation}
        (g \circ f)^\dagger = f^\dagger \circ g^\dagger, \qquad \id_H^\dagger = \id_H, \qqand (f^\dagger)^\dagger.
    \end{equation}
    These may be more familiar if we consider the finite dimensional case and represent the maps as matrices, in which case \(\dagger\) is the Hermitian conjugate, that is transpose and take the complex conjugate, then we have
    \begin{equation}
        (AB)^{\dagger} = B^\dagger A^\dagger, \qquad I^\dagger = I, \qqand (A^\dagger)^\dagger = A.
    \end{equation}
    
    We can encode this information into a functor, which is what the next definition does.
    
    \begin{dfn}{Dagger Functor}{}
        The \define{dagger functor}\index{dagger functor!\(\Hilb\)} on \(\Hilb\) is the contravariant endofunctor \((-)^\dagger \colon \Hilb \to \Hilb\) which takes objects to themselves and morphisms to their adjoints as bounded linear maps.
    \end{dfn}
    
    Contravariance means that \(F(g \circ f) = Ff \circ Fg\), which in this case means \((g \circ f)^\dagger = f^\dagger \circ g^\dagger\), which is certainly true.
    Since the dagger acts as an identity on objects and \(\id_H^\dagger = \id_H = \id_{H^\dagger}\), satisfying the requirement that \(F\id_A = \id_{FA}\), so we see that this is indeed a functor.
    The dagger functor has the further property of being an \defineindex{involution}, meaning that \((f^{\dagger})^{\dagger} = f\).
    
    This functor captures all of the information in an inner product.
    To see this we demonstrate how we can recover the inner product just using the dagger.
    Consider some Hilbert space, \(H\), with states \(v, w \colon \complex \to H\), that is kets \(\ket{v}\) and \(\ket{w}\).
    We can turn \(v\) into an effect \(v^\dagger \colon H \to \complex\).
    Then consider the composite \(v^\hermit \circ w\).
    As a (bounded) linear map this is completely determined by where it sends 1, so consider this map evaluated at 1, we have
    \begin{equation}
        (v^\dagger \circ w)(1) = v^\dagger(w(1)) = \braket{1}{v^\dagger(w(1))}
    \end{equation}
    where \(\braket{-}{-}\) is the inner product on \(\complex\), that is \(\braket{x}{y} = x^*y\), so \(\braket{1}{v^\dagger(w(1))} = 1^*v^\dagger(w(1)) = v^\dagger(w(1))\).
    Then using the definition of the adjoint we have
    \begin{equation}
        (v^\dagger \circ w)(1) = \braket{1}{v^\dagger(w(1))} = \braket{(v^\dagger)^\dagger(1)}{w(1)} = \braket{v(1)}{w(1)} = \braket{v}{w}
    \end{equation}
    where the last step is to identify the state \(w\) evaluated at 1 as the ket \(\ket{w}\) and the effect \(v\) evaluated at 1 as the bra \(\bra{w}\).
    This calculation shows that using only the dagger, and the most basic inner product on \(\complex\), we can reconstruct the inner product.
    This suggests that we can use the dagger, a functor, to generalise the inner product to other categories.
    
    \section{Dagger Categories: The Definition}
    \begin{dfn}{}{}
        Let \(\cat{C}\) be a category.
        A \defineindex{dagger functor} on \(\cat{C}\) is an involutive contravariant endofunctor \((-)^\dagger \colon \cat{C} \to \cat{C}\) which acts as the identity on objects.
        
        A \defineindex{dagger category} is a category equipped with a dagger.
    \end{dfn}
    
    \section{Dagger Categories: The Examples}
    \epigraph{Is this a dagger I see before me?}{Macbeth in Macbeth}
    \subsection{\texorpdfstring{\(\Set\)}{Set}}
    \begin{lma}{}{}
        It is not possible to make \(\Set\) into a dagger category.
        
        \begin{proof}
            Suppose that \(\Set\) is a dagger category, so we have a involutive contravariant functor \((-)^{\dagger} \colon \Set \to \Set\).
            Write \(\cardinality{A}\) for the cardinality of a set \(A\).
            It is known that the homset \(\Set(A, B)\) contains \(\cardinality{B}^{\cardinality{A}}\) elements.
            Similarly, \(\Set(B, A)\) has \(\cardinality{A}^{\cardinality{B}}\) elements.
            For every \(f \in \Set(A, B)\) the dagger gives us some \(f^\dagger \in \Set(B, A)\).
            Further, the dagger is involutive, so is its own inverse on morphisms.
            This means that the mapping of sets \((-)^{\dagger} \colon \Set(A, B) \to \Set(B, A)\) is a bijection.
            However, if \(A = \{1\}\) and \(B = \{1, 2\}\) then we have \(\cardinality{\Set(A, B)} = 2^1 = 2\) and \(\cardinality{\Set(B, A)} = 1^2 = 1\), so clearly these sets are not in bijection, a contradiction.
        \end{proof}
    \end{lma}
    
    \subsection{\texorpdfstring{\(\Rel\)}{Rel}}
    \begin{dfn}{{\normalsize\(\Rel\)} as a Dagger Category}{}
        The category \(\Rel\)\index{Rel@\(\Rel\)!as a dagger category} can be promoted to a dagger category by defining the dagger to be the \defineindex{converse relation}.
        If we have a relation \(R \colon A \to B\) then the converse \(R^\dagger \colon B \to A\) is
        \begin{equation}
            R^{\dagger} = \{(b, a) \mid (a, b) \in R\} \subseteq B \times A.
        \end{equation}
    \end{dfn}
    
    \begin{lma}{}{}
        \(\Rel\) along with the converse relation is a dagger category.
        \begin{proof}
            Clearly the converse relation is an involution, we need only check then that it is a contravariant functor.
            Consider first the identity relation, \(\id_A = \{(a, a) \mid a \in A\}\).
            We have
            \begin{equation}
                \id_A^{\dagger} = \{(a, a) \mid (a, a) \in \id_A\} = \id_A = \id_{A^\dagger}
            \end{equation}
            where the last step uses that the dagger acts as the identity on objects.
            Now suppose we have two relations \(R \colon A \to B\) and \(S \colon B \to C\).
            Their composite is the relation
            \begin{equation}
                S \circ R = \{(a, c) \mid \exists b \in B \text{ such that } aRb \text{ and } bSc\}.
            \end{equation}
            The converse of this is
            \begin{align}
                (S \circ R)^\dagger &= \{(c, a) \mid (a, c) \in (S \circ R)\}\\
                &= \{(c, a) \mid \exists b \in B \text{ such that } aRb \text{ and } bSc\}\\
                &= \{(c, a) \mid \exists b \in B \text{ such that } bR^\dagger a \text{ and } cS^\dagger b\}\\
                &= \{(c, a) \mid \exists b \in B \text{ such that } cS^\dagger b \text{ and } bR^\dagger a\}\\
                &= R^\dagger \circ S^\dagger.
            \end{align}
            So the converse is indeed a contravariant functor.
        \end{proof}
    \end{lma}
    
    This is another example of \(\Rel\) being more like \(\Hilb\) than \(\Set\).
    
    \subsection{\texorpdfstring{\(\Hilb\)}{Hilb}}
    \begin{dfn}{{\normalsize\(\Hilb\)} as a Dagger Category}{}
        The category \(\Hilb\)\index{Hilb@\(\Hilb\)!as a dagger category} can be promoted to a dagger category by defining the dagger to be the adjoint.
        That is, if \(f \colon H \to K\) is a bounded linear map between Hilbert spaces \(H\) and \(K\) with inner products \(\braket{-}{-}_H\) and \(\braket{-}{-}_K\) respectively then \(f^{\dagger}\) is the unique bounded linear operator such that
        \begin{equation}
            \braket{f^\dagger(k)}{h}_{H} = \braket{k}{f(h)}_{K}
        \end{equation}
        for all \(h \in H\) and \(k \in K\).
    \end{dfn}
    
    We won't prove that this is a dagger, since this would require us to get into details like proving the uniqueness part of the definition and that the adjoint of a bounded linear map is again a bounded linear map.
    Since \(\Hilb\) is our model category for defining the dagger in the first place the important properties of the dagger, like being involutive and contravariant, should be familiar already, although perhaps not in this language.
    
    Notice that the definition requires the inner product.
    It turns out that this is one of the first times where we see something done with \(\Hilb\) which can't be done with \(\Vect\).
    To see this we first note that for \(\Vect\) and two given vector spaces \(V\) and \(W\) the set \(\Vect(V, W)\) can be made into a vector space by defining pointwise addition, that is given \(\varphi, \psi \in \Vect(V, W)\) define \(\varphi + \psi \in \Vect(V, W)\) to be the linear map
    \begin{equation}
        (\varphi + \psi)(v) = \varphi(v) + \psi(v).
    \end{equation}
    Similarly, scalar multiplication is given pointwise, given \(z \in \field\) we define \(z\varphi \in \Vect(V, W)\) to be the linear map
    \begin{equation}
        (z\varphi)(v) = z\varphi(v).
    \end{equation}
    It is easy to check that these definitions make \(\Vect(V, W)\) a vector space.
    
    Now, suppose there was a dagger functor \((-)^{\dagger} \colon \Vect \to \Vect\).
    This gives a mapping of morphisms \(\Vect(V, W) \to \Vect(W, V)\).
    Specifically, choosing \(W = \field\) we have a mapping \(\Vect(V, \field) \to \Vect(\field, V)\).
    However, if \(V\) is infinite dimensional then \(\Vect(\complex, V)\) has strictly smaller dimension than \(\Vect(V, \field)\), intuitively there are more ways to squash \(V\) down into \(\field\) than there are to expand \(\field\) to fill the infinite dimensions of \(V\).
    However, this means that we cannot have a bijection \(\Vect(V, W) \to \Vect(W, V)\) for all vector spaces \(V\) and \(W\), as since \(\Vect(V, W)\) and \(\Vect(W, V)\) are vector spaces themselves such a bijection, which can also be shown to be linear, would be an isomorphism of vector spaces, and isomorphic vector spaces must have the same dimension\footnote{note that it is possible infinite dimensional vector spaces of the same dimension are not isomorphic, but being isomorphic necessitates having the same dimension. For finite dimensional vector spaces being isomorphic and having the same dimension are equivalent statements (for a fixed field).}.
    
    Finite dimensional vector spaces, \(\FVect\), don't have this problem, so we \emph{can} define a dagger \((-)^\dagger \colon \FVect \to \FVect\), by choosing an inner product, which can be done once we've chosen a basis, and then constructing adjoints.
    However, there is no \enquote{canonical} way to do this, so it's not helpful.
    A \defineindex{canonical} way to do something is, intuitively a way to do something such that we don't make any arbitrary choices, this can often be made rigorous by defining something to be the \enquote{canonical} choice if all ways of doing it end up being naturally isomorphic.
    
    \subsection{More Examples}
    \begin{exm}{}{}
        \begin{itemize}
            \item Any monoid, \(M\), with an involutive map \(f \colon M \to M\) can be considered as a one-object dagger category by taking the dagger to be \(m^\dagger = f(m)\) for all \(m \in M\), recall that when viewing a monoid as a one-object category the elements of the monoid are the morphisms of the category.
            \item Any groupoid (category where all morphisms are isomorphisms) is a dagger category with the dagger being the inverse.
            \item Any discrete category is trivially a dagger category.
            \item Let \(\cat{C}\) and \(\cat{D}\) be dagger categories and \(F, G \colon \cat{C} \to \cat{D}\) functors.
            Then given a natural transformation \(\zeta\colon F \naturalTransformation G\) we can define a dagger componentwise by defining \(\zeta^\dagger\colon G \naturalTransformation F\) to be the natural transformation whose component at \(A \in \Ob(\cat{D})\) is \(\zeta_A^\dagger\).
            This makes the functor category \(\functorCategory{\cat{C}}{\cat{D}}\) into a dagger category.
        \end{itemize}
    \end{exm}
    
    \section{Terminology}
    We now introduce some terminology useful for talking about special properties of morphisms under daggers.
    \begin{dfn}{}{}
        A morphism \(f \colon A \to B\) in a dagger category is
        \begin{itemize}
            \item the \defineindex{adjoint} of \(g \colon B \to A\) if \(g = f^\dagger\);
            \item \defineindex{self-adjoint} if \(f = f^\dagger\) (requiring \(A = B\));
            \item a \defineindex{projection} if\footnote{sometimes the requirement of being self-adjoint is left out of this definition} \(f = f^\dagger\) and \(f \circ f = f\);
            \item \defineindex{unitary} when \(f^\dagger \circ f = \id_A\) and \(f \circ f^\dagger = \id_B\);
            \item an \defineindex{isometry} when \(f^\dagger \circ f = \id_A\);
            \item a \defineindex{partial isometry} when \(f^\dagger \circ f\) is a projection;
            \item \defineindex{positive} when \(f = g^\dagger \circ g\) for some morphism \(g \colon A \to A\) (requiring \(A = B\)).
        \end{itemize}
    \end{dfn}
    Many of these should be familiar from \(\Hilb\).
    Note that the definition of positive is inspired by \(\complex\) with the dagger being complex conjugate where \(z^*z = \abs{z}^2\) is non-negative.
    
    \section{Graphical Notation}
    Since daggers swap the direction of functions we can depict them in the graphical notation as a rotation in the horizontal axis:
    \begin{equation}
        \tikzsetnextfilename{dagger-graphical-notation-1}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[morphism] (f) {\(\phantomrlap{\,f}{f^\dagger}\)};
            \draw[wire] (f) -- ++ (0, 0.75) node [above] {\(B\)};
            \draw[wire] (f) -- ++ (0, -0.75) node [below] {\(A\)};
            \draw[|->] (0.5, 0) -- (1.5, 0) node [midway, above] {\((-)^\dagger\)};
            \node[morphism] (fdagger) at (2, 0) {\(f^\dagger\)};
            \draw[wire] (fdagger) -- ++ (0, 0.75) node [above] {\(A\)};
            \draw[wire] (fdagger) -- ++ (0, -0.75) node [below] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    Rather than writing daggers on every morphism that we dagger we instead use a shape which breaks the horizontal mirror symmetry, so that we can reflect it and it is clearly different without having to add in a dagger:
    \begin{equation}
        \tikzsetnextfilename{dagger-graphical-notation-2}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[daggerable morphism] (f) {\(f\)};
            \draw[wire] (f) -- ++ (0, 0.75) node [above] {\(B\)};
            \draw[wire] (f) -- ++ (0, -0.75) node [below] {\(A\)};
            \draw[|->] (0.75, 0) -- (1.5, 0) node [midway, above] {\((-)^\dagger\)};
            \node[daggered morphism] (fdagger) at (2, 0) {\(f\)};
            \draw[wire] (fdagger) -- ++ (0, 0.75) node [above] {\(A\)};
            \draw[wire] (fdagger) -- ++ (0, -0.75) node [below] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    Notice that if \(a \colon I \to A\) is a state in a dagger category then \(a^\dagger \colon A \to I\) is an effect in the same category.
    So daggers relate states and effects:
    \begin{equation}
        \tikzsetnextfilename{dagger-states-and-effects}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[state] (a state) at (0, -0.25) {\(a\)};
            \draw[wire] (a state) -- ++ (0, 0.75) node [above] {\(A\)};
            \draw[|->] (0.75, 0.15) -- (1.5, 0.15) node [midway, above] {\((-)^\dagger\)};
            \draw[|->] (1.5, -0.15) -- (0.75, -0.15) node [midway, below] {\((-)^\dagger\)};
            \node[effect] (a effect) at (2.25, 0.25) {\(a\)};
            \draw[wire] (a effect) -- ++ (0, -0.75) node [below] {\(A\)};
        \end{tikzpicture}
    \end{equation}
    
    We can use this to generalise the inner product and bra-ket notation to any dagger category by defining
    \begin{equation}
        \braket{a}{b} \coloneqq \hspace{0.2cm}
        \tikzsetnextfilename{dagger-generalise-bra-ket}
        \begin{tikzpicture}[baseline=(equal.base)]
            \node[effect] (a) {\(\phantomrlap{a}{b}\)};
            \node[state, below=of a] (b) {\(b\)};
            \draw[wire] (a) -- (b);
            \node (equal) at ($(a)!0.5!(b) + (1, 0)$) {\(=\)};
            \node[effect] (a2) at (2, -0.48) {\(\phantomrlap{a}{b}\)};
            \node[state, below=-0.005cm of a2] (b2) {\(b\)};
        \end{tikzpicture}
        .
    \end{equation}
    That is, for \(a \colon A \to I\) and \(b \colon I \to A\)
    \begin{equation}
        \braket{a}{b} \coloneqq a \circ b.
    \end{equation}
    
    \section{Monoidal Dagger Categories}
    In order to use both monoidal categories and dagger categories at the same time we want the two functors \(-\otimes-\) and \((-)^\dagger\) to play nicely together, we get this through the following definition.
    \begin{dfn}{Monoidal Dagger Category}{}
        A \defineindex{monoidal dagger category} is a dagger category which is also monoidal such that
        \begin{itemize}
            \item \((f \otimes g)^\dagger = f^\dagger \otimes g^\dagger\) for all morphisms \(f\) and \(g\);
            \item the associator and unitors are unitary at every object.
        \end{itemize}
    
        A \defineindex{braided monoidal dagger category} is a monoidal dagger category equipped with a unitary braiding.
        
        A \defineindex{symmetric monoidal dagger category} is a braided monoidal dagger category for which the braiding is symmetric.
    \end{dfn}
    
    More compactly, a (braided or symmetric) monoidal dagger category is exactly what you would expect with the requirements that all of the natural isomorphisms involved in the definition of the (braided or symmetric) monoidal part are unitary and the dagger on the tensor product reverses the order of the morphisms: \((f\otimes g)^\dagger = f^\dagger \otimes g^\dagger\).
    
    \begin{lma}{}{}
        \(\Hilb\) is a symmetric monoidal dagger category.
        
        \begin{proof}
            Let \(H\), \(J\), \(K\), \(L\) be Hilbert spaces.
            Suppose we have two morphisms \(f \colon H \to K\) and \(g \colon J \to L\).
            Then we have a morphism \(f \otimes g \colon H \otimes J \to K \otimes L\).
            With \(H \otimes J\) and \(K \otimes L\) Hilbert spaces with inner products \(\braket{-}{-}_{H\otimes J}\) and \(\braket{-}{-}_{K\otimes L}\).
            The adjoint to \(f \otimes g\) is defined to be the unique linear map \((f \otimes g)^\dagger \colon K \otimes L \to H \otimes J\) satisfying
            \begin{equation}
                \braket{(f \otimes g)^\dagger(v' \otimes w')}{v \otimes w}_{H\otimes J} = \braket{v' \otimes w'}{(f \otimes g)(v \otimes w)}_{K \otimes L}
            \end{equation}
            for all \(v \in H\), \(w \in J\), \(v' \in K\), and \(w' \in L\).
            We then have
            \begin{align}
                \braket{v' \otimes w'}{(f \otimes g)(v \otimes w)}_{K \otimes L} &= \braket{v' \otimes w'}{f(v) \otimes g(w)}_{K\otimes L}\\
                &= \braket{v'}{f(v)}_K \braket{w'}{g(w)}_{L}\\
                &= \braket{f^\dagger(v')}{v}_H \braket{g^\dagger(w')}{w}_J\\
                &= \braket{(f^\dagger \otimes g^\dagger)(v' \otimes w')}{v \otimes w}_{H \otimes J},\notag
            \end{align}
            so we have \((f \otimes g)^\dagger = f^\dagger \otimes g^\dagger\).
            
            For unitarity first consider the left unitor, \(\lambda\).
            Fixing some Hilbert space, \(H\), the left unitor gives an isomorphism \(\lambda_H \colon I \otimes H \to H\).
            The dagger of this is defined as the unique morphisms such that
            \begin{equation}
                \braket{\lambda_H^\dagger(v)}{1 \otimes w}_{I \otimes H} = \braket{v}{\lambda_H(1 \otimes w)}_{H} = \braket{v}{w}_H.
            \end{equation}
            Clearly, this holds if \(\lambda_H^\dagger = \lambda_H^{-1}\) since
            \begin{equation}
                \braket{\lambda_H^{-1}(v)}{1 \otimes w}_{I \otimes H} = \braket{1 \otimes v}{1 \otimes w}_{I\otimes H} = \braket{1}{1}_{I} \braket{v}{w}_{H} = \braket{v}{w}_{H}.
            \end{equation}
            So by uniqueness we have \(\lambda_H^\dagger = \lambda_H^{-1}\), hence \(\lambda_H\) is unitary.
            Unitarity of the right unitor follows similarly.
            
            Now consider the associator, \(\alpha\).
            Fixing some Hilbert spaces \(H\), \(J\), and \(K\), we get an isomorphism \(\alpha_{H,J,K} \colon (H \otimes J) \otimes K \to H \otimes (J \otimes K)\).
            Then \(\alpha_{H,J,K}^\dagger\) is defined as the unique morphism such that
            \begin{align}
                &\braket{\alpha_{H,J,K}^\dagger(u \otimes (v \otimes w))}{(u' \otimes v') \otimes w'}_{(H\otimes J)\otimes K}\\
                &\qquad= \braket{u \otimes (v \otimes w)}{\alpha_{H,J,K}((u' \otimes v') \otimes w')}_{H\otimes(J\otimes K)}\\
                &\qquad= \braket{u \otimes (v \otimes w)}{(u' \otimes (v' \otimes w'))}_{H\otimes(J\otimes K)}.
            \end{align}
            Clearly this holds if \(\alpha_{H,J,K}^\dagger = \alpha_{H,J,K}^{-1}\):
            \begin{align}
                &\braket{\alpha_{H,J,K}^{-1}(u \otimes (v \otimes w))}{(u' \otimes v') \otimes w'}_{(H\otimes J)\otimes K}\\
                &\qquad= \braket{(u \otimes v) \otimes w}{(u' \otimes v') \otimes w'}_{(H\otimes J)\otimes K}\\
                &\qquad= \braket{(u \otimes v)}{u' \otimes v'}_{H\otimes J}\braket{w}{w'}_K\\
                &\qquad= \braket{u}{u'}_H \braket{v}{v'}_J \braket{w}{w'}_K\\
                &\qquad= \braket{u}{u'}_{H} \braket{v \otimes w}{v' \otimes w'}_{J \otimes K}\\
                &\qquad= \braket{u \otimes (v \otimes w)}{u' \otimes (v' \otimes w')}_{H\otimes(J\otimes K)}.
            \end{align}
            So by uniqueness \(\alpha_{H,J,K}^\dagger = \alpha_{H,J,K}^{-1}\), and so \(\alpha_{H,J,K}\) is unitary.
        \end{proof}
    \end{lma}
    
    \(\Rel\) is also a symmetric monoidal dagger category.
    
    \part{Duals}
    \chapter{Duals}
    \section{Duals: The Idea}
    Consider some Hilbert space, \(H\).
    We can construct its dual space, \(H^* \coloneqq \Hilb(H, \complex)\).
    This is also a Hilbert space with vector addition given by pointwise addition, so for \(\varphi, \psi \in \Hilb(H, \complex)\) we define \(\varphi + \psi \in \Hilb(H, \complex)\) to be the bounded linear map
    \begin{equation}
        (\varphi + \psi)(v) = \varphi(v) + \psi(v)
    \end{equation}
    and for \(z \in \complex\) we define \(z\varphi \in \Hilb(H, \complex)\) to be the bounded linear map
    \begin{equation}
        (z\varphi)(v) = z\varphi(v).
    \end{equation}
    The inner product on \(H^*\) follows from the Riesz representation theorem, which states that for all \(\varphi \in H^*\) there exists a unique \(f_\varphi \in H\) such that
    \begin{equation}
        \varphi(v) = \braket{f_\varphi}{v}
    \end{equation}
    for all \(v \in H\).
    Using this we define an inner product on \(H^*\) according to
    \begin{equation}
        \braket{\varphi}{\psi}_{H^*} = \braket{f_{\varphi}}{f_{\psi}}_{H}.
    \end{equation}
    
    This idea generalises to monoidal categories, although it is not particularly obvious from the definition.
    
    \section{Duals: The Definition}
    \begin{dfn}{Dual}{}
        Let \((\cat{C}, \otimes, I, \lambda, \rho, \alpha)\) be a monoidal category.
        An object \(L \in \Ob(\cat{C})\) is \define{left dual}\index{left dual!object} to an object \(R \in \Ob(\cat{C})\), and \(R\) is \defineindex{right dual}\index{right dual!object} to \(L\), which we write as \(L \leftdual R\), when there exist morphisms
        \begin{itemize}
            \item \(\eta \colon I \to R \otimes L\), called the \defineindex{unit morphism}; and
            \item \(\varepsilon \colon L \otimes R \to I\), called the \defineindex{counit morphism}
        \end{itemize}
        such that the following diagrams commute
        \begin{gather}
            \begin{tikzcd}[ampersand replacement=\&]
                L \arrow[r, "\rho_L^{-1}"] \arrow[d, "\id_L"'] \& L \otimes I \arrow[r, "{\id_L} \otimes \eta"] \& L \otimes (R \otimes L) \arrow[d, "\alpha_{L,R,L}^{-1}"]\\
                L \& I \otimes L \arrow[l, "\lambda_L"] \& (L \otimes R) \otimes R, \arrow[l, "\varepsilon \otimes \id_L"]
            \end{tikzcd}
            \\
            \begin{tikzcd}[ampersand replacement=\&]
                R \arrow[r, "\lambda_R^{-1}"] \arrow[d, "\id_R"'] \& I \otimes R \arrow[r, "\eta \otimes \id_R"] \& (R \otimes L) \otimes R \arrow[d, "\alpha_{R,L,R}"]\\
                R \& R \otimes I \arrow[l, "\rho_R"] \& R \otimes (L \otimes R). \arrow[l, "{\id_R} \otimes \varepsilon"]
            \end{tikzcd}
        \end{gather}
                
        If \(L\) is both left and right-dual to \(R\) we call \(L\) the \define{dual}\index{dual!object} of \(R\).
    \end{dfn}
    
    Consider the first diagram here.
    It says that we can add \(R \otimes L\) on the right of \(L\) and then remove \(L \otimes R\) from the left and this is the same as doing nothing.
    The second diagram is similar but swapping \(R\) and \(L\).
    
    Notice that \(\eta\) is a state of \(R\otimes L\).
    We will later show that it is an entangled state, which is useful for quantum mechanics stuff.
    
    \section{Graphical Notation}
    Suppose we have a category and two objects \(L\) and \(R\) with \(L \leftdual R\).
    In the graphical notation we draw \(L\) as a wire with an arrow pointing upwards, and we draw \(R\) as a wire with an arrow pointing down:
    \begin{equation}
        \tikzsetnextfilename{duals-graphical-notation-dual-objects}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[left dual] (0, 0) node [below] {\(L\)} -- (0, 1);
            \draw[right dual] (2, 0) node [below] {\(R\)} -- (2, 1);
        \end{tikzpicture}
        .
    \end{equation}
    
    We draw the unit morphism, \(\eta \colon I \to R \otimes L\), as a bent wire:
    \begin{equation}
        \tikzsetnextfilename{duals-graphical-notation-unit-morphism}
        \begin{tikzpicture}
            \node[morphism, minimum width=1.5cm] (eta) {\(\eta\)};
            \draw[wire] ($(eta.north) + (-0.5, 0)$) -- ++ (0, 0.75) node [above] {\(R\)} coordinate (top);
            \draw[wire] ($(eta.north) + (0.5, 0)$) -- ++ (0, 0.75) node [above] {\(L\)};
            \draw[right dual] (2, 0 |- eta.north) coordinate (A) -- (2, 0 |- top);
            \draw[wire] (A) arc (180:360:0.5);
            \draw[left dual] (3, 0 |- eta.north) -- (3, 0 |- top);
            \node (equal) at (1.3, 0) {\(=\)};
        \end{tikzpicture}
        .
    \end{equation}
    Similarly, the counit morphism, \(\varepsilon \colon L \otimes R \to I\) is drawn as
    \begin{equation}
        \tikzsetnextfilename{duals-graphcial-notation-counit-morphism}
        \begin{tikzpicture}
            \node[morphism, minimum width=1.5cm] (epsilon) {\(\varepsilon\)};
            \draw[wire] ($(epsilon.south) + (-0.5, 0)$) -- ++ (0, -0.75) node [below] {\(L\)} coordinate (bottom);
            \draw[wire] ($(epsilon.south) + (0.5, 0)$) -- ++ (0, -0.75) node [below] {\(R\)};
            \draw[left dual] (2, 0 |- bottom) -- (2, 0 |- epsilon.south) coordinate (A);
            \draw[wire] (A) arc (180:0:0.5);
            \draw[right dual] (3, 0 |- bottom) -- (3, 0 |- epsilon.south);
            \node (equal) at (1.3, 0) {\(=\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    The duality conditions then becomes
    \begin{equation}
        \tikzsetnextfilename{duals-graphical-notation-snake-1}
        \begin{tikzpicture}[baseline=(equal.base)]
            \draw[left dual] (0, 0) -- (0, 1);
            \draw[wire] (0, 1) arc (180:0:0.5) coordinate (A);
            \draw[left dual] (A) -- ++ (0, -0.2) coordinate (B);
            \draw[wire] (B) arc (180:360:0.5) coordinate (C);
            \draw[left dual] (C) -- ++ (0, 1.2);
            \node (equal) at (2.5, 1) {\(=\)};
            \draw[left dual] (3, 0) -- (3, 2);
        \end{tikzpicture}
        \qqand
        \tikzsetnextfilename{duals-graphical-notation-snake-2}
        \begin{tikzpicture}[baseline=(equal.base)]
            \draw[left dual] (0, 2) -- (0, 1);
            \draw[wire] (0, 1) arc (180:360:0.5) coordinate (A);
            \draw[left dual] (A) -- ++ (0, 0.2) coordinate (B);
            \draw[wire] (B) arc (180:0:0.5) coordinate (C);
            \draw[left dual] (C) -- ++ (0, -1.2);
            \node (equal) at (2.5, 1) {\(=\)};
            \draw[right dual] (3, 0) -- (3, 2);
        \end{tikzpicture}
    \end{equation}
    which are known as the \defineindex{snake equations}.
    
    It can be a bit difficult to see how this is the same as the duality condition at first.
    Consider the first snake equation.
    At the bottom we have a single wire with an arrow pointing up, so \(L\), the space next to this can represent the unit, so we have \(L \otimes I\).
    Then we're applying \(\eta\) to this identity, giving \(L \otimes (R \otimes L)\).
    Next we reassociate, giving \((L \otimes R) \otimes L\), then apply \(\varepsilon\), giving \(I \otimes L\), and finally we remove the unit, leaving us with just \(L\), and the line on the right hand side is just \(\id_L\).
    It can be helpful to break the diagram up into the following sections:
    \begin{equation}
        \tikzsetnextfilename{duals-graphical-notation-snake-breakdown}
        \begin{tikzpicture}[baseline=(current bounding box), font=\scriptsize]
            \draw[left dual] (0, 0) -- (0, 1) node [midway, left, text=highlight] {\(L\)};
            \draw[wire] (0, 1) arc (180:0:0.5) coordinate (A);
            \draw[left dual] (A) -- ++ (0, -0.2) coordinate (B)  node [pos=0.7, left, text=highlight, xshift=0.075cm] {\(R\)};
            \draw[wire] (B) arc (180:360:0.5) coordinate (C);
            \draw[left dual] (C) -- ++ (0, 1.2) node [midway, right, text=highlight] {\(L\)};
            \draw[thick, Red] (0.5, -0.2) -- (0.5, 1);
            \draw[thick, Red] (0.5, 0.2) -- (2.2, 0.2);
            \draw[thick, Red] (-0.2, 1) -- (2.2, 1);
            \draw[thick, Red] (1.5, 1) -- (1.5, 2.2);
            \draw[thick, Red] (1.5, 1.6) -- (-0.2, 1.6);
            \node[Red] at (0.5, 1.3) {\(\varepsilon\)};
            \node[Red] at (1.5, 0.5) {\(\eta\)};
            \node[Navy!50] at (1.5, 0) {\(I\)};
            \node[Navy!50] at (0.5, 1.8) {\(I\)};
            \node[Blue] at (0.25, 0.2) {1};
            \node[Blue] at (0.8, 0) {2};
            \node[Blue] at (0.8, 0.4) {3};
            \node[Blue] at (1.2, 1.3) {4};
            \node[Blue] at (1.2, 1.8) {5};
            \node[Blue] at (1.7, 1.6) {8};
        \end{tikzpicture}
        .
    \end{equation}
    We can also draw this as
    \begin{equation}
        \tikzsetnextfilename{duals-graphical-notation-snake-rewrite}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[morphism, minimum width=1.5cm] (eta) at (1.5, 0.5) {\(\eta\)};
            \draw[wire, dashed] (1.5, 0) -- (eta);
            \node[morphism, minimum width=1.5cm] (epsilon) at (0.5, 1.5) {\(\phantomrlap{\varepsilon}{\eta}\)};
            \draw[wire, dashed] (epsilon) -- (0.5, 2);
            \draw[wire] (0, 0) node [below] {\(L\)} -- (0, 0 |- epsilon.south);
            \draw[wire] (1, 0 |- epsilon.south) -- (1, 0 |- eta.north) node [midway, left] {\(R\)};
            \draw[wire] (2, 0 |- eta.north) -- (2, 2) node [above] {\(L\)};
        \end{tikzpicture}
        .
    \end{equation}
    
    \section{Duals: The Examples}
    \subsection{\texorpdfstring{\(\FHilb\)}{Hilb}}
    The monoidal category \(\FHilb\) has all duals.
    Every finite-dimensional Hilbert space is both left and right dual to its dual space, \(H^*\) in a canonical way.
    The counit of the duality \(H \leftdual H^*\) is given by the map
    \begin{align}
        \varepsilon \colon H \otimes H^* &\to \complex\\
        \ket{\varphi} \otimes \bra{\psi} &\mapsto \braket{\varphi}{\psi}.
    \end{align}
    Given an orthonormal basis, \(\{\ket{i}\}\), the unit is given by the map
    \begin{align}
        \eta \colon \complex \to H^* \otimes H\\
        1 \mapsto \sum_i \bra{i} \otimes \ket{i}
    \end{align}
    extended by linearity to all of \(\complex\).
    We'll show later that despite using a basis in this definition the unit morphism is basis independent.
    
    We now need to show that the snake equations hold.
    We'll do the first one here, using the version split into different regions above.
    Region 1 consists of just \(L\), which in this case is \(H\).
    We can show that the snake equation holds on the basis vectors, and then by linearity it holds for all vectors, so consider some specific basis vector \(\ket{j}\) for fixed \(j\).
    The first thing we do is affix the unit on the right, using \(\rho_H^{-1}\), which gives us \(\ket{j} \otimes 1\), having now accounted for region 2.
    Region 3 says to apply \({\id_H} \otimes \eta\), which gives
    \begin{equation}
        \ket{j} \otimes \left( \sum_i \bra{i} \otimes \ket{i} \right) = \sum_i \ket{j} \otimes (\bra{i} \otimes \ket{i}) \isomorphic \sum_i (\ket{j} \otimes \bra{i}) \otimes \ket{i}
    \end{equation}
    having used linearity of the tensor product to pull the sum out, and then the associator to move the brackets.
    Now region 5 tells us to apply \(\varepsilon \otimes \id_H\), which gives
    \begin{equation}
        \sum_i \braket{i}{j} \otimes \ket{i} = \sum_i \delta_{ij} \otimes \ket{i} = 1 \otimes \ket{j}
    \end{equation}
    having used the orthonormality of the basis and then the fact that \(0 \otimes \ket{i}\) gives zero when we act on it with the left unitor, and so the final step, region 6, of applying the left unitor gives us \(\ket{j}\).
    So we see that this is indeed the identity.
    
    Infinite dimensional Hilbert spaces don't have duals.
    The issue is with the sum
    \begin{equation}
        \sum_i \bra{i} \otimes \ket{i},
    \end{equation}
    which is not defined in the infinite dimensional case.
    
    \subsection{\texorpdfstring{\(\Mat\)}{Mat}}
    Recall that the category \(\Mat\) has natural numbers as objects and \(\Mat(n, m)\) consists of all \(m \times n\) matrices with entries in \(\field\).
    This is a monoidal category with the monoidal product on morphisms given by the Kronecker product of matrices and on objects by the normal product of natural numbers.
    The unit object is the natural number \(1\).
    
    In this category every object is self-dual with the canonical choice of unit and counit being
    \begin{equation}
        \eta \colon 1 \mapsto \sum_i \ket{i} \otimes \ket{i}, \qqand \varepsilon \colon \ket{i} \otimes \ket{j} \mapsto \delta_{ij}1
    \end{equation}
    where we give linear maps to be interpreted as matrices.
    
    \subsection{\texorpdfstring{\(\Rel\)}{Rel}}
    In \(\Rel\) every object is self-dual.
    The unit morphism, \(\colon 1 \mapsto S \times S\), and counit morphism, \(S \times S \to 1\), with \(1 = \{\bullet\}\) being the singleton set, and \(S\) an arbitrary set, are defined to be the relations
    \begin{itemize}
        \item \(\bullet \sim_\eta (s, s)\) for all \(s \in S\); and
        \item \((s, s) \sim_\varepsilon \bullet\) for all \(s \in S\)
    \end{itemize}
    respectively.
    
    We should check that the snake equations hold, again we'll check the first here.
    We start with \(s \in S\), acting with the right unitor we have that \(s \sim_{\rho_S} (s, s)\).
    Acting with \({\id_S} \otimes \eta\) we get \((s, s) \sim_{{\id_S} \otimes \eta} (s, (t, t))\) for all \(t \in S\).
    Reassociating we have \((s, (t, t)) \sim_\alpha ((s, t), t)\).
    Acting with \(\varepsilon \otimes \id_S\) we have \(((s, t), t) \sim_{\varepsilon \otimes \id_S} (\bullet, t)\) where we now fix \(t = s\), since if this is not the case then there is no relation here, compare this with the use of \(\delta_{ij}\) in the case of \(\Hilb\).
    Finally, using the left unitor we have \((\bullet, t) \sim_{\lambda_S} t\), with \(t = s\), so this is indeed the identity relation as chaining these relations together we get \(s \sim s\).
    
    \subsection{\texorpdfstring{\(\Set\)}{Set}}
    The category of sets is quite boring when it comes to duals, only singletons have duals, and up to isomorphism there's only one singleton, and it is self dual.
    To show this we make the following definitions.
    
    \begin{dfn}{Names and Conames}{}
        Consider some monoidal category with dualities \(A \leftdual A^*\) and \(B \leftdual B^*\).
        Given a morphism \(f \colon A \to B\) its
        \begin{itemize}
            \item \defineindex{name}, \(\name{f} \colon I \to A^* \otimes B\); and
            \item \defineindex{coname}, \(\coname{f} \colon A \otimes B^* \to I\),
        \end{itemize}
        are the morphisms
        \begin{equation}
            \tikzsetnextfilename{duals-name}
            \begin{tikzpicture}[baseline=(f.base)]
                \node[daggerable morphism] (f) {\(f\)};
                \draw[left dual=0.7] (f) -- ++ (0, 0.75) node [above] {\(B\)};
                \draw[right dual] (-1, 0.75) node [above] {\(A^*\)} -- ++ (0, -1.5);
                \draw[wire] (-1, -0.75) arc (180:360:0.5);
                \draw[left dual=0.6] (0, -0.75) -- (f) node [midway, right] {\(A\)};
            \end{tikzpicture}
        \end{equation}
        and
        \begin{equation}
            \tikzsetnextfilename{duals-coname}
            \begin{tikzpicture}[baseline=(f.base)]
                \node[daggerable morphism] (f) {\(f\)};
                \draw[right dual] (f) -- ++ (0, -0.75) node [below] {\(A\)};
                \draw[left dual=0.6] (f) -- ++ (0, 0.75) node [midway, left] {B};
                \draw[wire] (0, 0.75) arc (180:0:0.5);
                \draw[left dual] (1, 0.75) -- (1, -0.75) node [below] {\(B^*\)};
            \end{tikzpicture}
        \end{equation}
        respectively.
    \end{dfn}
    
    Note that the name is defined using the unit of the duality \(A \leftdual A^*\), and the coname is defined using the counit of the duality \(B \leftdual B^*\).
    
    The useful thing about names/conames is that we can recover the morphism from them.
    To recover the morphism from the coname we just have to add in a unit (and an associator) forming the morphism
    \begin{equation}
        \tikzsetnextfilename{duals-coname-recover-morphism}
        \begin{tikzpicture}[baseline=(f.base)]
            \node[daggerable morphism] (f) {\(f\)};
            \draw[right dual] (f) -- ++ (0, -0.75) node [below] {\(A\)};
            \draw[left dual=0.6] (f) -- ++ (0, 0.75) node [midway, left] {B};
            \draw[wire] (0, 0.75) arc (180:0:0.5);
            \draw[left dual] (1, 0.75) -- (1, 0) node [midway, right] {\(B^*\)};
            \draw[wire] (1, 0) arc (180:360:0.5);
            \draw[left dual] (2, 0) -- (2, 1.5) node [above] {\(B\)};
            \node at (2.5, 0) {\(=\)};
            \node[daggerable morphism] (f2) at (3.1, 0) {\(f\)};
            \draw[right dual] (f2) -- ++ (0, -0.75) node [below] {\(A\)};
            \draw[left dual=0.6] (f2) -- ++ (0, 1.5) node [above] {\(B\)};
        \end{tikzpicture}
    \end{equation}
    having applied one of the snake equations of \(B \leftdual B^*\) to straighten out the wire.
    Similarly to recover the morphism from the name we just have to add in a counit (and an associator) forming the morphism
    \begin{equation}
        \tikzsetnextfilename{duals-name-recover-morphism}
        \begin{tikzpicture}[baseline=(f.base)]
            \node[daggerable morphism] (f) {\(f\)};
            \draw[left dual=0.7] (f) -- ++ (0, 0.75) node [above] {\(B\)};
            \draw[right dual] (-1, 0) -- ++ (0, -0.75) node [midway, left] {\(A^*\)};
            \draw[wire] (-1, -0.75) arc (180:360:0.5);
            \draw[left dual=0.6] (0, -0.75) -- (f) node [midway, right] {\(A\)};
            \draw[wire] (-1, 0) arc (0:180:0.5);
            \draw[right dual] (-2, 0) -- ++ (0, -1.5) node [below] {\(A\)};
            \node at (0.75, 0) {\(=\)};
            \node[daggerable morphism] (f2) at (1.4, 0) {\(f\)};
            \draw[left dual] (f2) -- ++ (0, 0.75) node [above] {\(B\)};
            \draw[right dual] (f2) -- ++ (0, -1.5) node [below] {\(A\)};
        \end{tikzpicture}
    \end{equation}
    having applied one of the snake equations of \(A \leftdual A^*\) to straighten out the wire.
    
    In \(\Set\) the unit object, the singleton, \(1\), is terminal, meaning all morphisms into \(1\) from the same object are equal.
    This means that all conames \(\coname{f} \colon A \otimes B^* \to 1\) must be equal.
    Thus by the above construction all morphisms \(f \colon A \to B\) are equal.
    This can only be the case if \(A \isomorphic B \isomorphic 1\), or if \(B = \emptyset\), but \(\emptyset\) does not have a dual since there is no function \(1 \to \emptyset \times \emptyset^*\) for any set \(\emptyset^*\), since \(\emptyset \times X = \emptyset\) for any set \(X\), and there are no functions into \(\emptyset\), apart from the identity \(\emptyset \to \emptyset\), which isn't useful here as the domain we're interested in is 1.
    
    \chapter{Properties of Duals} 
    \section{Duals are Unique up to Isomorphism}
    Often in maths after making a definition one of the first things we ask is about the uniqueness of the thing that we are defining.
    Strict uniqueness is too strong a condition when it comes to uniqueness of many categorical definitions.
    Instead we look at the weaker condition of uniqueness up to isomorphism.
    That is, if some property defines an object up to isomorphism and both \(A\) and \(B\) have this property then we must have \(A \isomorphic B\).
    Duals are unique up to isomorphism.
    
    \begin{lma}{Duals are Unique up to Isomorphism}{}
        Let \(\cat{C}\) be a monoidal category with a duality \(L \leftdual R\), then, \(L \leftdual R'\) if and only if \(R \isomorphic R'\).
        Similarly \(L' \leftdual R\) if and only if \(L \isomorphic L'\).
        
        \begin{proof}
            We start with the forwards implication.
            Suppose that we have the dualities \((L, R, \eta, \varepsilon)\) and \((L, R', \eta', \varepsilon')\).
            We then want to define a map \(R \to R'\) and show that this map is an isomorphism.
            It is possible to construct a map \(R \to R'\) using the unit \(\eta'\) of \(L \leftdual R'\), and the counit \(\varepsilon\) of \(L \leftdual R\):
            \begin{equation}
                \tikzsetnextfilename{duals-unique-proof-1}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[left dual] (0, 2) node [above] {\(R'\)} -- (0, 1);
                    \draw[wire] (0, 1) arc (180:360:0.5) coordinate (A);
                    \draw[left dual] (A) -- ++ (0, 0.2) coordinate (B) node [midway, right] {\(L\)};
                    \draw[wire] (B) arc (180:0:0.5) coordinate (C);
                    \draw[left dual] (C) -- ++ (0, -1.2) node [below] {\(R\)};
                    \node[above] at (1.5, 1.75) {\(\varepsilon\)};
                    \node[below] at (0.5, 0.5) {\(\eta'\)};
                \end{tikzpicture}
                .
            \end{equation}
            Similarly, we can define a map \(R' \to R\) using \(\eta\) and \(\varepsilon'\):
            \begin{equation}
                \tikzsetnextfilename{duals-unique-proof-2}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[left dual] (0, 2) node [above] {\(R\)} -- (0, 1);
                    \draw[wire] (0, 1) arc (180:360:0.5) coordinate (A);
                    \draw[left dual] (A) -- ++ (0, 0.2) coordinate (B) node [midway, right] {\(L\)};
                    \draw[wire] (B) arc (180:0:0.5) coordinate (C);
                    \draw[left dual] (C) -- ++ (0, -1.2) node [below] {\(R'\)};
                    \node[above] at (1.5, 1.75) {\(\varepsilon'\)};
                    \node[below] at (0.5, 0.5) {\(\eta\)};
                \end{tikzpicture}
                .
            \end{equation}
            Now computing the composite we have
            \begin{equation*}
                \tikzsetnextfilename{duals-unique-proof-3}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[left dual] (0, 2) node [above] {\(R\)} coordinate (top) -- (0, 1);
                    \draw[wire] (0, 1) arc (180:360:0.5) coordinate (A);
                    \draw[left dual] (A) -- ++ (0, 0.2) coordinate (B) node [midway, right] {\(L\)};
                    \draw[wire] (B) arc (180:0:0.5) coordinate (C);
                    \draw[left dual] (C) -- ++ (0, -2.2) node [midway, left] (Rprime) {\(R'\)};
                    \node[above] at (1.5, 1.75) {\(\varepsilon'\)};
                    \node[below] at (0.5, 0.5) {\(\eta\)};
                    \begin{scope}[xshift=2cm, yshift=-2cm]
                        \draw[wire] (0, 1) arc (180:360:0.5) coordinate (A);
                        \draw[left dual] (A) -- ++ (0, 0.2) coordinate (B) node [midway, right] {\(L\)};
                        \draw[wire] (B) arc (180:0:0.5) coordinate (C);
                        \draw[left dual] (C) -- ++ (0, -1.2) node [below] {\(R\)} coordinate (bottom);
                        \node[above] at (1.5, 1.75) {\(\varepsilon\)};
                        \node[below] at (0.5, 0.5) {\(\eta'\)};
                    \end{scope}
                    \node[right= 2.25cm of Rprime] {\(\equaliso\)};
                    \fill[highlight!50] (6, -1) rectangle (9, 1);
                    \draw[right dual] (5.5, -1) -- (5.5, 0 |- top) node [above] {\(R'\)};
                    \draw[wire] (5.5, -1) arc (180:360:0.5);
                    \draw[left dual] (6.5, -1) -- (6.5, 0.25);
                    \draw[wire] (6.5, 0.25) arc (180:0:0.5);
                    \draw[left dual] (7.5, 0.25) -- (7.5, -0.25);
                    \draw[wire] (7.5, -0.25) arc (180:360:0.5);
                    \draw[left dual] (8.5, -0.25) -- (8.5, 1);
                    \draw[wire] (8.5, 1) arc (180:0:0.5);
                    \draw[left dual] (9.5, 1) -- (9.5, 0 |- bottom) node [below] {\(R\)};
                \end{tikzpicture}
                .
            \end{equation*}
            We can identify the shaded region as the snake equation for the duality \(L \leftdual R\), and so we can replace it with \(\id_L\).
            This leaves us with
            \begin{equation}
                \tikzsetnextfilename{duals-unique-proof-4}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[right dual] (5.5, -1) -- (5.5, 2) node [above] {\(R\)};
                    \draw[wire] (5.5, -1) arc (180:360:0.5);
                    \draw[left dual] (6.5, -1) -- (6.5, 1);
                    \draw[wire] (6.5, 1) arc (180:0:0.5);
                    \draw[right dual] (7.5, 1) -- (7.5, -2) node [below] {\(R\)};
                    \node at (8, 0) {=};
                    \draw[right dual] (9.5, -2) node [below] {\(R\)} -- (9.5, 2) node [above] {\(R\)};
                \end{tikzpicture}
                ,
            \end{equation}
            which is the snake equation for the duality \(L \leftdual R\).
            The composite the other way around can be worked out similarly, and is \(\id_{R'}\).
            This proves the forward direction, that dualities \(L \leftdual R\) and \(L \leftdual R'\) can only exist if \(R \isomorphic R'\).
            
            Now suppose that we have a duality \(L \leftdual R\) and an isomorphism \(f \colon R \to R'\).
            We can construct a duality \(L \leftdual R'\).
            In order to define a unit for this duality, \(\eta' \colon I \to R' \otimes L\), we need a way to produce \(R' \otimes L\).
            With the given data there is only one way to do this, we first produce \(R \otimes L\) with \(\eta\), then produce \(R' \otimes L\) by acting on this with \(f \otimes \id_L\).
            Thus, we define the unit of the duality \(L \leftdual R'\) to be
            \begin{equation}
                \tikzsetnextfilename{duals-unique-proof-5}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[wire] (0, 0) arc (180:360:0.5);
                    \draw[left dual] (1, 0) -- ++ (0, 1.5) node [above] {\(L\)};
                    \node[daggerable morphism] (f) at (0, 0.75) {\(f\)};
                    \draw[right dual] (0, 0) -- (f) node [midway, left] {\(R\)};
                    \draw[right dual] (f) -- (f |- 0, 1.5) node [above] {\(R'\)};
                \end{tikzpicture}
                .
            \end{equation}
            Similarly, we can define the counit of the dual \(L \leftdual R'\) to be
            \begin{equation}
                \tikzsetnextfilename{duals-unique-proof-6}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[wire] (0, 1) arc (180:0:0.5);
                    \draw[left dual] (0, -0.5) node [below] {\(L\)} -- (0, 1);
                    \node[daggerable morphism] (finv) at (1, 0.25) {\(f^{-1}\)};
                    \draw[right dual] (finv) -- (1, 1) node [midway, right] {\(R\)};
                    \draw[left dual] (finv) -- (1, -0.5) node [below] {\(R'\)};
                \end{tikzpicture}
            \end{equation}
            Computing the composite of this we have
            \begin{equation}
                \tikzsetnextfilename{duals-unique-proof-7}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \begin{scope}[xshift=1cm, yshift=-2cm]
                        \draw[wire] (0, 0) arc (180:360:0.5);
                        \draw[left dual] (1, 0) -- ++ (0, 3.75) node [above] {\(L\)};
                        \node[daggerable morphism] (f) at (0, 0.75) {\(f\)};
                        \draw[right dual] (0, 0) -- (f) node [midway, left] {\(R\)};
                    \end{scope}
                    \draw[wire] (0, 1) arc (180:0:0.5);
                    \draw[left dual] (0, -2.75) node [below] {\(L\)} -- (0, 1);
                    \node[daggerable morphism] (finv) at (1, 0.25) {\(f^{-1}\)};
                    \draw[right dual] (finv) -- (1, 1) node [midway, right] {\(R\)};
                    \draw[right dual] (f) -- (finv) node [midway, left] {\(R'\)};
                    \node at (2.5, -0.5) {\(=\)};
                    \begin{scope}[xshift=3cm]
                        \begin{scope}[xshift=1cm, yshift=-2cm]
                            \draw[wire] (0, 0) arc (180:360:0.5);
                            \draw[left dual] (1, 0) -- ++ (0, 3.75) node [above] {\(L\)};
                        \end{scope}
                        \draw[wire] (0, 1) arc (180:0:0.5);
                        \draw[left dual] (0, -2.75) node [below] {\(L\)} -- (0, 1);
                        \draw[right dual] (1, -2) -- (1, 1) node [midway, left] {\(R\)};
                    \end{scope}
                    \node at (5.5, -0.5) {\(=\)};
                    \draw[left dual] (6, -2.75) node [below] {\(L\)} -- (6, 1.75) node [above] {\(L\)};
                \end{tikzpicture}
            \end{equation}
            having used \(f^{-1} \circ f = \id_R\) and then the snake equation.
            Similarly, the composite in the opposite order is \(\id_{R'}\), we just use the snake equation and then inverses.
            
            The proof that \(L \leftdual R\) and \(L' \leftdual R\) if and only \(L \isomorphic L'\) follows the same steps.
        \end{proof}
    \end{lma}
    
    \section{Unit Determines Counit}
    Everything in this section holds if we swap unit and counit around, an example of duality in the broader sense of reversing morphisms.
    
    The definition of a dual is actually somewhat overdetermined, if we know the unit of a duality then this uniquely determines the counit.
    However, finding the counit is not always trivial, so we specify both the unit and counit when we specify a duality.
    
    The proof that the unit uniquely defines the counit is reminiscent of the proof of uniqueness of the inverse in a group, just using the snake equations in place of the group laws.
    This proof is as follows.
    Take a group, \(G\), and some element \(g \in G\).
    Then suppose that both \(h\) and \(h'\) are inverses of \(g\).
    In particular, we then have \(gh' = 1 = hg\).
    Hence,
    \begin{equation}
        h = h1 = hgh' = 1h' = h'.
    \end{equation}
    
    \begin{lma}{Unit Determines Counit}{}
        Let \(\cat{C}\) be a monoidal category with dualities \((L, R, \eta, \varepsilon)\) and \((L, R, \eta, \varepsilon')\).
        Then \(\varepsilon = \varepsilon'\).
        
        Similarly if we have dualities \((L, R, \eta, \varepsilon)\) and \((L, R, \eta', \varepsilon)\) then \(\eta = \eta'\).
        
        \begin{proof}
            First consider the counit \(\varepsilon\), we can take one leg of this and treat it as the identity, which we can then use the snake equation on.
            Rearranging the diagram allows us to apply the snake equation again, leaving us with just the counit \(\varepsilon'\).
            That is,
            \begin{equation}
                \tikzsetnextfilename{duals-unit-determines-counit}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \node[morphism, minimum width=1.5cm] (A) {\(\phantomrlap{\varepsilon}{\varepsilon'}\)};
                    \draw[right dual] ($(A.south) + (-0.5, -1)$) -- ($(A.south) + (-0.5, 0)$);
                    \draw[left dual] ($(A.south) + (0.5, -1)$) -- ($(A.south) + (0.5, 0)$);
                    \node (equal) at (1.25, -0.5) {\(=\)};
                    \node[morphism, minimum width=1.5cm] (B) at (2.5, -0.5) {\(\varepsilon'\)};
                    \node[morphism, minimum width=1.5cm] (C) at (4.5, 0) {\(\phantomrlap{\varepsilon}{\varepsilon'}\)};
                    \draw[right dual] ($(B.south) + (-0.5, -0.5)$) -- ($(B.south) + (-0.5, 0)$);
                    \draw[wire] ($(B.south) + (0.5, 0)$) arc (180:360:0.5);
                    \draw[right dual] ($(C.south) + (-0.5, -0.5)$) -- ($(C.south) + (-0.5, 0)$);
                    \draw[left dual] ($(C.south) + (0.5, -1)$) -- ($(C.south) + (0.5, 0)$);
                    \node at (1.25, -2.25) {\(\equaliso\)};
                    \begin{scope}[yshift=-2cm]
                        \node[morphism, minimum width=1.5cm] (B) at (2.5, 0) {\(\varepsilon'\)};
                        \node[morphism, minimum width=1.5cm] (C) at (4.5, -0.5) {\(\phantomrlap{\varepsilon}{\varepsilon'}\)};
                        \draw[right dual] ($(B.south) + (-0.5, -1)$) -- ($(B.south) + (-0.5, 0)$);
                        \draw[left dual] ($(B.south) + (0.5, -0.5)$) -- ($(B.south) + (0.5, 0)$);
                        \draw[wire] ($(B.south) + (0.5, -0.5)$) arc (180:360:0.5);
                        \draw[left dual] ($(C.south) + (0.5, -0.5)$) -- ($(C.south) + (0.5, 0)$);
                    \end{scope}
                    \node at (1.25, -4.5) {\(=\)};
                    \begin{scope}[xshift=2.5cm, yshift=-4cm]
                        \node[morphism, minimum width=1.5cm] (A) {\(\varepsilon'\)};
                        \draw[right dual] ($(A.south) + (-0.5, -1)$) -- ($(A.south) + (-0.5, 0)$);
                        \draw[left dual] ($(A.south) + (0.5, -1)$) -- ($(A.south) + (0.5, 0)$);
                        \node at (1, -1) {\(.\)};
                    \end{scope}
                \end{tikzpicture}
            \end{equation}
        \end{proof}
    \end{lma}
    
    Note that if we have dualities \((L, R, \eta, \varepsilon)\) and \((L, R, \eta', \varepsilon')\) we cannot say whether \(\eta = \eta'\) or \(\varepsilon = \varepsilon'\).
    
    \section{Duals Respect Tensor Products}
    In a monoidal category we can take tensor products and sometimes find dualities.
    We should check if these are compatible, that is, is a duality of tensor products a tensor product of dualities?
    The answer is yes.
    
    \begin{lma}{Duals Respect Tensor Products}{}
        Let \(\cat{C}\) be a monoidal category with dualities \(L \leftdual R\) and \(L' \leftdual R'\), then \(L \otimes L' \leftdual R' \otimes R\).
        
        \begin{proof}
            Suppose \(L \leftdual R\) and \(L' \leftdual R'\).
            We can define a duality \(L \otimes L' \leftdual R' \otimes R\) by defining the unit using the two units of \(L \leftdual R\) and \(L' \leftdual R'\):
            \begin{equation}
                \tikzsetnextfilename{duals-product-unit}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[right dual] (0, 0) -- (0, 0.5) node [above] {\(R\mathrlap{'}\)} coordinate (top);
                    \draw[right dual] (0.5, 0) -- (0.5, 0.5) node [above] {\(R\)};
                    \draw[left dual] (1.5, 0) -- (1.5, 0.5) node [above] {\(L\)};
                    \draw[left dual] (2, 0) -- (2, 0.5) node [above] {\(L\mathrlap{'}\)};
                    \draw[wire] (0.5, 0) arc (180:360:0.5);
                    \draw[wire] (0, 0) arc (180:360:1);
                    \node (equal) at (-0.75, 0) {\(=\)};
                    \begin{scope}[xshift=-2.5cm]
                        \node[morphism, minimum width=2cm] (unit) at (0, -0.75) {\(\eta\)};
                        \draw[right dual] ($(unit.north) + (-0.75, 0)$) coordinate (here) -- (here |- top) node [above] {\(R\mathrlap{'} \otimes R\)};
                        \draw[left dual] ($(unit.north) + (0.75, 0)$) coordinate (here) -- (here |- top) node [above] {\(L \otimes L\mathrlap{'}\)};
                    \end{scope}
                \end{tikzpicture}
                .
            \end{equation}
            Similarly, we an define the counit using the two units of \(L \leftdual R\) and \(L' \leftdual R'\):
            \begin{equation}
                \tikzsetnextfilename{duals-product-counit}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[left dual] (0, 0) coordinate (bottom)  node [below] {\(L\)} -- (0, 0.5);
                    \draw[left dual] (0.5, 0) node [below] {\(L\smash{\mathrlap{'}}\)} -- (0.5, 0.5);
                    \draw[right dual] (1.5, 0)  node [below] {\(R\smash{\mathrlap{'}}\)} -- (1.5, 0.5);
                    \draw[right dual] (2, 0) node [below] {\(R\)} -- (2, 0.5);
                    \draw[wire] (0.5, 0.5) arc (180:0:0.5);
                    \draw[wire] (0, 0.5) arc (180:0:1);
                    \node (equal) at (-0.75, 0.5) {\(=\)};
                    \begin{scope}[xshift=-2.5cm]
                        \node[morphism, minimum width=2cm] (counit) at (0, 1) {\(\varepsilon\)};
                        \draw[right dual] ($(counit.south) + (-0.75, 0)$) coordinate (here) -- (here |- bottom) node [below] {\(L \otimes L\mathrlap{'}\)};
                        \draw[right dual] ($(counit.south) + (0.75, 0)$) coordinate (here) -- (here |- bottom) node [below] {\(R\mathrlap{'} \otimes R\)};
                    \end{scope}
                \end{tikzpicture}
            \end{equation}
            
            We can prove the snake equations using the graphical notation and the snake equations for the two individual dualities:
            \begin{equation}
                \tikzsetnextfilename{duals-product-snakes}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[left dual] (0, -1.25) node [below] {\(L\)} -- (0, 1);
                    \draw[wire] (0, 1) arc (180:0:0.5);
                    \draw[left dual] (1, 1) -- (1, 0.5) node [midway, left] {\(R\)};
                    \draw[wire] (1, 0.5) arc (180:360:1);
                    \draw[left dual] (3, 0.5) -- (3, 2.5);
                    
                    \draw[left dual] (-0.5, -1.25) node [below] {\(L\smash{\mathrlap{'}}\)} -- (-0.5, 1);
                    \draw[wire] (-0.5, 1) arc (180:0:1);
                    \draw[left dual] (1.5, 1) -- (1.5, 0.5) node [midway, right] {\(R\smash{'}\)};
                    \draw[wire] (1.5, 0.5) arc (180:360:0.5);
                    \draw[left dual] (2.5, 0.5) -- (2.5, 2.5);
                    
                    \node (equal) at (3.5, 0.75) {\(\equaliso\)};
                    
                    \begin{scope}[xshift=4.5cm]
                        \draw[left dual] (0, -1.25) node [below] {\(L\)} -- (0, 0);
                        \draw[wire] (0, 0) arc (180:0:0.5);
                        \draw[left dual] (1, 0) -- (1, -0.1) node [midway, right] {\(R\)};
                        \draw[wire] (1, -0.1) arc (180:360:1);
                        \draw[left dual] (3, -0.1) -- (3, 2.5);
                        
                        \draw[left dual] (-0.5, -1.25) node [below] {\(L\smash{\mathrlap{'}}\)} -- (-0.5, 1.3);
                        \draw[wire] (-0.5, 1.3) arc (180:0:1);
                        \draw[left dual] (1.5, 1.3) -- (1.5, 1.2) node [midway, left] {\(R\smash{'}\)};
                        \draw[wire] (1.5, 1.2) arc (180:360:0.5);
                        \draw[left dual] (2.5, 1.2) -- (2.5, 2.5);
                        
                        \node at (3.5, 0.75) {\(=\)};
                    \end{scope}
                    
                    \draw[left dual] (8.5, -1.25) node [below] {\(L\)} -- (8.5, 2.5);
                    \draw[left dual] (9, -1.25) node [below] {\(L\smash{\mathrlap{'}}\)}  -- (9, 2.5);
                \end{tikzpicture}
            \end{equation}
            The other snake equation can be proved in the same way.
        \end{proof}
    \end{lma}
    
    Along with the tensor product we also get the unit object, this is always self dual.
    \begin{lma}{The Unit Object is Self Dual}{}
        Let \(\cat{C}\) be a monoidal category and \(I\) the monoidal unit.
        Then \(I \leftdual I\).
        \begin{proof}
            First, notice that we have morphisms \(\lambda_I^{-1} \colon I \to I \otimes I\) and \(\lambda_I \colon I \otimes I \to I\), which have the correct domain and codomain to be candidates for the unit, \(\eta\), and counit, \(\varepsilon\), respectively.
            If we consider the commuting diagrams defining duals then taking the unit to be \(\eta = \lambda_I^{-1}\) and the counit to be \(\varepsilon = \lambda_I\) these diagrams are formed entirely from unitors, identities, and associators, and so commute by the coherence theorem.
            Hence, \((I, I, \lambda_I^{-1}, \lambda_I)\) is a duality.
        \end{proof}
    \end{lma}
    
    \section{Duals Respect Braidings}
    In a braided monoidal category as well as the monoidal structure of the last section we also have a braiding.
    We should check if this is compatible with dualities.
    Since braidings are about swapping the orders of objects in a tensor product duality respecting braiding would mean that if we have a duality and we swap left and right using a braiding we should still have a duality.
    
    
    \begin{lma}{Duals Respect Braidings}{}
        Let \(\cat{C}\) be a braided monoidal category with duality \(L \leftdual R\) then there is a duality \(R \leftdual L\).
        \begin{proof}
            Given a duality \(L \leftdual R\) and a braiding, \(\sigma\), we can construct a new duality \(R \leftdual L\) by defining the new unit and counit as
            \begin{equation}
                \tikzsetnextfilename{duals-braiding-unit}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[left dual=1] (1, 1.25) coordinate (top) -- (1, 1);
                    \draw[wire, rounded corners] (1, 1) -- (1, 0.75) -- (0, 0.25) -- (0, 0);
                    \draw[left dual=1] (0, 0) -- (0, -0.25);
                    \draw[wire] (0, -0.25) arc (180:360:0.5);
                    \draw[left dual] (1, -0.25) -- (1, 0);
                    \draw[over wire, rounded corners] (1, 0) -- (1, 0.25) -- (0, 0.75) -- (0, 1);
                    \draw[left dual=0] (0, 1) -- (0, 1.25);
                    \node (equal) at (-0.5, 0.5) {\(=\)};
                    \begin{scope}[xshift=-1.6cm]
                        \node[morphism, minimum width=1.5cm] (unit) at (0, -0.5) {\(\eta\)};
                        \draw[left dual] ($(unit.north) + (-0.5, 0)$) coordinate (here) -- (here |- top);
                        \draw[right dual] ($(unit.north) + (0.5, 0)$) coordinate (here) -- (here |- top);
                    \end{scope}
                \end{tikzpicture}
                \quad\text{and}\quad
                \tikzsetnextfilename{duals-braiding-counit}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \begin{scope}[yscale=-1]
                        \draw[left dual=1] (1, 1.25) coordinate (bottom) -- (1, 1);
                        \draw[wire, rounded corners] (1, 1) -- (1, 0.75) -- (0, 0.25) -- (0, 0);
                        \draw[left dual=1] (0, 0) -- (0, -0.25);
                        \draw[wire] (0, -0.25) arc (180:360:0.5);
                        \draw[left dual] (1, -0.25) -- (1, 0);
                        \draw[over wire, rounded corners] (1, 0) -- (1, 0.25) -- (0, 0.75) -- (0, 1);
                        \draw[left dual=0] (0, 1) -- (0, 1.25);
                    \end{scope}
                    \node (equal) at (-0.5, 0) {\(=\)};
                    \begin{scope}[xshift=-1.6cm]
                        \node[morphism, minimum width=1.5cm] (counit) at (0, 0.5) {\(\phantomrlap{\varepsilon}{\eta}\)};
                        \draw[left dual] ($(counit.south) + (-0.5, 0)$) coordinate (here) -- (here |- bottom);
                        \draw[right dual] ($(counit.south) + (0.5, 0)$) coordinate (here) -- (here |- bottom);
                    \end{scope}
                \end{tikzpicture}
                .
            \end{equation}
            We can prove the snake equations using the graphical notation and the snake equations for the original duality:
            \begin{equation}
                \tikzsetnextfilename{duals-braiding-snakes}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[left dual] (0, -0.25) -- (0, 1.75);
                    \draw[wire, rounded corners] (1, 1.75) -- (1, 2) -- (0, 2.5) -- (0, 2.75);
                    \draw[over wire, rounded corners] (0, 1.75) -- (0, 2) -- (1, 2.5) -- (1, 2.75);
                    \draw[left dual=1] (1, 2.75) -- (1, 2.8);
                    \draw[wire] (0, 2.8) arc (180:0:0.5);
                    \draw[left dual=1] (0, 2.8) -- (0, 2.75);
                    \draw[left dual] (1, 1.75) -- (1, 1.5);
                    \draw[wire, rounded corners] (2, 1.5) -- (2, 1.25) -- (1, 0.75) -- (1, 0.5);
                    \draw[over wire, rounded corners] (1, 1.5) -- (1, 1.25) -- (2, 0.75) -- (2, 0.5);
                    \draw[left dual=1] (1, 0.4) -- (1, 0.5);
                    \draw[left dual=1] (2, 0.5) -- (2, 0.4);
                    \draw[wire] (1, 0.4) arc (180:360:0.5);
                    \draw[left dual] (2, 1.5) -- (2, 3.55);
                    
                    \node (equal) at (2.5, 1.65) {\(\equaliso\)};
                    
                    \begin{scope}[xshift=3cm]
                        \draw[left dual] (2, -0.25) -- (2, 1.9);
                        \draw[wire] (2, 1.9) arc (0:180:0.5);
                        \draw[left dual] (1, 1.9) -- (1, 1.4);
                        \draw[wire] (1, 1.4) arc (0:-180:0.5);
                        \draw[left dual] (0, 1.4) -- (0, 3.55);
                    \end{scope}
                    
                    \node at (5.5, 1.65) {\(=\)};
                    
                    \draw[left dual] (6, -0.26) -- (6, 3.55);
                \end{tikzpicture}
                .
            \end{equation}
            The other snake equation can be proved in the same way.
        \end{proof}
    \end{lma}
    
    This allows us to be a bit less careful with what is left and right in a duality in any braided monoidal category, which is most of the categories we're interested in.
    
    \section{Dual Morphisms}
    If we have a morphism, say \(f \colon A \to B\), in which \(A\) and \(B\) happen to have duals, \(A \leftdual A^*\) and \(B \leftdual B^*\), then it makes sense to ask if we can find a morphism between the right duals.
    We might initially look for a morphism \(A^* \to B^*\), but we won't find one in general that works nicely with the duality.
    Instead we find a morphism \(B^* \to A^*\) instead.
    
    \begin{dfn}{Dual Morphism}{}
        Given a morphism \(f \colon A \to B\) with dualities \(A \leftdual A^*\) and \(B \leftdual B^*\) the \define{right dual}\index{right dual!morphism}, \(f^* \colon B^* \to A^*\), is defined as
        \begin{equation}
            \tikzsetnextfilename{duals-morphism-dual}
            \begin{tikzpicture}
                \node[daggerable morphism] (f dual) {\(f^*\)};
                \draw[right dual] (0, -1) node [below] {\(B\mathrlap{^*}\)} -- (f dual);
                \draw[right dual] (f dual) -- (0, 1) node [above] {\(A\mathrlap{^*}\)};
                \node (equal) at (1.1, 0) {\(\coloneqq\)};
                \node[daggerable morphism] (f) at (3, 0) {\(\phantomrlap{f}{f^*}\)};
                \draw[right dual] (2, 1) node [above] {\(A\mathrlap{^*}\)} -- (2, -0.3);
                \draw[wire] (2, -0.3) arc (180:360:0.5);
                \draw[left dual] (3, -0.3) -- (f);
                \draw[left dual=1] (f) -- (3, 0.4);
                \draw[wire] (3, 0.4) arc (180:0:0.5);
                \draw[right dual] (4, -1) node [below] {\(B\mathrlap{^*}\)} --(4, 0.4);
                \node at (4.9, 0) {\(\eqqcolon\)};
                \node[dual morphism] (f) at (6, 0) {\(\phantomrlap{f}{f^*}\)};
                \draw[right dual] (6, -1) node [below] {\(B\mathrlap{^*}\)} -- (f);
                \draw[right dual] (f) -- (6, 1) node [above] {\(A\mathrlap{^*}\)};
            \end{tikzpicture}
        \end{equation}
        where the first equality defines \(f^*\) and the second defines the graphical notation for \(f^*\), we rotate the box, as opposed to \(f^{\dagger}\), where we reflect the box.
    \end{dfn}
    
    The idea behind duals is that as well as sliding morphisms along wires we can also slide them through units and counits, and in doing so they are rotated.
    This is the next lemma.
    
    \begin{lma}{Sliding}{lma:sliding duals}
        For all morphisms \(f \colon A \to B\) in a monoidal category with chosen duals \(A \leftdual A^*\) and \(B \leftdual B^*\) we have
        \begin{equation}
            \tikzsetnextfilename{duals-slide-1}
            \begin{tikzpicture}[baseline=(equal.base)]
                \node[daggerable morphism] (f) at (0, 0) {\(f\)};
                \draw[left dual] (0, -1) -- (f);
                \draw[left dual=1] (f) -- (0, 0.5);
                \draw[wire] (0, 0.5) arc (180:0:0.5);
                \draw[right dual] (1, -1) -- (1, 0.5);
                \node (equal) at (1.5, 0) {\(=\)};
                \node[dual morphism] (f) at (3, 0) {\(f\)};
                \draw[left dual] (2, -1) -- (2, 0.5);
                \draw[wire] (2, 0.5) arc (180:0:0.5);
                \draw[right dual=1] (f) -- (3, 0.5);
                \draw[right dual] (3, -1) -- (f);
            \end{tikzpicture}
            \qquad\text{and}\qquad
            \tikzsetnextfilename{duals-slide-2}
            \begin{tikzpicture}[baseline=(equal.base)]
                \draw[right dual] (0, -0.5) -- (0, 1);
                \draw[wire] (0, -0.5) arc (180:360:0.5);
                \node[daggerable morphism] (f) at (1, 0) {\(f\)};
                \draw[left dual] (1, -0.5) -- (f);
                \draw[left dual] (f) -- (1, 1);
                \node (equal) at (1.75, 0) {\(=\)};
                \node[dual morphism] (f) at (2.5, 0) {\(f\)};
                \draw[right dual] (2.5, 1) -- (f);
                \draw[right dual] (f) -- (2.5, -0.5);
                \draw[wire] (2.5, -0.5) arc (180:360:0.5);
                \draw[left dual] (3.5, -0.5) -- (3.5, 1);
            \end{tikzpicture}
        \end{equation}
        \begin{proof}
            We'll prove the first of these.
            It follows immediately in the graphical notation by expanding the definition of \(f^*\) and applying the snake equations:
            \begin{equation}
                \tikzsetnextfilename{duals-sliding-proof}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \node[dual morphism] (f) at (1, 0) {\(f\)};
                    \draw[left dual] (0, -1) -- (0, 0.5);
                    \draw[wire] (0, 0.5) arc (180:0:0.5);
                    \draw[right dual=1] (f) -- (1, 0.5);
                    \draw[right dual] (1, -1) -- (f);
                    
                    \node (equal) at (1.55, 0) {\(=\)};
                    
                    \draw[left dual] (2, -1) -- (2, 0.5);
                    \draw[wire] (2, 0.5) arc (180:0:0.5);
                    \draw[left dual] (3, 0.5) -- (3, -0.3);
                    \draw[wire] (3, -0.3) arc (180:360:0.5);
                    \node[daggerable morphism] (f) at (4, 0) {\(f\)};
                    \draw[left dual=0] (4, -0.3) -- (f);
                    \draw[left dual=1] (f) -- (4, 0.4);
                    \draw[wire] (4, 0.4) arc (180:0:0.5);
                    \draw[left dual] (5, 0.4) -- (5, -1);
                    
                    \node at (5.5, 0) {\(=\)};
                    
                    \begin{scope}[xshift=4.1cm]
                        \node[daggerable morphism] (f) at (2, 0) {\(f\)};
                        \draw[left dual] (2, -1) -- (f);
                        \draw[left dual=1] (f) -- (2, 0.5);
                        \draw[wire] (2, 0.5) arc (180:0:0.5);
                        \draw[right dual] (3, -1) -- (3, 0.5);
                    \end{scope}
                \end{tikzpicture}
                .
            \end{equation}	
            The other equation can be proved similarly.
        \end{proof}
    \end{lma}
    
    Now we have defined duals of objects and morphisms it makes sense to ask if taking duals defines a functor, and indeed it does.
    
    \begin{dfn}{Dual Functor}{}
        In a monoidal category, \(\cat{C}\), with chosen right duals, that is where we have made a choice for every object, \(A\), of some \(A^*\) such that \(A \leftdual A^*\), there is a contravariant functor
        \begin{align}
            (-)^* \colon \cat{C} &\to \cat{C}\\
            A &\mapsto A^*\\
            f &\mapsto f^*
        \end{align}
        where \(A \leftdual A^*\) is the chosen duality, and \(f^*\colon B^* \to A^*\) is the right dual of the morphism \(f\colon A \to B\).
        This is the \defineindex{dual functor}.
    \end{dfn}
    
    \begin{lma}{}{}
        The dual functor is a functor.
        \begin{proof}
            We start by proving that \((\id_A)^* = \id_{A^*}\) for all objects \(A\).
            This is immediately clear from the definition of \(\id_{A^*}\) in graphical notation and the equations:
            \begin{equation}
                (\id_A)^* = 
                \tikzsetnextfilename{duals-dual-id}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \node[daggerable morphism] (id) at (0, 0) {\((\id_{A})^*\)};
                    \draw[right dual] (0, -1) node [below] {\(A\mathrlap{^*}\)} -- (id);
                    \draw[right dual] (id) -- (0, 1) node [above] {\(A\mathrlap{^*}\)};
                    \node (equal) at (1, 0) {\(=\)};
                    \node[daggerable morphism] (id) at (2.5, 0) {\(\id_{A}\)};
                    \draw[left dual] (1.5, 1) node [above] {\(A\mathrlap{^*}\)} -- (1.5, -0.3);
                    \draw[wire] (1.5, -0.3) arc (180:360:0.5);
                    \draw[left dual=0] (2.5, -0.3) -- (id);
                    \draw[left dual=1] (id) -- (2.5, 0.4);
                    \draw[wire] (2.5, 0.4) arc (180:0:0.5);
                    \draw[left dual] (3.5, 0.4) -- (3.5, -1) node [below] {\(A\mathrlap{^*}\)};
                    \node at (4.25, 0) {\(=\)};
                    \draw[left dual] (5, 1) node [above] {\(A\mathrlap{^*}\)} -- (5, -0.3);
                    \draw[wire] (5, -0.3) arc (180:360:0.5);
                    \draw[left dual] (6, -0.3) -- (6, 0.4);
                    \draw[wire] (6, 0.4) arc (180:0:0.5);
                    \draw[left dual] (7, 0.4) -- (7, -1) node [below] {\(A\mathrlap{^*}\)};
                    \node at (7.75, 0) {\(=\)};
                    \draw[right dual] (8.5, -1) node [below] {\(A\mathrlap{^*}\)} -- (8.5, 1) node [above] {\(A\mathrlap{^*}\)};
                \end{tikzpicture}
                .
            \end{equation}
            This proves \((\id_A)^* = \id_{A^*}\), as required for a functor.
            
            Given morphisms \(f \colon A \to B\) and \(g \colon B \to C\) we can form \((g \circ f) \colon A \to C\) and consider \((g \circ f)^* \colon C^* \to A^*\).
            We can then prove that \((g \circ f)^* = f^* \circ g^*\), as required for a contravariant functor.
            To do so we insert the identity, using the snake equation, between \(f\) and \(g\):
            \begin{equation}
                \tikzsetnextfilename{duals-dual-composite}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \pgfdeclarelayer{behind}
                    \pgfsetlayers{behind, main}
                    \node[daggerable morphism] (gf star) at (0, 0) {\((g \circ f)^*\)};
                    \draw[right dual] (0, -1.8) node [below] {\(C\mathrlap{^*}\)} -- (gf star);
                    \draw[right dual] (gf star) -- (0, 1.8) node [above] {\(A\mathrlap{^*}\)};
                    \node (equal) at (1, 0) {\(=\)};
                    \node[daggerable morphism] (f) at (2.5, -0.5) {\(f\)};
                    \node[daggerable morphism] (g) at (2.5, 0.5) {\(\phantomrlap{g}{f}\)};
                    \draw[left dual=0.6] (f) -- (g);
                    \draw[left dual=0] (2.5, -1) -- (f);
                    \draw[left dual=1] (g) -- (2.5, 1);
                    \draw[wire] (1.5, -1) arc (180:360:0.5);
                    \draw[wire] (2.5, 1) arc (180:0:0.5);
                    \draw[right dual] (1.5, -1) -- (1.5, 1.8) node [above] {\(A\mathrlap{^*}\)};
                    \draw[right dual] (3.5, -1.8) node [below] {\(C\mathrlap{^*}\)} -- (3.5, 1);
                    \node at (4, 0) {\(=\)};
                    \node[daggerable morphism] (f) at (5.5, 0) {\(f\)};
                    \node[daggerable morphism] (g) at (7.5, 0) {\(g\)};
                    \draw[left dual] (4.5, 1.8) -- (4.5, -0.5);
                    \draw[wire] (4.5, -0.5) arc (180:360:0.5);
                    \draw[left dual] (5.5, -0.5) -- (f);
                    \draw[left dual=1] (f) -- (5.5, 0.5);
                    \draw[wire] (5.5, 0.5) arc (180:0:0.5);
                    \draw[left dual] (6.5, 0.5) -- (6.5, -0.5);
                    \draw[wire] (6.5, -0.5) arc (180:360:0.5);
                    \draw[left dual] (7.5, -0.5) -- (g);
                    \draw[left dual=1] (g) -- (7.5, 0.5);
                    \draw[wire] (7.5, 0.5) arc (180:0:0.5);
                    \draw[left dual] (8.5, 0.5) -- (8.5, -1.8);
                    \node at (9, 0) {\(=\)};
                    \node[daggerable morphism] (g) at (9.5, -0.5) {\(\phantomrlap{g^*}{f^*}\)};
                    \node[daggerable morphism] (f) at (9.5, 0.5) {\(f^*\)};
                    \draw[left dual=0.6] (g) -- (f);
                    \draw[left dual] (9.5, -1.8) -- (g);
                    \draw[left dual] (f) -- (9.5, 1.8);
                    \begin{pgfonlayer}{behind}
                        \filldraw[Blue!50] (4.2, 1.2) -- (4.2, -1.2) -- (6, -1.2) -- (7, 1.2);
                        \filldraw[Navy!50] (6, -1.2) -- (7, 1.2) -- (8.7, 1.2) -- (8.7, -1.2);
                        \node[Blue, above left] at (6.2, -1.2) {\(f^*\)};
                        \node[Navy, above right] at (6.05, -1.2) {\(g^*\)};
                    \end{pgfonlayer}
                \end{tikzpicture}
                .
            \end{equation}
            This proves that \((g \circ f)^* = f^* \circ g^*\).
            Hence, \((-)^*\) is a contravariant functor.
        \end{proof}
    \end{lma}
    
    \subsection{Examples}
    In \(\FVect\) and \(\FHilb\) a morphism is a linear map \(f \colon V \to W\), and the dual spaces are spaces of linear maps into \(\field\) or \(\complex\) respectively.
    The dual of \(f\) is the map of linear maps \(f \colon W^* \to V^*\) defined by \(f^*(e) = e \circ f\) where \(e \colon W \to \field\) or \(e \colon W \to \complex\) respectively is an arbitrary linear map.
    This is sometimes called the \defineindex{pullback}.
    
    In \(\Mat\) the morphisms are matrices, and the dual is just the transpose.
    
    In \(\Rel\) the dual of a relation is its converse.
    This means the right duals functor, \((-)^*\), and the dagger functor, \((-)^\dagger\), are the same, since both act as identities on objects since all objects are self dual in \(\Rel\) and the dagger acts as an identity on objects by definition.
    
    \section{Double Duals}
    We've already seen that every vector space is canonically isomorphic to its double dual, that is \(V \isomorphic V^{**} = (V^*)^*\).
    In fact this holds in an arbitrary category with chosen right duals.
    Further, the action of taking double duals is compatible with the monoidal product.
    
    \begin{lma}{}{}
        In a monoidal category with chosen right duals \(A^{**} \otimes B^{**} \isomorphic (A \otimes B)^{**}\).
        
        \begin{proof}
            To prove this we need to witness an isomorphism \(A^{**} \otimes B^{**} \to (A \otimes B)^{**}\).
            One such isomorphism is
            \begin{equation}
                \tikzsetnextfilename{duals-double-dual-tensor-product}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \node[morphism, minimum width=2cm] (counit) at (0, 0) {\(\varepsilon_{A\otimes B}\)};
                    \draw[right dual=0.6] ($(counit.south) + (-0.5, 0)$) -- ++ (0, -0.3) coordinate (A);
                    \draw[right dual=0.6] (counit.south) -- ++ (0, -0.3) coordinate (B);
                    \draw[wire] (A) arc (360:180:0.5) coordinate (C);
                    \draw[wire] (B) arc (360:180:1) coordinate (D);
                    \draw[right dual] (C) -- ++ (0, 1) coordinate (E);
                    \draw[right dual] (D) -- ++ (0, 1) coordinate (F);
                    \draw[wire, xscale=3] (E) arc (180:0:0.5) coordinate (G);
                    \draw[wire, xscale=2] (F) arc (180:0:1) coordinate (H);
                    \draw[right dual] (G) -- ++ (0, -3.8) node [below] {\(A\mathrlap{^{**}}\;\)};
                    \draw[right dual] (H) -- ++ (0, -3.8) node [below] {\(B\mathrlap{^{**}}\)};
                    \node[morphism, minimum width=2cm] (unit) at (-1, -3) {\(\eta_{(A\otimes B)^*}\)};
                    \coordinate (I) at ($(unit.north) + (0.5, 0)$);
                    \draw[left dual, rounded corners] ($(counit.south) + (0.5, 0)$) coordinate (here) -- (here |- 0, -1) -- (I |- 0, -2) -- (I);
                    \draw[left dual=0.61, rounded corners] ($(unit.north) + (-0.5, 0)$) coordinate (here) -- (here |- 0, -2) -- (-2.5, -1) -- (-2.5, 1.7) node [above] {\((A \otimes B)\mathrlap{^{**}}\)};
                \end{tikzpicture}
                .
            \end{equation}
            The inverse to this morphism, proving it is an isomorphism, is
            \begin{equation}
                \tikzsetnextfilename{duals-double-dual-tensor-product-inverse}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \node[morphism, minimum width=2cm] (counit) at (1, 2) {\(\varepsilon_{(A\otimes B)^*}\)};
                    \draw[right dual=0.57, rounded corners] ($(counit.south) + (0.5, 0)$) coordinate (here) -- (here |- 0, 1) -- ++ (1, -1) coordinate (here) -- (here |- 0, -3.3) node [below] {\((A \otimes B)^{**}\)};
                    \node[morphism, minimum width=2cm] (unit) at (0, -1) {\(\eta_{A \otimes B}\)};
                    \coordinate (A) at ($(unit.north) + (-0.5, 0)$);
                    \draw[left dual, rounded corners] ($(counit.south) + (-0.5, 0)$) coordinate (here) -- (here |- 0, 1) -- (A |- 0, 0) -- (A);
                    \draw[left dual=1] ($(unit.north) + (0.5, 0)$) -- ++ (0, 0.3) coordinate (C);
                    \draw[left dual=1] ($(unit.north)$) -- ++ (0, 0.3) coordinate (D);
                    \draw[wire] (C) arc (180:0:0.5) coordinate (E);
                    \draw[wire] (D) arc (180:0:1) coordinate (F);
                    \draw[left dual] (E) -- ++ (0, -1) coordinate (G);
                    \draw[left dual] (F) -- ++ (0, -1) coordinate (H);
                    \draw[wire, xscale=3] (G) arc (360:180:0.5) coordinate (I);
                    \draw[wire, xscale=2] (H) arc (360:180:1) coordinate (J);
                    \draw[left dual] (I) -- ++ (0, 4) node [above] {\(B\mathrlap{^{**}}\)};
                    \draw[left dual] (J) -- ++ (0, 4) node [above] {\(A\mathrlap{^{**}}\)};
                \end{tikzpicture}
            \end{equation}
            where the cups and caps are the units and counits of the relevant dualities.
            To show these are inverses compose them and then apply the appropriate snake equations for each unit-counit pair which appears.
        \end{proof}
    \end{lma}
    
    \chapter{Teleportation}
    \section{Quantum Teleportation}
    \begin{rmk}
        For more details see my notes from \course{Principles of Quantum Mechanics}.
    \end{rmk}
    Suppose Alice is in possession of a quantum state, which for simplicity we take to be a single qubit, \(\ket{\psi}\).
    Quantum teleportation is a protocol by which Alice and Bob can prepare a joint set of states and then by communicating only classical information manipulate a system in Bob's presence into the state \(\ket{\psi}\).
    Alice's state is necessarily destroyed in this process, otherwise the no cloning theorem is violated, and so this process makes a state disappear from Alice's possession and appear in Bob's possession, i.e.\@ it teleports.
    
    The process is as follows.
    Alice and Bob prepare ahead of time an extra two qubits in the Bell state
    \begin{equation}
        \ket{\Psi^-} = \frac{1}{\sqrt{2}}[\ket{{+}{-}} - \ket{{-}{+}}].
    \end{equation}
    Alice then takes the first qubit here into her possession, and Bob takes the second qubit into their possession.
    This creates a \enquote{quantum channel} between Alice and Bob.
    
    Alice now has two qubits, the original qubit \(\ket{\psi}\), and her qubit from the entangled pair.
    In general Alice's qubit will be in the state
    \begin{equation}
        \ket{\psi} = a\ket{+} + b\ket{-}
    \end{equation}
    for some \(a, b \in \complex\) with \(a^2 + b^2 = 1\).
    
    If we then consider the joint system of all three qubits the state of this system is
    \begin{align}
        \ket{\varphi} &= \ket{\psi} \otimes \ket{\Psi^-}\\
        &= [a\ket{+} + b\ket{-}] \otimes \frac{1}{\sqrt{2}}[\ket{+-} - \ket{-+}]\\
        &= \frac{c}{\sqrt{2}}[\ket{{+}{+}{-}} - \ket{{+}{-}{+}}] + \frac{d}{\sqrt{2}}[\ket{{-}{+}{-}} - \ket{{-}{-}{+}}].
    \end{align}
    The four Bell states form a basis for the space \(\complex^2 \otimes \complex^2\), so we can write Alice's pair of states as a superposition of Bell states.
    For example,
    \begin{equation}
        \ket{{+}{+}} = \frac{1}{\sqrt{2}}[\ket{\Phi^+} + \ket{\Phi^-}].
    \end{equation}
    If we do this with all of Alice's states then we find that
    \begin{multline}
        \ket{\varphi} = \frac{1}{2}[\ket{\Psi^-}\otimes (-a\ket{+} - b\ket{-}) + \ket{\Psi^+}\otimes (-a\ket{+} + b\ket{-})\\
        + \ket{\Phi^-}\otimes (a\ket{-} + b\ket{+}) + \ket{\Phi^+}\otimes (a\ket{-} - b\ket{+})].
    \end{multline}
    Thus when Alice measures her states, changing them in the process, she will measure them to be in one of the Bell states.
    There are four of these, so by sending two classical bits to Bob, establishing a \enquote{classical channel}, Alice can inform Bob of the result of her measurement.
    
    This tells Bob what their state is, since the wavefunction collapses\footnote{or insert your favourite interpretation of quantum mechanics here} to leave just one of the terms above.
    For example, if Alice measures \(\ket{\Phi^+}\) then Bob knows that their qubit is in the state \(a\ket{-} - b\ket{+}\).
    Bob knows that they can then put their qubit into the state \(\ket{\psi}\) by applying a unitary transformation, in this example,
    \begin{equation}
        \begin{pmatrix}
            0 & 1\\
            -1 & 0
        \end{pmatrix}
        .
    \end{equation}
    This can be done regardless of which state Alice measures, and so Bob can always put their qubit in the state \(\ket{\psi}\).
    
    \section{Abstract Teleportation}
    We will now define a general notion of teleportation, which becomes quantum teleportation in \(\Hilb\).
    
    \begin{dfn}{Teleportation}{}
        Consider a monoidal dagger category with right duals.
        A \defineindex{teleportation procedure} is a finite family of effects, \(e_i \colon A \otimes A^* \to I\), and unitary morphisms \(U_i \colon A \to A\) such that
        \begin{equation}
            \tikzsetnextfilename{teleportation-teleportation-procedure}
            \begin{tikzpicture}[baseline=(equal.base)]
                \node[daggerable morphism] (e) at (0, 0) {\(\vphantom{e_i}\ooalign{\hspace{2cm} \cr \hfill \(e_i\) \hfill}\)};
                \coordinate (A) at ($(e.south) + (-0.5, 0)$);
                \coordinate (B) at ($(e.south) + (0.5, 0)$);
                \draw[left dual] (A |- 0, -1.5) node [below] {\(A\)} coordinate (bottom) -- (A);
                \draw[left dual=1] (B) -- ++ (0, -0.3) coordinate (C);
                \draw[wire] (C) arc (180:360:0.7) coordinate (D);
                \draw[left dual] (D) -- ++ (0, 1) coordinate (E);
                \node[daggered morphism, above] (U) at (E) {\(U_i\)};
                \draw[left dual] (U) -- ++ (0, 1) node [above] {\(A\)} coordinate (top);
                \node (equal) at (2.75, 0) {\(=\)};
                \draw[left dual] (3.5, 0 |- bottom) node [below] {\(A\)} -- (3.5, 0 |- top) node [above] {\(A\)};
            \end{tikzpicture}
        \end{equation}
        for all \(i\).
    \end{dfn}
    
    This definition actually carries more information than required, since given unitary morphisms \(U_i\) we can always take \(e_i\) to be
    \begin{equation}
        \tikzsetnextfilename{teleportation-e-in-terms-of-U}
        \begin{tikzpicture}[baseline=(U.base)]
            \node[dual morphism] (U) {\(U_i\)};
            \draw[left dual] (-1, -1) -- (-1, 0.5);
            \draw[wire] (-1, 0.5) arc (180:0:0.5) coordinate (A);
            \draw[left dual] (A) -- (U);
            \draw[left dual] (U) -- (0, -1);
        \end{tikzpicture}
        .
    \end{equation}
    
    This works since
    \begin{equation}
        \tikzsetnextfilename{teleportation-solve-teleporatation-procedure}
        \begin{tikzpicture}[baseline=(equal.base)]
            \node[dual morphism] (U) {\(U_i\)};
            \draw[left dual] (-1, -1.3) coordinate (bottom) -- (-1, 0.5);
            \draw[wire] (-1, 0.5) arc (180:0:0.5) coordinate (A);
            \draw[left dual] (A) -- (U);
            \draw[left dual=1] (U) -- (0, -0.5);
            \draw[wire] (0, -0.5) arc (180:360:0.5);
            \draw[left dual] (1, -0.5) -- (1, 0.5);
            \node[daggered morphism, above] (U2) at (1, 0.5) {\(U_i\)};
            \draw[left dual] (U2) -- (1, 1.5) coordinate (top);
            \node (equal) at (1.75, 0) {\(=\)};
            \draw[left dual] (2.5, 0 |- bottom) -- (2.5, 0.5);
            \draw[wire] (2.5, 0.5) arc (180:0:0.5);
            \draw[left dual] (3.5, 0.5) -- (3.5, -0.5);
            \draw[wire] (3.5, -0.5) arc (180:360:0.5);
            \draw[left dual] (4.5, -0.5) -- (4.5, -0.25);
            \node[above, daggerable morphism] (U3) at (4.5, -0.25) {\(U_i\)};
            \draw[left dual=0.8] (U3) -- ++ (0, 0.5) coordinate (B);
            \node[above, daggered morphism] (U4) at (B) {\(U_i\)};
            \draw[left dual] (U4) -- (U4 |- top);
            \node (equal) at (5.25, 0) {\(=\)};
            \draw[left dual] (6, 0 |- bottom) -- (6, -0.25);
            \node[above, daggerable morphism] (U5) at (6, -0.25) {\(U_i\)};
            \draw[left dual=0.8] (U5) -- ++ (0, 0.5) coordinate (C);
            \node[above, daggered morphism] (U6) at (C) {\(U_i\)};
            \draw[left dual] (U6) -- (U6 |- top);
            \node at (6.75, 0) {\(=\)};
            \draw[left dual] (7.25, 0 |- bottom) -- (7.25, 0 |- top);
        \end{tikzpicture}
        \,.
    \end{equation}
    This uses the sliding of \cref{lma:sliding duals}, then the snake equation, and then the definition of unitarity, namely \(U_i \circ U_i^\dagger = \id_A\).
    
    Consider the first diagram above, we can interpret this as a procedure as follows:
    \begin{enumerate}
        \item Start with a single system, \(A\).
        \item Independently prepare a joint system, \(A^* \otimes A\), in the state \(\eta\), so the combined system is \(A \otimes (A^* \otimes A)\).
        \item Reassociate, so that we consider the initial system and one of the two systems in the joint system, \((A \otimes A^*) \otimes A\).
        \item Perform a joint measurement on the first two systems, with the result given by the effect \(\varepsilon \circ ({\id_L} \otimes U_i^*)\).
        \item Perform a unitary operation, \(U_i^\dagger\), on the remaining system.
    \end{enumerate}
    
    \section{Quantum Teleportation Again}
    In \(\Hilb\) a teleportation procedure is exactly the process of quantum teleportation.
    We can see this by considering the steps listed above and comparing to the qubit teleportation procedure at the start of the chapter.
    \begin{enumerate}
        \item Start with Alice's system that she wants to send to Bob.
        This is some state in in a Hilbert space \(A = \complex^2\).
        \item Prepare a state in the Bell state, this is an element of the joint system \(\complex^2 \otimes \complex^2\) (note that \(\complex^2 \isomorphic (\complex^2)^*\)).
        \item Reassociate, which means considering the pair of qubits in Alice's possession.
        \item Alice performs a measurement.
        \item Bob can then perform a unitary operation to get their qubit into the same state as Alice's at the start of the process.
    \end{enumerate}
    
    We can be more explicit for the case of teleporting a qubit.
    We have \(A = \complex^2\) and \(A \isomorphic \complex^2\).
    We can represent morphisms with matrices after choosing some orthonormal basis.
    Then we have
    \begin{equation}
        \eta^\dagger = \varepsilon = 
        \begin{pmatrix}
            1 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    If we choose the unitary operators
    \begin{equation}
        \begin{pmatrix}
            1 & 0\\
            0 & 1
        \end{pmatrix}
        , \qquad
        \begin{pmatrix}
            1 & 0\\
            0 & -1
        \end{pmatrix}
        , \qquad 
        \begin{pmatrix}
            0 & 1\\
            1 & 0
        \end{pmatrix}
        , \qqand 
        \begin{pmatrix}
            0 & 1\\
            -1 & 0
        \end{pmatrix}
        ,
    \end{equation}
    which we might recognise as the identity, \(I\), and the Pauli matrices, \(\sigma^3\), \(\sigma^1\), and \(i\sigma^2\) respectively.
    The family of effects are then given by \(\varepsilon \circ (I \otimes U_i^*)\), which gives
    \begin{equation*}
        \begin{pmatrix}
            1 & 0 & 0 & 1
        \end{pmatrix}
        , \quad 
        \begin{pmatrix}
            1 & 0 & 0 & -1
        \end{pmatrix}
        , \quad 
        \begin{pmatrix}
            0 & 1 & 1 & 0
        \end{pmatrix}
        , \qand
        \begin{pmatrix}
            0 & 1 & -1 & 0
        \end{pmatrix}
        .
    \end{equation*}
    This is a complete set of effects, since it is a basis for \(\Hilb(\complex^2 \otimes \complex^2, \complex) = (\complex^2 \otimes \complex^2)^* \isomorphic \complex^2 \otimes \complex^2 \isomorphic \complex^4\), and so this teleportation procedure is guaranteed to work no matter which result we produce at the measurement step.
    
    \section{One Time Pads}
    Consider an implementation of the teleportation procedure in \(\Rel\).
    We'll take the simple case of \(L = R = \{0, 1\}\) and
    \begin{equation}
        \eta^\dagger = \varepsilon = 
        \begin{pmatrix}
            1 & 0 & 0 & 1
        \end{pmatrix}
        .
    \end{equation}
    There are only two unitary relations \(\{0, 1\} \to \{0, 1\}\), and they are represented by the matrices
    \begin{equation}
        U_1 = 
        \begin{pmatrix}
            1 & 0\\
            0 & 1
        \end{pmatrix}
        , \qqand U_3 = 
        \begin{pmatrix}
            0 & 1\\
            1 & 0
        \end{pmatrix}
        .
    \end{equation}
    Taking these as our family of unitary morphisms, \(U_i\), we get the effects
    \begin{equation}
        \begin{pmatrix}
            1 & 0 & 0 & 1
        \end{pmatrix}
        , \qqand
        \begin{pmatrix}
            0 & 1 & 1 & 0
        \end{pmatrix}
        .
    \end{equation}
    These form a complete set of effects.
    
    The teleportation procedure is then as follows.
    \begin{enumerate}
        \item Alice starts with a single bit she wishes to communicate to Bob.
        \item Alice and Bob each take one bit from a preprepared joint state of \(\{0, 1\} \times \{0, 1\}\).
        This state, \(\eta\), is \(\{(0, 0), (1, 1)\}\), so they will both take the same bit.
        \item Reassociate, which means considering the pair of bits in Alice's possession.
        \item Alice looks at the shared bit and sees it either matches the bit she wants to send or it doesn't.
        \item Alice tells Bob which of these outcomes and if it matches he does nothing, \(U_1\), and if it doesn't match he swaps it, \(U_2\).
        Bob's bit from the shared system will now be the bit that Alice wanted to send in the first place.
    \end{enumerate}
    
    This is how a one time pad works.
    The joint state is the one time pad, Alice then communicates information about the one time pad, and Bob can turn this into information about the state that Alice is trying to communicate.
    One can imagine a one time pad containing all messages Alice could possibly have reason to send, generating a shared message and a set of unitary operations swapping that message with a message at a different position on the one time pad.
    These are unitary since they are transpositions and so square to the identity.
    In reality at some point it becomes easier to have a fixed number of possible messages, say the 26 letters, and then break down longer messages into these.
    To do so securely Alice and Bob should use a different one time pad for each letter they send.
    
    
    \chapter{Compact Categories}
    \section{Compact Categories}
    \begin{dfn}{Compact Category}{}
        A \defineindex{compact category} is a symmetric monoidal category where every object has a dual.
    \end{dfn}
    
    All of \(\FVect\), \(\FHilb\), \(\Mat\), and \(\Rel\) are compact categories.
    
    In the graphical notation wires now come with arrows, this gives us a new notion of the orientation of a wire, and we have to preserve this property.
    This leads to us updating what it means for two diagrams to be equal.
    
    \begin{thm}{Correctness of the Graphical Calculus for Compact Categories}{}
        Any well-formed equation of the form \(f = g\) for morphisms \(f\) and \(g\) in a compact category follows from the axioms of a compact category if and only if it holds in the graphical notation up to four-dimensional oriented isotopy.
    \end{thm}
    
    Intuitively this just means that we can't turn arrows around, which shouldn't happen when manipulating a diagram anyway.
    
    \section{Dagger Compact Category}
    \begin{lma}{}{}
        In a monoidal dagger category if \(L \leftdual R\) then \(R \leftdual L\).
        \begin{proof}
            Suppose \(\eta \colon I \to R \otimes L\) and \(\varepsilon \colon L \otimes R \to I\) are the unit and counit of the \(L \leftdual R\) duality.
            Then \(\eta' = \varepsilon^\dagger \colon L \otimes R \to I\) and \(\varepsilon' = \eta^\dagger \colon R \otimes L \to I\) can be taken as the unit and counit of the \(R \leftdual L\) duality.
            It is possible to show that the diagrams defining a duality commute by taking one, and applying the dagger functor, flipping all the arrows, and then using \((f \otimes g)^\dagger = f^\dagger \otimes g^\dagger\), which turns one of these diagrams into the other but with \(L\) and \(R\) exchanged.
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Dagger Dual}{}
        In a monoidal dagger category a \defineindex{dagger dual} is a duality \(A \leftdual A^*\) witnessed by morphisms \(\eta \colon I \to A^* \otimes A\) and \(\varepsilon \colon A \otimes A^* \to I\) satisfying
        \begin{equation}
            \tikzsetnextfilename{compact-categories-dagger-dual}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \node[daggerable morphism] (unit) {\vphantom{\eta}\ooalign{\hspace{1.5cm} \cr \hfil \(\eta\) \hfil}};
                \draw[left dual] ($(unit.north) + (-0.5, 0)$) -- ++ (0, 1);
                \draw[right dual] ($(unit.north) + (0.5, 0)$) -- ++ (0, 1);
                \node at (1.75, 0) {\(=\)};
                \begin{scope}[xshift=4cm]
                    \node[daggered morphism] (counit) {\vphantom{\eta}\ooalign{\hspace{1.5cm} \cr \hfil \(\varepsilon\) \hfil}};
                    \draw[left dual] ($(counit.north) + (-0.5, 0)$) -- ++ (0, 1) coordinate (top);
                    \draw[left dual] ($(top) + (-1, 0)$) -- ++ (0, -2) coordinate (A);
                    \draw[wire, xscale=3] (A) arc (180:360:0.5) coordinate (B);
                    \draw[left dual] (B) -- ++ (0, 1.3) coordinate (C);
                    \draw[wire] (C) arc (0:180:0.5) coordinate (D);
                    \draw[left dual] (D) -- ++ (0, -0.3);
                \end{scope}
            \end{tikzpicture}
            ,
        \end{equation}
        
        A \defineindex{compact dagger category} is a symmetric dagger category which can be made into a compact category by taking all duals to be dagger duals.
    \end{dfn}
    
    \(\FHilb\), \(\Mat\), and \(\Rel\) are all compact dagger categories.
    
    \section{Trace}
    \begin{dfn}{Trace}{def:trace}
        In a compact dagger category the \defineindex{trace} of an endomorphism, \(f \colon A \to A\), is the scalar
        \begin{equation}
            \tr_A f \coloneqq 
            \tikzsetnextfilename{compact-categories-trace}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \node[daggerable morphism] (f) {\(f\)};
                \draw[left dual=1] (f) -- ++ (0, 0.5) coordinate (A);
                \draw[wire] (A) arc (180:0:0.5) coordinate (B);
                \draw[left dual] (B) -- ++ (0, -1) coordinate (C);
                \draw[wire] (C) arc (360:180:0.5) coordinate (D);
                \draw[left dual] (D) -- (f);
            \end{tikzpicture}
            ,
        \end{equation}
        that is
        \begin{equation}
            \tr_A f = \varepsilon \circ (f \otimes \id_A^*) \circ \eta.
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Dimension}{}
        In a compact dagger category the \defineindex{dimension} of an object, \(A\), is the trace of the identity on that object, \(\dim A \coloneqq \tr_A(\id_A)\).
    \end{dfn}
    
    The names trace and dimension are used as these generalise these concepts from \(\FHilb\).
    
    \begin{lma}{}{}
        In \(\FHilb\) the trace as defined in \cref{def:trace} is the ordinary trace and the dimension is the ordinary dimension.
        
        \begin{proof}
            Recall that in \(\FHilb\) the unit is given by
            \begin{align}
                \eta \colon \complex &\to H^* \otimes H\\
                1 &\mapsto \sum_i \bra{i} \otimes \ket{i}
            \end{align}
            where \(\{i\}\) is a basis for \(H\).
            The counit is defined by
            \begin{align}
                \varepsilon \colon H \otimes H^* &\to \complex\\
                \ket{i} \otimes \bra{j} &\mapsto \braket{i}{j}.
            \end{align}
            Then the trace of \(f\) in \(H\) is
            \begin{align}
                \tr_H f &= (\varepsilon \circ (f \otimes \id_{H^*}) \circ \eta)(f)\\
                &= (\varepsilon \circ (f \otimes \id_{H^*})) \left( \sum_i \bra{i} \otimes \ket{i} \right)\\
                &= \varepsilon\left( \sum_i \bra{f(i)} \otimes \ket{i} \right)\\
                &= \braket{i}{f(i)}\\
                &= \bra{i} f \ket{i}
            \end{align}
            which is just the usual definition of the trace.
            
            The trace of an identity matrix, and hence an identity linear transformation, is just the number of rows/columns in the matrix, and hence is the dimension of the vector space.
            So since the trace in \(\FHilb\) is just the usual trace the dimension is just the usual dimension.
        \end{proof}
    \end{lma}
    
    The trace has many of the expected properties.
    For example, it is cyclic.
    
    \begin{lma}{Trace is Cyclic}{}
        The trace is cyclic, that is \(\tr_A(g \circ f) = \tr_B(f \circ g)\) for all morphisms \(f \colon A \to B\) and \(g \colon B \to A\) in a compact category.
        
        \begin{proof}
            \begin{equation*}
                \tr_A (g \circ f) = 
                \tikzsetnextfilename{compact-categories-trace-cyclic-1}
                \begin{tikzpicture}[baseline=(b.base)]
                    \node[daggerable morphism] (f) at (0, -0.5) {\(f\)};
                    \node[daggerable morphism] (g) at (0, 0.5) {\(\phantomrlap{g}{f}\)};
                    \draw[left dual=0.6] (f) -- (g) node [midway, left, font=\scriptsize] (b) {\(B\)};
                    \draw[left dual=1] (g) -- ++ (0, 0.5) coordinate (A) node [midway, left, font=\scriptsize] {\(A\)};
                    \draw[wire] (A) arc (180:0:0.5) coordinate (B);
                    \draw[left dual] (B) -- ++ (0, -2) coordinate (C) node [midway, right, font=\scriptsize] {\(A^*\)};
                    \draw[wire] (C) arc (360:180:0.5) coordinate (D);
                    \draw[left dual] (D) -- (f) node [midway, left, font=\scriptsize] {\(A\)};
                \end{tikzpicture}
                =
                \tikzsetnextfilename{compact-categories-trace-cyclic-2}
                \begin{tikzpicture}[baseline=(f.base)]
                    \node[daggerable morphism] (f) at (0, 0) {\(f\)};
                    \node[dual morphism] (g) at (1, 0) {\(\phantomrlap{g}{f}\)};
                    \draw[left dual=1] (f) -- ++ (0, 0.5) coordinate (A) node [pos=0.6, left, font=\scriptsize] {\(B\)};
                    \draw[wire] (A) arc (180:0:0.5) coordinate (B);
                    \draw[left dual] (B) -- (g) node [pos=0.4, right, font=\scriptsize] {\(B^*\)};
                    \draw[left dual=1] (g) -- ++ (0, -0.5) coordinate (C) node [pos=0.6, right, font=\scriptsize] {\(A^*\)};
                    \draw[wire] (C) arc (360:180:0.5) coordinate (D);
                    \draw[left dual] (D) -- (f) node [pos=0.4, left, font=\scriptsize] {\(A\)};
                \end{tikzpicture}
                =
                \tikzsetnextfilename{compact-categories-trace-cyclic-3}
                \begin{tikzpicture}[baseline=(b.base)]
                    \node[daggerable morphism] (g) at (0, 0.5) {\(f\)};
                    \node[daggerable morphism] (f) at (0, -0.5) {\(\phantomrlap{g}{f}\)};
                    \draw[right dual=0.6] (g) -- (f) node [midway, left, font=\scriptsize] (b) {\(A\)};
                    \draw[left dual=1] (g) -- ++ (0, 0.5) coordinate (A) node [midway, left, font=\scriptsize] {\(B\)};
                    \draw[wire] (A) arc (180:0:0.5) coordinate (B);
                    \draw[left dual] (B) -- ++ (0, -2) coordinate (C) node [midway, right, font=\scriptsize] {\(B^*\)};
                    \draw[wire] (C) arc (360:180:0.5) coordinate (D);
                    \draw[left dual] (D) -- (f) node [midway, left, font=\scriptsize] {\(B\)};
                \end{tikzpicture}
                = \tr_B (f \circ g). \qedhere
            \end{equation*}
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:trace of scalar is that scalar}
        The trace of a scalar is that scalar, that is \(\tr_I s = s\).
        \begin{proof}
            The identity is self dual with the unit and counit given by \(\lambda_I^{-1}\) and \(\lambda_I\) respectively.
            We don't draw these in the diagram, and the objects are all unit objects, which we also don't draw.
            Hence, the diagram defining the trace is simply the diagram consisting of the scalar \(s\).
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:trace of tensor product}
        In a compact category with morphisms \(f \colon A \to A\) and \(g \colon B \to B\) we have \(\tr_{A\otimes B}(f \otimes g) = \tr_A(f) \circ \tr_B(g)\).
        \begin{proof}
            \begin{equation*}
                \tr_{A \otimes B} (f \otimes g) = 
                \tikzsetnextfilename{compact-categories-trace-tensor-product-1}
                \begin{tikzpicture}[baseline=(f.base)]
                    \node[daggerable morphism] (f) at (0, 0) {\(f\)};
                    \node[daggerable morphism] (g) at (1, 0) {\(\phantomrlap{g}{f}\)};
                    \draw[left dual=1] (f) -- ++ (0, 0.5) coordinate (A1);
                    \draw[left dual=1] (g) -- ++ (0, 0.5) coordinate (A2);
                    \draw[wire] (A2) arc (180:0:0.5) coordinate (B2);
                    \draw[wire, xscale=2] (A1) arc (180:0:0.75) coordinate (B1);
                    \draw[left dual] (B1) -- ++ (0, -1) coordinate (C1);
                    \draw[left dual] (B2) -- ++ (0, -1) coordinate (C2);
                    \draw[wire] (C2) arc (360:180:0.5) coordinate (D2);
                    \draw[wire, xscale=2]{} (C1) arc (360:180:0.75) coordinate (D1);
                    \draw[left dual] (D1) -- (f);
                    \draw[left dual] (D2) -- (g);
                \end{tikzpicture}
                =
                \tikzsetnextfilename{compact-categories-trace-tensor-product-2}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \node[daggerable morphism] (f) {\(f\)};
                    \draw[left dual=1] (f) -- ++ (0, 0.5) coordinate (A);
                    \draw[wire] (A) arc (180:0:0.5) coordinate (B);
                    \draw[left dual] (B) -- ++ (0, -1) coordinate (C);
                    \draw[wire] (C) arc (360:180:0.5) coordinate (D);
                    \draw[left dual] (D) -- (f);
                    \begin{scope}[yshift=2.2cm]
                        \node[daggerable morphism] (f) {\(g\)};
                        \draw[left dual=1] (f) -- ++ (0, 0.5) coordinate (A);
                        \draw[wire] (A) arc (180:0:0.5) coordinate (B);
                        \draw[left dual] (B) -- ++ (0, -1) coordinate (C);
                        \draw[wire] (C) arc (360:180:0.5) coordinate (D);
                        \draw[left dual] (D) -- (f);
                    \end{scope}
                \end{tikzpicture}
                = \tr_A(f) \circ \tr_B(g) \qedhere
            \end{equation*}
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        In a compact dagger category with morphism \(f \colon A \to A\) we have \((\tr_A f)^\dagger = \tr_A(f^\dagger)\).
        \begin{proof}
            Take the diagram defining the trace, flip it upside-down to take the dagger, this is the diagram defining the trace of \(f^\dagger\).
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        In a compact dagger category with unit object \(I\) we have \(\dim I = \id_I\).
        \begin{proof}
            Using \cref{lma:trace of scalar is that scalar} we have
            \begin{equation*}
                \dim I = \tr_I \id_I = \id_I. \qedhere
            \end{equation*}
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        In a compact dagger category with objects \(A\) and \(B\) we have \(\dim(A \otimes B) = \dim(A) \circ \dim(B)\).
        \begin{proof}
            Using the functoriality of the tensor product and \cref{lma:trace of tensor product}
            \begin{multline*}
                \dim(A \otimes B) = \tr_{A \otimes B}(\id_{A \otimes B}) = \tr_{A \otimes B}(\id_A \circ \id_B)\\
                = \tr_A(\id_A) \circ \tr_B(\id_B) = \dim(A) \circ \dim(B). \qedhere
            \end{multline*}
        \end{proof}
    \end{lma}
    
    We can now show that any infinite-dimensional space does not have a dual.
    To do so we need to accept a few facts first.
    To start with given two vector spaces, \(V\) and \(W\), we can form their direct sum, the vector space
    \begin{equation}
        V \oplus W \coloneqq \{(v, w) \mid v \in V \text{ and } w \in W\}
    \end{equation}
    with pairwise vector addition:
    \begin{equation}
        (v, w) + (v', w') = (v + v', w + w'),
    \end{equation}
    and scalar multiplication according to
    \begin{equation}
        z(v, w) = (zv, zw).
    \end{equation}
    Writing \(v + w\) for \((v, w)\) and treating this sum as a formal (non-commutative) sum of vectors these operations naturally become
    \begin{equation}
        (v + w) + (v' + w') = (v + v') + (w + w'), \qqand z(v + w) = zv + zw.
    \end{equation}
    The dimension of \(V \oplus W\) is simply
    \begin{equation}
        \dim(V \oplus W) = \dim V + \dim W,
    \end{equation}
    which is clear for the traditional definition of the dimension and finite dimensional vector spaces, but holds for the generalised definition and infinite dimensional vector spaces.
    
    For an infinite dimensional vector space \(V\) we have an isomorphism \(V \oplus \field \isomorphic V\).
    An explicit construction is to take a basis\footnote{assuming the axiom of choice} \(\{v_i\}\) for \(V\) and then \(\{v_i \oplus 1\}\) is a basis for \(V \oplus \field\), which clearly has the same cardinality.
    This can then be extended linearly to all elements of \(V \oplus \field\).
    
    \begin{lma}{}{}
        Infinite dimensional Hilbert spaces don't have duals.
        
        \begin{proof}
            Suppose \(H\) is an infinite dimensional vector space with dual.
            Then we can construct a scalar \(\dim H\).
            We have an isomorphism \(H \oplus \complex \isomorphic H\), and hence \(\dim(H \oplus \complex) = \dim(H) + 1\).
            But isomorphic vector spaces have the same dimension, so \(\dim(H \oplus \complex) = \dim H\).
            This is not possible since in \(\Hilb\) the scalars can be identified with the complex numbers, and there is no complex number, \(z\) making \(z + 1 = z\) true, so there is no way to identify \(\dim H\) with a complex number, a contradiction since \(\dim H\) is by assumption a scalar.
        \end{proof}
    \end{lma}
    
    \part{Structures Within Categories}
    \chapter{(Co)Monoids}
    \textit{I'm going to diverge from the order in which things were approached in lectures, because I think it makes more sense to define monoids before comonoids.}
    
    \section{Universal Algebra}
    \textit{This section will be a bit technical, and is beyond the scope of the course, the important parts will be covered again at the start of the next section.}
    
    In category theory we aim to work with morphisms between objects, rather than with \enquote{elements} of the objects, which isn't even a meaningful concept in some categories.
    Something like a group is defined as a set, \(G\), a binary operation, \(G \times G \to G\), a distinguished element, \(1\), and for every element, \(g\), another element, \(g^{-1}\).
    This is too much talk about elements for category theory.
    Let's have a look at an alternative characterisation without reference to elements.
    
    A set, \(G\), and binary operation, \(G \times G \to G\) is fine, no mention of elements here.
    A binary operation is a 2-ary operation, taking two things to one, we might think of a binary operation as a map \(m \colon G^2 \to G\).
    Next we need to define the identity.
    We can do so by introducing a 0-ary operation \(e \colon G^0 \to G\)?
    This takes in no things and produces one thing, which we can take to be \(1\), the identity.
    Finally, we need the inverse, we can characterise this as a map \(i \colon G \to G\) taking in one element, \(g\), and returning one element, \(i(g) = g^{-1}\).
    
    The axioms of a group then become requirements on these operations, namely for all \(g, h, k \in G\)
    \begin{itemize}
        \item \(m(e(), g) = m(g, e()) = g\), writing \(e()\) to remind us that \(e\) is an operation with no arguments
        \item \(m(g, i(g)) = m(i(g), g) = e()\),
        \item \(m(g, m(h, k)) = m(m(g, h), k)\).
    \end{itemize}
    Notice that there are no quantifiers here apart from the \enquote{for all} at the start, this is a desirable feature.
    
    Universal algebra studies algebraic structures by characterising them as a set and some collection of operations on that set.
    The signature of an algebraic structure is just a list of the arity of the operations.
    We can then define a variety of algebras as a collection of all sets with some fixed signature and all satisfying some set of equational laws, like those above for a group, with only outermost quantifiers.
    
    Examples are
    \begin{itemize}
        \item The variety of groups which has signature \((2, 1, 0)\) and satisfy the equational laws above.
        \item The variety of abelian groups which has the signature \((2, 1, 0)\) and the additional equational law that for all \(g, h \in G\) we have \(m(g, h) = m(h, g)\)
        \item The variety of monoids which has signature \((2, 0)\) and satisfies the equational laws above, apart from the second one with inverses.
        \item The variety of rings, which has the signature \((2, 2, 0, 0, 1)\) and satisfies the mononoid and abelian group laws for appropriately chosen operations.
        \item The variety of vector spaces, which has the signature \((2, 1, 1, 1, \dotsc)\) with one unary operation for each scalar, namely multiplication by that scalar.
        These satisfy equational laws like \(m_\lambda(p(v, w)) = p(m_\lambda(v), m_\lambda(w))\) where \(m_\lambda \colon V \to V\) is multiplication by the scalar \(\lambda\) and \(p\colon V^2 \to V\) is vector addition.
    \end{itemize}
    
    One of the nice things about varieties of algebras is once we've defined one we can generalise it by replacing the set with an object in some other category, so long as it has finite products (to form \(G^2\) etc.) and an initial object (to take the place of \(G^0\)), and functions with morphisms of this category.
    This is what we will do here, focusing on varieties of monoids.
    
    An example of this is group objects, which are objects of the variety of groups.
    \begin{itemize}
        \item Interpreted in \(\Set\) we just get normal groups.
        \item Interpreted in \(\Grp\) we instead get Abelian groups
        This follows since we require the inverse to be a homomorphism, that is \(g \mapsto g^{-1}\) is a homomorphism, and this is only the case in an Abelian group.
        \item Interpreting in \(\Top\) we get topological groups, which are topological spaces with a group structure in which the product and inversion maps are continuous, this isn't that interesting and is basically spelling out the definition of a group object in the specific context of \(\Top\).
        \item Interpreting in \(\makeatletter\c@egory{SmthMan}\makeatother\), the category of smooth manifolds, with smooth functions as morphisms, a group object is exactly a Lie group.
    \end{itemize}
    
    \section{Monoids: The Idea}
    A monoid can be characterised without reference to explicit elements as follows.
    \begin{dfn}{Monoid}{}
        A \defineindex{monoid}, \((M, m, u)\), is a set, \(M\), equipped with a binary operation, \(m \colon M \times M \to M\), and a 0-ary operation, \(u \colon \{\bullet\} \to M\) satisfying the following axioms for all \(x, y, z \in M\):
        \begin{itemize}
            \item \(m(u(\bullet), x) = m(x, u(\bullet)) = x\),
            \item \(m(x, m(y, z)) = m(m(x, y), z)\).
        \end{itemize}
    \end{dfn}
    
    Clearly we can identify this with the usual definition of a monoid by taking \(m(x, y) = xy\), multiplication, and \(u(\bullet) = 1\), the identity, and then the laws become \(1x = x1 = 1\), the identity law, and \(x(yz) = (xy)z\), the associativity law.
    
    The nice thing about this definition is that removing explicit reference to elements\footnote{apart from \(\bullet \in \{\bullet\}\), but this is characterised as being the initial object in \(\Set\) and so is unique up to isomorphism and doesn't really count as talking about an element} we can make this more general by interpreting it in any monoidal category making the following replacements:
    \begin{itemize}
        \item Replace \enquote{a set, \(M\)} with \enquote{an object, \(M\)}.
        \item Replace \(M \times M\) with \(M \otimes M\).
        \item Replace functions with morphisms.
        \item Replace \(\{\bullet\}\) with the unit object.
    \end{itemize}
    
    Before we give the more general definition of a monoid lets look at the monoid laws in the graphical notation.
    First, the identity is characterised by a morphism \(e \colon I \to M\), so this is exactly a state,
    \begin{equation}
        \tikzsetnextfilename{monoid-identity-state}
        \begin{tikzpicture}[baseline=(u.base)]
            \node[state] (u) {\(u\)};
            \draw[wire] (u) -- ++ (0, 1);
        \end{tikzpicture}
        .
    \end{equation}
    Multiplication is then characterised by a morphism \(M \otimes M \to M\), so this can be drawn as
    \begin{equation}
        \tikzsetnextfilename{monoid-multiplication-map}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[morphism, minimum width=1cm] (m) {\(m\)};
            \draw[wire] ($(m.south) + (-0.25, 0)$) -- ++ (0, -1);
            \draw[wire] ($(m.south) + (0.25, 0)$) -- ++ (0, -1);
            \draw[wire] (m.north) -- ++ (0, 1);
        \end{tikzpicture}
        .
    \end{equation}
    Rather than give a name to these morphisms we can just represent them with a dot with the appropriate number of inputs.
    
    \section{Monoids: The Definition}
    \begin{dfn}{Monoid}{}
        A \defineindex{monoid} in a monoidal category, \(\cat{C}\), is a triple, \((A, \monoidProduct, \monoidIdentity)\), consisting of an object, \(A \in \Ob(\cat{C})\), a morphism, \(\monoidProduct \colon A \otimes A \to A\), and a state, \(\monoidIdentity\colon I \to A\), satisfying the \defineindex{associativity} and \defineindex{unitality} equations,
        \begin{equation}
            \tikzsetnextfilename{monoid-associativity-law}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw[wire] (0, 0) node [below] {\(A\)} arc (180:0:0.5) node [below] {\(A\)};
                \node[monoid dot] at (0.5, 0.5) {};
                \draw[wire] (0.5, 0.5) arc (180:0:0.75) -- (2, 0) node [below] {\(A\)};
                \node[monoid dot] at (1.25, 1.25) {};
                \draw[wire] (1.25, 1.25) -- (1.25, 2) node [above] {\(A\)};
                \begin{scope}[xshift=5cm, xscale=-1]
                    \draw[wire] (0, 0) node [below] {\(A\)} arc (180:0:0.5) node [below] {\(A\)};
                    \node[monoid dot] at (0.5, 0.5) {};
                    \draw[wire] (0.5, 0.5) arc (180:0:0.75) -- (2, 0) node [below] {\(A\)};
                    \node[monoid dot] at (1.25, 1.25) {};
                    \draw[wire] (1.25, 1.25) -- (1.25, 2) node [above] {\(A\)};
                \end{scope}
                \node at (current bounding box) {\(=\)};
            \end{tikzpicture}
            ,
        \end{equation}
        and
        \begin{equation}
            \tikzsetnextfilename{monoid-unitality-law}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw[wire] (0, 0) node [below] {\(A\)} -- (0, 0.5) arc (180:0:0.5) node [monoid dot] {};
                \draw[wire] (0.5, 1) node [monoid dot] {} -- (0.5, 1.75) node [above] {\(A\)};
                \node at (current bounding box -| 1.5, 0) {\(=\)};
                \node at (current bounding box -| 2.5, 0) {\(=\)};
                \draw[wire] (2, 0) node [below] {\(A\)} -- (2, 1.75) node [above] {\(A\)};
                \begin{scope}[xscale=-1, xshift=-4cm]
                    \draw[wire] (0, 0) node [below] {\(A\)} -- (0, 0.5) arc (180:0:0.5) node [monoid dot] {};
                    \draw[wire] (0.5, 1) node [monoid dot] {} -- (0.5, 1.75) node [above] {\(A\)};
                \end{scope}
            \end{tikzpicture}
        \end{equation}
        respectively.
        
        These relations can also be expressed as commutative diagrams for a monoid \((A, m, e)\) we require
        \begin{equation}
            \begin{tikzcd}[row sep=small]
                A \otimes (A \otimes A) \arrow[dd, "\alpha_{A,A,A}"'] \arrow[r, "{\id_A} \otimes m"] & A \otimes A \arrow[dr, "m"]\\
                && A,\\
                (A \otimes A) \otimes A \arrow[r, "m \otimes \id_A"'] & A \otimes A \arrow[ur, "m"']
            \end{tikzcd}
        \end{equation}
        and
        \begin{equation}
            \begin{tikzcd}
                A \otimes I \arrow[d, "{\id_A} \otimes u"'] & A \arrow[l, "\rho_A^{-1}"'] \arrow[d, "\id_A"'] \arrow[r, "\lambda_A^{-1}"] & I \otimes A \arrow[d, "{\id_A} \otimes u"] \\
                A \otimes A \arrow[r, "m"'] & A & A \otimes A. \arrow[l, "m"]
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Commutative Monoid}{}
        A \defineindex{commutative monoid} in a braided monoidal category, \(\cat{C}\), is a monoid, \((A, \monoidProduct, \monoidIdentity)\), satisfying the \defineindex{commutativity} equation
        \begin{equation}
            \tikzsetnextfilename{monoid-commutativity-law}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw[wire] (0, 0) node [below] {\(A\)} -- ++ (0, 0.5) arc (180:0:0.5) -- ++ (0, -0.5) node [below] {\(A\)};
                \draw[wire] (0.5, 1) node [monoid dot] {} -- ++ (0, 0.75) node [above] {\(A\)};
                \draw[wire, rounded corners] (3, 0) node [below] {\(A\)} -- ++ (0, 0.2) -- ++ (-1, 0.2) -- ++ (0, 0.1);
                \draw[over wire, rounded corners] (2, 0) node [below] {\(A\)} -- ++ (0, 0.2) -- ++ (1, 0.2) -- ++ (0, 0.1);
                \draw[wire] (2, 0.5) arc (180:0:0.5);
                \draw[wire] (2.5, 1) node [monoid dot] {} -- ++ (0, 0.75) node [above] {\(A\)};
                \node at (current bounding box -| 1.5, 0) {\(=\)};
            \end{tikzpicture}
            .
        \end{equation}
        This can also be expressed as a commutative diagram for the monoid \((A, m, e)\):
        \begin{equation}
            \begin{tikzcd}
                A \otimes A \arrow[r, "m"] \arrow[d, "\sigma_{A,A}"'] & A\\
                A \otimes A. \arrow[ur, "m"']
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    \section{Monoids: The Examples}
    In any monoidal category \((I, \lambda_I, \id_I)\) is trivially a monoid, all diagrams we write for the monoid laws are empty.
    
    A monoid in \(\Set\) is exactly the normal definition of a monoid.
    
    A monoid in \(\Mon\) is a commutative monoid.
    To see this note that if \(((M, \cdot), m, u)\) is a monoid object in \(\Mon\) then \(m\) must be a monoid homomorphism between \(M \otimes M \to M\), and elements of \(M \otimes M\) are of the form \((a, b)\).
    Then for \(a, b, a', b' \in M\) we have
    \begin{equation}
        aba'b' = m(a, b)m(a', b') = m((a,b), (a',b')) = m(aa',bb') = aa'bb'.
    \end{equation}
    Cancelling the \(a\) and \(b'\), which can be done without requiring inverses, we have \(ba' = a'b\).
    
    A monoid \((G, \otimes_{\integers}, \integers)\) in the category of Abelian groups, \(\Ab\), is a ring.
    That is, if we treat Abelian groups as \(\integers\)-modules and use the tensor product of \(\integers\)-modules as the product in our monoid then we already have addition, the Abelian group operation, and now we have multiplication, which follows the monoid laws, so this is both an Abelian group, \((G, +)\) and a monoid, \((G, \otimes_{\integers})\), and it is possible to demonstrate distributivity from distributivity of the tensor product, so this is a ring.
    
    A monoid \((V, \times, 1)\) in the category of vector spaces, \(\Vect\), is a unital (associative) algebra.
    The monoid product in this case gives us a way of multiplying two vectors and is such that \(1\times v = v = v\times 1\) for all vectors \(v \in V\).
    This is exactly a unital (associative) algebra.
    Examples of algebras are \(\complex^n\) under pointwise multiplication (\((a, \dotsc, z) \times (a', \dotsc, z') = (aa', \dotsc, zz')\), with the unit \((1, \dotsc, 1)\)) and \(n \times n\) matrices, \(\matrices{n}{\field}\) with the usual matrix multiplication and the identity matrix as the unit.
    
    A monoid \((\lstinline[style=Haskell]|a|, \lstinline[style=Haskell]|mappend|, \lstinline[style=Haskell]|mempty|)\) in the category \(\Hask\) of \Haskell{} types and functions is exactly an instance of the \lstinline[style=Haskell]|Monoid| typeclass.
    
    A monoid \((T, \mu, \eta)\) in the category of endofunctors, \(\functorCategory{\cat{C}}{\cat{C}}\), is a monad.
    That is, \(T \colon \cat{C} \to \cat{C}\) is a functor for some category \(\cat{C}\), and \(\mu \colon T \otimes T \to T\) and \(\eta \colon \id_{\cat{C}} \naturalTransformation T\) are natural transformations.
    This leads to the famously confusing statement
    \begin{important}
        A \defineindex{monad} is a monoid in the category of endofunctors.
    \end{important}
    
    \subsection{Pair of Pants Monoid}
    \begin{dfn}{Pair of Pants Monoid}{}
        In a category in which every object has a chosen dual \((A^* \otimes A, \pairofpantsProduct, \pairofpantsIdentity)\) is a monoid.
    \end{dfn}
    
    \begin{lma}{Pair of Pants is Monoid}{}\index{pair of pants}
        In a category where every object has a chosen dual \((A^* \otimes A, \pairofpantsProduct, \pairofpantsIdentity)\) is a monoid.
        
        \begin{proof}
            First we demonstrate unitality:
            \begin{equation}
                \tikzsetnextfilename{monoids-pair-of-pants-unitality}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[right dual, rounded corners] (0, 0) -- ++ (0, 0.9) -- ++ (1, 1) -- ++ (0, 0.3) coordinate (top);
                    \draw[left dual] (1, 0) -- ++ (0, 0.7);
                    \draw[wire] (1, 0.7) arc (180:0:0.5);
                    \draw[left dual=0] (2, 0.7) arc (180:360:0.5);
                    \draw[left dual, rounded corners] (3, 0.7) -- ++ (0, 0.2) -- ++ (-1, 1) -- ++ (0, 0.3);
                    \node at (3.5, 0 |- current bounding box) {\(=\)};
                    \draw[right dual] (4, 0) -- (4, 0 |- top);
                    \draw[left dual] (4.5, 0) -- (4.5, 0 |- top);
                    \node at (5, 0 |- current bounding box) {\(=\)};
                    \draw[left dual, rounded corners] (9, 0) -- ++ (0, 0.9) -- ++ (-1, 1) -- ++ (0, 0.3);
                    \draw[right dual] (8, 0) -- ++ (0, 0.7);
                    \draw[wire] (8, 0.7) arc (0:180:0.5);
                    \draw[right dual=0] (7, 0.7) arc (360:180:0.5);
                    \draw[right dual, rounded corners] (6, 0.7) -- ++ (0, 0.2) -- ++ (1, 1) -- ++ (0, 0.3);
                \end{tikzpicture}
                .
            \end{equation}
            Next we demonstrate associativity:
            \begin{equation}
                \tikzsetnextfilename{monoids-pair-of-pants-associativity}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[right dual, rounded corners] (0, 0) -- ++ (0, 0.2) -- ++ (1, 1) -- ++ (0, 0.4) -- ++ (1, 1) -- ++ (0, 0.3);
                    \draw[left dual, rounded corners] (1, 0) -- ++ (0, 0.2) -- ++ (1, 1) -- ++ (0, 0.2) coordinate (A);
                    \draw[wire] (A) arc (180:0:0.5) coordinate (B);
                    \draw[left dual, rounded corners] (B) -- ++ (0, -0.2) -- ++ (-1, -1) -- ++ (0, -0.2);
                    \draw[left dual=0.55] (3, 0) arc (180:0:0.5);
                    \draw[left dual, rounded corners] (5, 0) -- ++ (0, 0.2) -- ++ (-1, 1) -- ++ (0, 0.4) -- ++ (-1, 1) -- ++ (0, 0.3);
                    \node at (5.5, 0 |- current bounding box) {\(=\)};
                    \begin{scope}[xshift=6cm]
                        \draw[right dual, rounded corners] (0, 0) -- ++ (0, 0.2) -- ++ (1, 1) -- ++ (0, 0.4) -- ++ (1, 1) -- ++ (0, 0.3);
                        \draw[left dual=0.55] (1, 0) arc (180:0:0.5);
                        \draw[left dual, rounded corners] (4, 0) -- ++ (0, 0.2) -- ++ (-1, 1) -- ++ (0, 0.2) coordinate (A);
                        \draw[wire] (A) arc (0:180:0.5) coordinate (B);
                        \draw[left dual, rounded corners] (B) -- ++ (0, -0.2) -- ++ (1, -1) -- ++ (0, -0.2);
                        \draw[left dual, rounded corners] (5, 0) -- ++ (0, 0.2) -- ++ (-1, 1) -- ++ (0, 0.4) -- ++ (-1, 1) -- ++ (0, 0.3);
                    \end{scope}
                \end{tikzpicture}
                .
            \end{equation}
        \end{proof}
    \end{lma}
    
    Note that this monoid is not, in general, commutative.
    
    It turns out that lots of monoids can be recast as monoids of this form.
    For example, consider the algebra \(\matrices{n}{\complex}\) viewed as a monoid object in \(\FHilb\).
    A matrix can be considered as linear map, so it can be interpreted as an element of \(\FHilb(\complex^n, \complex^n)\).
    This is isomorphic to \((\complex^n)^* \otimes \complex^n\).
    More generally, \(\FVect(V, V) \isomorphic V^* \otimes V\).
    Making this identification leads to the following lemma.
    
    \begin{lma}{Pair of Pants on {\normalsize\(\FHilb\)}}{}
        The pair of pants monoid \(\FHilb\) is simply the matrix algebra \(\matrices{n}{\complex}\).
        \begin{proof}
            Let \(A = \complex^n\), then \(A^* = (\complex^n)^{*} \isomorphic \complex^n\).
            Fix some orthonormal basis \(\{\ket{i}\}\) for \(\complex^n\), then \(\{\bra{i}\}\) is a basis for \((\complex^n)^*\).
            Define a map
            \begin{align}
                \varphi\colon(\complex^n)^* \otimes \complex^n &\to \matrices{n}{\complex}\\
                \bra{i} \otimes \ket{j} &\mapsto e_{ij}
            \end{align}
            where \(e_{ij} \in \matrices{n}{\complex}\) is defined as the matrix with zeros everywhere except a single entry on row \(i\) and column \(j\) which is 1.
            
            This map is clearly a bijection, and it respects multiplication.
            Recall that the cap on \(\FHilb\) is defined by
            \begin{equation}
                \tikzsetnextfilename{monoids-cap-FHilb}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[left dual=0.55] (0, 0) node [below] {\(\ket{j}\)} arc (180:0:0.5) node [below] {\(\bra{k}\)};
                \end{tikzpicture}
                = \varepsilon(\ket{j} \otimes \bra{k}) = \braket{k}{j} = \delta_{kj}.
            \end{equation}
            Then we have
            \begin{equation}
                \tikzsetnextfilename{monoids-pair-of-pants-FHilb}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \draw[left dual=0.55] (0, 0) node [below] {\(\ket{j}\)} arc (180:0:0.5) node [below] {\(\bra{k}\)};
                    \draw[right dual, rounded corners] (-1, 0) node [below] {\(\bra{i}\)}-- ++ (0, 0.2) -- ++ (1, 1) -- ++ (0, 0.2) node [above] {\(\bra{i}\)};
                    \draw[left dual, rounded corners] (2, 0) node [below] {\(\ket{l}\)} -- ++ (0, 0.2) -- ++ (-1, 1) -- ++ (0, 0.2) node [above] {\(\ket{l}\)};
                \end{tikzpicture}
                = \delta_{jk} \bra{i} \otimes \ket{l} = 
                \begin{cases}
                    \bra{i} \otimes \ket{\ell} & j = k,\\
                    0 & j \ne k.
                \end{cases}
            \end{equation}
            Applying the map \(\varphi\) this result becomes
            \begin{equation}
                \begin{cases}
                    e_{il} & j = k\\
                    0 & j \ne k
                \end{cases}
                = e_{ij} e_{kl}
            \end{equation}
            where \(e_{ij}e_{kl}\) is the product of these two matrices (note that \(i\), \(j\), \(k\), and \(l\) are not indexing particular components of the matrix, they label the matrices).
            This shows that \(\varphi\) preserves multiplication.
            
            The identity in the pair of pants monoid is \(\sum_i \bra{i} \otimes \ket{i}\).
            This is just the usual completeness relation for Hilbert spaces, but with the order of the bras and kets swapped.
            We then have \(\sum_i \bra{i} \otimes \ket{i} \mapsto \sum_i e_{ii}\), which is just the identity matrix.
            So \(\varphi\) also preserves identities, and so is a bijective monoid homomorphism, and hence is a monoid isomorphism.
        \end{proof}
    \end{lma}
    
    \section{Comonoids: The Idea}
    Consider a process which takes an object and copies it.
    This should give a copying, or duplication, operation \(d \colon A \to A \otimes A\).
    We can draw this as
    \begin{equation}
        \tikzsetnextfilename{monoids-comultiplication}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[morphism, minimum width=1cm] (d) {\(d\)};
            \draw[wire] ($(d.north) + (-0.25, 0)$) -- ++ (0, 1);
            \draw[wire] ($(d.north) + (0.25, 0)$) -- ++ (0, 1);
            \draw[wire] (d.south) -- ++ (0, -1);
        \end{tikzpicture}
        .
    \end{equation}
    
    What other properties should this copying have?
    The first thing we might look for is that this is really copying.
    One way to test this is to take an object, copy it, then throw away one of the copies, the result should be the original object, and it shouldn't matter which of the two objects we delete.
    That is, we should have
    \begin{equation}
        \tikzsetnextfilename{monoiods-counitality}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[morphism, minimum width=1cm] (d) {\(d\)};
            \draw[wire] ($(d.north) + (-0.25, 0)$) -- ++ (0, 1.2) coordinate (top);
            \draw[wire] ($(d.north) + (0.25, 0)$) -- ++ (0, 0.5) coordinate (A);
            \draw[wire] (d.south) -- ++ (0, -1) coordinate (bottom);
            \node[effect, minimum width=0.7cm, inner sep=0pt, above] at (A) {\smash{\raisebox{-0.5ex}{\(e\)}}};
            \node at (current bounding box -| 1, 0) {\(=\)};
            \draw[wire] (1.5, 0 |- bottom) -- (1.5, 0 |- top);
            \node at (current bounding box -| 2, 0) {\(=\)};
            \begin{scope}[xshift=3cm, xscale=-1]
                \node[morphism, minimum width=1cm] (d) {\(d\)};
                \draw[wire] ($(d.north) + (-0.25, 0)$) -- ++ (0, 1.2);
                \draw[wire] ($(d.north) + (0.25, 0)$) -- ++ (0, 0.5) coordinate (A);
                \draw[wire] (d.south) -- ++ (0, -1);
                \node[effect, minimum width=0.7cm, inner sep=0pt, above] at (A) {\smash{\raisebox{-0.5ex}{\(e\)}}};
            \end{scope}
        \end{tikzpicture}
    \end{equation}
    where \(e \colon A \to I\) can be thought of as deletion.
    
    Another property that we should consider is what happens if we make multiple copies?
    It shouldn't matter which of the two copies from the first copying process we use to make the copies, the result should be the same both ways.
    That is,
    \begin{equation}
        \tikzsetnextfilename{monoids-coassociativity}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[morphism, minimum width=1cm] (d) {\(d\)};
            \draw[wire] ($(d.north) + (-0.25, 0)$) -- ++ (0, 2) coordinate (top);
            \draw[wire] ($(d.north) + (0.25, 0)$) -- ++ (0, 0.5) coordinate (A);
            \draw[wire] (d.south) -- ++ (0, -1) coordinate (bottom);
            \node[morphism, minimum width=0.7cm, above] (d2) at (A) {\(d\)};
            \draw[wire] ($(d2.north) + (-0.25, 0)$) coordinate (here) -- (here |- top);
            \draw[wire] ($(d2.north) + (0.25, 0)$) coordinate (here) -- (here |- top);
            \node at (1, 0 |- current bounding box) {\(=\)};
            \begin{scope}[xshift=2cm, xscale=-1]
                \node[morphism, minimum width=1cm] (d) {\(d\)};
                \draw[wire] ($(d.north) + (-0.25, 0)$) -- ++ (0, 2) coordinate (top);
                \draw[wire] ($(d.north) + (0.25, 0)$) -- ++ (0, 0.5) coordinate (A);
                \draw[wire] (d.south) -- ++ (0, -1) coordinate (bottom);
                \node[morphism, minimum width=0.7cm, above] (d2) at (A) {\(d\)};
                \draw[wire] ($(d2.north) + (-0.25, 0)$) coordinate (here) -- (here |- top);
                \draw[wire] ($(d2.north) + (0.25, 0)$) coordinate (here) -- (here |- top);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    
    Sometimes, but not always, swapping the two objects after we copy won't matter, if this is the case then we have
    \begin{equation}
        \tikzsetnextfilename{monoids-cocommutativity}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[morphism, minimum width=1cm] (c) {\(c\)};
            \draw[wire, rounded corners] ($(c.north) + (0.25, 0)$) -- ++ (0, 0.2) -- ++ (-0.5, 0.5) -- ++ (0, 0.2);
            \draw[over wire, rounded corners] ($(c.north) + (-0.25, 0)$) -- ++ (0, 0.2) -- ++ (0.5, 0.5) -- ++ (0, 0.2);
            \draw[wire] (c.south) -- ++ (0, -0.9);
            \node at (1, 0 |- current bounding box) {\(=\)};
            \begin{scope}[xshift=2cm]
                \node[morphism, minimum width=1cm] (c) {\(c\)};
                \draw[wire] ($(c.north) + (0.25, 0)$) -- ++ (0, 0.9);
                \draw[wire] ($(c.north) + (-0.25, 0)$) -- ++ (0, 0.9);
                \draw[wire] (c.south) -- ++ (0, -0.9);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}

    Now consider the monoid laws, and we see that
    \begin{itemize}
        \item the \enquote{true copy rule} is just the unitality law upside-down,
        \item the \enquote{multiple copy rule} is just the associativity law upside-down,
        \item the \enquote{copy then swap rule} is just the commutativity rule upside-down.
    \end{itemize}
    More formally, by upside-down we mean that the direction of the morphisms has been reversed, so we're in the opposite category.
    We can therefore consider these rules to be counitality, coassociativity, and cocommutativity respectively, and define a comonoid as a monoid in the opposite category.
    Expanding this definition leads to the definition of the comonoid.
    
    \section{Comonoids: The Definition}
    \begin{dfn}{Comonoid}{}
        A \defineindex{comonoid} in a monoidal category, \(\cat{C}\), is a triple, \((A, \comonoidProduct, \comonoidIdentity)\), consisting of an object, \(A \in \Ob(\cat{C})\), a morphism, \(\comonoidProduct \colon A \to A \otimes A\), and an effect, \(\comonoidIdentity\colon A \to I\), satisfying the \defineindex{coassociativity} and \defineindex{counitality} equations,
        \begin{equation}
            \tikzsetnextfilename{monoids-coassociativity-law}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw[wire] (0, 0) node [above] {\(A\)} arc (180:360:0.5) node [above] {\(A\)};
                \draw[wire] (0.5, -0.5) arc (180:360:0.75) -- ++ (0, 0.5) node [above] {\(A\)};
                \draw[wire] (1.25, -1.25) -- ++ (0, -0.75) node [below] {\(A\)};
                \node[comonoid dot] at (0.5, -0.5) {};
                \node[comonoid dot] at (1.25, -1.25) {};
                \begin{scope}[xshift=5cm, xscale=-1]
                    \draw[wire] (0, 0) node [above] {\(A\)} arc (180:360:0.5) node [above] {\(A\)};
                    \draw[wire] (0.5, -0.5) arc (180:360:0.75) -- ++ (0, 0.5) node [above] {\(A\)};
                    \draw[wire] (1.25, -1.25) -- ++ (0, -0.75) node [below] {\(A\)};
                    \node[comonoid dot] at (0.5, -0.5) {};
                    \node[comonoid dot] at (1.25, -1.25) {};
                \end{scope}
                \node at (current bounding box) {\(=\)};
            \end{tikzpicture}
            ,
        \end{equation}
        and
        \begin{equation}
            \tikzsetnextfilename{monoids-counitality-law}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw[wire] (0, 0) node [above] {\(A\)} -- ++ (0, -0.5) arc (180:360:0.5) node [comonoid dot] {};
                \draw[wire] (0.5, -1) node [comonoid dot] {} -- ++ (0, -1) node [below] {\(A\)} coordinate (bottom);
                \node at (1.5, 0 |- current bounding box) {\(=\)};
                \draw[wire] (2, 0 |- bottom) node [below] {\(A\)} -- (2, 0) node [above] {\(A\)};
                \node at (2.5, 0 |- current bounding box) {\(=\)};
                \begin{scope}[xshift=4cm, xscale=-1]
                    \draw[wire] (0, 0) node [above] {\(A\)} -- ++ (0, -0.5) arc (180:360:0.5) node [comonoid dot] {};
                    \draw[wire] (0.5, -1) node [comonoid dot] {} -- ++ (0, -1) node [below] {\(A\)};
                \end{scope}
            \end{tikzpicture}
        \end{equation}
        respectively.
        
        These relations can also be expressed as commutative diagrams for a comonoid \((A, d, e)\) we require
        \begin{equation}
            \begin{tikzcd}[row sep=small]
                & A \otimes A \arrow[r, "d \otimes \id_A"] & (A \otimes A) \otimes A \arrow[dd, "\alpha_{A,A,A}"]\\
                A \arrow[ur, "d"] \arrow[dr, "d"']\\
                & A \otimes A \arrow[r, "{\id_A} \otimes d"'] & A \otimes (A \otimes A),
            \end{tikzcd}
        \end{equation}
        and
        \begin{equation}
            \begin{tikzcd}
               A \otimes I \arrow[r, "\rho_A"] & A \arrow[d, "\id_A"] & I \otimes A \arrow[l, "\lambda_A"'] \\
               A \otimes A \arrow[u, "{\id_A} \otimes e"] & A \arrow[l, "d"] \arrow[r, "d"'] & A \otimes A \arrow[u, "e \otimes \id_A"']
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Cocommutative Comonoid}{}
        A \defineindex{cocommutative comonoid} in a braided monoidal category, \(\cat{C}\), is a comonoid, \((A, \comonoidProduct, \comonoidIdentity)\), satisfying the \defineindex{cocommutativity} equation
        \begin{equation}
            \tikzsetnextfilename{monoids-cocommutativity-law}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw[wire] (0, 0) node [below] {\(A\)} -- (0, 1);
                \draw[wire] (-0.5, 1.5) arc (180:360:0.5);
                \node[comonoid dot] at (0, 1) {};
                \draw[wire, rounded corners] (0.5, 1.5) -- ++ (0, 0.2) -- ++ (-1, 0.5) -- ++ (0, 0.2) node [above] {\(A\)};
                \draw[over wire, rounded corners] (-0.5, 1.5) -- ++ (0, 0.2) -- ++ (1, 0.5) -- ++ (0, 0.2) node [above] {\(A\)};
                \begin{scope}[xshift=2cm]
                    \draw[wire] (0, 0) node [below] {\(A\)} -- (0, 1);
                    \draw[wire] (-0.5, 1.5) arc (180:360:0.5);
                    \node[comonoid dot] at (0, 1) {};
                    \draw[wire] (0.5, 1.5) -- ++ (0, 0.9) node [above] {\(A\)};
                    \draw[wire] (-0.5, 1.5) -- ++ (0, 0.9) node [above] {\(A\)};
                \end{scope}
                \node at (current bounding box) {\(=\)};
            \end{tikzpicture}
            .
        \end{equation}
        This can also be expressed as a commutative diagram for the comonoid \((A, d, e)\):
        \begin{equation}
            \begin{tikzcd}
                A \arrow[r, "d"] \arrow[d, "d"'] & A \otimes A\\
                A \otimes A. \arrow[ur, "\sigma_{A,A}"']
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    It doesn't matter if we use the swap or the inverse swap in a cocommutative monoid, things work out the same as the next lemma shows.
    
    \begin{lma}{}{}
        In a cocommutative monoid \(\sigma_{A,A}^{-1} \circ d = d\),
        \begin{proof}
            In a cocommutative monoid by definition \(\sigma_{A,A} \circ d = d\).
            Applying this to the equation \(\sigma_{A,A}^{-1} \circ \sigma_{A,A} \circ d = \id_A \circ d = d\) we are left with \(\sigma_{A,A}^{-1} \circ d = d\).
        \end{proof}
    \end{lma}
    
    \section{Comonoids: The Examples}
    In \(\Set\) any given nonempty set, \(A\), forms exactly one comonoid, \((A, c, d)\), with \(c(a) = (a, a)\) and \(d(a) = \bullet\).
    This is cocommutative.
    Since there is only one monoid we can define for any set there is no \enquote{a comonoid is a set with\dots} definition of a comonoid.
    
    Take some group, \(G\), and we can form a comonoid in \(\Rel\) by defining a comultiplication with the relation \(g \sim (h, h^{-1}g)\) for any \(g, h \in G\) and the counit as the relation \(1 \sim \bullet\).
    Here \(\{\bullet\}\) is the unit object in \(\Rel\).
    We can check that this is a comonoid:
    \begin{itemize}
        \item Counitality: Take some \(g \in G\), and apply \(c\), so now we have \(g \sim (h, h^{-1}g)\) for any \(h \in G\).
        Now apply \({\id_G} \otimes d\), the result depends on \(h^{-1}g\).
        If \(h^{-1}g = 1\) then we get \((h, h^{-1}g) = (h, 1) \sim (h, \bullet)\), on the other hand if \(h^{-1}g \ne 1\) then there is no relation.
        So, we must have \(h^{-1}g = 1\), which we can rearrange to get \(g = h\).
        Finally applying \(\rho_I\) we get \((h, \bullet) \sim h = g\).
        Chaining these relations we have \(g \sim g\), which is to say that this chain is the identity relation, as required.
        Counitality on the other side can be demonstrated similarly.
        \item Coassociativity: Take some \(g \in G\), and apply \(c\), we get \(g \sim (h, h^{-1}g)\) for any \(h \in G\).
        Now apply \(c \otimes \id_A\) and we get \((h, h^{-1}g) \sim ((k, k^{-1}h), h^{-1}g)\) for any \(k \in G\).
        Reassociating gives \(((k, k^{-1}h), h^{-1}g) \sim (k, (k^{-1}h, h^{-1}g))\), so \(g \sim (k, (k^{-1}h, h^{-1}g))\).
        If instead we had applied \({\id_A} \otimes c\) we would get \((h, h^{-1}g) \sim (h, (k, k^{-1}h^{-1}g))\).
        Chaining these relations together we have \(g \sim (h, (k, k^{-1}h^{-1}g))\).
        Now set \(h \to k\) and \(k \to k^{-1}h\), then we get \(g \sim (k, (k^{-1}h, (k^{-1}h)^{-1}k^{-1}g)) = (k, (k^{-1}h, h^{-1}kk^{-1}g)) = (k, (k^{-1}h, h^{-1}g))\), which is exactly the result we got before.
        \item Cocommutativity: Take some \(g \in G\) and apply \(c\), giving \(g \sim (h, h^{-1}g)\).
        If we apply \(\sigma_{A,A}\) we get \((h, h^{-1}g) \sim (h^{-1}g, h)\).
        This comonoid will be cocommutative if and only if \((h, h^{-1}g) = (k^{-1}g, k)\).
        Clearly this holds if and only if \(k = h^{-1}g\), that is \(hk = g\), but we also require \(h = k^{-1}g\), so \(kh = g\).
        Thus, in order for this to be a cocommutative monoid we need to have \(hk = g = kh\), so the group must be Abelian.
    \end{itemize}
    
    In \(\FHilb\) choose some basis, \(\{e_i\}\) for a Hilbert space \(H\).
    We can define a cocommutative comonoid by defining comultiplication according to \(e_i \mapsto e_i \otimes e_i\) and the counit according to \(e_i \mapsto 1\), and extending these by linearity to all of \(H\).
    We can check that this defines a cocommutative comonoid, and thanks to linearity we need only check the laws hold on each basis vectors:
    \begin{itemize}
        \item Coassociativity: Going one way around the diagram we have
        \begin{equation}
            e_i \xmapsto{c} e_i \otimes e_i \xmapsto{c \otimes \id_H} (e_i \otimes e_i) \otimes e_i \xmapsto{\alpha_{H,H,H}} e_i \otimes (e_i \otimes e_i)
        \end{equation}
        and going the other way we get
        \begin{equation}
            e_i \xmapsto{c} e_i \otimes e_i \xmapsto{{\id_H} \otimes c} e_i \otimes (e_i \otimes e_i),
        \end{equation}
        which are both the same so coassociativity is satisfied.
        \item Counitality: Going one way around the diagram we have
        \begin{equation}
            e_i \xmapsto{c} e_i \otimes e_i \xmapsto{{\id_H} \otimes d} e_i \otimes 1 \xmapsto{\rho_H} e_i
        \end{equation}
        so this chain is simply the identity.
        The other counitality law can be checked similarly.
        \item Cocommutativity: We have
        \begin{equation}
            e_i \xmapsto{c} e_i \otimes e_i \xmapsto{\sigma_{H,H}} e_i \otimes e_i
        \end{equation}
        so clearly this is cocommutative.
    \end{itemize}
    
    \chapter{Uniform Copying and Deleting}
    \section{Uniform Deleting}
    Consider a comonoid, \((A, d, e)\).
    The counit, \(e \colon A \to I\) gives us a way to \enquote{delete} copies of the system, \(A\).
    Suppose we have a way of systematically deleting every object.
    Such a process must respect the tenor product, leading to the following definition.
    
    \begin{dfn}{Uniform Deleting}{}
        Consider a monoidal category, \(\cat{C}\), with unit object \(I\).
        This category has \defineindex{uniform deleting} if there exists a natural transformation \(e \colon \id_{\cat{C}} \naturalTransformation \const_I\), with components \(e_A \colon A \to I\), such that \(e_I = \id_I\) and
        \begin{equation}
            \begin{tikzcd}
                & A \otimes B \arrow[dl, "e_A \otimes e_B"'] \arrow[dr, "e_{A\otimes B}"] &\\
                I \otimes I \arrow[rr, "\lambda_I"'] && I
            \end{tikzcd}
        \end{equation}
        commutes.
    \end{dfn}
    
    Writing out the naturality condition for \(e\) the diagram
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "e_A"] \arrow[d, "f"'] & I \arrow[d, "\id_I"]\\
            B \arrow[r, "e_B"'] & I
        \end{tikzcd}
    \end{equation}
    must commute.
    
    This definition states that uniform deleting is a collection of isomorphisms deleting systems, that is sending them to the empty system, \(I\).
    The commutativity definition is simply that deleting both systems independently, then combining the two empty systems, is the same as deleting the combined system.
    
    \begin{lma}{}{lma:uniform deleting iff terminal unit}
        Let \(\cat{C}\) be a monoidal category with unit object \(I\).
        Then \(\cat{C}\) has uniform deleting if and only if \(I\) is terminal.
        \begin{proof}
            Suppose that \(\cat{C}\) has uniform deleting
            Then for each object \(A\) we have a morphism \(e_A \colon A \to I\).
            We need to show that this is unique.
            The naturality condition for \(e\), specialised to the case where the second object is \(I\), becomes the commuting square
            \begin{equation}
                \begin{tikzcd}
                    A \arrow[r, "e_A"] \arrow[d, "f"'] & I \arrow[d, "\id_I"]\\
                    I \arrow[r, "e_I = \id_I"'] \arrow[ur, equal] & I.
                \end{tikzcd}
            \end{equation}
            which commutes if and only if \(f = e_A\), proving that there is a \emph{unique} morphism \(A \to I\) for all objects \(A\).
            
            Conversely, suppose that \(I\) is terminal.
            Then simply define \(e\) such that \(e_A \colon A \to I\) is the unique morphism \(A \to I\).
            The commutativity property is satisfied by uniqueness of morphisms into \(I\).
        \end{proof}
    \end{lma}
    
    Deleting causes problems in quantum mechanics.
    To delete a state we must lose information, which is not generally possible.
    A quantum system can be modelled as a monoidal category with duals.
    The way that this issue with deleting arises in category theory is that any quantum system with uniform deleting will be boring, in the sense it will be a preorder.
    
    \begin{dfn}{Preorder}{}
        A \defineindex{preorder} is a category with at most one morphism \(A \to B\) for each pair of objects \(A\) and \(B\).
    \end{dfn}
    
    Note that a preorder can be defined similarly to a poset, but relaxing the requirement of antisymmetry, and then interpreted as a category in the same way we think of a preorder as a category.
    
    Viewed as processes preorders are particularly boring categories, there is at any one point at most one process which can occur.
    
    \begin{thm}{No Deleting}{}
        If a monoidal category with duals has uniform deleting then it is a preorder.
        
        \begin{proof}
            Let \(f, g \colon A \to B\) be morphisms in this category.
            We will show that \(f = g\).
            Recall that we can define the coname, \(\coname{f} \colon A \otimes B^* \to I\).
            Naturality of \(e\) for this morphism gives
            \begin{equation}
                \begin{tikzcd}
                    A \otimes B^* \arrow[r, "e_{A\otimes B^*}"] \arrow[d, "\coname{f}"'] & I \arrow[d, "\id_I"]\\
                    I \arrow[r, "e_I = \id_I"'] & I.
                \end{tikzcd}
            \end{equation}
            Terminality of the unit (\cref{lma:uniform deleting iff terminal unit}) implies that \(\coname{f} = e_{A\otimes B^*}\).
            
            Exactly the same logic implies that \(\coname{g} = e_{A \otimes B^*}\).
            Since the coname uniquely specifies the morphism we must have that \(f = g\).
            Hence this category is a preorder.
        \end{proof}
    \end{thm}
    
    \section{Uniform Copying}
    Consider a comonoid, \((A, d, e)\).
    The comultiplication, \(d \colon A \to A \otimes A\) gives us a way to create an extra copy of our system.
    Suppose we have a way of systematically copying every object.
    Such a process must respect the tensor product, leading to the following definition.
    A subtlety in this is that if we copy \(A \otimes B\) as a single object we get \((A \otimes B) \otimes (A \otimes B)\), but if we copy each part independently we get \((A \otimes A) \otimes (B \otimes B)\), so we must work in a braided monoidal category.
    
    \begin{dfn}{Uniform Copying}{}
        Consider a braided monoidal category, \(\cat{C}\), with unit object \(I\).
        This category has \defineindex{uniform copying} if there exists a natural transformation \(d \colon \id_{\cat{C}} \naturalTransformation \Delta\), with components \(d \colon A \to A \otimes A\), satisfying coassociativity and cocommutativity, with \(d_I = \rho_I^{-1}\) and 
        \begin{equation}\label{eqn:uniform copying definition}
            \tikzsetnextfilename{uniform-copying-respects-tensor-products}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \node[morphism, minimum width=1.3cm] (dA) at (0, 0) {\(d_A\)};
                \node[morphism, minimum width=1.3cm] (dB) at (2, 0) {\(d_B\)};
                \draw[wire] (dA) -- ++ (0, -1) node [below] {\(A\)};
                \draw[wire] (dB) -- ++ (0, -1) node [below] {\(B\)};
                \draw[wire] ($(dA.north) + (-0.5, 0)$) -- ++ (0, 1) node [above] {\(A\)};
                \draw[wire] ($(dB.north) + (0.5, 0)$) -- ++ (0, 1) node [above] {\(B\)};
                \draw[wire, rounded corners] ($(dB.north) + (-0.5, 0)$) -- ++ (0, 0.2) -- ++ (-1, 0.6) -- ++ (0, 0.2) node [above] {\(B\)};
                \draw[over wire, rounded corners] ($(dA.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (1, 0.6) -- ++ (0, 0.2) node [above] {\(A\)};
                \node at (3, 0) {\(=\)};
                \node[morphism, minimum width=3.3cm] (d) at (5, 0) {\(d_{A \otimes B}\)};
                \draw[wire] ($(d.south) + (-0.5, 0)$) -- ++ (0, -1) node [below] {\(A\)};
                \draw[wire] ($(d.south) + (0.5, 0)$) -- ++ (0, -1) node [below] {\(B\)};
                \draw[wire] ($(d.north) + (-0.5, 0)$) -- ++ (0, 1) node [above] {\(B\)};
                \draw[wire] ($(d.north) + (-1.5, 0)$) -- ++ (0, 1) node [above] {\(A\)};
                \draw[wire] ($(d.north) + (0.5, 0)$) -- ++ (0, 1) node [above] {\(A\)};
                \draw[wire] ($(d.north) + (1.5, 0)$) -- ++ (0, 1) node [above] {\(B\)};
            \end{tikzpicture}
        \end{equation}
        that is
        \begin{equation*}
            \begin{tikzcd}
                A \otimes B \arrow[dd, "d_{A \otimes B}"'] \arrow[r, "d_A \otimes d_B"] & (A \otimes A) \otimes (B \otimes B) \arrow[r, "\alpha_{A,A,B\otimes B}"{yshift=0.1cm}] & A \otimes (A \otimes (B \otimes B)) \arrow[d, "\id_A \otimes \alpha_{A,B,B}^{-1}"] \\
                && A \otimes ((A \otimes B) \otimes B) \arrow[d, "\id_A \otimes (\sigma_{A,B} \otimes B)"]\\
                (A \otimes B) \otimes (A \otimes B) & A \otimes (B \otimes (A \otimes B)) \arrow[l, "\alpha_{A,B,A\otimes B}"{yshift=-0.1cm}] & A \otimes ((B \otimes A) \otimes B) \arrow[l, "\id_A \otimes \alpha_{B,A,B}"{yshift=-0.1cm}]
            \end{tikzcd}
        \end{equation*}
        commutes.
    \end{dfn}
    
    The naturality condition for \(d\), commutativity of
    \begin{equation}
        \begin{tikzcd}
            A \arrow[r, "d_A"] \arrow[d, "f"'] & A \otimes A \arrow[d, "f \otimes f"]\\
            B \arrow[r, "d_B"'] & B \otimes B,
        \end{tikzcd}
    \end{equation}
    can be expressed as
    \begin{equation}
        \tikzsetnextfilename{uniform-copying-naturality-condition}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node[morphism, minimum width=1.3cm] (dA) {\(d_A\)};
            \draw[wire] (dA) -- ++ (0, -1) node [below] {\(A\)};
            \draw[wire] ($(dA.north) + (-0.5, 0)$) -- ++ (0, 0.5) coordinate (f1);
            \draw[wire] ($(dA.north) + (0.5, 0)$) -- ++ (0, 0.5) coordinate (f2);
            \node[morphism, above] (f3) at (f1) {\(f\)};
            \node[morphism, above] (f4) at (f2) {\(f\)};
            \draw[wire] (f3.north) -- ++ (0, 0.5) node [above] {\(B\)};
            \draw[wire] (f4.north) -- ++ (0, 0.5) node [above] {\(B\)} coordinate (top);
            \node at (1.25, 0) {\(=\)};
            \node[morphism] (f) at (2, 0) {\(f\)};
            \node[morphism, above] (dA) at (2, 0.75) {\(d_A\)};
            \draw[wire] (f) -- (dA);
            \draw[wire] (f) -- ++ (0, -1) node [below] {\(A\)};
            \draw[wire] ($(dA.north) + (-0.2, 0)$) coordinate (here) -- (here |- top) node [above] {\(B\)};
            \draw[wire] ($(dA.north) + (0.2, 0)$) coordinate (here) -- (here |- top) node [above] {\(B\)};
        \end{tikzpicture}
        .
    \end{equation}
    That \(d_I = \id_I\) can be expressed as
    \begin{equation}
        \tikzsetnextfilename{uniform-copying-copy-unit}
        \begin{tikzpicture}[baseline=(d.base)]
            \node[morphism] (d) {\(d_I\)};
            \node[right=0.1cm of d] {\(=\)};
            \draw[thick, dashed] (1, -0.25) rectangle (1.5, 0.25);
        \end{tikzpicture}
        \,.
    \end{equation}
    
    \begin{exm}{}{}
        \(\Set\) has uniform copying defined by the maps \(a \mapsto (a, a)\).
        Writing \(1 = \{\bullet\}\) for the singleton set, which is the unit of \(\Set\), and recalling that \(\rho_A(a, \bullet) = a\) for all sets \(A\), we have \(d_1(\bullet) = (\bullet, \bullet) = \rho_1^{-1}(\bullet)\).
    \end{exm}
    
    \begin{dfn}{Copyable}{}
        In a braided monoidal category a state, \(u \colon I \to A\), is \defineindex{copyable} with respect to a map \(d_A \colon A \to A \otimes A\) when
        \begin{equation}
            \tikzsetnextfilename{uniform-copying-copyable}
            \begin{tikzpicture}[baseline=(equal.base)]
                \node[morphism] (d) {\(d_A\)};
                \node[state, below=0.5cm of d] (u) {\(u\)};
                \draw[wire] (d) -- (u);
                \draw[wire] (d.north) -- ++ (0, 0.5) coordinate (top);
                \node[state, right=of u] (u1) {\(u\)};
                \node[state, right=0.5cm of u1] (u2) {\(u\)};
                \draw[wire] (u1) -- (u1 |- top);
                \draw[wire] (u2) -- (u2 |- top);
                \node (equal) at (0.8, -0.4) {\(=\)};
            \end{tikzpicture}
            ,
        \end{equation}
        that is, \(d_A \circ u = (u \otimes u) \circ \rho_I\).
    \end{dfn}
    
    \begin{clm}{}{}
        In a braided monoidal category with uniform copying all states are copyable.
        
        \begin{proof}
            Naturality of the uniform copying map specialised to morphisms \(u \colon I \to A\) gives the commuting square
            \begin{equation}
                \begin{tikzcd}
                    I \arrow[r, "d_I = \rho_I^{-1}"] \arrow[d, "u"'] & I \otimes I \arrow[d, "u \otimes u"]\\
                    A \arrow[r, "d_A"'] & A \otimes A.
                \end{tikzcd}
            \end{equation}
            That is \(d_A \circ u = (u \otimes u) \circ \rho_I\), which is exactly the requirement for \(u\) to be copyable.
        \end{proof}
    \end{clm}
    
    We saw that uniform deleting rendered a quantum theory trivial, in the sense that we lost all but one process per system and were left with a preorder.
    We will similarly show that uniform copying renders a quantum theory trivial, although this time in the sense that all maps simply become scalar multiplies of the identity, so we can't do anything interesting, especially since we usually normalise states and ignore overall phases, so these scalings don't change the physics.
    Before we show this we'll need a couple of lemmas, which already start to show that if we have duals and uniform copying then things are intuitively not quite right.
    
    \begin{lma}{}{lma:double cup braided mon cat duals and copying}
        In a braided monoidal category with duals and uniform copying
        \begin{equation}
            \tikzsetnextfilename{uniform-copying-duals}
            \begin{tikzpicture}[baseline=(equal.base)]
                \draw[left dual] (0, 0) node [above] {\(A\mathrlap{^*}\)} arc (180:360:0.5) node [above] {\(A\)};
                \draw[left dual] (2, 0) node [above] {\(A\mathrlap{^*}\)} arc (180:360:0.5) node [above] {\(A\)};
                \node (equal) at (3.5, -0.25) {\(=\)};
                \draw[left dual, xshift=4cm, xscale=3] (0, 0) node [above] {\(A\mathrlap{^*}\)} -- ++ (0, -1.1) arc (180:360:0.5) -- ++ (0, 1.1) node [above] {\(A\)};
                \draw[wire] (5, -0.9) arc (180:360:0.5);
                \draw[wire, rounded corners] (5, 0) -- (5, -0.35) -- (6, -0.6) -- ++ (0, -0.3);
                \draw[over wire, rounded corners] (6, 0) -- (6, -0.35) -- (5, -0.6) -- ++ (0, -0.3);
                \draw[right dual=1] (5, 0) node [above] {\(A\)} -- ++ (0, -0.2);
                \draw[left dual=1] (6, 0) node [above] {\(A\mathrlap{^*}\)} -- ++ (0, -0.2);
            \end{tikzpicture}
            .
        \end{equation}
        \begin{proof}
            Recalling that \(d_I = \rho_I^{-1}\) we can insert the scalar \(d_I\) into our diagram without changing anything, since it's really the empty diagram.
            This gives
            \begin{equation}\label{eqn:proving uniform copying duals 1}
                \tikzsetnextfilename{uniform-copying-duals-proof-1}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[left dual] (0, 0) node [above] {\(A\mathrlap{^*}\)} arc (180:360:0.5) node [above] {\(A\)};
                    \draw[left dual] (2, 0) node [above] {\(A\mathrlap{^*}\)} arc (180:360:0.5) node [above] {\(A\)};
                    \node (equal) at (3.5, -0.25) {\(=\)};
                    \begin{scope}[xshift=4cm]
                        \draw[left dual] (0, 0) node [above] {\(A\mathrlap{^*}\)} arc (180:360:0.5) node [above] {\(A\)};
                        \draw[left dual] (2, 0) node [above] {\(A\mathrlap{^*}\)} arc (180:360:0.5) node [above] {\(A\)};
                        \node[morphism] at (1.5, -1) {\(d_I\)};
                    \end{scope}
                \end{tikzpicture}
                .
            \end{equation}
            Naturality tells us that we can copy and then perform a process on each copy or perform the process and then copy.
            In this case the process is creating the system \(A^* \otimes A\).
            As written above we are copying the empty system then applying the process to create \(A^* \otimes A\).
            Instead, we can create \(A^* \otimes A\) from the empty system and then copy it.
            In terms of diagrams this gives
            \begin{equation}
                \tikzsetnextfilename{uniform-copying-duals-proof-2}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[left dual] (0, 0) node [above] {\(A\mathrlap{^*}\)} arc (180:360:0.5) node [above] {\(A\)} coordinate (top);
                    \draw[left dual] (2, 0) node [above] {\(A\mathrlap{^*}\)} arc (180:360:0.5) node [above] {\(A\)};
                    \node[morphism] at (1.5, -1) {\(d_I\)};
                    \node (equal) at (3.5, -0.25) {\(=\)};
                    \node[morphism, minimum width=3.3cm] (d) at (5.6, -1) {\(d_{A^* \otimes A}\)};
                    \draw[left dual=0.5] ($(d.south) - (0.5, 0)$) arc (180:360:0.5);
                    \draw[right dual] ($(d.north) - (1.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual] ($(d.north) - (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \draw[right dual] ($(d.north) + (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual] ($(d.north) + (1.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                \end{tikzpicture}
                .
            \end{equation}
            We can now apply the definition of uniform copying (\cref{eqn:uniform copying definition}) to write
            \begin{equation}\label{eqn:proving uniform copying duals 2}
                \tikzsetnextfilename{uniform-copying-duals-proof-3}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \coordinate (top) at (0, 0.2);
                    \node[morphism, minimum width=3.3cm] (d) at (0, -1) {\(d_{A^* \otimes A}\)};
                    \draw[left dual=0.5] ($(d.south) - (0.5, 0)$) arc (180:360:0.5);
                    \draw[right dual] ($(d.north) - (1.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual] ($(d.north) - (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \draw[right dual] ($(d.north) + (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual] ($(d.north) + (1.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \node (equal) at (2, -0.25) {\(=\)};
                    \node[morphism, minimum width=1.2cm] (d1) at (3, -1) {\(d_{A^*}\)};
                    \node[morphism, minimum width=1.2cm] (d2) at (5, -1) {\(\phantomrlap{d_A}{d_{A^*}}\)};
                    \draw[left dual, xscale=2] (d1.south) arc (180:360:0.5);
                    \draw[right dual] ($(d1.north) - (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual=0.3, rounded corners] ($(d2.north) - (0.5, 0)$) -- ++ (0, 0.2) -- ++ (-1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[over wire, right dual=0.3, rounded corners] ($(d1.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \draw[right dual] ($(d2.north) + (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                \end{tikzpicture}
                .
            \end{equation}
            Applying cocommutativity of \(d_{A^*}\), using the inverse swap, we have
            \begin{equation}
                \tikzsetnextfilename{uniform-copying-duals-proof-4}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \coordinate (top) at (0, 1);
                    \node[morphism, minimum width=1.2cm] (d1) at (0, -1) {\(d_{A^*}\)};
                    \node[morphism, minimum width=1.2cm] (d2) at (2, -1) {\(\phantomrlap{d_A}{d_{A^*}}\)};
                    \draw[left dual, xscale=2] (d1.south) arc (180:360:0.5);
                    \draw[right dual] ($(d1.north) - (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual=0.8, rounded corners] ($(d2.north) - (0.5, 0)$) -- ++ (0, 0.2) -- ++ (-1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \draw[over wire, right dual=0.8, rounded corners] ($(d1.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[right dual] ($(d2.north) + (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \node (equal) at (3, -0.25) {\(=\)};
                    \node[morphism, minimum width=1.2cm] (d1) at (4, -1) {\(d_{A^*}\)};
                    \node[morphism, minimum width=1.2cm] (d2) at (6, -1) {\(\phantomrlap{d_A}{d_{A^*}}\)};
                    \draw[left dual, xscale=2] (d1.south) arc (180:360:0.5);
                    \draw[left dual=0.24, rounded corners] ($(d2.north) - (0.5, 0)$) -- ++ (0, 1) -- ++ (-1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \draw[right dual, rounded corners, over wire] ($(d1.north) - (0.5, 0)$) -- ++ (0, 0.2) -- ++ (1, 0.5) -- ++ (0, 0.3) -- ++ (1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[over wire, right dual=0.8, rounded corners] ($(d1.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (-1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual] ($(d2.north) + (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                \end{tikzpicture}
                .
            \end{equation}
            Redrawing this diagram we have
            \begin{equation}
                \tikzsetnextfilename{uniform-copying-duals-proof-5}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \coordinate (top) at (0, 1.5);
                    \node[morphism, minimum width=1.2cm] (d1) at (0, -1) {\(d_{A^*}\)};
                    \node[morphism, minimum width=1.2cm] (d2) at (2, -1) {\(\phantomrlap{d_A}{d_{A^*}}\)};
                    \draw[left dual, xscale=2] (d1.south) arc (180:360:0.5);
                    \draw[left dual=0.24, rounded corners] ($(d2.north) - (0.5, 0)$) -- ++ (0, 1) -- ++ (-1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \draw[right dual=0.5, rounded corners, over wire] ($(d1.north) - (0.5, 0)$) -- ++ (0, 0.2) -- ++ (1, 0.5) -- ++ (0, 0.3) -- ++ (1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[over wire, right dual=0.8, rounded corners] ($(d1.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (-1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual] ($(d2.north) + (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \node (equal) at (3, 0) {\(\equaliso\)};
                    \node[morphism, minimum width=1.2cm] (d1) at (4, -1) {\(d_{A^*}\)};
                    \node[morphism, minimum width=1.2cm] (d2) at (6, -1) {\(\phantomrlap{d_A}{d_{A^*}}\)};
                    \draw[left dual, xscale=2] (d1.south) arc (180:360:0.5);
                    \draw[left dual=0.9, rounded corners] ($(d2.north) - (0.5, 0)$) -- ++ (0, 0.2) -- ++ (-1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \draw[right dual=0.4, rounded corners, over wire] ($(d1.north) - (0.5, 0)$) -- ++ (0, 0.7) -- ++ (2, 1) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[over wire, right dual=0.8, rounded corners] ($(d1.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (1, 0.5) -- ++ (0, 0.3) -- ++ (-2, 1) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual] ($(d2.north) + (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                \end{tikzpicture}
                .
            \end{equation}
            Now notice that the lower half of this diagram is exactly the right hand side of \cref{eqn:proving uniform copying duals 2}, so we can replace it with the double cup making up the left hand side of \cref{eqn:proving uniform copying duals 1}.
            This gives
            \begin{equation}
                \tikzsetnextfilename{uniform-copying-duals-proof-6}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \coordinate (top) at (0, 1.5);
                    \node[morphism, minimum width=1.2cm] (d1) at (0, -1) {\(d_{A^*}\)};
                    \node[morphism, minimum width=1.2cm] (d2) at (2, -1) {\(\phantomrlap{d_A}{d_{A^*}}\)};
                    \draw[left dual, xscale=2] (d1.south) arc (180:360:0.5);
                    \draw[left dual=0.9, rounded corners] ($(d2.north) - (0.5, 0)$) -- ++ (0, 0.2) -- ++ (-1, 0.5) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \draw[right dual=0.4, rounded corners, over wire] ($(d1.north) - (0.5, 0)$) -- ++ (0, 0.7) -- ++ (2, 1) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[over wire, right dual=0.8, rounded corners] ($(d1.north) + (0.5, 0)$) -- ++ (0, 0.2) -- ++ (1, 0.5) -- ++ (0, 0.3) -- ++ (-2, 1) coordinate (here) -- (here |- top) node [above] {\(A\mathrlap{^*}\)};
                    \draw[left dual] ($(d2.north) + (0.5, 0)$) coordinate (here) -- (here |- top) node [above] {\(A\)};
                    \node (equal) at (3, 0) {\(=\)};
                    \begin{scope}[xshift=-0.5cm]
                        \draw[right dual=0.2, rounded corners] (5, 1.5) node [above] {\(A\)} -- ++ (0, -1.7);
                        \draw[over wire, left dual=0.7, rounded corners] (6, 1.5) node [above] {\(A\mathrlap{^*}\)} -- ++ (0, -0.5) -- ++ (-2, -1) -- ++ (0, -0.2) coordinate (A);
                        \draw[over wire, left dual=0.3, rounded corners] (4, 1.5) node [above] {\(A\mathrlap{^*}\)} -- ++ (0, -0.2) -- ++ (2, -1) -- ++ (0, -0.5) coordinate (B);
                        \draw[right dual] (7, 1.5) node [above] {\(A\)} -- ++ (0, -1.7);
                        \draw[wire] (A) arc (180:360:0.5);
                        \draw[wire] (B) arc (180:360:0.5);
                    \end{scope}
                \end{tikzpicture}
                .
            \end{equation}
            Finally, we can rewrite this with an isotopy of diagrams giving
            \begin{equation}
                \tikzsetnextfilename{uniform-copying-duals-proof-7}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \coordinate (top) at (0, 1.5);
                    \draw[right dual=0.2, rounded corners] (1, 1.5) node [above] {\(A\)} -- ++ (0, -1.7);
                    \draw[over wire, left dual=0.7, rounded corners] (2, 1.5) node [above] {\(A\mathrlap{^*}\)} -- ++ (0, -0.5) -- ++ (-2, -1) -- ++ (0, -0.2) coordinate (A);
                    \draw[over wire, left dual=0.3, rounded corners] (0, 1.5) node [above] {\(A\mathrlap{^*}\)} -- ++ (0, -0.2) -- ++ (2, -1) -- ++ (0, -0.5) coordinate (B);
                    \draw[right dual] (3, 1.5) node [above] {\(A\)} -- ++ (0, -1.7);
                    \draw[wire] (A) arc (180:360:0.5);
                    \draw[wire] (B) arc (180:360:0.5);
                    \node (equal) at (3.5, 0) {\(=\)};
                    \draw[left dual, xscale=3] (4/3, 0 |- top) node [above] {\(A\mathrlap{^*}\)} -- ++ (0, -1.7) arc (180:360:0.5) -- ++ (0, 1.7) node [above] {\(A\)};
                    \draw[wire, rounded corners] (5, 0 |- top) node [above] {\(A\)} -- ++ (0, -0.5) -- ++ (1, -0.5) -- ++ (0, -0.4);
                    \draw[over wire, rounded corners] (6, 0 |- top) node [above] {\(A\mathrlap{^*}\)} -- ++ (0, -0.5) -- ++ (-1, -0.5) -- ++ (0, -0.4) coordinate (A);
                    \draw[left dual] (A) arc (180:360:0.5);
                \end{tikzpicture}
                .\qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    This is intuitively not quite right because on the left hand side we have the second system, \(A\), and the third system, \(A^*\), unconnected, but on the right they become connected.
    This makes it hard to create independent states.
    
    \begin{lma}{}{lma:swap trivial in braided mon cat duals uniform copying}
        In a braided monoidal category with duals and uniform copying the swap is trivial, that is
        \begin{equation}
            \tikzsetnextfilename{uniform-copying-trivial-swap}
            \begin{tikzpicture}[baseline=(equal.base)]
                \draw[wire, rounded corners] (1, 0) node [below] {\(A\)} -- ++ (0, 0.25) -- ++ (-1, 0.5) -- ++ (0, 0.25) node [above] {\(A\)};
                \draw[over wire, rounded corners] (0, 0) node [below] {\(A\)} -- ++ (0, 0.25) -- ++ (1, 0.5) -- ++ (0, 0.25) node [above] {\(A\)};
                \node (equal) at (1.5, 0.5) {\(=\)};
                \draw[wire] (2, 0) node [below] {\(A\)} -- ++ (0, 1) node [above] {\(A\)};
                \draw[wire] (3, 0) node [below] {\(A\)} -- ++ (0, 1) node [above] {\(A\)};
            \end{tikzpicture}
            .
        \end{equation}
        \begin{proof}
            Start with the left hand side above and deform it according to the snake equation:
            \begin{equation}
                \tikzsetnextfilename{uniform-copying-swap-to-snakes}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[wire, rounded corners] (1, 0) node [below] {\(A\)} -- ++ (0, 0.5) -- ++ (-1, 0.5) -- ++ (0, 0.5) node [above] {\(A\)};
                    \draw[over wire, rounded corners] (0, 0) node [below] {\(A\)} -- ++ (0, 0.5) -- ++ (1, 0.5) -- ++ (0, 0.5) node [above] {\(A\)};
                    \node (equal) at (1.5, 0.5) {\(\equaliso\)};
                    \draw[wire] (2, 1.5) node [above] {\(A\)} -- ++ (0, -0.75) arc (180:360:0.25) coordinate (A);
                    \draw[wire, xscale=2.5] (A) arc (180:0:0.5) coordinate (B);
                    \draw[wire] (B) -- ++ (0, -0.75) node [below] {\(A\)};
                    \draw[over wire] (3, 1.5) node [above] {\(A\)} -- ++ (0, -0.75) arc (180:360:0.25) arc (180:0:0.25) -- ++ (0, -0.75) node [below] {\(A\)};
                \end{tikzpicture}
                .
            \end{equation}
            Now apply \cref{lma:double cup braided mon cat duals and copying} to get
            \begin{equation}
                \tikzsetnextfilename{uniform-copying-swap-to-tangled-snakes}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[wire, rounded corners] (1, 0) node [below] {\(A\)} -- ++ (0, 0.5) -- ++ (-1, 0.5) -- ++ (0, 0.5) node [above] {\(A\)};
                    \draw[over wire, rounded corners] (0, 0) node [below] {\(A\)} -- ++ (0, 0.5) -- ++ (1, 0.5) -- ++ (0, 0.5) node [above] {\(A\)};
                    \node (equal) at (1.5, 0.75) {\(=\)};
                    \draw[wire] (2, 1.5) node [above] {\(A\)} -- ++ (0, -1.2) coordinate (A);
                    \draw[wire, xscale=3] (A) arc (180:360:0.3) coordinate (B);
                    \draw[wire] (B) -- ++ (0, 0.3) arc (180:0:0.3) -- ++ (0, -0.6) node [below] {\(A\)};
                    \draw[wire, rounded corners] (2.6, 1.5) node [above] {\(A\)} -- ++ (0, -0.5) -- ++ (0.6, -0.3) -- ++ (0, -0.1) coordinate (C);
                    \draw[wire] (C) arc (360:180:0.3) coordinate (D);
                    \draw[over wire, rounded corners] (D) -- ++ (0, 0.1) -- ++ (0.6, 0.3) -- ++ (0, 0.2) coordinate (E);
                    \draw[wire, xscale=3] (E) arc (180:0:0.3) -- ++ (0, -1.2) node [below] {\(A\)};
                    \node at (5.5, 0.9) {\(\equaliso\)};
                    \draw[wire] (6, 0) node [below] {\(A\)} -- ++ (0, 1.5) node [above] {\(A\)};
                    \draw[wire] (7, 0) node [below] {\(A\)} -- ++ (0, 1.5) node [above] {\(A\)};
                \end{tikzpicture}
                . \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    So exchanging two identical systems is the same as doing nothing.
    From a quantum mechanical point of view this means that exchanging two electrons is the same as doing nothing, but we know that exchanging two electrons should result in an overall minus sign, since electrons are fermions and so their wave functions are antisymmetric.
    Thus we cannot have duals and uniform copying if we want fermions, and we most certainly want fermions as they make up all matter.
    
    The failure of these structures to model reality results in the \defineindex{no cloning theorem}, which says that a system modelling quantum reality cannot have uniform copying.
    Rephrased in the following theorem it says that if we do have uniform copying then the only endomorphisms are multiplies of the identity.
    From a quantum mechanical perspective this means that all observables are simply defined by a single eigenvalue and take the same value on all states, so there are no interesting observables.
    
    \begin{thm}{No Cloning Theorem}{}
        In a braided monoidal category with duals and uniform copying every endomorphism is a multiple of the identity.
        In particular, if \(f \colon A \to A\) is an endomorphism in this category then \(f = \tr(f) \cdot \id_A\), that is
        \begin{equation}
            \tikzsetnextfilename{uniform-copying-no-cloning-theorem}
            \begin{tikzpicture}[baseline=(equal.base)]
                \node[morphism] (f) {\(f\)};
                \draw[wire] (f) -- ++ (0, 1) node [above] {\(A\)};
                \draw[wire] (f) -- ++ (0, -1) node [below] {\(A\)};
                \node (equal) at (0.75, 0) {\(=\)};
                \draw[wire] (1.4, -1) node [below] {\(A\)} -- ++ (0, 2) node [above] {\(A\)};
                \node[morphism] (f2) at (2, 0.3) {\(f\)};
                \draw[wire] (f2) -- ++ (0, 0.2) arc (180:0:0.5) -- ++ (0, -0.4) coordinate (A);
                \path[rounded corners] (A) -- ++ (0, -0.2) -- ++ (-1, -0.3) -- ++ (0, -0.2) coordinate (B);
                \draw[wire] (B) arc (180:360:0.5) coordinate (C);
                \draw[wire, rounded corners] (C) -- ++ (0, 0.2) -- ++ (-1, 0.3) -- (f2);
                \draw[over wire, rounded corners] (A) -- ++ (0, -0.2) -- ++ (-1, -0.3) -- ++ (0, -0.2) coordinate (B);
            \end{tikzpicture}
            .
        \end{equation}
        \begin{proof}
            Start by applying the snake equation to \(f\), then sliding the lower wire over \(f\):
            \begin{equation}
                \tikzsetnextfilename{uniform-copying-no-cloning-proof}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \node[morphism] (f) {\(f\)};
                    \draw[wire] (f) -- ++ (0, 1) node [above] {\(A\)} coordinate (top);
                    \draw[wire] (f) -- ++ (0, -1) node [below] {\(A\)};
                    \node (equal) at (0.75, 0) {\(=\)};
                    \draw[wire] (1.4, -1) node [below] {\(A\)} -- ++ (0, 0.75) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 0.3) coordinate (A);
                    \node[morphism, above] (f2) at (A) {\(f\)};
                    \draw[wire] (f2) -- (top -| f2) node [above] {\(A\)};
                    \node at (4.2, 0.15) {\(\equaliso\)};
                    \draw[wire] (4.85, -1) node [below] {\(A\)} -- ++ (0, 1.25) coordinate (B);
                    \draw[wire, xscale=2] (B) arc (180:0:0.5) coordinate (C);
                    \node[morphism] (f3) at (5.85, 0.25) {\(f\)};
                    \draw[over wire] (f3) -- (f3 |- top) node [above] {\(A\)};
                    \draw[wire, rounded corners] (f3.south) -- ++ (0, -0.2) -- ++ (1, -0.5) -- ++ (0, -0.15) coordinate (D);
                    \draw[wire] (D) arc (360:180:0.5) coordinate (E);
                    \draw[over wire, rounded corners] (E) -- ++ (0, 0.15) -- ++ (1, 0.5) -- (C);
                \end{tikzpicture}
                .
            \end{equation}
            Now apply \cref{lma:swap trivial in braided mon cat duals uniform copying} to the swap and we are left with \(\tr(f) \cdot \id_A\).
        \end{proof}
    \end{thm}
    
    \chapter{Frobenius Structures}
    \section{Frobenius Structures: The Idea}
    While quantum mechanics doesn't allow for copying of states there is nothing preventing us copying classically.
    In this way classical mechanics can be phrased as quantum mechanics plus the ability to copy and delete.
    We take \(\FHilb\) as our motivation.
    Fix some Hilbert space with orthogonal basis \(\{e_i\}\).
    Then copying is the map defined by
    \begin{equation}
        d = \comonoidProduct \colon e_i \mapsto e_i \otimes e_i
    \end{equation}
    extended by linearity to all inputs.
    The adjoint to this copying map is the \defineindex{comparison map}
    \begin{equation}
        d^\hermit = \comonoidProductAdjoint \colon e_i \otimes e_j \mapsto
        \begin{cases}
            e_i & i = j,\\
            0 & \text{otherwise}.
        \end{cases}
    \end{equation}
    This map defines a monoid on this Hilbert space.
    These two structures can be combined to give
    \begin{equation}
        \tikzsetnextfilename{FS-frobenius-law-in-FHilb}
        \begin{tikzpicture}[baseline=(equal.base)]
            \draw[wire] (0, 0) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1);
            \draw[wire] (0.5, 1.5) -- ++ (0, 0.5);
            \draw[wire] (1.5, 0.5) -- ++ (0, -0.5);
            \node[comonoid dot] at (0.5, 1.5) {};
            \node[comonoid dot] at (1.5, 0.5) {};
            \node[state, below, minimum width=0.75cm] (here) at (0, 0) {};
            \node at (here) {\(e_i\)};
            \node[state, below, minimum width=0.75cm] (here) at (1.5, 0) {};
            \node at (here) {\(e_j\)};
            \node (equal) at (2.4, 0.75) {\(=\)};
            \node[state, below, minimum width=0.75cm] (here) at (3.25, 1) {};
            \node at (here) {\(e_i\)};
            \draw[wire] (here) -- ++ (0, 0.75);
            \node[state, below, minimum width=0.75cm] (here) at (4, 1) {};
            \node at (here) {\(e_j\)};
            \draw[wire] (here) -- ++ (0, 0.75);
            \node at (3.25, 0) {\(0\)};
            \node at (5.25, 1) {\(i = j\)};
            \node at (5.25, 0) {\(i \ne j\)};
            \draw[decorate, decoration={calligraphic brace}, thick] (2.8, -0.25) -- (2.8, 1.7);
            \draw[decorate, decoration={calligraphic brace, mirror}, thick] (5.75, -0.25) -- (5.75, 1.7);
            \node (equal) at (6.1, 0.75) {\(=\)};
            \begin{scope}[xshift=8.5cm, xscale=-1]
                \draw[wire] (0, 0) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1);
                \draw[wire] (0.5, 1.5) -- ++ (0, 0.5);
                \draw[wire] (1.5, 0.5) -- ++ (0, -0.5);
                \node[comonoid dot] at (0.5, 1.5) {};
                \node[comonoid dot] at (1.5, 0.5) {};
                \node[state, below, minimum width=0.75cm] (here) at (0, 0) {};
                \node at (here) {\(e_j\)};
                \node[state, below, minimum width=0.75cm] (here) at (1.5, 0) {};
                \node at (here) {\(e_i\)};
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    The shape of this equation motivates our next definition.
    
    \section{Frobenius Structures: The Definition}
    \begin{dfn}{Frobenius Structure}{}
        In a monoidal category a \defineindex{Frobenius structure} is formed from a monoid, \((A, \monoidProduct, \monoidIdentity)\), and comonoid, \((A, \comonoidProduct, \comonoidIdentity)\), on some object \(A\), such that the product and coproduct satisfy the \defineindex{Frobenius law}
        \begin{equation}
            \tikzsetnextfilename{FS-frobenius-law}
            \begin{tikzpicture}[baseline=(equal.base)]
                \draw[wire] (0, 0) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1);
                \draw[wire] (0.5, 1.5) -- ++ (0, 0.5);
                \draw[wire] (1.5, 0.5) -- ++ (0, -0.5);
                \node[monoid dot] at (0.5, 1.5) {};
                \node[comonoid dot] at (1.5, 0.5) {};
                \node (equal) at (2.5, 1) {\(=\)};
                \begin{scope}[xshift=5cm, xscale=-1]
                    \draw[wire] (0, 0) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1);
                    \draw[wire] (0.5, 1.5) -- ++ (0, 0.5);
                    \draw[wire] (1.5, 0.5) -- ++ (0, -0.5);
                    \node[monoid dot] at (0.5, 1.5) {};
                    \node[comonoid dot] at (1.5, 0.5) {};
                \end{scope}
            \end{tikzpicture}
            .
        \end{equation}
        If \(\monoidProduct = \comonoidProductAdjoint\) then we call this a \defineindex{dagger Frobenius structure}.
    \end{dfn}

    \section{Frobenius Structures: The Examples}
    \subsection{Group Algebra}
    We've seen that in \(\FHilb\) we get a Frobenius structure by taking the comparison monoid and copying comonoid.
    There is another way to form a Frobenius structures in \(\FHilb\).
    Start with a finite group, \(G\), and define the \defineindex{group algebra}, \(\complex[G]\), to be the Hilbert space spanned by the elements of the group.
    A vector in this space is of the form \(\sum_i z_i g_i\) where \(g_i \in G\) and \(z_i \in \complex\).
    The sum of two such vectors is \(\sum_{i}z_i g_i + \sum_{i}w_i g_i = \sum_i (z_i + w_i)g_i\), scalar multiplication is given by \(w\sum_i z_i g_i = \sum_i (wz_i)g_i\), the product of two vectors is \(\left( \sum_i z_ig_i \right) \left( \sum_j w_jg_j \right) = \sum_{i,j}(z_iw_j)(g_ig_j)\), and the scalar product is \(\braket{\sum_iz_ig_i}{\sum_jw_jg_j} = \sum_i z_i^*w_i\).
    These sums are purely formal, we don't try to evaluate them.
    
    The group algebra comes equipped with a natural Frobenius structure.
    Define a monoid on \(\complex[G]\) in the obvious way using the fact that every group is a monoid and defining
    \begin{equation}
        m = \monoidProduct\colon g \otimes h \mapsto gh, \qquad \monoidIdentity \colon z \mapsto z 1_G
    \end{equation}
    for \(g, h \in G\) and \(1_G \in G\) the group identity, and extending this by linearity to all of \(\complex[G]\).
    The adjoint of this monoid is the comonoid defined by
    \begin{equation}
        d = \monoidProductAdjoint \colon g \mapsto \sum_{h\in G} gh^{-1}\otimes h, \qquad \monoidIdentityAdjoint \colon 
        \begin{cases}
            1_G \mapsto 1 & g = 1_G,\\
            g \mapsto 0 & g \ne 1_G.
        \end{cases}
    \end{equation}
    again, extending this by linearity to all of \(\complex[G]\).
    
    The left hand side of the Frobenius law takes in two vectors in \(\complex[G]\), but we can simplify the analysis by acting just on the basis vectors, which are elements of the group, \(G\), embedded in the group algebra.
    So consider the input \(g \otimes h\).
    Algebraically the left hand side of the Frobenius law is
    \begin{equation}
        f = (m \otimes \id_A) \circ \alpha_{A,A,A}^{-1} \circ ({\id_A} \otimes d)
    \end{equation}
    where \(m\) and \(d\) are the monoid multiplication and comonoid copy maps.
    Applying this to \(g \otimes h\) we get
    \begingroup\allowdisplaybreaks
    \begin{align}
        f(g \otimes h) &= [(m \otimes \id_A) \circ \alpha_{A,A,A}^{-1}]\left( g \otimes \left[ \sum_{k\in G} hk^{-1} \otimes k \right] \right)\\
        &= \sum_{k\in G} [(m \otimes \id_A) \circ \alpha_{A,A,A}^{-1}](g \otimes (hk^{-1} \otimes k))\\
        &= \sum_{k\in G} [(m \otimes \id_A)]((g \otimes hk^{-1}) \otimes k)\\
        &= \sum_{k\in G} ghk^{-1} \otimes k\\
        &= \sum_{k\in G} gk^{-1} \otimes kh.
    \end{align}
    \endgroup
    In the last step here we've used Cayley's theorem to recognise that if \(k\) takes on all values in \(G\) then so does \(kh\) as this is simply the group acting on itself by permutation.
    Thus, these two sums are equal but the order of terms in the sum may change.
    The right hand side of the Frobenius law is
    \begin{equation}
        \tilde{f} = ({\id_A} \otimes m) \circ \alpha_{A,A,A} \circ (d \otimes \id_A).
    \end{equation}
    Applying this to \(g \otimes h\) we get
    \begin{align}
        \tilde{f}(g \otimes h) &= [({\id_A} \otimes m) \circ \alpha_{A,A,A}]\left( \left[ \sum_{k\in G} gk^{-1} \otimes k \right] \otimes h \right)\\
        &= \sum_{k\in G} [({\id_A} \otimes m) \circ \alpha_{A,A,A}]( (gk^{-1} \otimes k) \otimes h )\\
        &= \sum_{k\in G} [({\id_A} \otimes m)](gk^{-1} \otimes (k \otimes h))\\
        &= \sum_{k\in G} [gk^{-1} \otimes kh].
    \end{align}
    These are equal, so the Frobenius law holds.
    
    \subsection{Groupoids}
    Recall that a \defineindex{groupoid} is a category in which all morphisms are invertible.
    In order to work with groupoids it is often easier to work with a slightly different, but completely equivalent, definition of a category.
    \begin{dfn}{Category}{}
        A \defineindex{category}, \(\cat{C}\), consists of the following data:
        \begin{itemize}
            \item a class, \(\Ob(\cat{C})\), of objects;
            \item a class, \(\hom(\cat{C})\), of morphisms;
            \item two maps \(s, t \colon \hom(\cat{C}) \to \Ob(\cat{C})\), called the \defineindex{source} and \defineindex{target} maps;
            \item for all pairs of morphisms \(f, g \in \hom(\cat{C})\) with \(t(f) = s(g)\) a morphism \(g \circ f \in \hom(\cat{C})\);
            \item for every object \(A \in \Ob(\cat{C})\) a morphism \(\id_A \in \hom(\cat{C})\).
        \end{itemize}
        This data is constrained by the following:
        \begin{itemize}
            \item source and target respect composition: \(s(g \circ f) = s(f)\) and \(t(g\circ f) = t(g)\);
            \item source and target respect identities: \(s(\id_A) = t(\id_A) = A\);
            \item associativity: \((h \circ g) \circ f = h \circ (g \circ f)\) whenever these maps are defined;
            \item unit laws: \(\id_{t(f)} \circ f = f = f \circ \id_{s(f)}\).
        \end{itemize}
    \end{dfn}
    
    Basically, take the definition we usually use of a category, do away with individual hom-sets and put all morphisms into one bag, \(\hom(\cat{C})\), and then define functions \(s\) and \(t\) which give the domain and codomain.
    To map back to the original definition of a category simply define \(\hom(A, B) = \{f \in \hom(\cat{C}) \mid s(f) = A \text{ and } t(f) = B\}\).
    With this new definition of a category a groupoid, \(\cat{G}\), consists of objects, \(\Ob(\cat{G})\), and isomorphisms \(\hom(\cat{G})\).
    
    In \(\Rel\) we can construct a Frobenius structure on a groupoid, \(\cat{G}\).
    First, let \(G = \hom(\cat{G})\) and we can then construct relations on this set (assuming that \(\cat{G}\) is a small category).
    Then define multiplication:
    \begin{align}
        \monoidProduct \colon G \times G &\to G\\
        (g, h) &\sim g \circ h \text{ if } s(g) = t(h).
    \end{align}
    That is, identify the pair of isomorphisms \((h, g)\) with their composite \(h \circ g\) if this is defined, and with nothing if this composite doesn't exist.
    The unit is then defined as
    \begin{align}
        \monoidIdentity \colon 1 &\to G\\
        \bullet &\sim \id_A \text{ for all } A \in \Ob(\cat{G})
    \end{align}
    where \(1 = \{\bullet\}\) is the singleton set.
    That is, we identify the element of the singleton set with identities and nothing else.
    
    The adjoint of these maps are defined by the converse relations, in particular,
    \begin{equation}
        \monoidProductAdjoint \colon g \sim (h, h^{-1} \circ g) \text{ for all } h \in G \text{ with } t(g) = s(h^{-1}) = t(h)
    \end{equation}
    and
    \begin{equation}
        \monoidIdentityAdjoint \colon \id_A \sim \bullet \text{ for all } A \in \Ob(\cat{G}).
    \end{equation}
    This defines a Frobenius structure on \(G\).
    
    \subsection{Pair of Pants}
    In a dagger monoidal category with duality \(A \leftdual A^*\) the pair of pants monoid on \(A^* \otimes A\) forms a dagger Frobenius structure.
    This can be seen directly as the left and right hand sides of the Frobenius law are isotopic for the pair of pants monoid:
    \begin{equation}
        \tikzsetnextfilename{FS-pair-of-pants-frobenius-law}
        \begin{tikzpicture}[baseline=(equal.base)]
            \draw[left dual, rounded corners] (1, -0.5) -- ++ (0, 1) arc (180:90:0.5) -- ++ (1, -1) -- ++ (0, -0.5);
            \draw[left dual, rounded corners] (3.5, 2) -- ++ (0, -1) arc (0:-90:0.5) -- ++ (-1, 1) -- ++ (0, 0.5);
            \draw[right dual, rounded corners] (0, -0.5) -- ++ (0, 1.3) -- ++ (1, 1) -- ++ (0, 0.2);
            \draw[left dual, rounded corners] (3.5, -0.5) -- ++ (0, 0.2) -- ++ (1, 1) -- ++ (0, 1.3);
            \node (equal) at (5, 0.65) {\(\equaliso\)};
            \begin{scope}[xshift=6.5cm]
                \draw[right dual, rounded corners] (0, 2) -- ++ (0, -1) arc (180:270:0.5) -- ++ (1, 1) -- ++ (0, 0.5);
                \draw[right dual, rounded corners] (2.5, -0.5) -- ++ (0, 1) arc (0:90:0.5) -- ++ (-1, -1) -- ++ (0, -0.5);
                \draw[right dual, rounded corners] (0, -0.5) -- ++ (0, 0.2) -- ++ (-1, 1) -- ++ (0, 1.3);
                \draw[left dual, rounded corners] (3.5, -0.5) -- ++ (0, 1.3) -- ++ (-1, 1) -- ++ (0, 0.2);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}
    Notice that both sides here are isotopic to
    \begin{equation}
        \tikzsetnextfilename{FS-extended-frobenius-law-pair-of-pants}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[left dual] (0.5, 0) arc (180:0:0.5);
            \draw[left dual] (1.5, 2) arc (360:180:0.5);
            \draw[right dual, rounded corners] (0, 0) -- ++ (0, 0.2) -- ++ (0.3, 0.3) -- ++ (0, 1) -- ++ (-0.3, 0.3) -- ++ (0, 0.2);
            \draw[left dual, rounded corners] (2, 0) -- ++ (0, 0.2) -- ++ (-0.3, 0.3) -- ++ (0, 1) -- ++ (0.3, 0.3) -- ++ (0, 0.2);
        \end{tikzpicture}
    \end{equation}
    which motivates the following lemma.
    
    \section{Extended Frobenius Law}
    \begin{lma}{Extended Frobenius Law}{}
        Any Frobenius structure satisfies
        \begin{equation}
            \tikzsetnextfilename{FS-extended-frobenius-law}
            \begin{tikzpicture}[baseline=(equal.base)]
                \draw[wire] (0, 0) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1);
                \draw[wire] (0.5, 1.5) -- ++ (0, 0.5);
                \draw[wire] (1.5, 0.5) -- ++ (0, -0.5);
                \node[monoid dot] at (0.5, 1.5) {};
                \node[comonoid dot] at (1.5, 0.5) {};
                \node (equal) at (2.5, 1) {\(=\)};
                \draw[wire] (3, 0) arc (180:0:0.5);
                \draw[wire] (3, 2) arc (180:360:0.5);
                \draw[wire] (3.5, 0.5) -- ++ (0, 1);
                \node[monoid dot] at (3.5, 0.5) {};
                \node[comonoid dot] at (3.5, 1.5) {};
                \node at (4.5, 1) {\(=\)};
                \draw[wire] (5, 2) -- ++ (0, -1) arc (180:360:0.5) arc (180:0:0.5) -- ++ (0, -1);
                \draw[wire] (5.5, 0.5) -- ++ (0, -0.5);
                \draw[wire] (6.5, 1.5) -- ++ (0, 0.5);
                \node[comonoid dot] at (5.5, 0.5) {};
                \node[monoid dot] at (6.5, 1.5) {};
            \end{tikzpicture}
            \,.
        \end{equation}
        \begin{proof}
            Start with the middle term above and apply the counitality law to the bottom left leg.
            Then identify a subdiagram we can apply the Frobenius law to.
            This gives
            \begin{equation}
                \tikzsetnextfilename{FS-extended-frobenius-law-proof-1}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \pgfdeclarelayer{behind}
                    \pgfsetlayers{behind, main}
                    \draw[wire] (0, -1) -- ++ (0, 1) arc (180:0:0.5) -- ++ (0, -1);
                    \draw[wire] (0, 2) arc (180:360:0.5);
                    \draw[wire] (0.5, 0.5) -- ++ (0, 1);
                    \node[monoid dot] at (0.5, 0.5) {};
                    \node[comonoid dot] at (0.5, 1.5) {};
                    \node (equal) at (1.4, 1) {\(=\)};
                    \draw[wire] (2.5, -1) -- ++ (0, 0.5);
                    \draw[wire] (2, 1) -- ++ (0, -1) arc (180:360:0.5) arc (180:0:0.5) -- ++ (0, -1);
                    \draw[wire] (3.5, 0.5) -- ++ (0, 1);
                    \draw[wire] (3, 2) arc (180:360:0.5);
                    \node[comonoid dot] at (2.5, -0.5) {};
                    \node[comonoid dot] at (2, 1) {};
                    \node[monoid dot] at (3.5, 0.5) {};
                    \node[comonoid dot] at (3.5, 1.5) {};
                    \begin{pgfonlayer}{behind}
                        \fill[Blue!50] (1.8, -0.8) rectangle (4.2, 0.8);
                    \end{pgfonlayer}
                    \node at (4.5, 1) {\(=\)};
                    \draw[wire] (5.5, 1) -- ++ (0, -0.5);
                    \draw[wire] (5, -1) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1.5);
                    \draw[wire] (6.5, 2) arc (180:360:0.5);
                    \draw[wire] (6.5, -0.5) -- ++ (0, -0.5);
                    \node[comonoid dot] at (5.5, 1) {};
                    \node[monoid dot] at (5.5, 0.5) {};
                    \node[comonoid dot] at (6.5, -0.5) {};
                    \node[comonoid dot] at (7, 1.5) {};
                    \begin{pgfonlayer}{behind}
                        \fill[Blue!50] (4.8, -0.8) rectangle (7.2, 0.8);
                    \end{pgfonlayer}
                \end{tikzpicture}
                \,.
            \end{equation}
            We can now apply coassociativity to the right hand side of this diagram and then again identify a subdiagram to which we can apply the Frobenius law:
            \begin{equation}
                \tikzsetnextfilename{FS-extended-frobenius-law-proof-2}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \pgfdeclarelayer{behind}
                    \pgfsetlayers{behind, main}
                    \draw[wire] (0, -1) -- ++ (0, 1) arc (180:0:0.5) -- ++ (0, -1);
                    \draw[wire] (0, 2) arc (180:360:0.5);
                    \draw[wire] (0.5, 0.5) -- ++ (0, 1);
                    \node[monoid dot] at (0.5, 0.5) {};
                    \node[comonoid dot] at (0.5, 1.5) {};
                    \node (equal) at (1.5, 1) {\(=\)};
                    \draw[wire] (2, -1) -- ++ (0, 1.5) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1.5);
                    \draw[wire] (3.5, 0) node [comonoid dot] {} arc (180:360:0.5) -- ++ (0, 2);
                    \draw[wire] (4, -0.5) node [comonoid dot] {} -- ++ (0, -0.5);
                    \draw[wire] (2.5, 1) node [monoid dot] {} -- ++ (0, 0.5) node [comonoid dot] {};
                    \begin{pgfonlayer}{behind}
                        \fill[Blue!50] (1.8, -0.25) rectangle (4.2, 1.25);
                    \end{pgfonlayer}
                    \node at (5, 1) {\(=\)};
                    \draw[wire] (5.5, 1.5) node [comonoid dot] {} -- ++ (0, -1) {} arc (180:360:0.5) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1.5);
                    \draw[wire] (6, 0) node [comonoid dot] {} -- ++ (0, -1);
                    \draw[wire] (7, 1) node [monoid dot] {} -- ++ (0, 1);
                    \draw[wire] (8, 0) node [comonoid dot] {} -- ++ (0, -1);
                    \begin{pgfonlayer}{behind}
                        \fill[Blue!50] (5.3, -0.25) rectangle (7.5, 1.25);
                    \end{pgfonlayer}
                \end{tikzpicture}
                \,.
            \end{equation}
            Finally, apply counitality to the left hand side of this diagram and we are left with
            \begin{equation}
                \tikzsetnextfilename{FS-extended-frobenius-law-proof-3}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[wire] (2, 0) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1);
                    \draw[wire] (2.5, 1.5) -- ++ (0, 0.5);
                    \draw[wire] (3.5, 0.5) -- ++ (0, -0.5);
                    \node[monoid dot] at (2.5, 1.5) {};
                    \node[comonoid dot] at (3.5, 0.5) {};
                    \node (equal) at (1.5, 1) {\(=\)};
                    \draw[wire] (0, 0) arc (180:0:0.5);
                    \draw[wire] (0, 2) arc (180:360:0.5);
                    \draw[wire] (0.5, 0.5) -- ++ (0, 1);
                    \node[monoid dot] at (0.5, 0.5) {};
                    \node[comonoid dot] at (0.5, 1.5) {};
                \end{tikzpicture}
                \,.
            \end{equation}
            
            Proving the second equality is done in exactly the same way but starting with the other leg.
        \end{proof}
    \end{lma}
    
    \section{Special Frobenius Structure}
    Consider the Frobenius structure given by an orthogonal basis, \(\{e_i\}\), on some Hilbert space.
    The norm squared of a basis vector is given by
    \begin{equation}
        \norm{e_i}^2 = \,
        \tikzsetnextfilename{FS-square-norm-of-unit-vector}
        \begin{tikzpicture}[baseline=11.5pt]
            \node[state, minimum width=0.75cm] (A) at (0, 0) {};
            \node[effect, minimum width=0.75cm] (B) at (0, 1) {};
            \draw[wire] (A) -- (B);
            \node at (A) {\(e_i\)};
            \node at (B) {\(e_i\)};
        \end{tikzpicture}
        .
    \end{equation}
    Since the comonoid in this case copies the basis we have
    \begin{equation}
        \tikzsetnextfilename{FS-motivating-speciality-in-FHilb}
        \begin{tikzpicture}[baseline=(equal.base)]
            \node[state, minimum width=0.75cm] (A) at (0, 0) {};
            \node[effect, minimum width=0.75cm] (B) at (0, 2) {};
            \draw[wire] (0, 1) circle [radius=0.4];
            \draw[wire] (A) -- ++ (0, 0.6) node [comonoid dot] {};
            \draw[wire] (B) -- ++ (0, -0.6) node [comonoid dot] {};
            \node at (A) {\(e_i\)};
            \node at (B) {\(e_i\)};
            \node (equal) at (0.75, 1) {\(=\)};
            \node[state, minimum width=0.75cm] (A) at (1.5, 0) {};
            \node[state, minimum width=0.75cm] (B) at (2.25, 0) {};
            \node[effect, minimum width=0.75cm] (C) at (1.5, 2) {};
            \node[effect, minimum width=0.75cm] (D) at (2.25, 2) {};
            \node at (A) {\(e_i\)};
            \node at (B) {\(e_i\)};
            \node at (C) {\(e_i\)};
            \node at (D) {\(e_i\)};
            \draw[wire] (A) -- (C);
            \draw[wire] (B) -- (D);
        \end{tikzpicture}
        \, = \norm{e_i}^4.
    \end{equation}
    From this we see that
    \begin{equation}
        \tikzsetnextfilename{FS-speciality-in-FHilb}
        \begin{tikzpicture}[baseline=(equal.base)]
            \draw[wire] (0, 1) circle [radius=0.4];
            \draw[wire] (0, 0) -- ++ (0, 0.6) node [comonoid dot] {};
            \draw[wire] (0, 2) -- ++ (0, -0.6) node [comonoid dot] {};
            \node (equal) at (0.8, 1) {\(=\)};
            \draw[wire] (1.25, 0) -- ++ (0, 2);
        \end{tikzpicture}
    \end{equation}
    if and only if \(\norm{e_i} = 1\).
    This motivates the following definition.
    
    \begin{dfn}{Special Frobenius Structure}{}
        A Frobenius structure is special if
        \begin{equation}
            \tikzsetnextfilename{FS-speciality-definition}
            \begin{tikzpicture}[baseline=(equal.base)]
                \draw[wire] (0, 1) circle [radius=0.4];
                \draw[wire] (0, 0) -- ++ (0, 0.6) node [comonoid dot] {};
                \draw[wire] (0, 2) -- ++ (0, -0.6) node [monoid dot] {};
                \node (equal) at (0.8, 1) {\(=\)};
                \draw[wire] (1.25, 0) -- ++ (0, 2);
            \end{tikzpicture}
            \,,
        \end{equation}
        that is \(m = d^{-1}\) where \(m\) is monoid multiplication and \(d\) is comonoid comultiplication.
    \end{dfn}
    
    As stated before the orthogonal basis Frobenius structure in \(\FHilb\) is special exactly when the basis is orthonormal.
    
    The group algebra Frobenius structure in \(\FHilb\) is only special for the trivial group, in which case \(\complex[\{1\}] \isomorphic \complex\).
    This whole example is trivial though, since \(m(1_G \otimes 1_G) = 1_G 1_G = 1_G\) and \(d(1_G) = 1_G \otimes 1_G\) are the only operations, neither of which allows any interesting computations.
    
    The groupoid Frobenius structure in \(\Rel\) is always special.
    
    \section{Classical Structures}
    A classical structure, which we shall define shortly, captures all of the structure of a classical computation, which is roughly speaking a quantum computation with the ability to copy and delete.
    
    \begin{dfn}{Classical Structure}{}
        A \defineindex{classical structure} in a braided monoidal dagger category is a special commutative dagger Frobenius structure.
    \end{dfn}
    
    The (second) \enquote{dagger} part of this definition means that the comonoid and monoid forming the Frobenius structure are adjoints, so we typically only specify one of them.
    The commutative part applies to the monoid, it is a commutative monoid, which then means that the comonoid is cocommutative.
    
    A classical structure in \(\FHilb\) corresponds to an orthonormal basis.
    A classical structure in \(\Rel\) corresponds to an Abelian group.
    
    The definition of a classical turns out to have several redundancies.
    It can be shown that \((A, \monoidProduct, \monoidIdentity)\) is a classical structure if all of the following hold:
    \begin{equation}
        \tikzsetnextfilename{FS-classical-structure-minimal-definition}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw[wire] (0, 0) -- ++ (0, 1) arc (180:0:0.5) -- ++ (0, -1);
            \draw[wire] (0.5, 1.5) node [monoid dot] {} -- ++ (0, 1);
            \node at (1.5, 1.5) {\(=\)};
            \draw[wire, rounded corners] (2, 0) -- ++ (0, 0.2) -- ++ (1, 0.6) -- ++ (0, 0.2) coordinate (A);
            \draw[wire] (A) arc (0:180:0.5) coordinate (B);
            \draw[over wire, rounded corners] (B) -- ++ (0, -0.2) -- ++ (1, -0.6) -- ++ (0, -0.2) node [right] {\(,\)};
            \draw[wire] (2.5, 1.5) node [monoid dot] {} -- ++ (0, 1);
            \begin{scope}[xshift=4cm]
                \draw[wire] (0, 0) -- ++ (0, 2.5);
                \draw[wire] (1.5, 1.25) circle [radius=0.5];
                \draw[wire] (1.5, 0) node [right, xshift=0.5cm] {\(,\)} -- ++ (0, 0.75) node [monoid dot] {};
                \draw[wire] (1.5, 2.5) -- ++ (0, -0.75) node [monoid dot] {};
                \node at (0.5, 1.5) {\(=\)};
            \end{scope}
            \begin{scope}[xshift=1cm, yshift=-3cm]
                \node at (-0.75, 1.5) {and};
                \draw[wire] (0, 0) -- ++ (0, 1.25) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1.25);
                \draw[wire] (0.5, 1.75) node [monoid dot] {} -- ++ (0, 0.75);
                \draw[wire] (1.5, 0.75) node [monoid dot] {} -- ++ (0, -0.75) node [right] {\(.\)};
                \draw[wire] (3, 0) arc (180:0:0.5);
                \draw[wire] (3, 2.5) arc (180:360:0.5);
                \draw[wire] (3.5, 0.5) node [monoid dot] {} -- ++ (0, 1.5) node [monoid dot] {};
                \node at (2.5, 1.5) {\(=\)};
            \end{scope}
        \end{tikzpicture}
    \end{equation}
    
    That is, a classical structure is a commutative map \(\monoidProduct \colon A \otimes A \to A\) in a dagger category such that the speciality condition holds and half of the extended Frobenius law holds.
    The other requirements from these.
    For example, combining speciality and the Frobenius law gives us associativity
    
    \section{Self Duality}
    \begin{lma}{Frobenius Structures are Self Dual}{lma:FS self dual}
        If \((A, \comonoidProduct, \comonoidIdentity, \monoidProduct, \monoidIdentity)\) is a Frobenius structure in some monoidal category then \(A\) is self dual with this duality expressed through
        \begin{equation}
            \tikzsetnextfilename{FS-self-duality}
            \begin{tikzpicture}[baseline=(equal.base)]
                \draw[wire] (0, 0) node [above] {\(A\)} arc (180:360:0.5) node [above] {\(A\)};
                \draw[wire] (2, 0) node [above] {\(A\)} arc (180:360:0.5) node [above] {\(A\)};
                \draw[wire] (2.5, -0.5) node [comonoid dot] {} -- ++ (0, -0.5) node [monoid dot] {};
                \node (equal) at (1.5, -0.5) {\(=\)};
                \node at (4, -0.5) {and};
                \begin{scope}[xshift=5cm, yshift=-1cm]
                    \draw[wire] (0, 0) node [below] {\(A\)} arc (180:0:0.5) node [below] {\(A\)};
                    \draw[wire] (2, 0) node [below] {\(A\)} arc (180:0:0.5) node [below] {\(A\)};
                    \draw[wire] (2.5, 0.5) node [monoid dot] {} -- ++ (0, 0.5) node [comonoid dot] {};
                    \node at (1.5, 0.5) {\(=\)};
                \end{scope}
            \end{tikzpicture}
            .
        \end{equation}
        \begin{proof}
            We need to demonstrate that the snake equations hold with this definition.
            This follows immediately by the extended Frobenius law and the definition of the (co)unit:
            \begin{equation}
                \tikzsetnextfilename{FS-self-dual-proof}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[wire] (0, 0) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1);
                    \node (equal) at (2.5, 1) {\(=\)};
                    \draw[wire] (3, 0) -- ++ (0, 1) arc (180:0:0.5) arc (180:360:0.5) -- ++ (0, 1);
                    \draw[wire] (3.5, 1.5) node [monoid dot] {} -- ++ (0, 0.5) node [comonoid dot] {};
                    \draw[wire] (4.5, 0.5) node [comonoid dot] {} -- ++ (0, -0.5) node [monoid dot] {};
                    \node at (5.5, 1) {\(=\)};
                    \draw[wire] (6, 0) arc (180:0:0.5) node [monoid dot] {};
                    \draw[wire] (6, 2) node [comonoid dot] {} arc (180:360:0.5);
                    \draw[wire] (6.5, 0.5) node [monoid dot] {} -- ++ (0, 1) node [comonoid dot] {};
                    \node at (7.5, 1) {\(=\)};
                    \draw[wire] (8, 0) -- ++ (0, 2);
                \end{tikzpicture}
                .
            \end{equation}
            The other snake equation is proven similarly.
        \end{proof}
    \end{lma}
    
    To state the next theorem we'll need a few definitions first.
    \begin{dfn}{Zero Object}{}
        An object, \(0\), is a \defineindex{zero object} if it is both initial and terminal.
        A \defineindex{zero morphism} \(0_{A,B} \colon A \to B\) is the unique morphism \(A \to 0 \to B\) factoring through the zero object.
    \end{dfn}
    
    Note that uniqueness of \(0_{A,B}\) follows since by the definition of an initial object the morphism \(A \to 0\) is unique and by the definition of a terminal object the morphism \(0 \to B\) is unique.
    
    \begin{dfn}{Dagger Kernel}{}
        In a dagger category with a zero object given a map \(f \colon A \to B\) a \defineindex{dagger kernel} of this map is a pair \((K, k)\) consisting of an object \(K\) and an isometry\footnote{recall that \(k\) is an isometry if \(k^\dagger \circ k = \id_K\)} \(k \colon K \to A\) such that when \(f \circ k = 0_{K,B}\) every morphism satisfying \(x \colon X \to A\) factors through \(k\) so that the following commutes:
        \begin{equation}
            \begin{tikzcd}
                K \arrow[r, "k"] & A \arrow[r, shift left, "f"] \arrow[r, shift right, "0_{A,B}"'] & B\\
                X\mathrlap{.} \arrow[ur, "x"'] \arrow[u, dashed]
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    The morphism \(m \colon X \to K\) posited by this commuting diagram is unique, and must by equal to \(k^\dagger \circ x\) since \(k\) is an isometry so
    \begin{equation}
        m = \id_K \circ m = k^\dagger \circ k \circ m = k^\dagger \circ x
    \end{equation}
    
    The intuition here is to generalise the notion of the kernel of a linear map as the subspace which is mapped to the zero vector, since the zero-dimensional subspace \(\{0\}\) is exactly the zero object in the category of vector spaces.
    The extra map \(k\) that we have to introduce here is simply the inclusion map from this zero-dimensional space into the space on which \(f\) is defined.
    
    \begin{dfn}{Nondegenerate}{}
        Fix some object \(A\) in a dagger monoidal category.
        A state \(a \colon I \to A\) is \defineindex{nondegenerate} if \(a \ne 0_{I,A}\) implies \(a^\dagger \circ a \ne 0_{I,I}\).
        A nondegenerate form is an effect \(\alpha \colon A \to I\) such that \(\alpha^\dagger\) is a nondegenerate state.
    \end{dfn}
    
    In the language of geometry a \defineindex{form} (aka a covariant vector in physics) is something which takes in a vector (aka a contravariant vector in physics) and returns a scalar, the interpretation here being that a nondegenerate form \(\alpha \colon A \to I\) takes in a state \(a \colon I \to A\) and returns the scalar \(\alpha \circ a \colon I \to I\).
    
    It can be shown that all morphisms in a category with zero objects and arbitrary dagger kernels are nondegenerate or identically zero.
    
    These definitions are quite a lot, so just think in terms of Hilbert spaces where a zero morphism is a map sending everything to the zero vector and a nondegenerate form is a linear map \(\alpha\colon V \to \complex\) such that \(\alpha(v) = 0 \implies v = 0\).
    In more familiar notation, \(\braket{\alpha}{v} = 0\) if and only if \(v = 0\) (assuming \(\alpha \ne 0\)).
    Don't worry too much about these concepts, they're just technicalities required for the following theorem.
    
    \begin{thm}{}{}
        Given a monoid \((A, \monoidProduct, \monoidIdentity)\) we can define a Frobenius structure with comonoid \((A, \comonoidProduct, \comonoidIdentity)\) (to be specified in the proof) if and only if \(A\) admits a nondegenerate form \(\comonoidIdentity \colon A \to I\) such that
        \begin{equation}
            \tikzsetnextfilename{FS-nondegenerate-form-implies-FS}
            \begin{tikzpicture}
                \draw[wire] (0, 0) arc (180:0:0.5);
                \draw[wire] (0.5, 0.5) node [monoid dot] {} -- ++ (0, 0.5) node [comonoid dot] {};
            \end{tikzpicture}
        \end{equation}
        is the cap of the self duality \(A \leftdual A\).
        
        \begin{proof}
            \Cref{lma:FS self dual} proves that all Frobenius structures give rise to a self duality with the required cap.
            So we need only show that the existence of such a self duality with unspecified cup \(\eta \colon I \to A \otimes A\) ensures this monoid-comonoid pair is a Frobenius structure.
            The snake equations for this self duality take the form
            \begin{equation}
                \tikzsetnextfilename{FS-snake-for-nondegenerate-counit}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[wire] (0, 0) -- ++ (0, 1) arc (180:0:0.5);
                    \draw[wire] (0.5, 1.5) node [monoid dot] {} -- ++ (0, 0.5) node [comonoid dot] {};
                    \node[morphism, minimum width=1cm, below, xshift=0.3cm] (cup1) at (1, 1) {\(\eta\)};
                    \draw[wire] ($(cup1.north) + (0.3, 0)$) -- ++ (0, 1.25);
                    \node (equal) at (2.25, 1) {\(=\)};
                    \draw[wire] (2.75, 0) -- ++ (0, 2.25);
                    \node at (3.25, 1) {\(=\)};
                    \draw[wire] (5.5, 0) -- ++ (0, 1) arc (0:180:0.5);
                    \draw[wire] (5, 1.5) node [monoid dot] {} -- ++ (0, 0.5) node [comonoid dot] {};
                    \node[morphism, minimum width=1cm, below, xshift=-0.3cm] (cup2) at (4.5, 1) {\(\eta\)};
                    \draw[wire] ($(cup2.north) + (-0.3, 0)$) -- ++ (0, 1.25);
                \end{tikzpicture}
                .
            \end{equation}
            We can then define comultiplication through
            \begin{equation}
                \tikzsetnextfilename{FS-nondegenerate-form-comultiplication}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[wire] (-0.5, 2.25) -- ++ (0, -0.75) arc (180:360:0.5) -- ++ (0, 0.75);
                    \draw[wire] (0, 0) -- (0, 1) node [comonoid dot] {};
                    \node (equal) at (1, 1) {\(=\)};
                    \draw[wire] (3.5, 0) -- ++ (0, 1) arc (0:180:0.5);
                    \draw[wire] (3, 1.5) node [monoid dot] {} -- ++ (0, 0.75);
                    \node[morphism, minimum width=1cm, below, xshift=-0.3cm] (cup2) at (2.5, 1) {\(\eta\)};
                    \draw[wire] ($(cup2.north) + (-0.3, 0)$) -- ++ (0, 1.25);
                \end{tikzpicture}
                .
            \end{equation}
            Note that we could have defined comultiplication with \(\eta\) on the other side since applying the snake equation to the line leaving the monoid product dot and then applying associativity of the monoid product we can exchange one of these diagrams for the other:
            \begin{equation}
                \tikzsetnextfilename{FS-nondegenerate-form-comultiplication-def-doesnt-matter}
                \begin{tikzpicture}[baseline=(equal.base), scale=0.7]
                    \draw[wire] (1, 0) -- ++ (0, 0.5) arc (0:180:0.5);
                    \draw[wire] (0.5, 1) node [monoid dot] {} -- ++ (0, 1.25);
                    \node[morphism, minimum width=1cm, below, xshift=-0.3cm] (cup2) at (0, 0.5) {\(\eta\)};
                    \draw[wire] ($(cup2.north) + (-0.3, 0)$) -- ++ (0, 1.75);
                    \node (equal) at (1.25, 1) {\(=\)};
                    \begin{scope}[xshift=2.75cm]
                        \draw[wire] (1, 0) -- ++ (0, 0.5) arc (0:180:0.5);
                        \draw[wire] (0.5, 1) node [monoid dot] {} arc (180:0:0.5);
                        \draw[wire] (1, 1.5) node [monoid dot] {} -- ++ (0, 0.4) node [comonoid dot] {};
                        \node[morphism, minimum width=1cm, below, xshift=0.3cm] (cup2) at (1.5, 1) {\(\eta\)};
                        \node[morphism, minimum width=1cm, below, xshift=-0.3cm] (cup1) at (0, 0.5) {\(\eta\)};
                        \draw[wire] ($(cup1.north) + (-0.3, 0)$) -- ++ (0, 1.75);
                        \draw[wire] ($(cup2.north) + (0.3, 0)$) -- ++ (0, 1.25);
                    \end{scope}
                    \node at (5.75, 1) {\(=\)};
                    \begin{scope}[xshift=7.25cm]
                        \draw[wire] (0.5, 0) -- ++ (0, 0.5) arc (180:0:0.5);
                        \draw[wire] (1, 1) node [monoid dot] {} arc (0:180:0.5);
                        \draw[wire] (0.5, 1.5) node [monoid dot] {} -- ++ (0, 0.4) node [comonoid dot] {};
                        \node[morphism, minimum width=1cm, below, xshift=0.3cm] (cup2) at (1.5, 0.5) {\(\eta\)};
                        \node[morphism, minimum width=1cm, below, xshift=-0.3cm] (cup1) at (0, 1) {\(\eta\)};
                        \draw[wire] ($(cup1.north) + (-0.3, 0)$) -- ++ (0, 1.25);
                        \draw[wire] ($(cup2.north) + (0.3, 0)$) -- ++ (0, 1.75);
                    \end{scope}
                    \node at (10, 1) {\(=\)};
                    \begin{scope}[xshift=10.5cm]
                        \draw[wire] (0, 0) -- ++ (0, 0.5) arc (180:0:0.5);
                        \draw[wire] (0.5, 1) node [monoid dot] {} -- ++ (0, 1.25);
                        \node[morphism, minimum width=1cm, below, xshift=0.3cm] (cup) at (1, 0.5) {\(\eta\)};
                        \draw[wire] ($(cup.north) + (0.3, 0)$) -- ++ (0, 1.75);
                    \end{scope}
                \end{tikzpicture}
                .
            \end{equation}
            Counitality is then simply the snake equation:
            \begin{equation}
                \tikzsetnextfilename{FS-nondegenerate-form-counitality}
                \begin{tikzpicture}[baseline=(equal.base)]
                    \draw[wire] (0.5, 1.5) node [comonoid dot] {} arc (360:180:0.5) -- ++ (0, 0.75);
                    \draw[wire] (0, 0) -- ++ (0, 1) node [comonoid dot] {};
                    \node (equal) at (0.8, 1) {\(=\)};
                    \begin{scope}[xshift=2cm]
                        \draw[wire] (1, 0) -- ++ (0, 0.75) arc (0:180:0.5);
                        \draw[wire] (0.5, 1.25) node [monoid dot] {} -- ++ (0, 0.5) node [comonoid dot] {};
                        \node[morphism, minimum width=1cm, below, xshift=-0.3cm] (cup2) at (0, 0.75) {\(\eta\)};
                        \draw[wire] ($(cup2.north) + (-0.3, 0)$) -- ++ (0, 1.5);
                    \end{scope}
                    \node at (3.5, 1) {\(=\)};
                    \draw[wire] (4, 0) -- ++ (0, 2.25);
                \end{tikzpicture}
            \end{equation}
            Coassociativity follows similarly using the symmetry of the definition of comultiplication in terms of the cup.
            The Frobenius law holds by applying associativity of the multiplication after writing the comultiplication in terms of the cup.
        \end{proof}
    \end{thm}
    Nondegneracy was implicitly used at several points in this proof.
    Without this assumption we would have been doing the equivalent of dividing by zero.
    
    \chapter{Normal Forms}
    There are two ways to think about diagrams in the graphical notation.
    The first is as a representation of a morphism.
    The second is to treat the diagram as an object in its own right, in which case diagrams are just graphs, although not all graphs are valid diagrams.
    The morphisms are nodes and the objects are edges.
    This way of thinking about diagrams allows us to do things like consider subdiagrams, and replace them with other equivalent diagrams, which we've been doing already without thinking too much about the details.
    
    If two diagrams are different when viewed as graphs they may still be equal as morphisms.
    We've seen already that really we want to consider homotopy classes of diagrams, which is just a fancy way of saying we can move things around and the morphism is the same so long as the connectivity stays the same.
    There are other operations we can perform on diagrams-as-graphs without changing the morphisms they represent.
    We can define an equivalence relation on graphs where two graphs are equivalent if they represent the same morphism.
    The particularly nice thing about this, which we shall prove shortly, is it is possible to define a normal form for a graph such that any equivalent graph can be manipulated into this normal form without changing the morphism it represents.
    This allows us to work with a fixed form for the representatives of the equivalence classes of diagrams representing morphisms.
    
    Similar to the coherence theorem, which says that given the data of a monoidal category there is a unique morphism between any two objects we will show that given the ability to copy, discard, fuse, and create data there is a unique way to do so if we start and finish with a fixed amount of data.
    This, stated formally, is the spider theorem.
    
    \section{Spider Theorem}
    \begin{thm}{Spider Theorem}{}
        Let \((A, \monoidProduct, \monoidIdentity, \comonoidProduct, \comonoidIdentity)\) be a special Frobenius structure.
        Any connected\footnote{By connected we mean that any diagram representing it is a connected graph, if this isn't the case we can treat the disconnected parts separately, so this isn't an issue.} morphism \(A ^{\otimes m} \to A^{\otimes n}\) built from finitely many pieces \(\monoidProduct\), \(\monoidIdentity\), \(\comonoidProduct\), \(\comonoidIdentity\), and \(\id_A\), using only \(\circ\) and \(\otimes\) is equal to the morphism represented by the diagram
        \begin{equation}
            \tikzsetnextfilename{spider-theorem-spider}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw[wire, xscale=2] (0, 0) arc (180:0:0.25);
                \draw[wire, xscale=2] (-0.25, 0) -- ++ (0, 0.25) arc (180:0:0.25) node [monoid dot] {};
                \draw[wire, xscale=2] (-0.5, 0) -- ++ (0, 0.5) arc (180:90:0.25) coordinate (A);
                \draw[wire, dashed, xscale=2] (A) arc (90:0:0.25) node [monoid dot] {};
                \draw[wire] (A) node [monoid dot] {} -- ++ (0, 0.5) coordinate (E);
                \draw[wire, xscale=2] (0, 2) arc (180:360:0.25);
                \draw[wire, xscale=2] (-0.25, 2) -- ++ (0, -0.25) arc (180:360:0.25) coordinate (D);
                \draw[wire, xscale=2] (-0.5, 2) -- ++ (0, -0.5) arc (180:270:0.25) coordinate (B);
                \draw[wire, dashed, xscale=2] (B) arc (270:360:0.25) coordinate (C);
                \node[comonoid dot, thick] at (C) {};
                \node[comonoid dot, thick] at (D) {};
                \node[comonoid dot, thick] at (E) {};
                \draw[decorate, thick, decoration={calligraphic brace, mirror}] (-1.1, -0.2) -- (1.1, -0.2) node [midway, below] {\(m\)};
                \draw[decorate, thick, decoration={calligraphic brace}] (-1.1, 2.2) -- (1.1, 2.2) node [midway, above] {\(n\)};
            \end{tikzpicture}
        \end{equation}
        \begin{proof}
            The proof proceeds by (strong) induction on the number of dots, that is the total number of multiplications, units, comultiplications, and counits.
            The base case of \(N = 1\) dots is trivial, since the diagram must simply be one of \(\monoidProduct\), \(\monoidIdentity\), \(\comonoidProduct\), or \(\comonoidIdentity\), all of which are already in this normal form.
            
            Now suppose that all diagrams with at most \(N\) dots can be brought into normal form.
            Consider a diagram with \(N + 1\) dots.
            We can use naturality to write this diagram in a form where one of the dots is physically higher than all other dots.
            We then proceed casewise.
            \begin{itemize}
                \item First, suppose that the topmost dot is the counit, \(\comonoidIdentity\).
                Then we can take the subdiagram consisting of the \(N\) lower dots and put this into normal form.
                There are two subcases:
                \begin{itemize}
                    \item If there are any other white dots in the diagram after placing it in normal form then they  will be a comultiplications, and the counit will be directly connected to one of these comultiplications, in which case it can be eliminated, leaving an \(N\) dot diagram which can be placed into normal form.
                    \item If there are no other white dots after placing the lower diagram into normal form then this diagram is already in normal form, specifically, the counit corresponds to the white dot on top of the vertical line in the middle of the normal form.
                \end{itemize}
                \item If the topmost dot is instead comultiplication, \(\comonoidProduct\), then place the \(N\) lower dots into normal form and use coassociativity repeatedly to move the topmost dot all the way to the right.
                Since the diagram is finite in extent this process terminates.
                This process will also result in the final diagram being in normal form.
                \item If the topmost dot is the unit, \(\monoidIdentity\), then the connectivity requirement tells us that this is the only dot present in the diagram, and hence the diagram is already in normal form.
                \item If the topmost dot is the multiplication, \(\monoidProduct\), then put the lower \(N\) dots into normal form.
                There are then two subcases, imagine removing the topmost dot and the wires leading into it.
                There are two possibilities, first, we could end up with two disconnected parts to the diagram, or we could have just one connected part if there were other connections between the two halves of the diagram
                \begin{itemize}
                    \item Suppose we are left one connected diagram after removing the topmost dot.
                    Then we can repeatedly us coassociativity and speciality to move the multiplication dot down until it is below all comultiplications and counits.
                    Then repeatedly use associativity to more it all the way to the right and the diagram will be in normal form.
                    \item If instead we are left with two disconnected diagrams then we can put these individually in normal form.
                    Then applying the extended Frobenius law will allow us to push the multiplication dot downwards.
                    Doing so repeatedly we move the multiplication to be below all comultiplications and counits, and then we can use associativity to pus it all the way to the right, placing the diagram in normal form.
                \end{itemize}
            \end{itemize}
        \end{proof}
    \end{thm}
    
    There are many variants of the spider theorem which we can obtain by allowing slight modifications on the initial set up.
    
    \begin{thm}{Non-special Spider Theorem}{}
        Let \((A, \monoidProduct, \monoidIdentity, \comonoidProduct, \comonoidIdentity)\) be a not-necessarily special Frobenius structure.
        Any connected morphism \(A^{\otimes m} \to A^{\otimes n}\) built out of finitely many pieces \(\monoidProduct\), \(\monoidIdentity\), \(\comonoidProduct\), \(\comonoidIdentity\), and \(\id_A\) using only \(\circ\) and \(\otimes\) is equal to the morphism represented by the diagram
        \begin{equation}\label{eqn:non-special normal form}
            \tikzsetnextfilename{spider-theorem-non-special}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw[wire, xscale=2] (0, 0) arc (180:0:0.25);
                \draw[wire, xscale=2] (-0.25, 0) -- ++ (0, 0.25) arc (180:0:0.25) node [monoid dot] {};
                \draw[wire, xscale=2] (-0.5, 0) -- ++ (0, 0.5) arc (180:90:0.25) coordinate (A);
                \draw[wire, dashed, xscale=2] (A) arc (90:0:0.25) node [monoid dot] {};
                \draw[wire] (A) node [monoid dot] {} -- ++ (0, 0.5) coordinate (E);
                \draw[wire] ($(A) + (0, 0.75)$) circle [radius=0.25];
                \draw[wire, dashed] ($(A) + (0, 1)$) -- ++ (0, 0.5);
                \draw[wire] ($(A) + (0, 1.75)$) circle [radius=0.25];
                \draw[wire] ($(A) + (0, 2)$) -- ++ (0, 0.5);
                \node[comonoid dot, thick] at ($(A) + (0, 0.5)$) {};
                \node[monoid dot] at ($(A) + (0, 1)$) {};
                \node[comonoid dot] at ($(A) + (0, 1.5)$) {};
                \node[monoid dot] at ($(A) + (0, 2)$) {};
                \draw[wire, xscale=2] (0, 4) arc (180:360:0.25);
                \draw[wire, xscale=2] (-0.25, 4) -- ++ (0, -0.25) arc (180:360:0.25) coordinate (D);
                \draw[wire, xscale=2] (-0.5, 4) -- ++ (0, -0.5) arc (180:270:0.25) coordinate (B);
                \draw[wire, dashed, xscale=2] (B) arc (270:360:0.25) coordinate (C);
                \node[comonoid dot, thick] at (C) {};
                \node[comonoid dot, thick] at (D) {};
                \node[comonoid dot, thick] at (E) {};
                \draw[decorate, thick, decoration={calligraphic brace, mirror}] (-1.1, -0.2) -- (1.1, -0.2) node [midway, below] {\(m\)};
                \draw[decorate, thick, decoration={calligraphic brace}] (-1.1, 4.2) -- (1.1, 4.2) node [midway, above] {\(n\)};
                \draw[decorate, thick, decoration={calligraphic brace, mirror}] (-0.1, 1) -- (-0.1, 3) node [midway, right] {\(k\)};
            \end{tikzpicture}
            .
        \end{equation}
    \end{thm}
    
    \begin{thm}{Symmetric Spider Theorem}{}
        Let \((A, \monoidProduct, \monoidIdentity, \comonoidProduct, \comonoidIdentity)\) be a symmetric Frobenius structure.
        Any connected morphism \(A^{\otimes m} \to A^{\otimes n}\) built out of finitely many pieces \(\monoidProduct\), \(\monoidIdentity\), \(\comonoidProduct\), \(\comonoidIdentity\), \(\swapMorphism\), \(\id_A\) using only \(\circ\) and \(\otimes\) is equal to the morphism represented by the diagram in \cref{eqn:non-special normal form}.
    \end{thm}
    
    However, a general Frobenius structure does not allow for a normal form.
    
    \begin{thm}{No Braided Spider Theorem}{}
        Let \((A, \monoidProduct, \monoidIdentity, \comonoidProduct, \comonoidIdentity)\) be a commutative Frobenius structure.
        There is no normal form for a connected morphism \(A^{\otimes m} \to A^{\otimes n}\) built out of finitely many pieces \(\monoidProduct\), \(\monoidIdentity\), \(\comonoidProduct\), \(\comonoidIdentity\), \(\braidMorphism\), \(\id_A\) using only \(\circ\) and \(\otimes\).
        \begin{proof}
            Consider the following diagram representing a morphism \(A \to A\):
            \begin{equation}
                \tikzsetnextfilename{knot-no-spider-theorem}
                \begin{tikzpicture}[baseline=(current bounding box)]
                    \pgfdeclarelayer{behind}
                    \pgfsetlayers{behind, main}
                    \draw[over wire, rounded corners] (0, 4.5) -- ++ (0, -1.5) -- ++ (-1, -0.5) -- ++ (0, -0.25) -- ++ (-1, -0.5) -- ++ (0, -1) coordinate (A);
                    \draw[wire] (A) arc (180:360:0.5) coordinate (B);
                    \draw[over wire, rounded corners] (B) -- ++ (0, 0.25) -- ++ (1, 0.5) -- ++ (0, 1) -- ++ (-1, 0.5) -- ++ (0, 0.25) coordinate (C);
                    \draw[wire] (C) arc (0:180:0.5) coordinate (D);
                    \begin{pgfonlayer}{behind}
                        \draw[wire, rounded corners] (D) -- ++ (0, -1) -- ++ (1, -0.5) -- ++ (0, -0.25) -- ++ (1, -0.5) -- ++ (0, -1.5);
                    \end{pgfonlayer}
                    \draw[wire] ($(A) + (0.5, -0.5)$) node [comonoid dot, thick] {} -- ++ (0, -0.5) node [monoid dot] {};
                    \draw[wire] ($(C) + (-0.5, 0.5)$) node [monoid dot, thick] {} -- ++ (0, 0.5) node [comonoid dot] {};
                \end{tikzpicture}
                .
            \end{equation}
            This is a knot (once we connect up the free ends).
            The Frobenius axioms, without the addition of speciality or symmetry, induce homotopy equivalences on diagrams.
            By definition a knot is \emph{not} homotopy equivalent to the circle, but clearly, for example, the identity \(\id_A \colon A \to A\), after connecting the ends, is homotopy equivalent to the circle.
            Hence, these two diagrams do not correspond to the same morphism.
        \end{proof}
    \end{thm}
    
    
    
    
    % TODO: Cartesian category and terminal objects at same time as products, then later prove all Cartesian categories are monoidal
    % TODO: Coherence theorems and strictification
    % TODO: Adjoints
    % TODO: Fill in skipped bits on dagger duals
    % TODO: FIll in skipped bits on (co)monoid homomorphisms & products of (co)monoids & universality of pair of pants & Cayley
    
    %   Appdendix
    \appendixpage
    \begin{appendices}
        \include{parts/maths-defs}
        \chapter{Adjoint Functors}
        Adjoint functors are, strictly, beyond the scope of this course, although we have already seen several examples.
        Broadly adjoint functors are to categories as dual objects are to objects in a monoidal category.
        
        \section{Adjoint Functors: The Definition}
        There are multiple equivalent definitions of functors, we'll look at two here.
        The first is easier to use but the second makes the relation to dual objects more explicit.
        
        Before we can give the first definition we need to introduce a special collection of functors.
        \begin{dfn}{Hom-Functor}{}
            Given a category, \(\cat{C}\), then for objects \(A\) and \(B\) there are functors \(\cat{C} \to \Set\), namely the covariant and contravariant \define{hom-functors}\index{hom-functor}
            \begin{equation}
                \cat{C}(A, -) \colon \cat{C} \to \Set, \qqand \cat{C}(-, B) \colon \cat{C} \to \Set.
            \end{equation}
            The first sends the object \(B \in \Ob(\cat{C})\) to the set of morphisms \(\cat{C}(A, B)\), and the second sends the object \(A \in \Ob(\cat{C})\) to the set of morphisms \(\cat{C}(A, B)\).
            
            The covariant hom-functor acts on maps \(f \colon X \to Y\) by sending them to maps
            \begin{align}
                \cat{C}(A, f) \colon \cat{C}(A, X) &\to \cat{C}(A, Y)\\
                g &\mapsto f\circ g.
            \end{align}
            The contravariant hom-functor acts on maps \(h \colon X \to Y\) by sending them to maps
            \begin{align}
                \cat{C}(h, B) \colon \cat{C}(X, B) &\to \cat{C}(Y, B)\\
                g &d\mapsto g \circ h.
            \end{align}
        \end{dfn}
        
        It is simple enough to check that these define functors, we'll do it here for the covariant hom-functor.
        First, \(\cat{C}(A, \id_X)\), is the map which sends \(g\) to \(g \circ \id_X = g\), so it is the identity.
        Second, \(\cat{C}(A, f \circ f')\) is the map which sends \(g\) to \(f \circ f' \circ g\), which is the map that \(\cat{C}(A, f)\) sends \(f' \circ g\) to.
        
        \begin{dfn}{Adjoint Functors}{}
            Let \(\cat{C}\) and \(\cat{D}\) be categories.
            Fix two functors \(F \colon \cat{C} \to \cat{D}\) and \(G \colon \cat{D} \to \cat{C}\).
            An \defineindex{adjunction} between \(F\) and \(G\) is a natural isomorphism
            \begin{equation}
                \Phi \colon \cat{D}(F-, -) \naturalTransformation \cat{C}(-, G-).
            \end{equation}
            That is, a collection of bijections
            \begin{equation}
                \Phi_{A,B} \colon \cat{D}(FA, B) \to \cat{C}(A, GC)
            \end{equation}
            for \(A \in \Ob(\cat{C})\) and \(B \in \Ob(\cat{D})\) such that the following diagram commutes
            \begin{equation}
                \begin{tikzcd}
                    \cat{D}(FA, B) \arrow[r, "\Phi_{A,B}"] \arrow[d, "{\cat{C}(Fg, f)}"'] & \cat{C}(A, GB) \arrow[d, "{\cat{D}(g, Gf)}"]\\
                    \cat{D}(FA', B') \arrow[r, "\Phi_{A',B'}"'] & \cat{C}(A', GB').
                \end{tikzcd}
            \end{equation}
            for all objects \(A, A' \in \Ob(\cat{C})\) and \(B, B' \in \Ob(\cat{D})\) and morphisms \(f \colon A \to A'\) and \(g \colon B \to B'\).
            
            We say that \(F\) is \defineindex{left adjoint} to \(G\) and \(G\) is \defineindex{right adjoint} to \(F\).
            We write \(F \leftadjoint G\)
        \end{dfn}
    
        This definition is a bit complex, but essentially it comes down to the existence of a bijection
        \begin{equation}
            \cat{D}(FA, B) \isomorphic \cat{C}(A, GB)
        \end{equation}
        for all objects in such a way that this bijection defines a natural isomorphism.
        We'll explore this with lots of examples.
        
        The idea is that adjoint functors translate between categories without changing the structure of the categories too much.
        In particular an equivalence of categories can be defined as an adjunction with a few more requirements.
        Consider categories as countries, with the objects being citizens and morphisms conversations in the countries language.
        Then functors are translations from one language to another preserving meaning.
        Then \(F\) and \(G\) are adjoints if it doesn't matter if Alice travel's to Bob's country and speaks Bob's language or Bob travel's to Alice's country and speaks Alice's language.
        
        \section{Adjoint Functors: The Examples}
        \subsection{Free-Forgetful Adjunction}
        Recall that a forgetful functor interprets objects of one category in another category by \enquote{forgetting} some of the details, such as the forgetful functor \(\Grp \to \Set\), which interprets each group as its underlying set, or the forgetful functor \(\CRing \to \Ring\) which forgets that a given ring is commutative.
        Often it is possible to go the other way, to add in some structure to turn a set into a group, or a ring into a commutative ring.
        There are usually many ways to do this, but often there is only one minimal way to do it, to add in only the strictly necessary structure in order for a set to become a group or a ring to become a commutative ring.
        Minimal here refers to the added structure, not the size of the free objects we produce, which tend to be much larger than the object we started with.
        Such a minimal addition of structure is called a free construction.
        
        \begin{exm}{Free Vector Space}{}
            Fix some field \(\field\).
            Write \(U \colon \Vect \to \Set\) for the forgetful functor which sends a vector space to its underlying set of vectors, simply forgetting the existence of the scalars and forgetting anything about adding vectors.
            This functor sends a linear map to the underlying function, which is all it really can do since we've forgotten about scalars and vector addition.
            
            Write \(F \colon \Set \to \Vect\) for the functor sending a set, \(A\), to the vector space of all formal linear combinations of elements of \(A\), that is
            \begin{equation}
                F(A) = \{\varphi \colon A \to \complex \mid \varphi(a) \ne 0 \text{ for only finitely many } a \in A\}.
            \end{equation}
            Taking \(A\) as the basis for \(F(A)\) we can interpret \(\varphi(a)\) as the \(a\) component of some vector \(\varphi\), in more standard notation we might write \(\varphi_a\).
            That is, a vector of \(F(A)\) is of the form
            \begin{equation}
                \sum_{a \in A} \varphi(a) a = \sum_{a \in A} \varphi_a a.
            \end{equation}
            The second part, that \(\varphi(a)\) is nonzero for a finitely many \(a \in A\), is a technical requirement for the vector space to be well-defined.
            
            Addition in this vector space \(F(A)\) is given by \((\varphi + \psi)(a) = \varphi(a) + \psi(a)\), and scalar multiplication by \((z\varphi)(a) = z[\varphi(a)]\).
            The zero vector is the zero map, \(a \mapsto 0\) for all \(a \in A\).
            For a finite set, \(A\), we have that \(F(A) \isomorphic \field^{\abs{A}}\).
            
            We also need to define how \(F\) acts on functions.
            Given a function (of sets) \(f \colon A \to B\) we need a linear map \(F(f) \colon F(A) \colon F(B)\).
            We can construct such a map: \(\varphi \mapsto \varphi \circ f\).
            
            We will show that \(F\) is left adjoint to \(U\).
            To do this we need to construct a one-to-one correspondence
            \begin{equation}
                (\text{Linear functions } g\colon F(A) \to V) \leftrightarrow (\text{Functions } f\colon A \to U(V)).
            \end{equation}
            Given some function \(f\colon A \to U(V)\) define \(g_f\) by \(\varphi \mapsto \sum_{a\in A} \varphi(a)f(a)\).
            Then the map \(f \mapsto g_f\) evaluates a formal linear combination in \(F(A)\) as a (not-necessarily-formal) linear combination in \(V\).
            Conversely, given a linear map \(g \colon F(A) \to V\) define \(f_g\) by sending \(a \in A\) to \(g(\delta_{a-})\) where \(\delta_{a-} \in F(A)\) is the characteristic function \(c \mapsto \delta_{ac}\), that is \(\delta_{a-}(b) = 1\) if \(a = b\) and \(\delta_{a-}(b) = 0\) otherwise.
            
            Then we have \(f_{g_f} = f\) and \(g_{f_g} = g\), so these two constructions are inverses.
            To see this notice that \(f_{g_f}\) takes in an element \(a \in A\), sends it to \(g_f(\delta_{a-})\), which according to the definition of \(g_f\) is the map sending \(\delta_{a-}\) to \(\sum_{a' \in A} \delta_{aa'} f(a') = f(a)\).
            
            Similarly, \(g_{f_g}\), which is linear, takes a basis vector, \(a \in A\), and sends it to \(f_g(\delta_{a-})\), which is by definition \(\sum_{a' \in A} \delta_{aa'}g(a') = g(a)\), which we can then extend by linearity to all of \(F(A)\).
            
            It is also possible to show that this defines a natural transformation, but we won't do so here.
        \end{exm}
        
        \begin{exm}{Free Monoid}{}
            Write \(U \colon \Mon \to \Set\) for the forgetful functor sending a monoid to its underlying set.
            
            Write \(F \colon \Set \to \Mon\) for the functor which takes a set, \(A\), to the free monoid on \(A\).
            The free monoid on \(A\) being the monoid made form all formal combinations of \enquote{letters} in \(A\) with string concatenation as the monoid product and the empty string as the identity.
            For example, the free monoid on one object is simply (isomorphic to) the natural numbers, \(F(\{\bullet\}) \isomorphic \naturals\).
            The free monoid on two generators has set of elements
            \begin{equation}
                F(\{0, 1\}) = \{\emptyset, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 110, 101, 111, 0000, \dotsc\}.
            \end{equation}
            Given a function (of sets) \(f \colon A \to B\) the free monoid functor sends this to the function \(F(f)\colon F(A) \to F(B)\) given by acting on each element of one of these \enquote{words} at a time, for example, \(F(f)(001) = f(0)f(0)f(1)\), and sending the empty string to the empty string.
            This is clearly a monoid homomorphism.
            
            To show that \(F \leftadjoint U\) we need to establish a one-to-one correspondence
            \begin{equation}
                (\text{monoid homomorphisms } g\colon F(A) \to M) \leftrightarrow (\text{functions } f \colon A \to U(M)).
            \end{equation}
            Given \(f\) define \(g_f\) to send the word \(a_1 \dotsm a_n\) to the product \(f(a_1)\dotsm f(a_2)\) in \(M\), and the empty string to \(1\), the identity of \(M\).
            Conversely, given \(g\) define \(f_g\) by applying \(g\) to the one-letter word \(a \in A\).
            Then we have \(f_{g_f} = f\) and \(g_{f_g} = g\), so these two constructions are inverses.
            
            To see this simply unwrap the definitions and we see that \(f_{g_f}(a) = g_f(a) = f(a)\) and \(g_{f_g}(a_1 \dotsm a_n) = f_g(a_1)\dotsm f_g(a_n) = g(a_1)\dotsm g(a_n) = g(a_1 \dotsm a_n)\), the last equality following as \(g\) is a monoid homomorphism.
            
            This construction is also natural.
        \end{exm}
        
        \begin{exm}{Free Category}{}
            Write \(\Graph\) for the category of directed graphs.
            For a graph \(G\) if \(V(G)\) is the set of vertices and \(E(G)\) the set of edges then morphisms \(f\colon G \to G'\) in \(\Graph\) are pairs of functions \(f_V \colon V(G) \to V(G')\) and \(f_E \colon E(G) \to E(G')\) such that the source and target of an edge is preserved.
            That is, given an edge \(e = (v_1, v_2)\), recall an edge of a directed graph is just an ordered pair of vertices, we have an edge \(f_E(e) = (f_V(v_1), f_V(v_2))\).
            
            Write \(\Cat\) for the category of all (small) categories with functors as morphisms.
            Then we can write down a directed graph \(U(\cat{C})\) whose vertex set is the objects of \(\cat{C}\), \(V(U(\cat{C})) = \Ob(\cat{C})\), with an edge between two vertices if there is a morphism between these vertices.
            Then \(U\) is a forgetful functor \(U \colon \Cat \to \Graph\).
            
            Write \(F \colon \Graph \to \Cat\) for the functor taking a graph, \(G\) to the category whose objects are elements of \(V(G)\) and morphisms \(v \to w\) are paths between the two vertices, \(v \xrightarrow{e_1} \dotso \xrightarrow{e_n} w\).
            Composition is concatenation of paths, and the identity is the path of length zero from a vertex to itself.
            
            On a graph morphisms, \(f \colon G \to G'\), the functor \(F\) acts to give a functor \(F(f) \colon F(G) \to F(G')\) sending objects \(v \in \Ob(F(G))\) to \(f(v)\), and morphisms \(v \xrightarrow{e_1} \dotso \xrightarrow{e_n} w\) to the morphism \(f(v) \xrightarrow{f(e_1)} \dotso \xrightarrow{f(e_n)} f(w)\).
            
            We will show that \(F \leftadjoint U\).
            To do so we need to establish a one-to-one correspondence
            \begin{equation}
                (\text{functors } G \colon F(H) \to \cat{C} \to) \leftrightarrow (\text{graph morphisms } f \colon H \to U(\cat{C}))
            \end{equation}
            for a graph \(H\).
            
            Given \(f \colon H \to U(\cat{C})\) define \(G_f\) as follows: send an object \(v\) to \(f(v)\) and a morphism \(v \xrightarrow{e_1} \dotso \xrightarrow{e_n} w\) to the composite \(f(e_n) \circ \dotsm \circ f(e_1)\).
            Conversely, given \(G\) define \(f_G\) as follows: send a vertex \(v\) to the object \(G(v)\), and send an edge \(e\) to the morphism \(G(e)\).
            Then \(f_{G_f} = f\) and \(G_{f_G} = G\), so these constructions are inverses.
            This is also natural.
        \end{exm}
        
        In general, given a category, \(\cat{C}\), of algebraic structures and structure preserving morphisms we can define a forgetful functor \(U \colon \cat{C} \to \Set\).
        If that functor has a left adjoint, \(F \colon \Set \to \cat{C}\), then this has a natural interpretation of constructing a free object from a set.
        
        This construction is useful because free objects are, as the name suggests, the most free objects of a given type of structure.
        We can then define more interesting objects as quotients of these free ones, basically imposing some other property by quotienting out by the equivalence relation it defines.
        For example, we can construct the free algebra on a vector space, \(V\), and the result is the tensor algebra,
        \begin{equation}
            T(V) = \bigoplus_{k=0}^{\infty} V^{\otimes k}.
        \end{equation}
        Then we can define other algebras of interest as quotients of this:
        \begin{itemize}
            \item \(T(V)/(x \otimes y - y \otimes x)\) is the symmetric algebra, the state space for a boson.
            \item \(T(V) / (x \otimes y + y \otimes x)\) is the exterior algebra, the state space for a fermion.
            \item If \(Q\colon V \to \field\) is a quadratic form, that is a map such that \(Q(zv) = z^2Q(v)\) for all \(v \in V\) and \(z \in \field\), then we can define the Clifford algebra associated with this quadratic form to be \(T(V) / (v \otimes v - Q(v)1)\) where \((v \otimes v - Q(v)1)\) is the ideal generated by all elements of the form \(v \otimes v - Q(v)1\) for all elements \(v \in V\), where 1 is the unit of the tensor algebra.
            \item If \(\lie{g}\) is a Lie algebra then we can define a bracket \(\lie{g} \otimes \lie{g} \to \lie{g}\) as \([a, b] = a \otimes b - b \otimes a\).
            We can then generalise this recursively to \(\lie{g}^{\otimes n}\) through \([a \otimes b, c] = a\otimes [b, c] + [a, c] \otimes b\) and \([a, b \otimes c] = [a, b] \otimes c + b \otimes [a, c]\).
            Then we can define the universal enveloping algebra to be the algebra \(U(\lie{g}) = T(\lie{g}) / (a \otimes b - b \otimes a - [a, b])\).
        \end{itemize}
        
    \end{appendices}
    
    \backmatter
    \printbibliography
    %    \renewcommand{\glossaryname}{Acronyms}
    %    \printglossary[acronym]
    \printindex
\end{document}
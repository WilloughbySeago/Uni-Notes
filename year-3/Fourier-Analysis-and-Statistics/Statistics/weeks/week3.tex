\section{Permutations and Combinations}
    \subsection{Permutations}
    \subsubsection{Multiplication Principle}
    Suppose we have an \(N\) step process that involves making a choice from \(k_n\) options for the \(n\)th step.
    How many different ways can we perform this process?
    For the first choice we have \(k_1\) options.
    For the second choice for each of the possible values of \(n_1\) we have \(k_2\) choices which is \(k_1k_2\) choices total.
    For the third choice for each of the possible pairs \((n_1, n_2)\) we have \(k_3\) choices.
    Continuing this on the total number of ways we can complete the process is
    \[W = k_1k_2k_3\dotsm k_N = \prod_{i=1}^N k_i.\]
    This is called the multiplication principle.
    
    \subsubsection{Permutations on \texorpdfstring{\(n\)}{n} Letters}
    A permutation of a set is a bijection of the set onto itself.
    Essentially all that the permutation does is rearrange the order of the group.
    In the case of a set of \(n\) things the set of all permutations, along with function composition, forms a group, \(S_n\), called the permutation group on \(n\) letters.
    Here letters could be any distinguishable set of things so often it is easiest to consider permutations of the set \(\{1, 2, \dotsc, n\}\).
    
    One question we may ask is how many different permutations are there for \(n\) objects, i.e. what is \(\abs{S_n}\)?
    We can use the multiplication principle to answer this question.
    We choose a permutation by picking which element goes first, there are \(n\) elements to choose from.
    Next we pick the second element.
    Having already picked the first element there are now only \(n - 1\) elements to choose from.
    We then pick the third element from the \(n - 2\) remaining elements.
    Continuing this process until we run out of elements we see that the number of permutations is
    \[W = n(n-1)(n-2)\dotsm 2\cdot 1 = \prod_{k=1}^n k = n!.\]
    
    \subsubsection{Permutations of \texorpdfstring{\(r\)}{r} Objects from a Set of \texorpdfstring{\(n\)}{n} Objects}
    If we have \(n\) objects and we want to select \(r\) and arrange them in some way how many ways can we do this?
    One way to think about selecting \(r\) objects is choosing a permutation of all \(n\) objects and taking the first \(r\) objects from this list.
    However this results in us over counting.
    For example if we have four objects and want to select two then the permutations \((1, 2, 3, 4)\) and \((1, 2, 4, 3)\) both give the same two objects, \((1, 2)\), in the same order.
    The number of ways that we over count each option is the number of ways we can arrange the \(n - r\) left over things, which we already know that the number of arrangements of \(n - r\) things is \((n - r)!\).
    So the number of ways of arranging \(r\) things from a set of \(n\) things is
    \[W = \frac{n!}{(n - r)!} = \perms{n}{r} = P(n, r).\]
    
    \subsection{Combinations}
    What if the objects we have are indistinguishable.
    Then the number of distinct ways of arranging \(n\) objects is \(1\).
    Any changes we make can't be seen as the objects are indistinguishable so all permutations are the same.
    If we want to select \(r\) things from a set of \(n\) objects how many ways can we do this?
    We start by assuming that the objects are distinguishable and so we have \(\perms{n}{r}\) ways of selecting \(r\) objects and ordering them.
    However since the objects aren't really distinguishable this over counts by a factor of \(r!\), the number of ways we can sort each selection of \(r\) things.
    Thus the total number of ways to select \(r\) indistinguishable objects from a set of \(n\) things is
    \[W = \frac{\perms{n}{r}}{r!} = \frac{n!}{r!(n-r)!} = \combs{n}{r} = C(n, r) = {n\choose r}.\]
    
    \subsection{Partitions}
    A set, \(S\), of \(n\) elements is partitioned into \(N\) subsets, \(S_i\subseteq S\), \(S_i\ne\emptyset\), if for all \(s\in S\) we have \(s\in S_i\) for exactly one value of \(i\).
    That is all elements are assigned to exactly one subset.
    Clearly this means that
    \[\sum_{i=1}^{N}\abs{S_i} = n.\]
    The number of ways that we can partition \(S\) into two sets of size \(r\) and \(n - r\) is simply the number of ways that we can select \(r\) objects from \(S\) to fill the first subset.
    Since the order of items in a set doesn't matter we have that there are
    \[W = {n\choose r} = \frac{n!}{r!(n-r)!}\]
    ways to partition \(S\) into these two subsets.
    
    If instead we have three subsets, \(S_i\) of size \(k_i\) then there are \(\combs{n}{k_1}\) ways to fill the first subset and \(\combs{n-k_1}{k_2}\) ways to fill the second subset and the leftover elements go into the third subset.
    By the multiplication principle this means that there are
    \[W = \combs{b}{k_1}\combs{n-k_1}{k_2} = \frac{n!}{k_1!(n - k_1)!}\frac{(n - k_1)!}{k_2!(n - k_1 - k_2)!} = \frac{n!}{k_1!k_2!k_3!} = {n \choose k_1, k_2, k_3}\]
    ways to partition \(S\) into these three subsets.
    Here we have used that since \(k_1 + k_2 + k_3 = n\) we must have \(n - k_1 - k_2 = k_3\).
    
    This argument generalises to \(N\) subsets, \(S_i\), of size \(k_i\).
    The number of ways of partitioning \(S\) into these particular subsets is
    \[W = \frac{n!}{\prod_{i=1}^{N}k_i!} = {n \choose k_1, k_2, \dotsc, k_N}.\]
    
    \subsubsection{Partitions in Physics}
    Suppose that we have \(n\) particles and each one is in one of \(N\) states.
    If the number of particles in state \(i\) is \(n_i\) then this gives us some information about the system.
    We call this the macrostate, where we know how many particles are in each state but not which particles are in any particular state.
    On the other hand the knowing the microstate is exactly knowing which particles are in a given state.
    
    One question that we may ask is for a given macrostate how many microstates are there.
    Fortunately we have already answered this question, there are
    \[W = \frac{n!}{\prod_{i=1}^{N}n_i!} = {n \choose n_1, n_2, \dotsc, n_N}.\]
    Assuming that all microstates are equally likely the more microstates a macrostate has the more likely that macrostate is.
    
    \begin{theorem}
        The most likely macrostate, i.e. the partition with the most distinct ways of partitioning, is the macrostate where the number of particles in each state is equal, i.e. the partition where the size of all subsets is equal.
    \end{theorem}
    \begin{proof}
        We aim to show that
        \[\frac{n!}{k_1!k_2!\dotsm k_N!}\]
        is globally maximised when \(k_i = k\) for all \(i = 1, \dotsc, N\).
        To do this we consider what happens if this is the case and we increase the size of one subset.
        To do this we must also reduce the size of another subset to keep the number of elements constant.
        Say that we add one element to the first subset from second then there are
        \[\frac{n!}{(k + 1)!(k - 1)!k!\dotsm k!}\]
        partitions.
        Consider the first two terms of the denominator:
        \begin{align*}
            (k + 1)!(k - 1)! &= (k + 1)k(k - 1) \dotsm 1(k - 1)(k - 2)\dotsm 1\\
            &= (k + 1)k[(k - 1)!]^2\\
            &= (k^2 + k)[(k - 1)!]^2.
        \end{align*}
        Now consider if we hadn't changed the size of the first subset.
        Then the first two terms would be
        \[k!k! = k^2[(k-1)!]^2.\]
        Clearly this is smaller than the case where we had moved one element.
        Since this term appears in the denominator this shows that there is a local maximum when \(k_1 = k_2 = k\) where \(k_i\), \(i > 2\), take on some set values.
        The same argument shows that \(k_i = k\) for all \(i\) produces a local maximum.
        Since any partitioning can be found by moving one element at a time from set to set a similar argument shows that this must be a global maximum as we have shown that this always decreases the total number of partitionings.
    \end{proof}
    It isn't always possible to achieve this `most likely' macrostate however.
    For example if the states that we are talking about are energy levels such that in state \(i\) a particle has energy \(\varepsilon_i\) then assuming the total energy of the system is a finite value, \(E\), we must have that
    \[\sum_{i} n_i\varepsilon_i = E.\]
    If some states are significantly higher energy than others there simply may not be enough available energy to have an equal number of particles in every energy state.
    
    For a small number of energy levels we can fairly easily count the number of partitionings by exhaustion.
    However the way to find the number of partitionings for a larger number of states is fairly complex and involves something called Lagrange multipliers.
\part{Thermodynamics}
    \section{The Basics of Thermodynamics}
    \subsection{What is Thermodynamics}
    The word thermodynamics comes from the Greek \textgreek{θερμός} (therm\`os), meaning heat, and \textgreek{δύναμις} (dunamis), meaning motive force.
    Thermodynamics is perhaps best described as the study of moving heat.
    Of particular interest is how to convert heat into motion and vice versa.
    
    One of the most important ideas that we can take away form thermodynamics is a sense of time and causality.
    In classical mechanics everything is based off of Newton's second law,
    \[F = m\dv[2]{x}{t}.\]
    If we replace \(t\) with \(-t\), this is a time transformation all that happens is that the coefficient, \(m\), changes to \(-m\):
    \[F = -m\dv[2]{x}{t}.\]
    This is simply a statement that all process in classical mechanics are reversible.
    In electromagnetism everything can be built up from Maxwell's equations.
    Two of these are
    \[\curl\vv{E} = -\partial_t\vv{B},\]
    and
    \[\curl\vv{H} = \vv{J} + \partial_t\vv{D}.\]
    If we replace \(t\) with \(-t\) in the first of these we get
    \[\curl\vv{E} = \partial_t\vv{B},\]
    all that happens is the cross product is negated.
    This is equivalent to changing from a right handed to left handed system, this is a parity transformation.
    If we replace \(t\) with \(-t\) in the second equation and change from a right to left handed system we get
    \[\curl\vv{H} = -\vv{J} + \partial_t\vv{D}.\]
    This is equivalent to setting all charges, \(q\), equal to \(-q\).
    This is a charge transformation.
    In quantum mechanics everything is based upon the Schr\"odinger equation:
    \[\frac{1}{2m}\laplacian\psi + V\psi = i\partial_t\psi\]
    if we replace \(t\) with \(-t\) we get
    \[\frac{1}{2m}\laplacian\psi + V\psi = -i\partial_t\psi\]
    however both \(i\) and \(-i\) have the important property of squaring to \(-1\).
    This means that this time transformation is the same as choosing to take the negative square root instead of the positive.
    
    The important thing about all of these transformations is that if we reverse time \((t \to -t)\), parity \((\text{right}\to\text{left})\), charge \((q\to-q)\), as well as which square root we take none of the physics changes.
    This is called \acrfull{cpt} symmetry and is the only symmetry of C, P, and T that exists.
    CP, CT, and PT symmetry are all violated by some processes in nature but \acrshort{cpt} symmetry isn't.
    This means that all of physics is time reversible if we just reverse a few other things at the same time.
    This is not true for thermodynamics.
    The second law of thermodynamics states that entropy will always increase.
    This means we cannot reverse time.
    This gives us a way to define cause and effect.
    
    \subsection{Equilibrium}
    An \define{equilibrium state} is one in which all bulk physical properties do not change with time and are uniform throughout the system.
    Uniformity only happens if the system is in the same phase throughout.
    If there is a phase boundary then he system can be split into two or more homogeneous portions, one for each phase.
    Within each of these portions the properties are uniform, this is what \define{homogeneous} means.
    
    Isolated systems have equilibrium states that they settle into.
    Equilibrium states have uniform macroscopic properties, such as pressure, temperature, density, and magnetisation.
    Surrounding systems need not be in equilibrium with each other.
    
    \subsection{System}
    The \define{system} is the sample of interest.
    The system interacts with its surroundings.
    The system and its surroundings are separated by some kind of boundary wall.
    The type of boundary wall defines what is held constant, for example
    \begin{enumerate}
        \item if the boundary wall is a moving piston then volume is not held constant.
        A system where volume \emph{is} held constant is called \define{isovolumetric}.
        \item if the wall is well insulated the amount of heat energy is held constant.
        A system where the amount of heat energy is held constant is called \define{adiabatic}.
        \item A system that is not insulated will have temperature held constant.
        A system where temperature is held constant is called \define{isothermic}.
    \end{enumerate}
    In a thermodynamic process the way in which system variables change depends on the surroundings.
    
    \subsection{State Functions/Variables}
    Thermodynamics is about macroscopic properties.
    We care about the temperature, not the speed of individual particles.
    We care about the volume, density, or mass, not the number of particles.
    The various properties that can be quantified without disturbing the system are called \define{state functions} or \define{state properties}.
    For example, the internal energy, volume, pressure, and temperature are state functions.
    Properties whose absolute (as opposed to relative) values are easily measured are called \define{state variables}.
    For example, the volume, pressure, and temperature are state variables but the internal energy is not.
    Relations between state functions are called the \define{equations of state} of the material.
    For example for an ideal gas one equation of state is the ideal gas law:
    \[PV = nRT.\]
    
    \subsection{Zeroth Law of Thermodynamics}
    We define two systems to be in thermal contact if it is possible for heat to move from one to the other.
    We define two systems to be in thermal equilibrium if they could be put in thermal equilibrium without any net heat flow between them.
    The zeroth law of thermodynamics is an experimentally law.
    It states:
    \paragraph{Zeroth Law:} If two systems are in thermal equilibrium with another system then the are in thermal equilibrium with each other.
    
    This can be extended to any number of systems.
    The relation, \(\sim\), defined by \(A\sim B\) if and only if \(A\) is in thermal equilibrium with \(B\) is an equivalence relation and the zeroth law is a statement of the transitivity of \(\sim\)\footnote{See proofs and problem solving course for a definition of an equivalence relation.}.
    
    \subsection{Temperature}
    If systems are in thermal equilibrium with each other they must have some property with a common value.
    We will call this property the thermodynamic temperature, \(T_0\), where the 0 denotes that this value is derived from the zeroth law.
    Note that this is not a numerical system.
    All we can say given two systems is that they are either in thermal equilibrium or they aren't and if they aren't then we define the hotter one to be the one that is losing heat energy.
    
    From the ideal gas equation we have
    \[T_\mathrm{IG} = \frac{PV}{nR}\]
    which gives us another definition of temperature.
    This time \(T_\mathrm{IG}\) is a numeric scale.
    This scale relates to macroscopic properties.
    
    We can also define a third temperature system based off of the kinetic properties of particles:
    \[\frac{1}{2}m\bar{v}^2 = \frac{3}{2}\boltzmann T_\mathrm{K}\]
    \(T_\mathrm{K}\) is again a numerical scale which now relates to the microscopic property of average kinetic energy.
    
    The amazing thing is that it turns out that all three temperature systems, \(T_0\), \(T_\mathrm{IG}\) and \(T_\mathrm{K}\), actually are all equivalent so can be viewed as one temperature system, \(T\), which relates to the zeroth law, the macroscopic properties and the microscopic properties of the system.
    
    The base point for temperature was changed recently.
    We used to use absolute zero and the triple point of water to define two points on the scale and then filled in around that.
    We still use absolute zero, we just use a different upper point.
    It is based off the speed of sound in a gas which is given by
    \begin{equation}\label{eqn:speed of sound in gas}
        c_0 = \sqrt{\pdvconst{P}{\rho}{S}} = \sqrt{\frac{\gamma P}{\rho}}\ = \sqrt{\frac{\gamma\avagadro\boltzmann T}{M}}.
    \end{equation}
    In July 2013 the \acrfull{npl} measured
    \[\boltzmann = \SI{1.38065156(98)e-23}{\joule.\kelvin^{-1}}.\]
    The speed of sound in argon was also measured using a copper sphere whose diameter was known to \(\SI{11.7}{\nano\metre}\), which is approximately 500 atoms in a row.
    The accurate measure of these values allowed for equation~\ref{eqn:speed of sound in gas} to be used to specify a particular temperature more accurately than the triple point of water could.
    For this reason this is now taken as part of the definition of our temperature scale.
    
    \section{Reversibility}
    \subsection{Processes}
    In Thermodynamics a process is the change of a state variable with time.
    If a process is reversible then every (infinitesimal) step is reversible for both the system and the surroundings.
    This involves moving in between equilibrium states.
    A reversible process is quasi-static, meaning that if the external driving force is removed then the process will halt as it is in an equilibrium state.
    
    An irreversible process is the opposite of a reversible process.
    It involves moving away from equilibrium states.
    This means that the state variables are not necessarily the same everywhere during an irreversible process.
    This means that even something as simple as
    \[\int_{P_1}^{P_2}\,\dd{P}\]
    can be undefined if \(P\) is not the same throughout the system.
    
    In general reversible processes involve a slow process such as a slow compression that allows the system to relax to equilibrium between each infinitesimal step.
    The equivalent irreversible process would be pushing the piston in quickly.
    This causes turbulence which is a dissipative process and causes energy to be lost to the surroundings as heat, which is hard to recapture.
    
    When drawing on an indicator diagram, such as a \((P, V)\) diagram, if a process is reversible it can be represented as a continuous line.
    If it is irreversible this may not be possible.
    Instead we draw a series of circles representing the range of values that the state variables take at any one time.
    
    It is possible to move between two states by a reversible or irreversible process.
    The final state of the system will be the same regardless of reversibility.
    However the state of the surroundings will be different for the different processes.
    
    \begin{example}
        Consider a piston of area \(A\) being pushed in a distance, \(\dd{x}\), by a force, \(F\).
        The force required is given by \(F = PA\) which is simply the definition of pressure, \(P\).
        The volume change is \(\dd{V} = A\dd{x}\).
        The work done is \(\dd{W} = F\dd{x} = PA\dd{x} = P\dd{V}\).
        For this reason we define work in thermodynamics as \(\dd{W} = (-)P\dd{V}\).
        If we now wish to find the work done for a finite volume change from \(V_1\) to \(V_2\) we simply integrate:
        \[W = \int_{W_1}^{W_2}\dd{W} = \int_{V_1}^{V_2}P\,\dd{V}.\]
        The total work depends exactly on what the process is.
        If \(P\) is a constant then this integral is trivially \(P(V_2 - V_1)\).
        If we assume an ideal gas then the integral depends on whether we take \(T\) as a constant or try to quantify how it changes.
    \end{example}
    \begin{example}\label{exa:reversible ideal gas expansion}
        Consider an ideal gas that expands reversibly from a state, \((P_1, V_1, T_1)\), to a state, \((P_2, V_2, T_2)\).
        Again the work done is
        \[W = \int_{V_1}^{V_2}P\,\dd{V}.\]
        \begin{figure}[ht]
            \centering
            \tikzsetnextfilename{PV-reversible-process}
            \begin{tikzpicture}
                \draw[<->] (0, 4) -- (0, 0)-- (4, 0);
                \draw (1, 3) -- (1, 1) -- (3, 1);
                \begin{scope}
                    \clip (1, 1) rectangle (3, 3);
                    \draw (3, 3) circle[radius=2cm];
                \end{scope}
                \draw[dashed] (1, 1) -- (1, 0);
                \draw[dashed] (3, 1) -- (3, 0);
                \draw[dashed] (1, 1) -- (0, 1);
                \draw[dashed] (1, 3) -- (0, 3);
                \node[below] at (1, 0) {\(V_1\)};
                \node[below] at (3, 0) {\(V_2\)};
                \node[left] at (0, 1) {\(P_1\)};
                \node[left] at (0, 3) {\(P_2\)};
                \node[right] at (4, 0) {\(V\)};
                \node[above] at (0, 4) {\(P\)};
                \draw[->] (1, 3) -- (1, 2);
                \draw[->] (1, 1) -- (2, 1);
                \draw[->] (1.59, 1.59) -- (1.591, 1.589);
                \node[left] at (1, 2) {\(A\)};
                \node[below] at (2, 1) {\(B\)};
                \node[above right] at (1.59, 1.59) {\(C\)};
            \end{tikzpicture}
            \caption{Two possible processes for a reversible expansion.}
            \label{fig:reversible ideal gas expansion}
        \end{figure}
        Consider figure~\ref{fig:reversible ideal gas expansion}.
        The path marked \(C\) is an isothermal process.
        This makes it easy to define the pressure using the ideal gas law with \(T = T_1 = T_2\):
        \[P = \frac{nRT}{V}.\]
        Thus the work done becomes
        \begin{align*}
            W &= \int_{V_1}^{V_2}P\,\dd{V}\\
            &= nRT\int_{V_1}^{V_2}\frac{\dd{V}}{V}\\
            &= nRT\ln\left(\frac{V_2}{V_1}\right).
        \end{align*}
        If instead we consider the non-isothermal process along path \(A\) then \(B\) we end up at the same point.
        This time we have to do two integrals.
        The first is along \(A\) and the second along \(B\).
        Thus the work is
        \begin{align*}
            W &= \int_A P\,\dd{V} + \int_B P\,\dd{V}\\
            &= \int_{V_1}^{V_1}P\,\dd{V} + \int_{V_1}^{V_2}P\,\dd{V}\\
            &= 0 + P\int_{V_1}^{V_2}\dd{V}\\
            &= P(V_2 - V_1).
        \end{align*}
        Here we have used the fact that an integral over \([V_1, V_1]\) is zero so we don't need to know the pressure.
        We then used the fact that the process that takes us along \(B\) is isobaric (constant pressure) so we can factor the pressure out of the integral.
        
        We see that we get two different results for the work done depending on the process, even for a reversible process like this one.
        In fact we could do an infinite amount of work by just cycling along path \(A\) then \(B\) and then backwards along \(C\).
    \end{example}
    
    \subsection{Sign Convention}
    In physics and chemistry we are usually interested in changes of the system.
    In engineering the interest is usually in changes of the surroundings.
    For this reason there is a difference in sign conventions used to define work.
    It could be \(\pm P\dd{V}\).
    We choose\footnote{We will explain the bar through the \(\dd\) in a couple of sections}
    \[\ddbar{W} = -P\dd{V}.\]
    The logic behind this is that if \(\dd{V} < 0\), i.e. the system is compressed, then the work done is positive.
    We expect the work done to compress a gas to be positive so this is the correct sign choice for us.
    The important thing is to think about the sign that you expect \emph{before} attempting a problem.
    
    \subsection{Dissipative Processes}
    For an irreversible process we can't specify the work done by the state variables as these aren't well defined.
    We can measure the work done on/by the surroundings and use conservation of energy to say what this means for work done on/by the system.
    
    An example of a dissipative process is stirring.
    The system heats up but \(P\dd{V} = 0\) as the volume doesn't change.
    This means this can't possibly be the correct form of the work done.
    So this process must be dissipating energy in some way.
    If you stir in the opposite direction, i.e. perform the reverse process, it isn't possible to un-mix or extract energy.
    We see that stirring is not a reversible process.
    
    Dissipative processes are always irreversible, however not all irreversible processes are dissipative.
    
    \subsection{The Maths of Thermodynamics}
    In thermodynamics we often ask the questions
    \begin{itemize}
        \item How does changing one state variable affect another -- this points us towards differential calculus as a solution.
        \item How does constraining one variable while changing a second affect a third variable -- this points us to partial derivatives.
    \end{itemize}
    
    \subsubsection{Exact Differentials}
    Consider a single valued state function, \(\varphi\), of the variables \(x\) and \(y\).
    If we change \(x\) by \(\dd{x}\) and \(y\) by \(\dd{y}\) the change in \(\varphi\) is given by
    \[\dd{\varphi} = \pdvconst{\varphi}{x}{y}\dd{x} + \pdvconst{\varphi}{y}{x}\dd{y}.\]
    The same can be done for a function of more variables.
    The finite change in \(\varphi\) is given by
    \[\Delta\varphi = \varphi(x_2, y_2) - \varphi(x_1, y_1) = \int_{(x_1, y_1)}^{(x_2, y_2)} \dd{\varphi}.\]
    So we see that \(\Delta\varphi\) is uniquely determined by its evaluation at the two points \((x_1, y_1)\), and \(x_2, y_2\).
    The integral is path independent.
    State variables in thermodynamics have path independent integrals.
    
    For example if a process takes a system from the state \((T_1, P_1)\) to \((T_2, P_2)\) then we can find another state variable by integrating along any path between these two points.
    So we may as well choose one that makes the integral as easy as possible.
    
    We can calculate the change of a state variable due to an irreversible process by calculating the change of that state variable due to an equivalent reversible process.
    This is only true for the system.
    We can't do this for the surroundings as the state of the surroundings changes depending on if we follow a reversible or irreversible process but by definition the system is in the same state at the beginning and end of the process.
    
    All state variables, \(f\), of the variables \(x_i\) have the property that
    \[\pdvsec{f}{x_i}{x_j} = \pdvsec{f}{x_j}{x_i}\]
    where all the variables that aren't explicitly included are held constant.
    This means that \(f\) is a well behaved, differentiable, function.
    State variables are almost always well behaved, the one exception is at a phase transition, for example the volume can change discontinuously upon boiling.
    
    If we have two independent variables and \(f\) depends on both of them then the variables that we hold constant are important.
    For the following derivatives describe isothermal and isobaric compression and are in general not equal:
    \[\pdvat{f}{V}{T} \ne \pdvat{f}{V}{P}.\]
    
    \subsubsection{Inexact Differentials}
    If something is not a state variable then it does not have an exact differential.
    We denote the inexact differential with a bar, such as \(\ddbar{W}\), to remind us of this.
    The integral over this variable is dependent on the path, this is what it means to be inexact.
    We consider the argument from before in reverse.
    Suppose that the integral over \(\ddbar{\varphi}\) is path dependent.
    Therefore we cannot simply write
    \[\int\ddbar{\varphi} = \varphi(\vv{x}_2) - \varphi(\vv{x}_1)\]
    where \(\vv{x}_1\) and \(\vv{x_2}\) are simply vectors of state variables.
    The fact that we can't write this means that \(\varphi\) is not a state function.
    
    Work is not a state function.
    We can show this by showing that the order of partial derivatives matters.
    The inexact differential for work is \(\ddbar{W} = -P\dd{V}\).
    The question that we ask is if there exists a function, \(W\), of variables \(P\) and \(T\) such that
    \[\dd{W} = \pdvconst{W}{P}{V}\dd{P} + \pdvconst{W}{V}{P}\dd{V}?\]
    We still require that \(\ddbar{W} = -P\dd{V}\) so from this we must have
    \[\pdvconst{W}{P}{V} = 0,\qquad\text{and}\qquad \pdvconst{W}{V}{P} = -P.\]
    Calculating second derivatives we get
    \begin{align*}
        \pdv{V}\left(\pdvconst{W}{P}{V}\right)_{P} &= \pdv{V}\left(0\right)_{P} = 0\\
        \pdv{P}\left(\pdvconst{W}{V}{P}\right)_{V} &= \pdv{P}\left(-P\right)_{V} = -1
    \end{align*}
    Since \(0 \ne -1\) we have that
    \[\pdvsec{W}{P}{V} \ne \pdvsec{W}{V}{P}.\]
    Hence \(W\) is not a state function.
    The order of derivatives is important and the integral is path dependent.
    Remember that in example~\ref{exa:reversible ideal gas expansion} the path that we chose affected the work done.
    
    To summarise
    \begin{itemize}
        \item State variables in thermodynamics have path independent integrals.
        \item Reversible processes in thermodynamics have path dependent integrals.
        \item Irreversible processes in thermodynamics have no well defined integrals.
    \end{itemize}
    
    \begin{example}
        The state of a fluid undergoes an infinitesimal, reversible, change from \((P, T)\) to \((P + \dd{P}, T + \dd{T})\).
        What is the change in volume?
        Start by writing the volume as a function of relevant state variables, \(V = V(P, T)\).
        The change in volume is then
        \begin{align*}
            \dd{V} &= \pdvconst{V}{P}{T}\dd{P} + \pdvconst{V}{T}{P}\dd{T}\\
            &= -\frac{K}{V}\dd{P} + \beta V\dd{T}
        \end{align*}
        where \(K\) is the bulk modulus, which characterises isothermic compression, and \(\beta\) is the thermal expansivity, which characterises isobaric compression.
        They are defined by
        \[K = -V\pdvconst{P}{V}{T},\qquad\text{and}\qquad\beta = \frac{1}{V}\pdvconst{V}{T}{P}.\]
        \(V\) is a state function so we can choose any path.
        We choose a reversible, two-stage path; an isotherm from \(P\) to \(P + \Delta P\) and then an isobar from \(T\) to \(T + \Delta T\).
        The total volume change is then given by the integral over these two paths.
        For the first path \(\dd{T} = 0\) as it is an isotherm.
        Thus we have
        \[\int\frac{1}{V}\,\dd{V} = -\int\frac{1}{K}\,\dd{P} \implies \ln\left(\frac{V_2}{V_1}\right) = -\frac{\Delta P}{K}.\]
        For the second path \(\dd{P} = 0\) as it is an isobar.
        Thus we have
        \[\int\frac{1}{V}\,\dd{V} = -\int\beta\,\dd{T} \implies \ln\left(\frac{V + \Delta V}{V_1}\right) = \beta\Delta T.\]
        After some rearranging we get
        \[V + \Delta V = Ve^{-\Delta P/K}e^{\beta\Delta T}.\]
        Notice that \(V_1\) disappears as this was just an intermediate value that we introduced to define a path.
        We would have also gotten the same result if we started with the isobar and then the isotherm.
        If we had considered an irreversible process the answer would have been the same even though the integral is not defined for that process.
    \end{example}
    
    \section{Heat}
    Take some ice, put it in some water.
    Does all the ice melt?
    What temperature does the water end up?
    The process for answering these questions is to consider the following process:
    \begin{itemize}
        \item The water cools to \(\SI{0}{\degreeCelsius}\).
        This releases \(Q_l = m_lc_l\Delta T_l\) where the symbols have their usual meanings and \(l\) denotes that these are for the liquid water.
        \item The ice warms to \(\SI{0}{\degreeCelsius}\).
        This takes \(Q_s = m_sc_s \Delta T_s\) where \(g\) denotes that these are for the solid water.
        \item The ice melts. This takes \(E = m_s l_\text{vap}\).
        \item Depending on the sign of the left over energy one of two things will happen next:
        \begin{itemize}
            \item If the left over energy is positive it is used to heat the water by \(\Delta T = (Q_l + Q_s + E)/[(m_l + m_s)c_l]\).
            \item If the left over energy is negative some of the ice freezes again.
            We end up with a mass of ice given by \(m = -(Q_1 + Q_s + E)/l_\text{vap}\).
        \end{itemize}
    \end{itemize}
    This successfully answers the question even though this process is not the same as what actually happens.
    For example if the energy left at the end is negative then not all the ice will melt in the first place.
    The fact that we can do this means there must be a state function related to the energies that we are computing with.
    We will see later in this section what that is.
    
    \subsection{What is Heat?}
    James Joule used falling weights to turn paddles in water and measured a temperature rise.
    He found that the same amount of mechanical work, \(mgh\), always produced the same rise in temperature.
    He also found that he could use electrical work to produce this same temperature rise.
    This showed that heat could be produced by work alone.
    Previously heat had been thought of as an element that each material had a finite amount of and could be released by processes such as burning.
    However by putting in an arbitrary amount of mechanical or electrical work Joule showed that an arbitrary amount of heat could be produced and so it necessarily had to come from the work.
    
    As a result of being able to use different forms of work to produce the same change in a system we must have that when a thermally isolated system is brought from one equilibrium state to another the work necessary is independent of the process used.
    This observation requires that there is a state function related to heat, we call it the internal energy, \(U\).
    It was found that
    \[\Delta U = U_2 - U_1 = W_\text{adiabatic}\]
    where the last term is the work done in a process that allows no heat to flow out of the system.
    Essentially this means that the work done in a process, if it can't go anywhere else, goes to increase the internal energy of the system.
    Notice that this equation doesn't give us a way to define a point where \(U = 0\), we can only define internal energy relative to the internal energy in some other state.
    
    This equation is a restricted form of the first law of thermodynamics.
    In this form it applies only to a thermally insulated system and cannot be applied to the whole universe.
    
    If we now allow heat transfer then we find that
    \[\Delta U = U_2 - U_1 = W + Q\]
    where \(W\) is the work done on the system and \(Q\) is the heat going into the system (in other contexts this may have a different sign for heat leaving the system).
    Heat is the exchange of internal energy between the system and the surroundings that cannot be identified as work.
    
    What counts as heat therefore depends on how we define our system.
    If we have a room with a heater and a box of gas then there are two logical ways to define the system.
    The first is as just the box of gas.
    In this case the energy given out by the heater which is absorbed by the gas is counted as heat.
    If instead we define the whole room as the system then the heater is no longer a source of heat for the system it is a source of electrical work and, assuming the walls are well insulated, \(Q = 0\).
    In a way heat and work are just two sides of the same coin.
    
    \subsection{First Law of Thermodynamics}
    \paragraph{First Law:} The change in internal energy, \(\dd{U}\), of a system is given by the sum of the work done on the system, \(\ddbar{W}\), and the heat transferred to the system, \(\ddbar{Q}\):
    \[\dd{U} = \ddbar{W} + \ddbar{Q}.\]
    
    Note that \(\int\ddbar{W}\) and \(\int\ddbar{Q}\) are both path dependent but their sum, \(\int\dd{U}\) is not.
    Often we are interested in the work done mechanically on a compressible fluid\footnote{Remember that fluid encompasses both liquids and gasses.}, we know in this case that the work done is \(\ddbar{W} = -P\dd{V}\) and so the first law of thermodynamics is
    \[\dd{U} = -P\dd{V} + \ddbar{Q}.\]
    
    Typical forms of work that we need to account for are electrical, magnetic, or gravitational.
    Typical forms of internal energy are kinetic energy and potential energy, which in turn includes chemical, nuclear, and gravitational energy, as well as mass energy due to relativity.
    Fortunately since internal energy is always defined relatively we only need to account for sources of internal energy that change.
    For example if we are compressing a gas we don't need to account for chemical, nuclear, gravitational, or mass energy unless we start compressing the gas so much that it starts reacting with itself or undergoing nuclear fusion, and this is unlikely.
    
    Another important feature is that due to the path independence of \(\int\dd{U}\) the first law of thermodynamics has time reversal symmetry.
    
    \subsection{Heat Capacity}
    The heat capacity, \(C\), of a material that is heated by \(\Delta Q\), and has a temperature increase of \(\Delta T\). is defined to be
    \[C = \lim_{\Delta T\to 0} \frac{\Delta Q}{\Delta T} = \dv{Q}{T}.\]
    This definition is useful for theoretical work but for practical calculations we define two more useful quantities: the specific heat capacity, \(c = C/m\),  where \(m\) is the mass of the system, the specific heat capacity then gives the energy needed to heat \(\SI{1}{\kilogram}\) of the substance by \(\SI{1}{\kelvin}\).
    We also define the molar heat capacity, \(c = C/n\)\footnote{Yes, that is the same symbol as specific heat capacity, thermodynamics has too many letters, and an inexplicable refusal to use Greek letters like any other self respecting field of physics would.}, where \(n\) is the number of moles of the substance, the molar heat capacity then gives the energy needed to heat \(\SI{1}{\mole}\) of the substance by \(\SI{1}{\kelvin}\).
    
    As normal with thermodynamics a definition that doesn't specify what is constant isn't very useful.
    If we hold volume constant then the heat capacity at constant volume is \(C_V\).
    We start with a rearrange form of the first law of thermodynamics:
    \[\dd{Q_V} = \dd{U_V} + P\dd{V_V}.\]
    Importantly we are holding \(V\) constant so \(\dd{V_V} = 0\) which means \(\dd{Q_V} = \dd{U_V}\).
    \[C_V = \pdvconst{Q}{T}{V} = \pdvconst{U}{T}{V}.\]
    If instead we hold the pressure constant then \(\dd{Q_P} = \dd{U_P} + P\dd{V_P}\).
    We define the enthalpy, \(H\), to be \(H = U + PV\).
    In differential form this becomes
    \[\dd{H} = \dd{(U + PV)} = \dd{U} + P\dd{V} + V\dd{P}.\]
    Holding \(P\) constant gives us
    \[\dd{H_P} = \dd{U_P} + P\dd{V_P}\]
    since \(\dd{P_P} = 0\) when pressure is held constant.
    Further we know that \(\dd{U_P} = -P\dd{V_P} + \dd{Q_P}\) so substituting this into the enthalpy gives us
    \[\dd{H_P} = -P\dd{V_P} + \dd{Q_P} + P\dd{V_P} = \dd{Q_P}.\]
    Finally this allows us to calculate the heat capacity at constant pressure:
    \[C_P = \pdvconst{Q}{T}{P} = \pdvconst{H}{T}{P}.\]
    Notice that heat capacity is a state variable, unlike heat.
    
    \subsubsection{Relation Between \texorpdfstring{\(C_V\)}{CV} and \texorpdfstring{\(C_P\)}{CP}}
    From the first law of thermodynamics
    \[\dd{Q_P} = \dd{U_P} P\dd{V_P}\]
    dividing through by \(\dd{T}\) and taking a limiting process while holding pressure constant gives us the heat capacity at constant pressure again:
    \begin{equation}\label{eqn:C_P}
        C_P = \pdvconst{Q}{T}{P} = \pdvconst{U}{T}{P} + P\pdvconst{V}{T}{P}
    \end{equation}
    Now let \(U = U(T, V)\), then a small change in these state variables produces the following change in \(U\):
    \begin{equation}\label{eqn:dU}
        \dd{U} = \pdvconst{U}{T}{V}\dd{T} + \pdvconst{U}{V}{T}\dd{V}
    \end{equation}
    Dividing through by \(\dd{T}\) and taking a limiting process while holding pressure constant gives us
    \[\pdvconst{U}{T}{P} = \pdvconst{U}{T}{V} + \pdvconst{U}{V}{T}\pdvconst{V}{T}{P}.\]
    We can identify the first term on the right hand side as \(C_V\):
    \[\pdvconst{U}{T}{P} = C_V + \pdvconst{U}{V}{T}\pdvconst{V}{T}{P}.\]
    We can now substitute this into equation~\ref{eqn:C_P} to get
    \[C_P = C_V + \pdvconst{U}{V}{T}\pdvconst{V}{T}{P} + P\pdvconst{V}{T}{P}.\]
    Subtracting \(C_V\) and factorising we get
    \[C_P - C_V = \left[P + \pdvconst{U}{V}{T}\right]\pdvconst{V}{T}{P}.\]
    This is the best we can do in general, however if we consider an ideal gas then the internal energy is dependent only on the pressure.
    This means that
    \[\pdvconst{U}{V}{T} = 0\]
    for an ideal gas.
    This means
    \[C_P - C_V = P\pdvconst{V}{T}{P}.\]
    From the ideal gas law we have
    \[V = \frac{nRT}{P} \implies \pdvconst{V}{T}{P} = \frac{nR}{P}.\]
    Thus
    \begin{equation}\label{eqn:C_P - C_V = nR ideal gas}
        C_P - C_V = P\frac{nR}{P} = nR.
    \end{equation}
    Now looking at equation~\ref{eqn:dU} for an ideal gas we see that final term is zero and so
    \begin{equation}\label{eqn:dU = C_VdT ideal gas}
        \dd{U} = \pdvconst{U}{T}{V}\dd{T} = C_V\dd{T}.
   \end{equation} 
    We will use this in future calculations.
    
    \subsubsection{Reversible, Isothermal, Ideal Gas Expansion}
    For an isothermal expansion \(\Delta T = 0\).
    The ideal gas law gives us
    \[P = \frac{nRT}{V} \propto \frac{1}{V}\]
    this means that on a \((P, V)\)-indicator diagram we expect isotherms, that is lines of constant temperature, to be hyperbola.
    Since we are considering a reversible process we know that the work done on the system is the negative of the work done on the surroundings.
    We can fairly easily find the work done on the surroundings:
    \[W = \int\ddbar{W} = \int P\dd{V_T} = nRT\int\frac{\dd{V}}{V} = nRT\ln\left(\frac{V_2}{V_1}\right).\]
    
    \subsubsection{Reversible, Adiabatic, Ideal Gas Expansion}
    For an adiabatic expansion \(\Delta Q = 0\) so \(\Delta U = \Delta W\).
    Similarly \(\ddbar{Q} = 0\) so applying equation~\ref{eqn:dU = C_VdT ideal gas} and the first law of thermodynamics gives us
    \[0 = \ddbar{Q} = \dd{U} + P\dd{V} = C_V\dd{T} + P\dd{V}.\]
    Using the ideal gas law to replace \(P\), and then the relationship that we derived for an ideal gas in equation~\ref{eqn:C_P - C_V = nR ideal gas}, that \(C_P - C_V = nR\), we get
    \[0 = C_V\dd{T} + \frac{nRT}{V}\dd{V} = C_V\dd{T} + (C_P - C_V)\frac{T}{V}\dd{V}.\]
    Rearranging gives us
    \[-\frac{C_V}{T}\dd{T} = (C_P - C_V)\frac{1}{V}\dd{V}.\]
    Integrating gives us
    \[-C_V\int\frac{\dd{T}}{T} = (C_P - C_V)\int\frac{\dd{V}}{V}\]
    which we evaluate as
    \[-C_V\ln T = (C_P - C_V)\ln V + \ln A\]
    where \(\ln A\) is a constant of integration.
    Applying log laws gives us
    \[\ln T^{-C_V} = \ln \left(AV^{C_P - C_V}\right).\]
    Exponentiating gives
    \[T^{-C_V} = AV^{C_P - C_V}.\]
    Multiplying through by \(T^{C_V}\) gives
    \[1 = AT^{C_V}V^{C_P - C_V}.\]
    Raising everything to the power of \(1/C_V\) gives
    \[1 = A^{1/C_V}TV^{C_P - 1}.\]
    Multiplying through by \(A^{-1/C_V}\) and defining \(\gamma = C_P/C_V\) we get
    \[A^{-1/C_V} = TV^{\gamma - 1}.\]
    Since \(A\) and \(C_V\) are constants we conclude that \(TV^{\gamma - 1}\) is a constant.
    From the ideal gas law we know that
    \[T = \frac{PV}{nR}\]
    and so the constant \(TV^{\gamma - 1}\) in this form is
    \[TV^{\gamma - 1} = \frac{PV}{nR}V^{\gamma - 1} = \frac{P}{nR}V^\gamma.\]
    Since \(n\) and \(R\) are constants and the left hand side is a constant we must have that \(PV^\gamma\) is constant.
    If instead we write the volume using the ideal gas law:
    \[V = \frac{nRT}{P}\]
    then the constant \(PV^\gamma\) becomes
    \[PV^\gamma = P\left(\frac{nRT}{P}\right)^\gamma = P^{1 - \gamma}(nR)^\gamma T^\gamma.\]
    Since \(\gamma\) is a constant and so are \(n\) and \(R\) we have that \(P^{1-\gamma}T^\gamma\) is a constant, or as it is more usually written by raising everything to the power of \(1/\gamma\): \(TP^{(1 - \gamma)/\gamma}\) is constant.
    
    \subsection{Irreversible Free Expansion and the Joule Coefficient}\label{sec:irreversible free expansion and the Joule coefficient}
    A rigid, adiabatic, container is partitioned into two equal volumes, one which is a vacuum and the other which contains a gas.
    The system we consider is this all of the gas in the container.
    The irreversible process in question is the breaking of the partition.
    No work is done on the system and there is no heat transfer so the internal energy is constant and \(\dd{U} = 0\).
    We can write the temperature as a state function, \(T = T(U, V)\).
    A small change in these state variables causes the following change in the temperature:
    \begin{align*}
        \dd{T} &= \pdvconst{T}{V}{U}\dd{V} + \pdvconst{T}{U}{V}\dd{U}\\
        &= \pdvconst{T}{V}{U}\dd{V}\\
        &= \mu_J\dd{V}.
    \end{align*}
    This is the definition of \(\mu_J\), called the \define{Joule coefficient}, it characterises temperature change due to a volume change.
    For an ideal gas \(U\) depends only on the temperature.
    This means that for \(U\) to be constant we must have the state start and end at the same temperature.
    Therefore \(\mu_J = 0\).
    
    \section{Heat Engines}
    A heat engine turns heat into work.
    They do so by a cyclic process.
    Every cycle the system returns to an initial equilibrium.
    However the surroundings do not return to the same state and work is done.
    All of the steps of the cycle are quasi-static meaning reversible.
    During each step the values of the state variables change and the system exchanges heat and mechanical energy with its surroundings.
    
    \subsection{Carnot Engine}
    The most efficient heat engine is the Carnot engine.
    It follows that Carnot cycle:
    \begin{itemize}
        \item Isothermal expansion
        \item Adiabatic expansion
        \item Isothermal compression
        \item Adiabatic compression
    \end{itemize}
    During the isothermal expansion heat, \(Q_1\), is absorbed, then during the isothermal compression heat, \(Q_2\), is given out.
    The stages drawn on a \((P, V)\)-indicator diagram form a closed loop.
    The work done is the area enclosed by this loop.
    The surroundings of the Carnot engine consist of two constant temperature heat reservoirs.
    In each step of the process there is a piston moving.
    The heat exchange can only occur along the isotherms since by definition there is no heat exchange along an adiabat.
    Every cycle the work done is \(W = Q_1 - Q_2\).
    Reversing the process requires an input of work and heat is moved from one reservoir to the other.
    This is the process by which a refrigerator works.
    \subsection{General Heat Engine}
    \begin{figure}
        \centering
        \tikzsetnextfilename{basic-heat-engine}
        \begin{tikzpicture}
            \tikzstyle{hot} = [color=red, ultra thick, fill=red!50!white, rounded corners]
            \tikzstyle{cold} = [color=blue, ultra thick, fill=blue!50!white, rounded corners]
            \tikzstyle{engine} = [ultra thick, rounded corners]
            \draw[hot] (0, 4) rectangle (3, 5);
            \draw[cold] (0, 0) rectangle (3, 1);
            \draw[engine] (1, 2) rectangle (2, 3);
            \draw[engine, ->] (1, 2.3) -- (1, 2.5);
            \node at (1.5, 2.5) {\(E\)};
            \node[cold] at (1.5, 0.5) {Cold Body, \(T_2\)};
            \node[hot] at (1.5, 4.5) {Hot Body, \(T_1\)};
            \draw[engine, ->] (1.5, 4) -- (1.5, 3);
            \draw[engine, ->] (1.5, 2) -- (1.5, 1);
            \draw[engine, ->] (2, 2.5) -- (3, 2.5);
            \node[right] at (1.5, 3.5) {\(Q_1\)};
            \node[right] at (1.5, 1.5) {\(Q_2\)};
            \node[right] at (3, 2.5) {\(W\)};
        \end{tikzpicture}
        \caption{The basic workings of a heat engine}
        \label{fig:heat engine}
    \end{figure}
    The internal energy is constant.
    Therefore by the first law of thermodynamics we know that
    \[0 = \Delta Q + \Delta W = Q_1 - Q_2 - W \implies W = Q_1 - Q_2.\]
    Notice that we define work done by the engine to be positive as opposed to the more normal work done on the system being positive.
    This is because an engine doing positive work makes more sense so we switch convention when talking about engines.
    The \define{efficiency} of any process is the ratio of useful energy out to total energy in.
    For the heat engine the efficiency is
    \[\eta = \frac{W}{Q_1} = \frac{Q_1 - Q_2}{Q_1} = 1 - \frac{Q_2}{Q_1}.\]
    Perhaps a more useful quantity in practice is the power.
    Power is simply the useful energy per second which is the work done per cycle times the number of cycles per second.
    There is a balance to be had here, we could have very efficient but slow cycles if we were going for peak efficiency but if we want peak power then there will be some optimal speed which may not be at peak efficiency.
    
    \subsection{Second Law of Thermodynamics}
    The second law of thermodynamics defines certain processes to be impossible even if they still follow the first law.
    There are two equivalent statements:
    \begin{center}
        \begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
            Clausius statement:
            \paragraph{Second Law:} It is impossible to construct a device that, operating in a cycle, produces no effect other than the transfer of heat from a colder to a hotter body. &
            
            Kelvin--Planck statement:
            \paragraph{Second Law:} It is impossible to construct a device that, operating in a cycle, produces no effect other than the extraction of heat from a single body at a uniform temperature and the performance of an equivalent amount of work.
        \end{tabular}
    \end{center}

    Clausius' statement forbids heat to flow from cold to hot even if the total energy is conserved.
    What this means is that for a refrigerator it is impossible to have \(Q_1 = Q_2\) without also putting in work.
    The Kelvin--Planck statement for bids heat to be converted into work with \SI{100}{\percent} efficiency.
    What this means is that for an engine it is impossible to have \(Q_2 = 0\).
    
    Broadly the first law forbids processes where the energy out is different than the energy in and the second law says that the energy out will always have a contribution that we cannot use for driving another process.
    
    We can draw heat engines for the forbidden processes described in both of these statements.
    This is done in figure~\ref{fig:illegal heat engines}
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{forbidden-heat-engines}
        \begin{tikzpicture}
            \tikzstyle{hot} = [color=red, ultra thick, fill=red!50!white, rounded corners]
            \tikzstyle{cold} = [color=blue, ultra thick, fill=blue!50!white, rounded corners]
            \tikzstyle{engine} = [ultra thick, rounded corners]
            
            \draw[hot] (0, 4) rectangle (3, 5);
            \draw[cold] (0, 0) rectangle (3, 1);
            \draw[engine] (1, 2) rectangle (2, 3);
            \draw[engine, ->] (1, 2.7) -- (1, 2.5);
            \node at (1.5, 2.5) {\(R\)};
            \node[cold] at (1.5, 0.5) {Cold Body};
            \node[hot] at (1.5, 4.5) {Hot Body};
            \draw[engine, <-] (1.5, 4) -- (1.5, 3);
            \draw[engine, <-] (1.5, 2) -- (1.5, 1);
            \node[right] at (1.5, 3.5) {\(Q\)};
            \node[right] at (1.5, 1.5) {\(Q\)};
            
            \begin{scope}[xshift=4cm]
                \draw[hot] (0, 4) rectangle (3, 5);
                \draw[cold] (0, 0) rectangle (3, 1);
                \draw[engine] (1, 2) rectangle (2, 3);
                \draw[engine, ->] (1, 2.3) -- (1, 2.5);
                \node at (1.5, 2.5) {\(E\)};
                \node[cold] at (1.5, 0.5) {Cold Body};
                \node[hot] at (1.5, 4.5) {Hot Body};
                \draw[engine, ->] (1.5, 4) -- (1.5, 3);
                \draw[engine, ->] (2, 2.5) -- (3, 2.5);
                \node[right] at (1.5, 3.5) {\(Q\)};
                \node[right] at (3, 2.5) {\(W = Q\)};
            \end{scope}
            \node[color=white, left] at (0, 2.5) {\(W=Q\)};
        \end{tikzpicture}
        \caption{Heat engines forbidden by the second law}
        \label{fig:illegal heat engines}
    \end{figure}
    We can fairly easily show that the two statements of the second law are equivalent.
    First suppose that Kelvin--Planck is false but Clausius still stands.
    We use a Kelvin--Planck violating engine, \(E\), to drive a refrigerator, \(R\).
    If both \(E\) and \(R\) have the same hot and cold bodies then \(E\) extracts heat, \(Q_1\), from the hot body and converts it entirely to work, \(W\), which is then used to run \(R\).
    \(R\) extracts heat \(Q_2\) from the cold body and delivers \(Q_2 + W = Q_2 + Q_1\) to the hot body since the total energy into \(R\) must be equal to the total energy out by the first law.
    If we then treat the system including both \(E\) and \(R\) as one composite refrigerator, \(E + R\), then \(E + R\) extracts \(Q_2\) from the hot body and delivers \(Q_2\) to the cold body.
    This violates Clausius so our assumption that Kelvin--Planck doesn't hold must be wrong.
    Hence Clausius implies Kelvin--Planck.
    We can see the systems in question in figure~\ref{fig:clausius implies kelvin-planck}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{clausius-violating-heat-engines}
        \begin{tikzpicture}
            \tikzstyle{hot} = [color=red, ultra thick, fill=red!50!white, rounded corners]
            \tikzstyle{cold} = [color=blue, ultra thick, fill=blue!50!white, rounded corners]
            \tikzstyle{engine} = [ultra thick, rounded corners]
            \draw[hot] (0, 4) rectangle (5, 5);
            \draw[cold] (0, 0) rectangle (5, 1);
            \draw[engine] (1, 2) rectangle (2, 3);
            \draw[engine, ->] (1, 2.3) -- (1, 2.5);
            \node at (1.5, 2.5) {\(E\)};
            \node[cold] at (2.5, 0.5) {Cold Body};
            \node[hot] at (2.5, 4.5) {Hot Body};
            \draw[engine, ->] (1.5, 4) -- (1.5, 3);
            \draw[engine, ->] (2, 2.5) -- (3, 2.5);
            \node[right] at (1.5, 3.5) {\(Q_1\)};
            \node[below] at (2.5, 2.5) {\tiny\(W = Q_1\)};
            
            \draw[engine] (3, 2) rectangle (4, 3);
            \draw[engine, ->] (4, 2.3) -- (4, 2.5);
            \draw[engine, ->] (3.5, 3) -- (3.5, 4);
            \draw[engine, ->] (3.5, 1) -- (3.5, 2);
            \node[right] at (3.5, 3.5) {\(Q_1 + Q_2\)};
            \node[right] at (3.5, 1.5) {\(Q_2\)};
            \node at (3.5, 2.5) {\(R\)};
            
            \begin{scope}[xshift=6cm]
                \draw[hot] (0, 4) rectangle (5, 5);
                \draw[cold] (0, 0) rectangle (5, 1);
                \draw[engine] (1, 2) rectangle (4, 3);
                \draw[engine, ->] (1, 2.3) -- (1, 2.5);
                \node at (2.5, 2.5) {\(E + R\)};
                \node[cold] at (2.5, 0.5) {Cold Body};
                \node[hot] at (2.5, 4.5) {Hot Body};
                \draw[engine, ->] (2.5, 3) -- (2.5, 4);
                \draw[engine, ->] (2.5, 1) -- (2.5, 2);
                \node[right] at (2.5, 3.5) {\(Q_2\)};
                \node[right] at (2.5, 1.5) {\(Q_2\)};
            \end{scope}
        \end{tikzpicture}
        \caption{A composite system which violates Clausius.}
        \label{fig:clausius implies kelvin-planck}
    \end{figure}
    We can show that Kelvin--Planck implies Clausius in a similar way.
    
    \subsection{Carnot Engine Again}
    Carnot's theorem is
    \paragraph{Carnot's Theorem:} No engine operating between two reservoirs can be more efficient than a Carnot engine operating between the same two reservoirs.
    
    This is fairly easy to prove.
    Suppose that we had an engine \(E'\) with efficiency \(\eta'\) and a Carnot engine \(C\) with efficiency \(\eta_C\).
    Now suppose that \(E'\) violates Carnot's theorem so that when operating between the same reservoirs \(\eta' > \eta_C\).
    We can tune both engines such that they do equal amounts of work, \(W = W'\).
    The efficiency of \(E'\) is
    \[\eta' = \frac{W'}{Q_1'} = \frac{W}{Q_1'}.\]
    The efficiency of \(C\) is
    \[\eta_C = \frac{W}{Q_1}.\]
    For \(\eta' > \eta_C\) to hold we require that
    \[Q_1 > Q_1'.\]
    Since a Carnot engine only exchanges heat along isotherms it is reversible with the same efficiency.
    Thus if we instead use \(C\) as a refrigerator it will have efficiency \(\eta_C\) still.
    If we use \(E'\) to drive \(C\) as a refrigerator then the process acts to move heat \(Q_1 - Q_1'\) from cold to hot without any external work being done.
    Since \(Q_1 > Q_1'\) this quantity is positive so this composite system, \(E' + C\), violates Clausius' statement of the second law.
    This means that our assumption that \(E'\) is more efficient than a Carnot engine is false.
    
    It is however possible that \(\eta' = \eta_C\), this would mean that there is no heat flow in the composite system.
    We conclude that
    \[\eta' \le \eta_C.\]
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{heat-engines-prove-carnot}
        \begin{tikzpicture}
            \tikzstyle{hot} = [color=red, ultra thick, fill=red!50!white, rounded corners]
            \tikzstyle{cold} = [color=blue, ultra thick, fill=blue!50!white, rounded corners]
            \tikzstyle{engine} = [ultra thick, rounded corners]
            \draw[hot] (0, 4) rectangle (5, 5);
            \draw[cold] (0, 0) rectangle (5, 1);
            \draw[engine] (1, 2) rectangle (2, 3);
            \draw[engine, ->] (1, 2.3) -- (1, 2.5);
            \node at (1.5, 2.5) {\(E'\)};
            \node[cold] at (2.5, 0.5) {Cold Reservoir};
            \node[hot] at (2.5, 4.5) {Hot Reservoir};
            \draw[engine, ->] (1.5, 4) -- (1.5, 3);
            \draw[engine, ->] (1.5, 2) -- (1.5, 1);
            \draw[engine, ->] (2, 2.5) -- (2.5, 2.5);
            \node[right] at (1.5, 3.5) {\(Q_1'\)};
            \node[below] at (2.5, 2.45) {\(W'\)};
            \node[right] at (1.5, 1.5) {\(Q_2'\)};
            
            \draw[engine] (3.5, 2.5) circle[radius=0.5cm];
            \draw[engine, ->] (3, 2.49) -- (3, 2.5);
            \draw[engine, <-] (3.5, 3) -- (3.5, 4);
            \draw[engine, <-] (3.5, 1) -- (3.5, 2);
            \draw[engine, ->] (4, 2.5) -- (4.5, 2.5);
            \node[right] at (3.5, 3.5) {\(Q_1\)};
            \node[right] at (3.5, 1.5) {\scriptsize\(Q_2 = Q_1 - W\)};
            \node[below] at (4.5, 2.45) {\(W\)};
            \node at (3.5, 2.5) {\(C\)};
            
            \begin{scope}[xshift=6cm]
                \draw[hot] (0, 4) rectangle (5, 5);
                \draw[cold] (0, 0) rectangle (5, 1);
                \draw[engine] (1, 2) rectangle (2, 3);
                \draw[engine, ->] (1, 2.3) -- (1, 2.5);
                \node at (1.5, 2.5) {\(E'\)};
                \node[cold] at (2.5, 0.5) {Cold Reservoir};
                \node[hot] at (2.5, 4.5) {Hot Reservoir};
                \draw[engine, ->] (1.5, 4) -- (1.5, 3);
                \draw[engine, ->] (1.5, 2) -- (1.5, 1);
                \draw[engine, ->] (2, 2.5) -- (3, 2.5);
                \node[right] at (1.5, 3.5) {\(Q_1'\)};
                \node[below] at (2.5, 2.45) {\(W\)};
                \node[right] at (1.5, 1.5) {\scriptsize\(Q_2' = Q_1' - W\)};
                
                \draw[engine] (3.5, 2.5) circle[radius=0.5cm];
                \draw[engine, ->] (4, 2.49) -- (4, 2.5);
                \draw[engine, ->] (3.5, 3) -- (3.5, 4);
                \draw[engine, ->] (3.5, 1) -- (3.5, 2);
                \node[right] at (3.5, 3.5) {\(Q_1\)};
                \node[right] at (3.5, 1.5) {\scriptsize\(Q_2 = Q_1 - W\)};
                \node at (3.5, 2.5) {\(C\)};
                \draw[dashed, rounded corners] (0.75, 1.75) rectangle (4.25, 3.25);
                \node[right] at (4.25, 2.5) {\(E' + C\)};
            \end{scope}
        \end{tikzpicture}
        \caption{Systems used to prove Carnot's theorem}
    \end{figure}
    
    Suppose that we have two Carnot engines, \(C_a\) and \(C_b\) with efficiencies \(\eta_{C_a}\) and \(\eta_{C_b}\) respectively.
    Since we can use the first to drive the second or vice versa then applying the above proof to these two engines we have \(\eta_{C_a}\le \eta_{c_b}\) and \(\eta_{C_b}\le\eta_{C_a}\) so \(\eta_{C_a} = \eta_{C_b}\).
    This means that all Carnot engines operating between the same two reservoirs have exactly the same efficiency independent of the way that the two engines operate, as long as they follow the Carnot cycle.
    This is a direct Corollary of Carnot's theorem.
    
    Another corollary of Carnot's theorem is a temperature scale.
    Since the efficiency of any reversible heat engine operating between the same two reservoirs is equal the efficiency,
    \[\eta_R = 1 - \frac{Q_2}{Q_1},\]
    can only depend on the temperatures of the reservoirs as these are the only variables that need to be the same for all engines.
    Therefore there must be a universal function, \(f\colon\reals^2 \to \reals\) such that
    \[\frac{Q_2}{Q_1} = f(T_1, T_2)\]
    for all heat engines.
    Here \(T_1\) and \(T_2\) are the temperatures of the hot and cold reservoirs respectively.
    We can consider the form of \(f\) by considering another composite system where one engine runs on the wast heat of another.
    This system is shown in figure~\ref{fig:heat engine running on the heat from a heat engine}.
    Tuning the engines so that \(Q_2 = Q_2'\) we can consider this composite system as a single engine since no net heat enters the reservoir at \(T_3\) so we don't need a reservoir at all.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{recycling-heat-engine}
        \begin{tikzpicture}
            \tikzstyle{hot} = [color=red, ultra thick, fill=red!50!white, rounded corners]
            \tikzstyle{cold} = [color=blue, ultra thick, fill=blue!50!white, rounded corners]
            \tikzstyle{medium temp} = [color=blue!50!red, ultra thick, fill=blue!50!red!50!white, rounded corners]
            \tikzstyle{engine} = [ultra thick, rounded corners]
            \draw[hot] (0, 4) rectangle (3, 5);
            \draw[medium temp] (0, 0) rectangle (3, 1);
            \draw[cold] (0, -4) rectangle (3, -3);
            \draw[engine] (1, 2) rectangle (2, 3);
            \draw[engine, ->] (1, 2.3) -- (1, 2.5);
            \node at (1.5, 2.5) {\(E_1\)};
            \node[medium temp] at (1.5, 0.5) {\(T_2\)};
            \node[hot] at (1.5, 4.5) {\(T_1\)};
            \node[cold] at (1.5, -3.5) {\(T_3\)};
            \draw[engine, ->] (1.5, 4) -- (1.5, 3);
            \draw[engine, ->] (1.5, 2) -- (1.5, 1);
            \draw[engine, ->] (2, 2.5) -- (3, 2.5);
            \node[right] at (1.5, 3.5) {\(Q_1\)};
            \node[right] at (1.5, 1.5) {\(Q_2\)};
            \node[right] at (3, 2.5) {\(W_1\)};
            \node[color=white, left] at (0, 0.5) {\(W_1 + W_2\)};
            \begin{scope}[yshift=-4cm]
                \draw[engine] (1, 2) rectangle (2, 3);
                \draw[engine, ->] (1, 2.3) -- (1, 2.5);
                \node at (1.5, 2.5) {\(E_2\)};
                \draw[engine, ->] (1.5, 4) -- (1.5, 3);
                \draw[engine, ->] (1.5, 2) -- (1.5, 1);
                \draw[engine, ->] (2, 2.5) -- (3, 2.5);
                \node[right] at (1.5, 3.5) {\(Q_2'\)};
                \node[right] at (1.5, 1.5) {\(Q_3\)};
                \node[right] at (3, 2.5) {\(W_2\)};
            \end{scope}
            \begin{scope}[xshift=5cm, yshift=-2cm]
                \draw[hot] (0, 4) rectangle (3, 5);
                \draw[cold] (0, 0) rectangle (3, 1);
                \draw[engine] (1, 2) rectangle (2, 3);
                \draw[engine, ->] (1, 2.3) -- (1, 2.5);
                \node at (1.5, 2.5) {\tiny\(E_1 + E_2\)};
                \node[hot] at (1.5, 4.5) {\(T_1\)};
                \node[cold] at (1.5, 0.5) {\(T_3\)};
                \draw[engine, ->] (1.5, 4) -- (1.5, 3);
                \draw[engine, ->] (1.5, 2) -- (1.5, 1);
                \draw[engine, ->] (2, 2.5) -- (3, 2.5);
                \node[right] at (1.5, 3.5) {\(Q_1\)};
                \node[right] at (1.5, 1.5) {\(Q_3\)};
                \node[right] at (3, 2.5) {\(W_1 + W_2\)};
            \end{scope}
        \end{tikzpicture}
        \caption{One heat engine running on the waste heat of another.}
        \label{fig:heat engine running on the heat from a heat engine}
    \end{figure}
    We know that
    \[f(T_1, T_2) = \frac{Q_2}{Q_1},\qquad\text{and}\qquad f(T_2, T_3) = \frac{Q_3}{Q_2}.\]
    For the composite engine
    \[f(T_1, T_3) = \frac{Q_3}{Q_1}.\]
    From this we se that
    \[f(T_1, T_3) = \frac{Q_3}{Q_1} = \frac{Q_3}{Q_2}\frac{Q_2}{Q_1} = f(T_2, T_3)f(T_1, T_1).\]
    From this we see that \(f\) must factorise as
    \[f(T_1, T_2) = \frac{\vartheta(T_1)}{\vartheta(T_2)},\]
    where \(\vartheta\) is a universal function of temperature for whatever temperature scale we use to measure \(T\).
    We can then use \(\vartheta\) as a temperature scale.
    We will show in a later section that \(\vartheta(T) = T_\text{IG}\) where \(T_\text{IG}\) is the temperature as defined by the ideal gas law.
    
    \section{More Heat Engines}
    \subsection{Thermodynamic Temperature Scale}
    In the last section we defined a temperature scale, \(\vartheta\), based on the efficiency of a Carnot engine.
    We will show that this temperature scale is the same as the temperature scale, \(T\), that appears in the ideal gas law.
    To do this we consider the Carnot cycle with an ideal gas as our working substance.
    
    The first stage of the Carnot cycle is isothermal expansion from a volume \(V_a\) to \(V_b\).
    For an ideal gas the internal energy is a function of the temperature, \(U = U(T)\).
    Since this is isothermal expansion \(T\) is constant so \(U\) must also be constant.
    The first law then gives us
    \[0 = \dd{U} = \ddbar{q} - P\dd{V_T} \implies \ddbar{q} = P\dd{V_T}.\]
    Integrating this we get that the heat into the ideal gas, \(q_{ab}\), is
    \[q_{ab} = \int_{q_a}^{q_b} \ddbar{q} = \int_{V_a}^{V_b} P\dd{V_T} = nRT_1 \int_{V_a}^{V_b} \frac{\dd{V_T}}{V} = nRT_1 \ln\left(\frac{V_b}{V_a}\right).\]
    This is positive as the process is expansion so \(V_b > V_a\) meaning the argument of the logarithm is greater than one.
    
    The third stage in the Carnot cycle is isothermal compression from \(V_c\) to \(V_d\).
    Applying the same logic as for expansion the heat absorbed by the system is
    \[q_{cd} = nRT_2\ln\left(\frac{V_d}{V_c}\right).\]
    Since this is compression \(V_d < V_c\) so the argument of the logarithm is less than one so \(q_{cd}\) is negative.
    
    The other two stages are adiabatic so there is no heat exchange.
    If we now consider the heat exchange with the sign conventions that we use with engines then we can identify that \(q_{ab} = Q_1\) and \(-q_{cd} = Q_2\).
    The ratio of these two heats is
    \begin{equation}\label{eqn:Q_2/Q_1 ideal gas Carnot}
        \frac{Q_2}{Q_1} = \frac{-nRT_2\ln(V_d/V_c)}{nRT_1\ln(V_b/V_a)} = \frac{T_2}{T_1}\frac{\ln(V_c/V_d)}{\ln(V_b/V_a)}.
    \end{equation}
    We now look at the adiabatic expansion from \(V_b\) to \(V_c\) and adiabatic compression from \(V_d\) to \(V_a\).
    Since these processes are adiabatic we know that \(TV^{\gamma - 1}\) must be constant.
    Hence
    \[T_1V_b^{\gamma-1} = T_2V_c^{\gamma-1}, \qquad\text{and}\qquad T_1V_a^{\gamma-1} = T_2V_d^{\gamma-1}.\]
    Dividing the first of these by the second we can show that
    \[\frac{V_b}{V_c} = \frac{V_c}{V_d}.\]
    This means that the values of the logarithms in equation~\ref{eqn:Q_2/Q_1 ideal gas Carnot} are the same so we have
    \[\frac{Q_2}{Q_1} = \frac{T_2}{T_1}.\]
    Therefore we can define \(\vartheta(T) = T\) and we will have that the ideal gas temperature and the thermodynamic temperature are the same.
    This allows us to rewrite the efficiency of the Carnot heat engine:
    \[\eta_C = 1 - \frac{T_2}{T_1}.\]
    
    \subsection{Efficiency of Carnot Engines}
    There are three ways to use the Carnot cycle, and more generally any heat engine.
    The first is as a heat engine where heat in from the hotter reservoir is turned into work.
    The efficiency of this is
    \[\eta = \frac{W}{Q_1} = \frac{Q_1 + Q_2}{Q_1} = 1 - \frac{Q_2}{Q_1}.\]
    For the Carnot cycle this becomes
    \[\eta_C = 1 - \frac{T_2}{T_1}.\]
    The second way we can use one of these cycles is as a refrigerator.
    In this case work in is used to remove heat from the cold reservoir.
    The efficiency of this, sometimes called the coefficient of performance, is
    \[\eta^R = \frac{Q_2}{W} = \frac{Q_2}{Q_1 - Q_2}.\]
    For a Carnot cycle this becomes
    \[\eta_C^R = \frac{T_2}{T_1 - T_2}.\]
    The third way that we can use the cycle is one we haven't discussed yet.
    We can use the cycle as a heat pump where work is done to add heat to the hot reservoir.
    This is subtly different from removing heat from the cold reservoir when it comes to efficiency since the goal is different.
    The efficiency is
    \[\eta^{HP} = \frac{Q_1}{W} = \frac{Q_1}{Q_1 - Q_2}.\]
    For a Carnot cycle this becomes
    \[\eta_C^{HP} = \frac{T_1}{T_1 - T_2}.\]
    
    One case that is worth considering is when \(Q_1 = Q_2\).
    Here the efficiencies of the refrigerator and heat pump become infinite.
    This is because the heat flow becomes spontaneous since \(Q_1 = Q_2\) means that \(T_1 = T_2\).
    
    \subsection{Steam Engines}
    A steam engine makes use of a phase change to create rapid expansion.
    Work can be extracted from this expansion with either a piston or turbine.
    It is also often possible to skip the compression stages by simply dumping the steam and adding new water.
    The steam engine operates at temperatures and pressures where there can be liquid and vapour at the same time.
    
    \subsection{Otto Cycle}
    The Otto cycle is a simplified version of a two stroke petrol engine.
    The simplification comes by assuming that instead of internal combustion there is an external heat source.
    The cycle is as follows:
    \begin{itemize}
        \item Reversible adiabatic compression from \((P_a, V_1, T_a)\) to \((P_b, V_2, T_b)\).
        \item Isochoric (constant volume) heat exchange from \((P_b, V_2, T_b)\) to \((P_c, V_2, T_c)\).
        Heat, \(Q_1\), is absorbed in this step.
        \item Reversible adiabatic expansion from \((P_c, V_2, T_c)\) to \((P_d, V_1, T_d)\).
        Heat, \(Q_2\), is extracted from the system
        \item Isochoric heat exchange from \((P_d, V_1, T_2)\) to \((P_a, V_1, T_a)\).
    \end{itemize}
    Isochoric heat exchange is much faster than isothermal heat exchange meaning that this engine can produce much more power than a Carnot engine.
    
    In the first stage a piston moves in compressing the gas.
    Since this is adiabatic \(\ddbar{Q} = 0\) so the first law gives \(\dd{U} = \ddbar{W}\).
    Also
    \[T_aV_1^{\gamma-1} = T_bV_2^{\gamma-1}.\]
    The work done is
    \[\ddbar{W} = C_V(T_a - T_b).\]
    
    In the second stage heat is added at constant volume.
    This means \(\ddbar{W} = 0\).
    The heat therefore is
    \[Q_1 = \dd{U} = C_V(T_c - T_b).\]
    
    In the third stage the piston moves out again and we have
    \[T_dV_1^{\gamma-1} = T_cV_2^{\gamma-1}\]
    and
    \[\ddbar{W} = C_V(T_c - T_d).\]
    
    In the final stage heat is given out and the working substance cools to its original temperature.
    In reality the working substance is just given out as exhaust and then replaced.
    The heat given out is
    \[Q_2 = C_V(T_d - T_a).\]
    
    The efficiency is
    \[\eta = 1 - \frac{Q_2}{Q_1} = 1 - \frac{T_d - T_a}{T_c - T_b} = 1 - \left(\frac{V_2}{V_1}\right)^{\gamma-1} = 1 - \frac{1}{r_c^{\gamma-1}}\]
    where \(r_c = V_1/V_2\) is the ratio of the volume of the cylinder with the piston out to the volume of the cylinder with the piston in.
    That is the compression ratio.
    Typically \(r_c \approx 5\) and \(\eta\approx\SI{50}{\percent}\).
    This ignores other inefficiencies that mean that in reality the efficiency is much less than \(\SI{50}{\percent}\).
    
    A four stroke engine has exhaust and intake stages as well.
    Note that the efficiency depends on \(\gamma\) which is a property of the gas being used.
    For a diatomic gas \(\gamma\) is greater than for a monatomic gas.
    This means that a diatomic gas based Otto cycle is less efficient than for a monatomic gas based Otto cycle.
    This is because for a diatomic gas some of the heat is used to increase the rotational energy of the molecules which cannot be extracted by a piston.
    For a monatomic gas this doesn't happen and all of the heat goes to increase the kinetic energy of the gas which we can extract using a piston.
    
    \subsection{A Real Engine}
    We can model a real engine as a Carnot engine with two more heat exchange processes.
    We include a term, \(Q_L\), that gives the heat lost to surroundings before it can be used for work.
    We also include \(Q_F\) which accounts for heat loss due to friction after work has been produced.
    This is shown in figure~\ref{fig:real engine}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{carnot-engine-with-inefficiencies}
        \begin{tikzpicture}
            \tikzstyle{hot} = [color=red, ultra thick, fill=red!50!white, rounded corners]
            \tikzstyle{cold} = [color=blue, ultra thick, fill=blue!50!white, rounded corners]
            \tikzstyle{engine} = [ultra thick, rounded corners]
            \draw[hot] (0, 4) rectangle (4, 5);
            \draw[cold] (0, 0) rectangle (4, 1);
            \draw[engine] (3, 2.5) circle[radius=0.5cm];
            \draw[engine, ->] (2.5, 2.49) -- (2.5, 2.5);
            \node at (3, 2.5) {\(C\)};
            \node[cold] at (2, 0.5) {Cold Reservoir};
            \node[hot] at (2, 4.5) {Hot Reservoir};
            \draw[engine, ->] (3, 4) -- (3, 3);
            \draw[engine, ->] (3, 2) -- (3, 1);
            \draw[engine, ->] (3.5, 2.5) -- (4, 2.5);
            \node[right] at (3, 3.5) {\(Q_1\)};
            \node[right] at (3, 1.5) {\(Q_2\)};
            \node[right] at (4, 2.5) {\(W - Q_F\)};
            \draw[engine, ->] (1, 4) -- (1, 1);
            \node[left] at (1, 2.5) {\(Q_L\)};
            \node[left] at (0, 2.5) {\hphantom{\(W - Q_f\)}};
        \end{tikzpicture}
        \caption{A real engine as a Carnot engine with inefficiencies.}
        \label{fig:real engine}
    \end{figure}
    The Kelvin-Planck statement of the second law means that \(Q_F > 0\) and the Clausius statement of the second law means that \(Q_L > 0\).
    The efficiency is
    \[\eta = \frac{W - Q_F}{Q_1 + Q_L} < \frac{W}{Q_1}\]
    so we see that a real engine must always be less efficient than an ideal engine.
    
    \section{Entropy}
    \subsection{The Clausius Inequality}
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{clausius-inequality-demo-heat-engine}
        \begin{tikzpicture}
            \tikzstyle{hot} = [color=red, ultra thick, fill=red!50!white, rounded corners]
            \tikzstyle{cold} = [color=blue, ultra thick, fill=blue!50!white, rounded corners]
            \tikzstyle{engine} = [ultra thick, rounded corners]
            \draw[engine] (0.5, 0.5) circle[radius=0.5cm];
            \draw[engine] (2.5, 0) rectangle (4.5, 1);
            \draw[engine] (6.5, 0.5) circle[radius=0.5cm];
            \node at (0.5, 0.5) {\(A\)};
            \node at (3.5, 0.5) {\(E\)};
            \node at (6.5, 0.5) {\(B\)};
            \draw[engine, ->] (0.146, 0.853) -- (0.143, 0.851);
            \draw[engine, ->] (2.5, 0.51) -- (2.5, 0.5);
            \draw[engine, ->] (6.146, 0.853) -- (6.143, 0.851);
            \draw[engine, ->] (0, 0.5) -- (-1, 0.5);
            \node[left] at (-1, 0.5) {\(W_A\)};
            \draw[engine, ->] (4.5, 0.5) -- (5.5, 0.5);
            \node[below] at (5.5, 0.4) {\(W\)};
            \draw[engine, ->] (7, 0.5) -- (8, 0.5);
            \node[right] at (8, 0.5) {\(W_B\)};
            \draw[cold] (-1, -2.5) rectangle (8, -1.5);
            \node[cold] at (3.5, -2) {Cold, \(T_0\)};
            \draw[hot] (-1, 2.5) rectangle (2, 3.5);
            \node[hot] at (0.5, 3) {Hot, \(T_1\)};
            \draw[hot] (5, 2.5) rectangle (8, 3.5);
            \node[hot] at (6.5, 3) {Hot, \(T_2\)};
            \draw[engine, ->] (0.5, 1) -- (0.5, 2.5);
            \draw[engine, ->] (6.5, 1) -- (6.5, 2.5);
            \node[left] at (0.5, 1.75) {\(Q_{1A}\)};
            \node[right] at (6.5, 1.75) {\(Q_{2B}\)};
            \draw[engine, ->] (3.5, 0) -- (3.5, -1.5);
            \node[right] at (3.5, -0.75) {\(Q_0\)};
            \draw[engine, ->] (0.5, -1.5) -- (0.5, 0);
            \node[left] at (0.5, -0.75) {\(Q_{0A}\)};
            \draw[engine, ->] (6.5, -1.5) -- (6.5, 0);
            \node[right] at (6.5, -0.75) {\(Q_{0B}\)};
            \draw[engine, ->] (1, 2.5) -- (3, 1);
            \draw[engine, ->] (6, 2.5) -- (4, 1);
            \node[above right] at (2, 1.75) {\(Q_1\)};
            \node[above left] at (5, 1.75) {\(Q_2\)};
        \end{tikzpicture}
        \caption{The setup for demonstrating Clausius' inequality}
        \label{fig:clausius inequality}
    \end{figure}
    Consider the setup in figure~\ref{fig:clausius inequality}.
    \(A\) and \(B\) are Carnot refrigerators ans \(E\) is an engine.
    There are two hot heat reservoirs at temperatures \(T_1\) and \(T_2\) and a cold heat reservoir at temperature \(T_0\).
    We adjust the engines so that there is no net heat flow to the hot heat reservoirs, that is
    \[Q_{1A} = Q_1,\qquad\text{and}\qquad Q_{2B} = Q_2.\]
    The net heat flow out of the cold reservoir is
    \[Q_{0A} + Q_{0B} - Q_0.\]
    We expect this to be negative (i.e. heat is entering the cold reservoir).
    
    We consider as a system both fridges and the engine.
    The total work done by the system is
    \[W - (W_A + W_B).\]
    We adjust the engine so that there is not heat flow into or out of the heat reservoirs.
    Hence the internal energy is a constant so \(Q_T = W_T\) where \(Q_T\) is the total heat absorbed by the system and \(W_T\) is the total work done by the system (following the engine sign convention).
    Hence we have
    \[Q_{0A} + Q_{0B} - Q_0 = W - (W_A + W_B).\]
    This will violate the Kelvin-Planck statement of the second law unless
    \[W \le W_A + W_B,\qquad\text{and}\qquad Q_{0A} + Q_{0B} - Q_0 \le 0.\]
    Using the efficiency of the Carnot refrigerators we know that
    \[\frac{Q_{1A}}{Q_{0A}} = \frac{T_1}{T_0} \implies Q_{0A} = Q_1\frac{T_0}{T_1},\]
    and similarly
    \[Q_{0B} = Q_2\frac{T_0}{T_2}.\]
    The heat leaving the cold reservoir is
    \[Q_{0A} + Q_{0B} - Q_0 = \left(\frac{Q_1}{T_1} - \frac{Q_2}{T_2}\right)T_0 - Q_0 \le 0.\]
    Hence
    \[\frac{Q_1}{T_1} + \frac{Q_2}{T_2} - \frac{Q_0}{T_0} \le 0.\]
    We now consider heat inputs, \(q_i\), into the working substance of the engine.
    The engine absorbs \(q_1 = Q_1\) and \(q_2 = Q_2\) from the hot reservoirs and returns heat \(Q_0\) to the cold reservoir so \(q_0 = -Q_0\).
    The inequality above then becomes
    \[\frac{q_1}{T_1} + \frac{q_2}{T_2} + \frac{q_0}{T_0} \le 0.\]
    This generalises to any number of engines and refrigerators.
    \[\sum_i \frac{q_i}{T_i} \le 0.\]
    Be careful, \(T_i\) are the temperatures of the reservoirs with which heat is exchanged, not the temperatures of the system.
    
    In the limit where each engine absorbs only an infinitesimal amount of heat, \(\ddbar{q}\), from a large number of reservoirs all at temperature \(T\) this becomes
    \[\int \frac{\ddbar{q}}{T} \le 0.\]
    This is the \define{Clausius inequality}.
    Be careful, \(T\) is the temperature of the surroundings, not the system.
    
    \subsection{Entropy}
    If we apply the Clausius inequality to a reversible system something interesting happens.
    For a reversible process the system and surroundings must be in equilibrium so the temperature of the system, \(T_\sys\), is the same as the temperature of the surroundings, \(T_\surr\).
    Performing the process forward the Clausius inequality must hold so
    \[\int\frac{\ddbar{q_R}}{T} \le 0.\]
    However since this is a reversible process we can also perform it in to opposite direction.
    If we do this then the heat absorbed in each infinitesimal stage is \(-\ddbar{q_R}\) but \(T\) is the same so the Clausius inequality becomes
    \[-\int\frac{\ddbar{q_R}}{T} \le 0.\]
    The only way that both of these can hold is if
    \[\int\frac{\ddbar{q_R}}{T} = 0.\]
    
    Consider a reversible cycle from state \(i\) to state \(f\) and back to \(i\) along path 1 from \(i\) to \(f\) and path 2 from \(f\) to \(i\).
    We can split the integral at \(i\) and \(f\) and we have
    \[\oint \frac{\ddbar{q_R}}{T} = \int_{i~\text{path 1}}^{f} \frac{\ddbar{q_R}}{T} + \int_{f~\text{path 2}}^{i} \frac{\ddbar{q_R}}{T} = 0\]
    Hence
    \[\int_{i~\text{path 1}}^{f} \frac{\ddbar{q_R}}{T} = -\int_{f~\text{path 2}}^{i} \frac{\ddbar{q_R}}{T} = \int_{i~\text{path 2}}^{f} \frac{\ddbar{q_R}}{T}.\]
    This shows that this integral is independent of the path.
    This means that \(\ddbar{q_R}/T\) is an exact differential even though \(\ddbar{q_R}\) is not.
    There must, therefore, be a state function, \(S\), such that
    \[\Delta S = S_f - S_i = \int_i^f \dd{S} = \int_i^f \frac{\ddbar{q_R}}{T}.\]
    We call this state function entropy.
    Notice that this equation only defines a change in entropy.
    It doesn't give us a point where \(S = 0\) and it doesn't tell us what physical property entropy is a measure of.
    
    If path 1 were instead irreversible then the Clausius inequality is
    \[\oint \frac{\ddbar{q}}{T} = \int_{i~\text{path 1}}^{f} \frac{\ddbar{q}}{T} + \int_{f~\text{path 2}}^{i} \frac{\ddbar{q_R}}{T} \le 0.\]
    This implies that
    \[\int_{i~\text{path 1}}^{f} \frac{\ddbar{q}}{T} \le -\int_{f~\text{path 2}}^{i} \frac{\ddbar{q_R}}{T} = \int_{i~\text{path 2}}^{f} \frac{\ddbar{q_R}}{T} = \Delta S.\]
    Note that even though this is an irreversible process we can still do the integral as \(T\) is the temperature of the surroundings and \(\ddbar{q}\) is the heat exchanged with the surroundings and these are defined quantities even though the temperature of the system may not be defined.
    What this means is that the change in entropy for an irreversible process is always less than the change in entropy for an equivalent reversible process, that is
    \[\int_i^f\frac{\ddbar{q}}{T} \le \int_i^f \dd{S},\]
    with equality if and only if the integral on the left is performed over some reversible process.
    
    \begin{example}\label{exa:heating water}
        Water at \SI{20}{\degreeCelsius} is placed in thermal contact with a heat reservoir at \SI{100}{\degreeCelsius}.
        The water is heated irreversibly at constant pressure to an equilibrium state at \SI{100}{\degreeCelsius}.
        What is the change in entropy of the water and of the surroundings?
        
        We consider the water as the system.
        The process is irreversible so we work with an alternative, reversible, process.
        The surroundings in this reversible process are a series of heat reservoirs each at a slightly higher temperature.
        This heats the water slowly in a way that is reversible.
        At each intermediate step in this process there is a reversible transfer of heat, \(\dd{Q} = C_P\dd{T}\), from the surroundings to the system.
        The change in entropy of this intermediate step is then
        \[\dd{S} = \frac{\dd{Q}}{T} = \frac{C_P\dd{T}}{T}.\]
        The total entropy change is then
        \[\Delta S^\sys = C_p\int_{T_i}^{T_f}\frac{\dd{T}}{T} = C_P\ln\left(\frac{T_f}{T_i}\right) \approx 0.241C_P.\]
        
        For the change in entropy of the surroundings we again consider an alternative, reversible, process.
        This time the process is a series of heat reservoirs, each slightly cooler than the last, which represents the heat transferred to the system.
        The total heat transferred is
        \[-C_P(T_f - T_i)\]
        and the change in entropy is
        \[\Delta S^\surr = \frac{C_P(T_f - T_i)}{T_f} \approx -0.21448C_P.\]
        The total entropy change is then
        \[\Delta S^\tot = \Delta S^\sys + \Delta S^\surr = 0.02693C_P > 0.\]
    \end{example}
    
    Consider an adiabatic, thermally isolated system.
    Then \(T\) is a constant and \(\ddbar{q} = 0\).
    Thus \(\dd{S} \ge 0\) where equality holds only for a reversible process.
    This is a statement that entropy never decreases.
    
    We now have four equivalent statements of the second law of thermodynamics:
    \begin{itemize}
        \item Heat can't flow from a cooler reservoir to a hotter reservoir.
        \item Heat cannot be converted to work with \SI{100}{\percent} efficiency.
        \item Reversible cycles are the most efficient engines possible.
        \item Entropy cannot decrease.
    \end{itemize}
    We can think of heat as the lowest form of energy.
    All forms of energy tend towards heat as time progresses.
    This is a statistical phenomenon.
    There are more ways for energy to be heat, that is the motion of particles, than any other form of energy.
    Therefore heat is the most likely form of energy.
    
    The second law is the only fundamental equation in physics which violates \gls{cpt} symmetry.
    We use this to define a `forward' direction in time.
    
    \section{The Central Equation of Thermodynamics}
    If we combine the first law,
    \[\dd{U} = \ddbar{Q} + \ddbar{W},\]
    and the definition of entropy,
    \[\dd{S} = \frac{\ddbar{Q}}{T},\]
    along with the mechanical work done, \(-P\dd{V}\), compressing a fluid, we get the \define{central equation of thermodynamics}:
    \[\dd{U} = T\dd{S} - P\dd{V}.\]
    This is useful as it involves only state variables.
    This means that we can integrate it along any path.
    For an irreversible process we can find an equivalent reversible process along which to integrate.
    This also gives us \(T\dd{S} = \dd{U} + P\dd{V}\) which is an easier way to measure \(\dd{S}\) than the original definition of \(\ddbar{Q}/T\).
    
    An extensive property of a material is one that depends on how much of the material there is, such as volume or internal energy.
    Contrastingly an intensive property of a material is one that doesn't depend on how much of the material there is, such as density or temperature.
    Since the central equation gives us the entropy in terms of the extensive properties \(U\) and \(V\) the entropy must also be an extensive property.
    
    This equation can be made more general if we consider different kinds of work.
    For example, if a charge, \(Z\), is moved by an emf \(\emf\), then the work done is \(\emf\dd{Z}\).
    The central equation is then
    \[\dd{U} = T\dd{S} - P\dd{V} + \emf\dd{Z}.\]
    If instead we consider a rubber band of length \(L\) being stretched by a force, \(F\), then the work done in stretching the rubber band dominates and the central equation becomes
    \[\dd{U} = T\dd{S} + F\dd{L}.\]
    
    \subsection{Entropy of an Ideal Gas}
    We showed in equation~\ref{eqn:dU = C_VdT ideal gas} that for an ideal gas \(\dd{U} = C_V\dd{T}\).
    Using this, and the ideal gas equation, the central equation becomes
    \[\dd{S} = C_V\frac{\dd{T}}{T} + nRT\frac{\dd{V}}{V} = C_V\frac{\dd{T}}{T} + N\boltzmann\frac{\dd{V}}{V}.\]
    If we look at values per mole this becomes
    \[\dd{s} = c_V\frac{\dd{T}}{T} + R\frac{\dd{v}}{v}\]
    where lowercase symbols have the same meaning as their uppercase counter parts but per mole.
    Integrating this gives
    \[s = c_V\ln T + R\ln v + s_0\]
    where \(s_0\) is some constant of integration.
    
    This breaks down as \(T \to 0\) as \(s\to-\infty\).
    This is more than just the fact that the ideal gas equation makes several assumptions and also that kinetic theory breaks down at low temperatures.
    Fundamentally the reason this doesn't work is that, as we will see later, entropy is about counting things and for that things need to be discretised.
    
    \subsection{Entropy change in Joule Expansion}
    In free expansion of an ideal gas there is no work done or heat exchanged.
    Hence \(\dd{U} = \ddbar{Q} + \ddbar{W} = 0\).
    This means the internal energy is constant.
    Since the internal energy of an ideal gas is a function only of its temperature this means that the temperature must be constant and so \(\dd{T} = 0\).
    The volume however does change.
    Therefore there is a change in entropy:
    \[\dd{s} = c_V\frac{\dd{T}}{T} + R\frac{\dd{v}}{v} = R\frac{\dd{v}}{v}.\]
    Integrating this from initial volume \(v_i\) to final volume \(v_f\) we get
    \[\Delta s = s_f - s_i = \int_{s_i}^{s_f}\dd{s} = R\int_{v_i}^{v_f}\frac{\dd{v}}{v} = R\ln\left(\frac{v_f}{v_i}\right)\]
    \[\implies v_f = v_ie^{(s_f - s_i)/R}.\]
    
    \begin{example}
        Consider the same setup as example~\ref{exa:heating water}.
        Water is heated from \SI{20}{\degreeCelsius} to \SI{100}{\degreeCelsius} at a constant pressure in an irreversible way.
        We have already seen one method for calculating the entropy and here we will discuss two more.
        Recall that the method that we have seen is to directly calculate
        \[\Delta S = \int_i^f \frac{\dd{q_R}}{T} = \int_i^f \frac{C_p\dd{T}}{T}\]
        for some alternative reversible process.
        
        The second method we can use is to integrate the central equation:
        \[\Delta S = \int_i^f \frac{\dd{U}}{T} + \int_i^f \frac{P}{T}\dd{V}.\]
        To do this we expand \(\dd{U}\) and \(\dd{V}\) in terms of \(P\) and \(T\).
        We choose \(P\) and \(T\) as \(P\) is constant and \(T\) is the variable that we are interested in changing.
        We get
        \[\Delta S = \int_i^f \frac{1}{T}\left[\pdvconst{U}{T}{P}\dd{T} + \pdvconst{U}{P}{T}\dd{P}\right] + \int_i^f \frac{P}{T}\left[\pdvconst{V}{T}{P}\dd{T} + \pdvconst{V}{P}{T}\dd{P}\right].\]
        Since pressure is constant this reduces to
        \begin{align*}
            \Delta S &= \int_i^f \frac{1}{T} \pdvconst{U}{T}{P}\dd{T} + \int_i^f \frac{P}{T} \pdvconst{V}{T}{P}\dd{T}\\
                &= \int_i^f \frac{1}{T} \left[\pdvconst{U}{T}{P} + P\pdvconst{V}{T}{P}\right]\\
                &= \int_i^f \frac{1}{T} \left(\pdv{T}[U + PV]\right)_{P}\dd{T}\\
                &= \int_i^f \frac{1}{T} \pdvconst{H}{T}{P}\dd{T}.
        \end{align*}
        Comparing this to the result we got with the first method we see that
        \[\pdvconst{H}{T}{P} = C_P.\]
        
        The third method we can use is to use the enthalpy for a constant pressure process:
        \[H = U + PV\]
        Differentiating we get
        \[\dd{H} = \dd{U} + P\dd{V} + V\dd{P}.\]
        Substituting for \(\dd{U}\) with the central equation we get
        \[\dd{H} = T\dd{S} - P\dd{V} + P\dd{V} + V\dd{P} = T\dd{S} + V\dd{P}.\]
        At constant pressure this becomes
        \[\dd{H_P} = T\dd{S_P}.\]
        Integrating this gives
        \[\int_i^f\dd{S_P} = \int\frac{1}{T}\dd{H_P}.\]
        Writing \(H = H(T, P)\) we see that
        \[\dd{H} = \pdvconst{H}{T}{P}\dd{T} + \pdvconst{H}{P}{T}\dd{P} = C_P\dd{T} + \pdvconst{H}{P}{T}\dd{P}.\]
        At constant pressure this becomes
        \[\dd{H_P} = C_P\dd{T}.\]
        Integrating this gives
        \[\int_i^f\dd{S_P} = \int_i^f\frac{1}{T}\dd{H_P} = \int_i^f\frac{C_P}{T}\dd{T},\]
        as we found previously.
    \end{example}
    These three methods are the most common approaches to calculating the entropy in a thermodynamics problem:
    \begin{enumerate}
        \item Try to define a process, or an alternative reversible process if the process in question is irreversible.
        This is usually the longest way to solve a problem but is mathematically the simplest.
        \item Integrate the central equation with the correct work term and with careful consideration of what is held constant and what isn't.
        This is often relatively quick but mathematically trickier.
        \item Use the `correct' thermodynamic variable.
        For example at constant pressure the enthalpy often gives useful values.
        This is why chemists like enthalpy so much.
        This is fast if we know the `correct' variable to use but not much use if we don't.
    \end{enumerate}
    
    \subsection{Entropy Change in the Universe}
    Heat water from \(T\) to \(T + \Delta T\).
    The entropy change of the water and surroundings are
    \[\Delta S_\sys = C_P \ln\left(\frac{T + \Delta T}{T}\right),\]
    and
    \[\Delta S_\surr = C_P\frac{\Delta T}{T}.\]
    These were calculated in example~\ref{exa:heating water}.
    If we define \(x = \Delta T/T\) then the total entropy change is
    \[\Delta S_\tot = C_P[x - \ln(1 + x)].\]
    This is non-negative for all \(x > -1\).
    We see that for positive \(T\) the entropy must increase.
    
    \begin{example}
        Consider an emf, \(\emf\), driving a current, \(I\), through a resistor, \(R\), in an adiabatic box.
        The power output is \(I^2R\).
        No heat or mechanical work is exchanged with the surroundings so the total internal energy change in time \(\Delta t\) is
        \[\Delta U = I^2R\Delta t.\]
        Electrical work is dissipated irreversibly as heat,
        \[\Delta U = C_V\Delta T,\]
        which must all go to heating up the box since it is adiabatic.
        The change in temperature can be calculated:
        \[\Delta T = \frac{I^2R\Delta t}{C_V}.\]
        What is the change in entropy?
        
        Volume is constant so \(\dd{V} = 0\), therefore the central equation becomes
        \[\dd{S} = \frac{\dd{U}}{T} \implies \dd{S} = \frac{C_V\dd{T}}{T}.\]
        Integrating gives us
        \[\Delta S = \int_{T}^{T + \Delta T} \frac{C_V}{T}\dd{T}.\]
        Alternatively we could have defined an equivalent reversible heating process and found that
        \[\Delta S = \int_{T}^{T + \Delta T}\frac{\ddbar{q_R}}{T} = \int_{T}^{T + \Delta T}\frac{C_V\dd{T}}{T}.\]
        Either way the entropy change is
        \[\Delta S = C_V\ln\left(1 + \frac{I^2R\Delta t}{TC_V}\right).\]
        Again as \(T \to 0\) we now have \(\Delta S \to\infty\).
        So this breaks down when temperatures are near zero.
    \end{example}
    
    \subsection{Statistical Entropy}
    \textit{This section will be expanded upon in the statistical mechanics part of this course.}
    
    The total entropy can be defined as
    \[S = \boltzmann\ln\Omega\]
    where \(\Omega\) is the total number of ways it is possible to arrange the system.
    For example, consider Joule expansion where one box, \(A\), full of gas is allowed to expand freely into a second box, \(B\), which is initially under a vacuum.
    After this expansion the probability that any one atom is in box \(A\) is \(1/2\).
    The probability that all atoms are in box \(A\) is \(1/2^N\) where \(N\sim N_A\) is the number of atoms, clearly this is incredibly unlikely.
    The probability that \SI{50}{\percent} of the atoms are in box \(A\) is
    \[{N\choose N/2} \frac{1}{2^N}.\]
    In general the probability that \(r\) of the atoms are in box \(A\) is
    \[{N\choose r}\frac{1}{2^N}.\]
    The maximum value of this comes at \(r = N/2\) which means that the state where the atoms are evenly distributed between the two boxes is the most likely.
    This is also the highest entropy state since it is more likely precisely because there are more arrangements of the system that lead to this arrangement.
    
    The number of possible states with all particles
    \begin{enumerate}
        \item in A is \(\Omega(N, V, E)\)
        \item in \(A\) or \(B\) is \(\Omega(N, 2V, E)\).
    \end{enumerate}
    If all accessible microstates are equally likely then
    \[\frac{\Omega_A}{\Omega_{A+B}} = \frac{\Omega(N, V, E)}{\Omega(N, 2V, E)} = \frac{1}{2^N}.\]
    
    We can justify the form of this definition of entropy heuristically.
    Suppose that there exists a function, \(f\), such that \(f(\Omega) = S\).
    Entropy, being an extensive property, is additive.
    That is if a system is formed from two subsystems with entropies \(S_1\) and \(S_2\) then the total entropy is
    \[S = S_1 + S_2.\]
    Permutations on the other hand are multiplicative.
    That is if a system is formed from two subsystems which can be arranged in \(\Omega_1\) and \(\Omega_2\) ways then the total number of arrangements of the whole system is
    \[\Omega = \Omega_1\Omega_2.\]
    Hence we must have
    \[f(\Omega) = f(\Omega_1\Omega_2) = f(\Omega_1) + f(\Omega_2) = S_1 + S_2.\]
    This holds if \(f(\omega) = \ln\Omega\).
    Thus for fixed energy, \(E\), we define
    \[S = \boltzmann\ln\Omega.\]
    One important factor that comes from this is that \(\Omega\) involves counting things which means that said `things' need to be discretised which leads to quantum mechanics.
    
    \section{Thermodynamic Potentials}
    A mechanical equilibrium is reached when the energy is minimised.
    A thermodynamic equilibrium is reached when entropy is maximised.
    We define the free energy as the thermodynamic variable that is minimised to reach a thermodynamic equilibrium.
    This is useful as the entropy is often difficult to measure and therefore not very useful for practical applications.
    We have already met two such variables that we can minimise for equilibrium and we will introduce some more.
    These variables are called \define{free energies} or \define{thermodynamic potentials}.
    Which thermodynamic potential we work with depends on which state variables we are working with.
    The thermodynamic potentials are themselves state variables and therefore can be written as a function of two other state variables.
    Typically we choose the two variables that simplify the maths, which often means one variable that is held constant and one variable that we control the change in.
    The thermodynamic potentials and the natural variables to work in are given in table~\ref{tab:the thermodynamic potentials}.
    \begin{table}[ht]
        \centering
        \begin{tabular}{lll}\hline
            Potential & Differential & Natural Units\\\hline
            Internal energy, \(U\) & \(\dd{U} = T\dd{S} - P\dd{V}\) & \(S\), \(V\)\\
            Enthalpy, \(H = U + PV\) & \(\dd{H} = T\dd{S} + V\dd{P}\) & \(S\), \(P\)\\
            Helmholtz free energy\tablefootnote{sometimes denoted \(A\)}, \(F = U - TS\) & \(\dd{F} = -P\dd{V} - S\dd{T}\) & \(T\), \(V\)\\
            Gibbs free energy, \(G = H - TS\) & \(\dd{G} = V\dd{P} - S\dd{T}\) & \(T\), \(P\)\\\hline
        \end{tabular}
        \caption{The thermodynamic potentials}
        \label{tab:the thermodynamic potentials}
    \end{table}
    Which potential we choose depends on what the situation is.
    In chemistry most experiments are open to the atmosphere so \(P\) is constant.
    The temperature is also readily measured and controlled.
    This means that often the Gibbs free energy, \(G\), is the best potential to use.
    In physics we often work with \(S\) and \(T\) as constants, for example, a pendulum swinging.
    Therefore we work with internal energy, \(\dd{U} = -\ddbar{W}\), for some appropriate expression of work.
    
    In mechanics we often start with a Hamiltonian, \(\hamiltonian\), and we assume that \(\hamiltonian\) is the sum of the kinetic and potential energies.
    This assumes that we have boundary conditions of \(S\) and \(V\) being constant and therefore we can use \(\hamiltonian = U\).
    If instead we study air as a sound wave passes then \(S\) and \(P\) are the variables of interest so we should use \(\hamiltonian = H\).
    If we study reagents dissolved in water then we have \(T\) and \(V\) being the variables of interest so we should use \(\hamiltonian = F\).
    If we study water exposed to the atmosphere then we have \(T\) and \(P\) being the variables of interest so we should use \(\hamiltonian = G\).
    In general we look at any boundaries in the system and at what variables are either constant or can change over those boundaries.
    
    \subsection{Maxwell Relations}
    State functions can usually be written as function of two independent variables.
    Therefore there must be a relationship between the state functions.
    The equation of state is one such relationship that is material specific.
    There are also relationships that are general and true due to the mathematical nature of the state variables.
    In particular we can use the fact that the potentials are state variables and therefore represented by nice functions of their natural variables and partial derivatives commute.
    This allows us to derive four relations called the Maxwell relations.
    
    We start with
    \[\dd{U} = T\dd{S} - P\dd{V}.\]
    Taking a derivative with respect to \(V\) at constant entropy we have
    \[\pdvconst{U}{V}{S} = -P.\]
    Taking a derivative with respect to \(S\) at constant volume we have
    \[\pdvsec{U}{S_V}{V_S} = -\pdvconst{P}{S}{V}.\]
    Since partial derivatives commute if we compute this in the other direction we must get the same result:
    First
    \[\pdvconst{U}{S}{V} = T,\]
    then
    \[\pdvsec{U}{V_S}{S_V} = \pdvconst{T}{V}{S}.\]
    So we conclude
    \[\pdvconst{T}{V}{S} = -\pdvconst{P}{S}{V}.\]
    
    Next we start with
    \[\dd{H} = T\dd{S} + V\dd{P}.\]
    \begin{align*}
        &\implies \pdvconst{H}{S}{P} = T \implies \pdvsec{H}{P_S}{S_P} = \pdvconst{T}{P}{S}\\
        &\implies \pdvconst{H}{P}{S} = V \implies \pdvsec{H}{S_P}{P_S} = \pdvconst{V}{S}{P}\\
        &\implies \pdvconst{T}{P}{S} = \pdvconst{V}{S}{P}.
    \end{align*}

    Next we start with
    \[\dd{F} = -P\dd{V} - S\dd{T}\]
    \begin{align*}
        &\implies \pdvconst{F}{V}{T} = -P \implies \pdvsec{F}{T_V}{V_T} = -\pdvconst{P}{T}{V}\\
        &\implies \pdvconst{F}{T}{V} = -S \implies \pdvsec{F}{V_T}{T_V} = -\pdvconst{S}{V}{T}\\
        &\implies \pdvconst{P}{T}{V} = \pdvconst{S}{V}{T}.
    \end{align*}

    Finally we start with
    \[\dd{G} = V\dd{P} - S\dd{T}\]
    \begin{align*}
        &\implies \pdvconst{G}{P}{T} = V \implies \pdvsec{G}{T_P}{P_T} = \pdvconst{V}{T}{P}\\
        &\implies \pdvconst{G}{T}{P} = -S \implies \pdvsec{G}{P_T}{T_P} = -\pdvconst{S}{P}{T}\\
        &\implies \pdvconst{V}{T}{P} = -\pdvconst{S}{P}{T}.
    \end{align*}
    
    All four relations are:
    \begin{align*}
        \hphantom{-}\pdvconst{T}{V}{S} &= -\pdvconst{P}{S}{V} = \pdvsec{U}{S}{V} \\
        \hphantom{-}\pdvconst{T}{P}{S} &= \hphantom{-}\pdvconst{V}{S}{P} = \pdvsec{H}{S}{P} \\
        \hphantom{-}\pdvconst{S}{V}{T} &= \hphantom{-}\pdvconst{P}{T}{V} = \pdvsec{F}{T}{V} \\
        -\pdvconst{S}{P}{T} &= \hphantom{-}\pdvconst{V}{T}{P} = \pdvsec{G}{T}{P} \\
    \end{align*}
    
    \subsection{Remembering the Thermodynamic Potentials and Maxwell Relations}
    In the Maxwell relations the natural variables of the relevant potential are the ones that appear in the denominator.
    For a derivative like
    \[\pdvconst{x}{y}{z}\]
    \(xz\) has units of energy.
    
    The following mnemonic gives us the potentials and Maxwell's relations:
    \begin{displayquote}
        \textbf{G}ood \textbf{P}hysicists \textbf{H}ave \textbf{S}tudied \textbf{U}nder \textbf{V}ery \textbf{F}ine \textbf{T}eachers.
    \end{displayquote}
    We use this to remember the potentials and state variables in the order \(G, P, H, S, U, V, F, T\).
    We then place these anticlockwise around a square as shown in figure~\ref{fig:mnemonic}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{mnemonic-1}
        \begin{tikzpicture}
            \draw[ultra thick] (0, 0) rectangle (2, 2);
            \node[right] at (2, 1) {\(G\)};
            \node[above right] at (2, 2) {\(P\)};
            \node[above] at (1, 2) {\(H\)};
            \node[above left] at (0, 2) {\(S\)};
            \node[left] at (0, 1) {\(F\)};
            \node[below left] at (0, 0) {\(V\)};
            \node[below] at (1, 0) {\(U\)};
            \node[below right] at (2, 0) {\(T\)};
        \end{tikzpicture}
        \caption{Mnemonic for remembering thermodynamic potentials and Maxwell relations.}
        \label{fig:mnemonic}
    \end{figure}
    The natural variables for describing a potential are then the two variables on the same line as it, for example \(H = H(S, P)\).
    This means that we can write
    \[\dd{H} = \text{sign}\,\text{const}\dd{S} + \text{sign}\,\text{const}\dd{P}.\]
    The constants are given by the diagonal opposite of the state variables that make up the differential:
    \[\dd{H} = \text{sign}\,T\dd{S} + \text{sign}\,V\dd{P}.\]
    Finally we get the sign by drawing an arrow from the constant to the differential.
    If the arrow is up then the sign is positive, if it is down then the sign is negative, the arrows are shown in figure~\ref{fig:mnemonic for H}:
    \[\dd{H} = T\dd{S} + V\dd{P}.\]
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{mnemonic-2}
        \begin{tikzpicture}
            \draw[ultra thick] (0, 0) rectangle (2, 2);
            \node[right] at (2, 1) {\(G\)};
            \node[above right] at (2, 2) {\(P\)};
            \node[above] at (1, 2) {\(H\)};
            \node[above left] at (0, 2) {\(S\)};
            \node[left] at (0, 1) {\(F\)};
            \node[below left] at (0, 0) {\(V\)};
            \node[below] at (1, 0) {\(U\)};
            \node[below right] at (2, 0) {\(T\)};
            \draw[ultra thick, ->] (0.2, 0.2) -- (1.8, 1.8);
            \draw[ultra thick, ->] (1.8, 0.2) -- (0.2, 1.8);
        \end{tikzpicture}
        \caption{Using the mnemonic to find the thermodynamic potentials.}
        \label{fig:mnemonic for H}
    \end{figure}
    We can get the Maxwell relations for two state variables by picking the two variables that aren't diagonally opposite, for example \(S\) and \(V\).
    We then start with the form
    \[\pdvconst{V}{?}{\text{const}} = \text{sign}\pdvconst{S}{?}{\text{const}}.\]
    We draw arrows along the diagonals from the variables of interest, this is shown in figure~\ref{fig:mnemonic maxwell relations}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{mnemonic-3}
        \begin{tikzpicture}
            \draw[ultra thick] (0, 0) rectangle (2, 2);
            \node[right] at (2, 1) {\(G\)};
            \node[above right] at (2, 2) {\(P\)};
            \node[above] at (1, 2) {\(H\)};
            \node[above left] at (0, 2) {\(S\)};
            \node[left] at (0, 1) {\(F\)};
            \node[below left] at (0, 0) {\(V\)};
            \node[below] at (1, 0) {\(U\)};
            \node[below right] at (2, 0) {\(T\)};
            \draw[ultra thick, ->] (0.2, 0.2) -- (1.8, 1.8);
            \draw[ultra thick, ->] (0.2, 1.8) -- (1.8, 0.2);
        \end{tikzpicture}
        \caption{Using the mnemonic to find Maxwell's relations}
        \label{fig:mnemonic maxwell relations}
    \end{figure}
    The thing that is held constant is given by the end of the arrow:
    \[\pdvconst{V}{?}{P} = \text{sign}\pdvconst{S}{?}{T}.\]
    The variable we differentiate with respect to is given by the end of the other arrow:
    \[\pdvconst{V}{T}{P} = \text{sign}\pdvconst{S}{P}{T}.\]
    If both arrows are up or both are down then the sign is positive, else the sign is negative:
    \[\pdvconst{V}{T}{P} = -\pdvconst{S}{P}{T}.\]
    
    \subsection{\texorpdfstring{\(T\dd{S}\)}{TdS} Equations}
    Using these relations we can find the entropy in terms of other, more easily measured variables:
    \[T\dd{S} = c_V\dd{T} + T\pdvconst{P}{T}{V}\dd{v},\]
    \[T\dd{S} = c_P\dd{T} - T\pdvconst{V}{T}{P}\dd{P},\]
    and
    \[T\dd{S} = c_p\pdvconst{T}{V}{P}\dd{v} + c_V\pdvconst{T}{P}{V}\dd{P}.\]
    These are called the \(T\dd{S}\) equations.
    
    \subsection{Gay--Lussac Law}
    The Gay--Lussac law, also known as Amonton's law, is that the ratio of pressure and temperature is a constant, known as the Gay--Lussac constant, \(\gaylussac\):
    \[\gaylussac = \frac{P}{T}.\]
    If we assume an isothermal process then
    \[\gaylussac = \pdvconst{P}{T}{V} = \pdvconst{S}{V}{T}.\]
    This constant, \(\gaylussac\), is a property of the material.
    
    \subsection{Iso-}
    Iso-, from the Greek, \textgreek{ἶσος} (isos) meaning equal, is a prefix usually used to denote that a quantity is constant for a certain process:
    \begin{itemize}
        \item Isotropic -- Equal in all directions,
        \item Isotopic -- Equal element,
        \item Isothermal -- Equal temperature,
        \item Isobaric -- Equal pressure,
        \item Isochoric/Isovolumetric -- Equal volume,
        \item Isenthalpic -- Equal enthalpy,
        \item Isoenergetic -- Equal energy,
        \item Isentropic -- Equal entropy,
        \item Adiabatic -- Equal heat.
    \end{itemize}

    \subsection{Minimising Thermodynamic Potentials}
    We started the section by claiming that minimising the relevant thermodynamic potential was the same as maximising the entropy.
    In this section we will show this for a specific system.
    Suppose a gas, initially at temperature \(T\) and pressure \(P\) is placed in thermal contact with a temperature and pressure reservoir at temperature \(T_0\) and pressure \(P_0\).
    Also the gas and reservoir are adiabatically separated from the rest of the universe and there is a free (meaning able to move) wall between the system and the reservoir.
    Initially the system is out of equilibrium.
    Heat, \(Q\), flows between the system and the reservoir.
    The change in entropy of the surroundings is
    \[\Delta S_0 = -\frac{Q}{T_0}.\]
    The change in volume of the surroundings is
    \[\Delta V_0 = \Delta V = \frac{W}{P_0}.\]
    The second law means
    \[\Delta S_\sys + \Delta S_0 \ge 0.\]
    The first law is
    \[Q = \Delta U + P_0\Delta V.\]
    Combining these we have
    \[\Delta U + P_0\Delta V - T_0\Delta S_0 \le 0.\]
    Since \(P_0\) and \(T_0\) are constant for a pressure and temperature reservoir this can be written as
    \[\Delta(U + PV - TS) \le 0. \implies \Delta(H - TS) \le 0 \implies \Delta G \le 0.\]
    We made no assumptions about which way the system had to move to reach equilibrium so we see that any step that the system makes towards equilibrium will decreases \(G\) therefore the system is in equilibrium when \(G\) is minimised, which is also when the entropy is maximised.
    
    \section{Availability}
    \subsection{General Relation between \texorpdfstring{\(C_P\)}{CP} and \texorpdfstring{\(C_V\)}{CV}}
    For an ideal gas we showed that
    \[C_P - C_V = nR.\]
    We are now ready to find a general relation between \(C_P\) and \(C_V\).
    At a constant volume from the first law we have \(\dd{U} = \ddbar{Q}\) the central equation we have \(\dd{U} = T\dd{S}\).
    Thus
    \[C_V = \pdvconst{Q}{T}{V} = \pdvconst{U}{T}{V} = T\pdvconst{S}{T}{V}.\]
    At constant pressure we have \(\dd{H} = \dd{U} = T\dd{S}\) and so
    \[C_P = \pdvconst{H}{T}{P} = T\pdvconst{S}{T}{P}.\]
    This suggests that we should work with the entropy, \(S = S(T, V)\).
    In this case \(\dd{S}\) is given by
    \[\dd{S} = \pdvconst{S}{T}{V}\dd{T} + \pdvconst{S}{V}{T}\dd{V}.\]
    Differentiating with respect to temperature at constant pressure this becomes
    \[\pdvconst{S}{T}{P} = \pdvconst{S}{T}{V}\pdvconst{T}{T}{P} + \pdvconst{S}{V}{T}\pdvconst{V}{T}{P} = \pdvconst{S}{T}{V} + \pdvconst{S}{V}{T}\pdvconst{V}{T}{P}.\]
    Multiplying by \(T\) we have
    \[C_P = T\pdvconst{S}{T}{P} = T\pdvconst{S}{V}{T} + T\pdvconst{S}{V}{T}\pdvconst{V}{T}{P} = C_V + T\pdvconst{S}{V}{T}\pdvconst{V}{T}{P}.\]
    We now introduce the isobaric thermal expansivity, \(\beta\), defined as
    \[\beta = \frac{1}{V}\pdvconst{V}{T}{P}.\]
    Thus
    \begin{equation}\label{eqn:C_P = C_V + TVbeta dS/dV_T}
        C_P = C_V + TV\beta\pdvconst{S}{V}{T}.
    \end{equation}
    By Maxwell's relations we have
    \[\pdvconst{S}{V}{T} = \pdvconst{P}{T}{V}.\]
    The triple product rule for partial derivatives is that
    \[\pdvconst{x}{y}{z}\pdvconst{y}{z}{x}\pdvconst{z}{x}{y} = -1.\]
    Using this and the reciprocal rule
    \[\frac{1}{\pdvconst{x}{y}{z}} = \pdvconst{y}{x}{z}\]
    we have
    \[\pdvconst{S}{V}{T} = \pdvconst{P}{T}{V} = -\pdvconst{P}{V}{T}\pdvconst{V}{T}{P}.\]
    Introducing the isothermal bulk modulus, \(K\), defined as
    \[K = -V\pdvconst{P}{V}{T}\]
    we have
    \[\pdvconst{S}{V}{T} = -V\pdvconst{P}{V}{T}\frac{1}{V}\pdvconst{V}{T}{P} = K\beta.\]
    Substituting this into equation~\ref{eqn:C_P = C_V + TVbeta dS/dV_T} we have
    \[C_P - C_V = TV\beta\pdvconst{S}{V}{T} = TV\beta^2 K = \frac{TV\beta^2}{\kappa}.\]
    Here we have introduced the isothermal compressibility, \(\kappa = 1/K\), which is more commonly used with gasses whereas \(K\) is more commonly used with solids.
    We see that
    \[C_P - C_V \propto V\]
    which makes sense as it is an extensive property meaning it depends on how much stuff there is.
    For all known substances \(K > 0\) and also \(T > 0\) in kelvins and \(\beta^2 > 0\) for all \(\beta\) so \(C_P - C_V \ge 0\) and \(C_P \ge C_V\).
    This even works if the thermal expansion, \(\beta\), is negative.
    This means that the difference is not due to the work done expanding at constant \(P\) as \(C_P \ge C_V\) even if the material shrinks upon heating, this goes against one of the most common (incorrect) explanations for why \(C_P\) is greater.
    \(C_P = C_V\) at the density maximum of water (which occurs at about \SI{4}{\degreeCelsius} and \SI{1}{\atm}).
    For solids and liquids \(\beta\) is usually small and therefore \(C_P \approx C_V\).
    For this reason we often don't differentiate between \(C_P\) and \(C_V\) and just quote a single value of heat capacity, which is usually \(C_P\).
    
    \subsection{Changes in Heat Capacities}\label{sec:changes in heat capacities}
    The values of \(C_V\) and \(C_P\), even for an ideal gas, depend on the structure of the molecules as the degrees of freedom are important.
    At room temperature however most vibrational degrees of freedom aren't accessible so only translational and rotational degrees of freedom play a part.
    
    One thing that we might want to know is how does \(C_V\) change with volume.
    That is what is
    \[\pdvconst{C_V}{V}{T}?\]
    We can calculate this fairly easily:
    \begin{align*}
        \pdvconst{C_V}{V}{T} &= T\left(\pdv{V}\pdvconst{S}{T}{V}\right)_T\\
        &= T\left(\pdv{T}\pdvconst{S}{V}{T}\right)_V\\
        &= T\left(\pdv{T}\pdvconst{P}{T}{V}\right)_V\\
        &= T\pdvconst[2]{P}{T}.
    \end{align*}
    Here we have used one of Maxwell's relations:
    \[\pdvconst{S}{V}{T} = \pdvconst{P}{T}{V}.\]
    Similarly we can show that
    \[\pdvconst{C_P}{P}{T} = -T\pdvconst[2]{V}{T}{P}.\]
    We can obtain these derivatives directly from the equations of state without even needing to know \(C_V\) and \(C_P\), for example an ideal gas has
    \[V = \frac{nRT}{P} \implies \pdvconst{C_P}{P}{T} = -T\left(\pdv{T}\left(\pdv{T}\frac{nRT}{P}\right)_P\right)_P = \left(\pdv{T}\frac{nR}{P}\right)_P = 0.\]
    Sometimes we can measure
    \[\pdvconst{C_P}{P}{T}\]
    at multiple pressures and integrate to get \(C_P\), or we could find \(C_V\) in a similar way.
    
    One interesting point is that \(C_P\) is infinite at phase transitions as at a phase transition \(\Delta T = 0\) between phases and \(\Delta V \ne 0\).
    Hence
    \[C_P = \pdvconst{Q}{T}{P} = -P\pdvconst{V}{T}{P}\]
    assuming an ideal gas so \(U\) is constant at a phase boundary since \(T\) is constant and \(U = U(T)\).
    The latent heat is
    \[L = \int_{T - \delta T}^{T + \delta T}C_P\dd{T}\]
    where \(T\) is the temperature at which the phase transition occurs and \(\delta T \to 0\).
    We know that \(L\) is finite and therefore we model \(C_P\) with a delta distribution at phase transitions.
    
    \subsection{Why Don't Experiments Work}
    Many classical physics experiments are based on the assumption that
    \[F = -\pdv{U}{x}.\]
    While this is true under ideal conditions things like friction get in the way.
    We can quantify how much using thermodynamics.
    Differentiating the central equation with respect to volume and at constant temperature we have
    \[\pdvconst{U}{V}{T} = T\pdvconst{S}{T}{T} - P = T\pdvconst{P}{T}{V} - P = \frac{T\beta}{\kappa} - P.\]
    Here we have used a Maxwell relation and the definitions of \(\beta\) and \(\kappa\).
    Since pressure is force per unit area we can divide by area to get
    \[\pdvconst{U}{x}{T} = \frac{T\beta}{\kappa A} -F \implies F = -\pdvconst{U}{x}{T} + \frac{T\beta}{\kappa A}.\]
    This extra term compared to the definition of potential energy explains why many experiments don't give quite the expected value.
    
    \subsection{Energy Change with Pressure}
    The energy increases under a pressure change.
    We can show this by differentiation the central equation with respect to pressure and holding \(T\) constant:
    \[\pdvconst{U}{P}{T} = T\pdvconst{S}{P}{T} - P\pdvconst{V}{P}{T} = T\pdvconst{V}{T}{P} - P\pdvconst{V}{P}{T}\]
    where we have used one of Maxwell's relations to get rid of \(S\).
    Identifying various terms this is
    \[\pdvconst{U}{P}{T} = -TV\beta + PV\kappa_T\]
    where \(\kappa_T\) is the isothermal compressibility, defined as
    \[\kappa_T = -\frac{1}{V}\pdvconst{V}{P}{T}.\]
    We can show that
    \[\frac{C_P}{C_V} = \frac{\kappa_T}{\kappa_S}\]
    where \(\kappa_S\) is the adiabatic compressibility, defined as
    \[\kappa_S = -\frac{1}{V}\pdvconst{V}{P}{S}.\]
    This gives us another link between thermal \((C_P/C_V)\) and mechanical \((\kappa_T/\kappa_S)\) properties.
    
    \subsection{Entropy of an Ideal Gas (Again)}
    The entropy per mole of an ideal gas is \(s = s(T, V)\).
    \[\dd{s} = \pdvconst{s}{T}{V}\dd{T} + \pdvconst{s}{V}{T}\dd{V} = c_V\frac{\dd{T}}{T} + \beta K\dd{V}.\]
    This applies to any fluid so far.
    For an ideal gas we have that \(\beta K = R/V\) and \(c_V\) is constant so integrating this we have
    \[s = c_V\ln T + R\ln v + s_0.\]
    Similarly we can show that
    \[s = c_p\ln T - R\ln P + s_0.\]
    So we have related changes in entropy to measurable properties via the equations of state.
    
    \subsection{Availability}
    Given a specific system how much work can we get out?
    The answer depends on the surroundings.
    Suppose that the system is surrounded by a \(T_0, P_0\) reservoir.
    The second law means that
    \[\Delta S_\sys  + \Delta S_\surr \ge 0.\]
    Which is
    \[\Delta S_\sys - \frac{\Delta Q}{T_0 \ge 0}\]
    where \(\Delta Q\) is the heat transferred from the the surroundings to the system.
    The first law for the system means that \(\Delta Q = \Delta U + P_0\Delta V\) and hence
    \[\Delta U + P_0\Delta V - T_0\Delta S \le 0.\]
    We define the availability, \(A\), to be
    \[A = U - T_0S + P_0V.\]
    Note that this is different from the Gibbs free energy as \(T_0\) and \(P_0\) are the temperature and pressure of the surroundings, not the system.
    
    \(A\) depends on the system and surroundings, \(A = A(S, V, P_0, T_0)\).
    We have already seen that spontaneous changes in \(A\) are negative since
    \[\Delta A = \Delta U + P_0\Delta V - T_0\Delta S \le 0.\]
    At equilibrium nothing can change and since \(\Delta A\) can only decrease it must be minimised at equilibrium.
    In differential form
    \[\dd{A} = \dd{U} - T_0\dd{S} + P_0\dd{V}.\]
    At equilibrium we will have \(T_0 = T\) and \(P_0 = P\) and \(\dd{A} = 0\).
    The following scenarios show how useful \(A\) is, at equilibrium by minimising \(A\) with certain conditions we have:
    \begin{align*}
        &T = T_0 & P = P_0 && A &= U - TS + PV && G~\text{minimised}\\
        &T = T_0 & V~\text{const} && A &= U - TS + \text{const} && F~\text{minimised}\\
        &S~\text{const} & V~\text{const} && A &= U + \text{const} && U~\text{minimised}\\
        &S~\text{const} & P = P_0 && A &= U - \text{const} + PV && H~\text{minimised}\\
        &U~\text{const} & V~\text{const} && A &= \text{const} - T_0S && S~\text{maximised}
    \end{align*}
    \(A\) gives us a measure of how far we are from equilibrium.
    
    The work, \(P_0\Delta V\) is used to push back the environment.
    It is wasted.
    The heat transferred could be used to run an engine producing useful work.
    We have
    \[\Delta U = Q - W = Q - W_\text{useful} - P_0\Delta V.\]
    \(Q\)  is the heat transported from the surroundings to the system.
    Rearranging this and substituting in \(Q = T_0\Delta S\) we have
    \[0 = \Delta U + P_0\Delta V - T_0\Delta S + W_\text{useful} = W_\text{useful} - \Delta A.\]
    Therefore the maximum amount of useful work that we can extract is 
    \[W_{\max} = -\Delta A.\]
    This occurs if and only if all changes are reversible.
    In all other cases the work that we can extract is less.
    In differential form we have
    \begin{align*}
        \dd{W_\text{useful}} &\le -\dd{A}\\
        &= -\dd{U} + T_0\dd{S} - P_0\dd{V}\\
        &= -T\dd{S} + P\dd{V} + T_0\dd{S} - P_0\dd{V}\\
        &= (T_0 - T)\dd{S} - (P_0 - P)\dd{V}
    \end{align*}
    so we see that the work we can do is directly related to the difference in pressure and temperature of the system and surroundings.
    
    \section{Free Expansion of a Real Gas}
    In section~\ref{sec:irreversible free expansion and the Joule coefficient} we considered a gas expanding into a vacuum to twice its original volume in a rigid adiabatic container.
    We argued that these properties meant no work was done and no heat was transferred, therefore bu the first law \(\dd{U} = 0\).
    Then we looked at an ideal gas and said that \(\dd{U} = 0\) implies \(\dd{T} = 0\) since for an ideal gas \(U = U(T)\) so if \(U\) is constant \(T\) must be also.
    Finally we defined the Joule coefficient
    \[\mu_J = \pdvconst{T}{V}{U}\]
    such that \(\dd{T} = \mu_J\dd{V}\), which we found by expanding a differential of \(T = T(V, U)\) and setting \(\dd{U} = 0\).
    For an ideal gas \(\mu_J = 0\) since \(\dd{T} = 0\).
    
    We now have enough thermodynamics to reconsider this problem for real gases.
    Since the container is adiabatic the entropy of the surroundings, \(S_0\), is constant, so \(\dd{S_0} = 0\).
    Since the container is rigid the volume of the surroundings, \(V_0\), is constant so \(\dd{V_0} = 0\).
    For the surroundings \(\dd{U} = \ddbar{Q} + \ddbar{W} = 0\) since no work is done and no heat is transferred.
    Turning now to the system we want to know how the change in volume effects the temperature.
    We want to know what
    \[\mu_J = \pdvconst{T}{V}{U}\]
    is since then we know that
    \[\dd{T} = \pdvconst{T}{V}{U}\dd{V} = \mu_J\dd{V}.\]
    Integrating this we have
    \[\Delta T = \int_{V_i}^{V_f}\pdvconst{T}{V}{U}\dd{V} = \int_{V_i}^{V_f}\mu_J\dd{V}.\]
    We can't directly measure \(U\) so we would like to get rid of it.
    To do this we use the partial derivative triple product:
    \[\pdvconst{T}{V}{U} = -\pdvconst{T}{U}{V}\pdvconst{U}{V}{T}.\]
    Identifying
    \[\pdvconst{U}{T}{V} = C_V\]
    and using the reciprocal rule for partial derivatives we have
    \[\pdvconst{T}{V}{U} = -\frac{1}{C_V}\pdvconst{U}{V}{T}.\]
    Differentiating the central equation with respect to \(V\) at constant \(T\) we get
    \[\pdvconst{U}{V}{T} = T\pdvconst{S}{V}{T} - P.\]
    Therefore
    \[\mu_J = \pdvconst{T}{V}{U} = \frac{1}{C_V}\left[P - T\pdvconst{P}{T}{V}\right],\]
    where we have used a Maxwell relation to get rid of the entropy.
    This allows us to calculate \(\mu_J\) from the equation of state, \(U = U(P, T)\).
    For an ideal gas
    \[P = \frac{nRT}{V} \implies \pdvconst{P}{T}{V} = \frac{nR}{V} \implies T\pdvconst{P}{T}{V} = T\frac{nR}{V} = P\]
    so \(\mu_J = 0\), which is what we expected.
    Note that in general \(\mu_J\) can be positive or negative.
    
    We can also calculate \(\mu_J\) for a slightly more realistic gas, in this case a Van der Waal's gas:
    \[\left(P + \frac{a}{v^2}\right)(v - b) = RT.\]
    This can be expanded in powers of the number density in what is called a Virial expansion:
    \begin{align*}
        Pv &= RT\left[1 + \left(b - \frac{a}{RT}\right)\left(\frac{1}{v}\right) + b^2\left(\frac{1}{v}\right)^2 + \dotsb\right]\\
        &= RT\left[1 + \frac{B_2}{v} + \frac{B_3}{v^2} + \dotsb\right].
    \end{align*}
    Here \(B_i\) are the temperature dependent Virial coefficients.
    We want to use this to estimate \(\Delta T\) for a given \(\Delta V\)
    For this we need \(\mu_J\), we approximate it keeping only up to the second Virial coefficient and we have
    \[\mu_J \approx -\frac{1}{c_V}\frac{RT}{v^2}\dv{B_2}{T}.\]
    For argon \(\inlinedv{B_2}{T} = \SI{0.25}{\centi\metre^3.\mole^{-1}.\kelvin^{-1}}\) so if the volume of a mole of argon doubles then the temperature change is approximately \(\Delta T = \SI{-0.6}{\kelvin}\).
    
    \subsection{Fridges}
    Fridges use this effect of gases cooling upon expansion to cool the refrigerant.
    Except that in a fridge this is done continuously by forcing gas through a porous plug which can also hold a pressure difference on either side.
    The system of interest here is a sample of gas that starts on one side of the plug and then ends up on the other side.
    We assume that the plug stays the same and so the surroundings have no entropy change.
    We also assume that the surroundings exert a constant external pressure, this is a reasonable assumption for continuous flow.
    
    Since external pressure is constant enthalpy is the thermodynamic potential of choice.
    We consider a reversible isenthalpic process.
    The work done is
    \[\Delta W = P_iV_i - P_fV_i\]
    where \(V_i\) is the initial volume and \(P_i\) and \(P_f\) are the initial and final velocities respectively.
    Note that this is slightly different to the standard \(p\dd{V}\) work as it is the pressure that is changing, not the volume.
    
    For this continuous process every state between the initial and final state is visited along an isenthalp.
    Isenthalps are typically shaped to have some maximum temperature.
    The curve that connects the maximum temperatures is called the inversion curve.
    At higher pressures than that at which the maximum temperature occurs a volume change results in heating as the pressure drops so the temperature moves up towards the maximum (which is to the left on a \((P, T)\) diagram).
    At lower pressures an increase in volume and therefore decrease in pressure still causes movement in the same direction but on the other side of the maximum so the temperature decreases.
    We can think of this being because at high pressures the molecules start to form intermolecular bonds which decreases the energy and so reducing the pressure increases the energy as these bonds start to break.
    
    Since we are interested in changing \(P\) and conserving \(H\) we use \(T = T(P, H)\) and expand the differential as
    \[\dd{T} = \pdvconst{T}{P}{H}\dd{P} + \pdvconst{T}{H}{P}\dd{H} = \pdvconst{T}{P}{H}\dd{P} = \mu_{JK}\dd{P}.\]
    Here \(\mu_{JK}\) is called the Joule--Kelvin coefficient.
    The temperature change is then given by
    \[\Delta T = \int_{P_i}^{P_f}\mu_{JK}\dd{P}.\]
    We want to eliminate \(H\) from this as we can't measure it.
    We start by using the triple product and reciprocal rule for partial derivatives as well as recalling that \((\partial_TH)_P = C_P\):
    \[\mu_{JK} = \pdvconst{T}{P}{H} = -\pdvconst{T}{H}{P}\pdvconst{H}{P}{T} = -\frac{1}{C_P}\pdvconst{H}{P}{T}.\]
    Recalling that in differential form the enthalpy is
    \[\dd{H} = T\dd{S} + V\dd{P}\]
    and differentiating with respect to \(P\) at constant temperature we have
    \[\pdvconst{H}{P}{T} = T\pdvconst{S}{P}{T} + V\]
    so
    \[\mu_{JK} = \frac{1}{C_P}\left[T\pdvconst{V}{T}{P} - V\right]\]
    where we have used a Maxwell relation to get rid of entropy.
    To make a good fridge we want to choose a material with a small heat capacity and that undergoes a large volume change for a small temperature change.
    We can approximate the temperature change for a given pressure change as follows
    \[\pdvconst{T}{P}{H} = \mu_{JK} \implies \Delta T \approx \mu_{JK}\Delta P.\]
    For an ideal gas
    \[V = \frac{nRT}{P} \implies \pdvconst{V}{T}{P} = \frac{nR}{P} \implies T\pdvconst{V}{T}{P} = T\frac{nR}{P} = V\]
    and so \(\mu_{JK} = 0\).
    In general \(\mu_{JK}\) can be positive or negative.
    
    We can find the inversion temperature, where expansion goes from a cooling process to a heating process, by noting that at this point \(\mu_{JK} = 0\) as this is the maximum of an isenthalp in a \((P, T)\) diagram.
    Not all isenthalps have a maximum, above a certain temperature expansion will always be a heating process.
    We can find the temperature by setting \(\mu_{JK} = 0\) and then setting \(P = 0\).
    This maximum inversion temperature is important as above this temperature a gas cannot be cooled by expansion.
    Some values of the maximum inversion temperature are given in table~\ref{tab:maximum inversion temperatures}.
    \begin{table}[ht]
        \centering
        \begin{tabular}{ll}\hline
            Gas & Maximum inversion temperature\\\hline
            Argon & \(\SI{723}{K} = \SI{450}{\degreeCelsius}\)\\
            Nitrogen & \(\SI{621}{K} = \SI{348}{\degreeCelsius}\)\\
            Hydrogen & \(\SI{205}{K} = \SI{-68}{\degreeCelsius}\)\\
            Helium & \(\SI{51}{K} = \SI{-222}{\degreeCelsius}\)\\\hline
        \end{tabular}
        \caption{Maximum inversion temperatures}
        \label{tab:maximum inversion temperatures}
    \end{table}
    At very low temperatures helium is typically used as a refrigerant, not because it works well (it doesn't) but because it is one of the only gases that is still a fluid at only a few kelvin.
    The other problem with this is that since the maximum inversion temperature of helium is so low another refrigeration process has to be used above \SI{51}{\kelvin} before the helium can be used.
    
    Real fridges use a phase change to have a very large value of
    \[\pdvconst{V}{T}{P}.\]
    For this freon-12 (dichlorodifluromethane) used to be the gas of choice as at \SI{1}{\atm} it boils at \SI{-29.8}{\degreeCelsius} so in the slightly higher pressure that a refrigerant is stored at it boils at approximately the temperature we want the fridge to be.
    Unfortunately freon-12 is a \gls{cfc} which, when released into the atmosphere, destroy the ozone layer and act as an incredibly efficient green house gas.
    For this reason use of freon-12 has been banned in many countries and alternatives are used.
    
    \subsection{Other Forms of Work}
    So far we have considered mostly gas processes.
    In these processes an overwhelming amount of the total work is work done expanding against a pressure.
    However different systems have other, more important, forms of work.
    Some examples are given in table~\ref{tab:alternative forms of work}.
    \begin{table}
        \centering
        \begin{tabular}{llll}\hline
            System & Intensive Variable & Extensive Variable & Infinitesimal work on the system\\\hline
            Gas/fluid & Pressure, \(P\) & Volume, \(V\) & \(-P\dd{V}\)\\
            Wire/rod & Tension, \(\mathcal{F}\) & Length, \(L\) & \(\mathcal{F}\dd{L}\)\\
            Electric cell & Emf, \(\mathcal{E}\) & Charge, \(Z\) & \(\mathcal{E}\dd{Z}\)\\
            Magnetic material & Induction, \(B_0\) & Magnetic moment, \(\mathcal{M}\) & \(B_0\dd{\mathcal{M}}\)\\
            Dielectric material & Electric field, \(E\) & Polarisation, \(\mathcal{P}\) & \(E\dd{\mathcal{P}}\)\\\hline
        \end{tabular}
        \caption{Alternative forms of work}
        \label{tab:alternative forms of work}
    \end{table}

    \subsubsection{Stretching a Rubber Band}
    The work done stretching a rubber band is
    \[\dd{W} = \mathcal{F}\dd{L} - P\dd{V}\]
    however \(P\dd{V}\ll \mathcal{F}\dd{L}\) so we typically only consider
    \[\dd{W} = \mathcal{F}\dd{L}.\]
    We can use many results we have already derived by substituting \(L\) for \(V\) and \(-\mathcal{F}\) for \(P\).
    For example the following Maxwell relation:
    \[\pdvconst{S}{V}{T} = \pdvconst{P}{T}{V} \rightarrow \pdvconst{S}{L}{T} = \pdvconst{(-\mathcal{F})}{T}{L} = -\pdvconst{\mathcal{F}}{T}{L}.\]
    Consider an isothermal stretch.
    In this process rubber molecules, which are long chains, become less tangled and align, this means there is more order so the entropy decreases.
    Therefore we expect \((\partial_LS)_T < 0\) and therefore we expect \((\partial_T\mathcal{F})_L\) to be negative as well.
    Using this Maxwell relation and also the triple product we have
    \[\pdvconst{S}{L}{T} = -\pdvconst{\mathcal{F}}{T}{L} = \pdvconst{\mathcal{F}}{L}{T}\pdvconst{L}{T}{\mathcal{F}} < 0.\]
    The first derivative is certainly positive as we must increase the force to increase the length.
    We conclude that the second derivative will be negative and rubber will have a negative coefficient of linear expansion:
    \[\alpha_L = \frac{1}{L}\pdvconst{L}{T}{\mathcal{F}}.\]
    So a rubber band stretched with a constant force will cool.
    
    We can use this to make (a very bad) engine.
    We could heat a rubber band to make it contract and use this to do work, say lifting a mass, \(M\).
    The heat that we input will be \(Q = C_P\Delta T\).
    The work done is
    \[W = Mg\Delta L = MgL\alpha_L\Delta T.\]
    The efficiency is then
    \[\eta = \frac{W}{Q} = \frac{MgL\alpha_L}{C_P}\]
    This appears to increase with \(Mg\), which cannot be as this would allow for infinite efficiency, which is forbidden by the second law.
    We conclude that \(\alpha_L\) must decrease at least as fast as \(1/Mg\).
    Using Maxwell's relations we have
    \[\alpha_L = \frac{1}{L}\pdvconst{S}{\mathcal{F}}{T}.\]
    We posit that for sufficiently large forces all the molecules are aligned and therefore there is no scope for decreasing entropy by stretching further.
    In fact further stretching will cause the molecules to start breaking increasing entropy.
    We have managed to discover some important information for what seems to be a purely mechanical system using only thermodynamic principles.
    
    \section{Thermodynamics and Electrodynamics}
    We can apply many thermodynamic ideas to electromagnetism.
    For example we can take the internal energy to be the energy stored in the electromagnetic fields.
    We can consider the radiation pressure, \(\expected{S}/c\) where \(\vv{S} = \vv{E}\times\vv{H}\) is the Poynting vector and \(c\) is the speed of light.
    If we consider a black body as a cavity containing radiation then the volume is crucial, as well as the temperature.
    We can consider a magnetic field, \(\vv{B} = \mu_0(\vv{H} + \vv{M})\) as being a sum of an extensive property, \(\vv{H}\), and intensive property \(\vv{M}\).
    
    For simplicity we assume that \(H \gg M\) and therefore \(\vv{B} \approx \mu_0\vv{H}\).
    The magnetic energy is then \(-BM\).
    We also assume that the volume is constant, or at least close to it, and therefore we can ignore any work done to expand, \(P\dd{V}\).
    Instead we define the work done to be the energy used to increase magnetisation:
    \[\dd{W} = \vv{B}\cdot\dd{\vv{M}}.\]
    The central equation is then
    \[\dd{u} = T\dd{s} + B\dd{M}.\]
    We can find the Maxwell relations by substituting \(-B\) for \(P\) and \(M\) for \(V\), for example:
    \[-\pdvconst{P}{S}{V} = \pdvconst{T}{V}{S} \rightarrow \pdvconst{B}{S}{M} = \pdvconst{T}{M}{S}.\]
    It is also common to define the \define{total energy}, \(e_{\tot}\), which is the magnetic equivalent of enthalpy:
    \[H = U + PV \rightarrow e_{\tot} = u - BM.\]
    Taking derivatives this becomes
    \[\dd{e_{\tot}} = \dd{u} - B\dd{M} - M\dd{B} = T\dd{s} - M\dd{B}\]
    where we have substituted the central equation for \(\dd{u}\) in the last step.
    We can also do something similar for the other thermodynamic potentials.
    
    \subsection{Magnetic Cooling}
    For the rest of this section we will consider a process of magnetic cooling.
    The approximations that we make are
    \begin{itemize}
        \item The induced magnetisation is small so \(\vv{B} = \mu_0(\vv{H} + \vv{M}) \approx \mu_0\vv{H}\).
        \item The susceptibility is scalar so the magnetisation is parallel to the field:
        \[\vv{B}\cdot\vv{M} = BM.\]
        Generalisation to a tensor susceptibility is straight forward but tedious.
        \item Volume is constant so \(\dd{V} = 0\) and crucially there is no \(P\dd{V}\) work.
    \end{itemize}
    The equipartition theorem in classical mechanics states that all degrees of freedom have the same energy and that all excited degrees of freedom contribute to the energy.
    The important degree of freedom that is introduced when considering a magnetic field is the alignment of any magnetic moments.
    For example if we turn on a magnetic field then the magnetic moments will align causing a decrease in entropy.
    
    There are many degrees of freedom that we may consider normally:
    \begin{itemize}
        \item Rotational degrees of freedom -- important in polyatomic gases.
        \item Vibrational degrees of freedom -- important when considering chemical bonds.
        \item Electronic degrees of freedom (ability of electrons to move about) -- important when considering materials with delocalised electrons, such as metals or plasmas.
        \item Orientational degrees of freedom -- important when considering long molecules, such as those in rubber.
        \item Magnetic degrees of freedom -- important when considering materials in an external magnetic field.
    \end{itemize}
    All of these are quantised in some way by \(\hbar\).
    Only excited degrees of freedom contribute to the entropy.
    
    \subsubsection{The Magnetic Cooling Process}
    A paramagnet (magnetises in an external field and then loses magnetisation after the field is removed) is placed in contact with a heat bath.
    An external magnetic field is applied isentropically.
    The total entropy must therefore be constant, however by applying a magnetic field magnetic moments align decreasing the magnetic entropy.
    Thus another form of entropy must increase to balance this.
    In this case the only form of entropy that can increase is the thermal entropy.
    The paramagnet is then removed from the heat bath and then the field is removed isentropically.
    The thermal entropy then returns back to being magnetic entropy cooling the magnet.
    
    During the magnetisation process the magnet loses heat to the heat bath.
    If this process is done isothermally, as is the most efficient (but slowest) way to do it, then
    \[\pdvconst{Q}{B}{T} = T\pdvconst{S}{B}{T} = T\pdvconst{M}{T}{B}\]
    using a Maxwell relation in the last step.
    We can measure the properties in the right hand side of this.
    During the adiabatic/isenthalpic demagnetisation the temperature is reduced:
    \[\pdvconst{T}{B}{S} = -\pdvconst{T}{S}{B}\pdvconst{S}{B}{T} = -\frac{T}{C_B}\pdvconst{S}{B}{T}\]
    Here we have used the triple product rule for partial derivatives and then defined the heat capacity at constant \(B\) as
    \[C_B = \pdvconst{U}{T}{B} = \pdvconst{T}{S}{T},\]
    which uses the derivative of the magnetic central equation at constant \(B\).
    If we use a Maxwell relation and define the magnetic susceptibility to be \(\chi = M/B\) then we have
    \begin{equation}\label{eqn:dT/dB const S}
        \pdvconst{T}{B}{S} = -\frac{T}{C_B}\pdvconst{M}{T}{B} = -\frac{TB}{C_B}\pdvconst{\chi}{T}{B}.
    \end{equation}
    We now use Curie's law of susceptibility at lower temperatures:
    \[M = a\frac{B}{T} \iff \chi^{-1} = \frac{T}{a}\]
    where \(a\) is the Curie constant that we can measure (this is normally denoted \(C\) but we don't want to confuse it with a heat capacity).
    This law was empirically created by Pierre Curie.
    
    The Schottky non-magnetic heat capacity is
    \[c_B(T, B = 0) = \frac{b}{T^2}\]
    for some constant \(b\).
    Thus at some non-zero field strength, \(B\):
    \begin{align*}
        c_B(T, B) &= \frac{b}{T^2} + \int_0^B \pdvconst{c_B}{B}{T} \dd{B}\\
        &= \frac{b}{T^2} + \int_0^B T\pdvconst[2]{M}{T}{B}\\
        &= \frac{b}{T^2} + \int_0^B T\frac{2aB}{T^3}\dd{B}\\
        &= (b + aB^2)\frac{1}{T^2}
    \end{align*}
    Where we have used that
    \[\pdvconst{C_B}{B}{T} = T\pdvconst[2]{M}{T}{B},\]
    which is the magnetic analogue of
    \[\pdvconst{C_P}{P}{T} = -T\pdvconst[2]{V}{T}{P}\]
    which we have shown in section~\ref{sec:changes in heat capacities}.
    We then wrote \(M\) using Curie's Law.
    Using this and Curie's law in equation~\ref{eqn:dT/dB const S} we have
    \begin{align*}
        \pdvconst{T}{B}{S} &= -BT\frac{T^2}{b + aB^2}\left(\pdv{T}\frac{a}{T}\right)_B\\
        &= -BT\frac{T^2}{b + aB^2}\frac{(-a)}{T^2}\\
        &= \frac{aBT}{b + aB^2}\\
        \implies \int \frac{\dd{T}}{T} &= \int \frac{aB}{b + aB^2}\dd{B}\\
        \implies \ln\left(\frac{T_f}{T_i}\right) = \frac{1}{2}\ln\left(\frac{b + aB_f^2}{b + aB_i^2}\right)\\
        \implies T_f &= T_i\sqrt{\frac{b + aB_f^2}{b + aB_i^2}}
    \end{align*}
    We see that each cycle \(T\) is decreased by a fixed fraction.
    This means that we cannot get to \(T = 0\) with a finite number of cycles.
    
    Another issue is that Curie's law diverges at \(T = 0\).
    A more accurate set of measurements reveals that
    \[\chi = \frac{a}{T - T_C}\]
    where \(T_C\) is the Curie temperature.
    Below \(T_C\) a paramagnet will spontaneously magnetise as there is no longer enough thermal energy to disrupt the alignment of magnetic moments.
    We can use this and perform the same analysis above with the substitution that \(T \rightarrow T - T_C\) and we get
    \[T_f = T_C + (T_i - T_C)\sqrt{\frac{b + aB_f^2}{b + aB_i^2}}\]
    so we see that even after an infinite number of cycles the temperature cannot go below \(T_C\).
    There is also a balance that needs to be struck.
    A weaker paramagnet will have a lower value of \(T_C\) so allows for more cooling but this cooling will occur much slower than if we use a stronger paramagnet which will have a higher value of \(T_C\).
    
    \section{Thermal Radiation}
    In this section we will consider a system consisting of radiation in a cavity.
    The surroundings are the cavity walls which act as a heat bath at temperature \(T\), as well as providing a fixed volume.
    Because of this we express \(U\) as a function of temperature and volume, \(U = U(T, V)\).
    It can be shown that the pressure is \(P = u/3\) where \(u\) is the energy per unit volume (not the energy per mole as lowercase usually denotes).
    This can be shown by considering kinetic theory with the Maxwell--Boltzmann distribution with \(v = c\) for all particles, or as a consequence of relativity or with electromagnetism.
    
    We split the cavity into two halves, \(A\) and \(B\), with a small gap in the barrier.
    In the most general case the walls of both sides of the cavity can be made of different materials.
    The two sides are in thermal equilibrium and also due to the ability to exchange matter between the sides they are also at the same pressure.
    This means that \(u_A = u_B\) since there is no heat flow.
    Therefore the Clausius statement of the second law means that we must have \(u(T, V) = u(T)\).
    
    We consider a different system for each wavelength of light.
    This is like replacing the gap with a filter that only allows one wavelength through.
    The same argument as above means that heat flows between the two sides until equilibrium is reached and the temperature, pressure, and internal energy per unit volume is the same on both sides.
    The internal energy per volume at wavelength \(\lambda\) is \(u_\lambda(T)\) which is a thermodynamic function of \(T\) for some given value of \(\lambda\).
    The form of this function will be the same for all \(\lambda\).
    We know that at high temperatures things glow visibly and at slightly lower temperatures they glow in the infrared part of the spectrum.
    The peak of \(u_\lambda\) must therefore move with temperature.
    The total internal energy per unit volume is simply
    \[u = \int_{0}^{\infty} u_\lambda(T)\dd{\lambda}.\]
    Using the central equation we have
    \[\pdvconst{U}{V}{T} = T\pdvconst{S}{V}{T} - P\pdvconst{V}{V}{T} = T\pdvconst{P}{T}{V} - P\]
    where we have used a Maxwell relation in the last step.
    Substituting in \(P = u/3\), \(U = uV\) and \(u = u(T)\) we have
    \[\left(\pdv{V}uV\right)_T = u = T\left(\pdv{T}\frac{u}{3}\right)_V - \frac{u}{3} = \frac{T}{3}\dv{u}{T} - \frac{u}{3} \implies 4u = T\dv{u}{T}.\]
    Solving this we have
    \[4\ln T = \ln u + \ln k\]
    for some constant, \(k\).
    Rearranging we have
    \[T^4 = ku.\]
    Applying boundary conditions we find that
    \[u(T) = \frac{4\sigma}{c}T^4\]
    where \(c\) is the speed of light and \(\sigma = \SI{5.67e-8}{\watt.\meter^{-2}.\kelvin^{-4}}\) is Stefan's constant which is related to by the flux, the flux is given by \(uc/4 = \sigma T^4\).
    The fact that the exponent is 4 will lead to terms like \(P = u/3\) when we integrate.
    This is a direct consequence of living in three dimensions where the velocity is split equally amongst the three directions leading to \(1/3\) of the energy contributing to pressure in each direction.
    
    We can get other thermodynamic quantities in terms of \(u\) as well.
    For example, the heat capacity
    \[C_V = \pdvconst{U}{T}{V} = \left(\pdv{T}uV\right)_V = 4\sigma_0VT^3\]
    where \(\sigma_0 = 4\sigma/c = \SI{7.566e-16}{\joule.\meter^{-3}.\kelvin^{-4}}\) is the radiation constant.
    As well as this we can find the state variables such as entropy of the system:
    \[S = \int \frac{C_V\dd{T}}{T} = \frac{4}{3}\sigma_0VT^3,\]
    the enthalpy:
    \[H = U + PV = \frac{4}{3}\sigma_0VT^4 = TS,\]
    and the Gibbs free energy:
    \[G = U - TS + PV = uV - TS + PV = \sigma_0VT^4 - \frac{4}{3}\sigma_0VT^4 + \frac{1}{3}VT^4 = 0.\]
    This means that photons have zero Gibbs free energy.
    This is because photons appear and disappear and therefore must have \(G = 0\) for this to be allowed so that \(G\) is constant if a photon appears/disappears.
    
    The energy distribution looks like a slightly asymmetric bell curve (but is not normally distributed).
    We can fit power laws to the tails.
    For long wavelengths
    \[u_\lambda \propto T\lambda^{-4}\]
    and for short wavelengths
    \[u_\lambda \propto \lambda^{-5}e^{-k/\lambda T}\]
    for some constant \(k\).
    It can also be shown that the wavelength that maximises \(u_{\lambda}\) is
    \[\lambda_{\text{peak}} = \frac{1}{T}\cdot\SI{2.898e-3}{\meter}.\]
    Note that this maximises the energy per unit volume, not the energy per photon, clearly \(\lambda \to 0\) (\(f \to\infty\)) maximises the energy per photon, the reason that this is not the same is because \(u_{\lambda}\) takes into account the number of photons of wavelength \(\lambda\).
    As well as this we know that
    \[u(T) = \int_{0}^{\infty} u_{\lambda}(T) \dd{\lambda}.\]
    Integrating the black body equation of state we get Stefan's law:
    \[u(T) = \int_{0}^{\infty} u_{\lambda}(T)\dd{\lambda} = \frac{4\sigma}{c}T^4,\]
    we then also require that
    \begin{itemize}
        \item \(u_{\lambda}(T)\) decays as \(\lambda^{-5}\) for large values of \(\lambda\),
        \item \(u_{\lambda}(T)\) doesn't blow up for high frequencies (low \(\lambda\)),
        \item \(u_{\lambda}(T)\) gives the same temperature,
        \[T = \pdvconst{u_{\lambda}}{s_{\lambda}}{V},\]
        for all values of \(\lambda\).
    \end{itemize}
    Using this we can come up with an exact form for \(u_{\lambda}(T)\).
    Planck discovered that it is easier to relate \(u\) and \(s\) than \(u\) and \(T\) so that is what we will do here.
    Planck started with
    \[\pdvconst{s}{u}{V} = \frac{1}{T},\]
    which comes from the third condition on \(u_{\lambda}(T)\) above.
    From this we know that
    \[\pdvconst[2]{s}{u}{V} = \pdvconst{(1/T}){u}{V}.\]
    At long wavelengths we want
    \[\pdvconst{(1/T)}{u}{V} \propto -\frac{1}{u^2},\]
    and at short wavelengths we want
    \[\pdvconst{(1/T)}{u}{V} \propto -\frac{1}{u}.\]
    The simplest way to combine these two conditions is
    \[\pdvconst{(1/T)}{u}{V} = \pdvconst[2]{s}{u}{V} = \frac{C_1}{u(u + C_2)}\]
    for some constants \(C_1\) and \(C_2\).
    Integrating this we have
    \[\frac{1}{T} = \int \frac{C_1}{u(u + C_2)}\dd{u} = \frac{C_1}{C_2}\ln\left(\frac{u}{u + C_2}\right).\]
    Rearranging this and reintroducing \(\lambda\) we have
    \[u_{\lambda}(T) \propto \lambda^{-5}\left(\frac{1}{e^{k/\lambda T} - 1}\right).\]
    This fits the data perfectly.
    
    A geometric series of the form \(1 + r + r^2 + r^3 + \dotsb\) can be shown to sum to
    \[\sum_{n=0}^{\infty} r^n = \frac{1}{1 - r},\]
    if \(\abs{r} < 1\).
    Using this we see that
    \[u_{\lambda}(T) \propto \sum_{n=0}^{\infty} e^{-nk/\lambda T} = \sum_{n=0}^{\infty} e^{-nhc/\lambda \boltzmann T}\]
    where we use \(k = hc/\boltzmann\).
    This shows that \(u_{\lambda}(T)\) follows the Boltzmann distribution with \(E = nch/\lambda = nhf\).
    This assumes discrete energy levels quantised by \(nhf = nhc/\lambda\).
    With these constants we also have
    \[\sigma = \frac{2\pi^5\boltzmann^4}{15 h^3 c^2}.\]
    
    
    \section{Phase Diagrams}
    A phase diagram shows the different phases (states) of matter on some set of axes.
    The simplest phase diagram is for an ideal gas, since an ideal gas, by definition, is always a gas under any conditions, so the phase diagram is simply one phase, `gas', everywhere.
    
    A real phase diagram is more complicated.
    Mathematically this is because, at a phase boundary, the equations of state are in general
    \begin{itemize}
        \item not analytic,
        \item not continuous,
        \item not differentiable.
    \end{itemize}
    This makes mathematics at phase boundaries really hard.
    Phase diagrams can be of any dimensionality.
    For example for pure water the equation of state is in three-dimensional \((P, V, T)\)-space.
    This means that given two state variables we can find the third from the equation of state.
    The phases are separated by curves one dimension down from the dimension of the equation of state, so in the case of water phases are separated by two-dimensional surfaces.
    We can fix one variable, for example if we work at room temperature then the equation of state at room temperature is simply a plane through the equation of state and the phases are now separated by lines in one dimension.
    
    For a physics perspective within a phase we expect that the equation of state will be nice (by this we mean analytic, continuous, and differentiable) since it would be non-physical to have, for example, an instantaneous change of volume by some finite-amount.
    However at the boundaries, say between liquid and gas, we can have such sudden expansions as the liquid vaporises and increases rapidly in volume.
    We label each region of the phase diagram with the phase that is stable at that point.
    This is the phase with the lowest Gibbs free energy at this point.
    Sometimes phases can coexist.
    For example above the critical temperature water can be a liquid or gas and will spontaneously boil.
    This means that liquid is not stable at this point.
    
    Since low Gibbs free energy is favoured and \(G = U - TS + PV\) higher temperatures favour higher entropies, for example gasses, whereas higher pressures favour higher densities (i.e. lower volumes), for example solids.
    
    Some important features of phase diagrams are given here:
    \begin{itemize}
        \item The equilibrium point between two phases is where the free energy is minimised.
        
        \item Conjugate variables are those which appear together in free energy equations, for example \(P\) and \(V\) are conjugate, as are \(T\) and \(S\).
        Typically derivatives involving conjugate variables must be positive, for example \(\inlinepdv{P}{V} > 0\), or equivalently the bulk modulus is positive, as if this where not the case then the free energy could be spontaneously decreased by volume collapse and everything would shrink to nothingness.
        Similarly \(\inlinepdv{S}{T} > 0\), or equivalently the heat capacity is positive.
        
        \item A triple point is where solid, liquid, and gas coexist.
        These are unique points on a two dimensional phase diagram.
        For example the triple point of water is at approximately \SI{273.1575}{\kelvin} (\SI{0.0075}{\degreeCelsius}) and \SI{611.657}{\pascal} (\SI{0.006037}{\atm}).
        
        \item A sublimation line is the solid-gas boundary.
        A substance crossing this point will move between solid and gas without ever becoming liquid.
        For example at room temperature and pressure solid carbon dioxide will sublimate, this is the reason that dry ice is dry.
    \end{itemize}

    At a phase boundary two parts of a system, one part for each phase, are in equilibrium.
    This means that \(P\) and \(T\) are constant throughout the system.
    The relevant potential is then the Gibbs free energy, \(\dd{G} = V\dd{P} - S\dd{T}\).
    A system in mechanical (pressure) and thermal (temperature) equilibrium minimises the Gibbs free energy.
    For an irreversible process \(\dd{G} < 0\) and for a reversible process \(\dd{G} = 0\).
    If instead \(T\) and \(V\) where in equilibrium then the relevant potential would be the Helmholtz free energy, \(\dd{F} = -P\dd{V} - S\dd{T}\).
    
    \subsection{Forbidden Properties}
    We can show that some properties are thermodynamically impossible (i.e. they violate the second law).
    For example the bulk modulus,
    \[K_S = -V\pdvconst{P}{V}{S},\]
    is always positive.
    Consider the system split into two parts, \(A\) and \(B\), by a constant \(S\) and \(V\) boundary.
    If \(A\) grows by \(\Delta V\) then \(B\) must shrink by \(\Delta V\).
    Thus Taylor expanding
    \[\Delta U_A = \pdvconst{U}{V}{S}\Delta V + \pdvconst[2]{U}{V}{S}\frac{(\Delta V)^2}{2} + \order{(\Delta V)^3}.\]
    Similarly
    \[\Delta U_B = -\pdvconst{U}{V}{S}\Delta V + \pdvconst[2]{U}{V}{S}\frac{(\Delta V)^2}{2} - \order{(\Delta V)^3}.\]
    So the total energy change is
    \[\Delta U = \Delta U_A + \Delta U_B = \pdvconst[2]{U}{V}{S}(\Delta V)^2 + \order{(\Delta V)^4} = -V\pdvconst{P}{V}{S}\frac{(\Delta V)^2}{V}.\]
    This last part comes from differentiating the central equation with respect to volume.
    By the second law \(U\) is minimised at equilibrium so \(\dd{U} > 0\).
    Volume, \(V\), and \((\Delta V)^2\) are both positive quantities.
    This means that
    \[K_S = \frac{V\Delta U}{(\Delta V)^2} > 0.\]
    
    Similarly if we consider the same system at constant volume then
    \[\dd{U} = \dd{U_A} + \dd{U_B} = \pdvconst[2]{U}{S}{V}(\Delta S)^2 = \frac{1}{T}\pdvconst{T}{S}{V}T(\Delta S)^2 = \frac{T(\Delta S)^2}{c_V}.\]
    Since \(\dd{U}, T, (\Delta S) > 0\) we must also have that \(c_V > 0\).
    
    \subsection{Coexisting Phases}
    Suppose we have a system that has two coexisting phases.
    For example ice in water.
    The total Gibbs free energy, \(G\) is
    \[G = g_1M_1 + g_2M_2\]
    where \(g_i\) and \(M_i\) are the Gibbs free energy and mass of the relevant phase.
    Since mass is conserved \(\dd{M_1} = -\dd{M_2}\).
    Minimising \(G\) at equilibrium we have \(\dd{G} = 0\) so
    \[\dd{G} = g_1\dd{M_1} + g_2\dd{M_2} = (g_1 - g_2)\dd{M_1} = 0.\]
    This means that \(g_1 = g_2\).
    So two phases can coexist if they have the same Gibbs free energy.
    Conversely if \(g_1 > g_2\) then material will transfer from phase 1 to phase 2 until equilibrium is reached.
    The fact that there are three variables of interest, \((P, V, T)\), and two constraints, the equation of state and \(g_1 = g_2\), means that the coexistence can happen only along a line in \((P, T)\)-space.
    
    If instead a system is at constant \(T\) and \(V\) then we minimise the Helmholtz free energy, \(F = U - TS\) meaning that \(\dd{F} = P\dd{V} - S\dd{T} = 0\) at equilibrium.
    Our constraint is then \(V = \nu_1M_1 + \nu_2M_2\) where \(\nu_i\) is the density of the relevant phase.
    Typically this will lead to multiple lines, one for each phase.
    These lines can be linked by a common tangent line in \((V, F)\)-space, the gradient of which gives the pressure at coexistence:
    \[\pdvconst{F}{V}{T} = P.\]
    Notice that \(f_1 \ne f_2\) necessarily at equilibrium.
    Compression occurs when material transfers to a denser phase.
    Along an isotherm (\(\dd{T} = 0\)) at coexistence (\(\dd{G} = 0\)) is
    \[\dd{P} = \frac{1}{V}(\dd{G} + S\dd{T}) = 0.\]
    So pressure is constant along the coexistence line.
    This means that material can compress without changing pressure at this point which means that the isothermal compressibility at a phase boundary is infinite.
    
    \subsection{Van der Waal's Gas Phase Diagram}
    Van der Waal's equation of state can be written as
    \[Pv^3 - (Pb + RT)v^2 + av - ab = 0\]
    where \(v = V/n\) is the volume per molecule and \(n\) is the number of molecules in the sample.
    This is a cubic which means that in general \(v(P, T)\) is multi-valued.
    The critical point occurs when the three roots of this cubic coincide.
    It can be shown that the critical point occurs at
    \[vu_C = 3b, \qquad P_C = \frac{a}{27b^2}, \qquad\text{and}\qquad T_C = \frac{8a}{27Rb}.\]
    For \(T > T_C\) \(v(P, T)\) is single valued.
    The Boyle temperature,
    \[T_B = \frac{a}{Rb} = \frac{27}{8}T\]
    is the point when a Van der Waal's gas behaves most like an ideal gas.
    At lower temperatures intermolecular forces become important, characterised by \(a\), and at higher temperatures the volume of the molecules becomes important, characterised by \(b\).
    
    We can rewrite the Van der Waal's equation in a dimensionless form using the critical point:
    \[\left(\frac{P}{P_C} + \frac{3}{(v/v_C)^3}\right)\left(3\frac{v}{v_C} - 1\right) = 8\frac{T}{T_C}.\]
    
    \subsubsection{Maxwell's Construction}
    To deal with the multi-valued nature of an isotherm Maxwell replaced the problem part of the isotherm with a straight line such that the area below the line and above the isotherm is equal to the area above the line and below the isotherm.
    This essentially flattens out the curvy part of the cubic in a way that keeps \(\dd{G} = 0\) since integrating the density along the isotherm with respect to the pressure gives the area between the line and the isotherm and is zero if the line is constructed properly.
    This is shown in figure~\ref{fig:maxwell's construction}.
    
    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.6]{maxwell_construction.png}
        \caption{A \((P, V)\)-diagram showing an isotherm for a Van der Waal's gas as well as Maxwell's construction. The two shaded areas are equal.}
        \label{fig:maxwell's construction}
    \end{figure}

    \section{More Phase Diagrams}
    \subsection{More Forbidden Properties}
    In the last section we saw that applying the second law to conjugate variables \(P\) and \(V\), and \(T\) and \(S\) that \(K_S\), the bulk modulus, and \(C_V\), are always positive.
    We showed this by considering the change in \(U\) of a two part system.
    If instead of \(U\) we consider \(F\) and the conjugate variables \(P\) and \(V\) then we find that \(K_T > 0\).
    If instead of \(U\) we consider \(H\) and the conjugate variables \(T\) and \(S\) then we find that \(C_P > 0\).
    
    This only applies for conjugate variables, for example there is no equivalent restriction on \(\inlinedv{V}{T}\), we know this because water below \SI{4}{\degreeCelsius} will contract as it cools whereas water above \SI{4}{\degreeCelsius} will expand.
    This means that depending on the temperature \(\inlinedv{V}{T}\) can be positive or negative.
    
    We can view these restrictions as an application of the central equation, \(\dd{U} = T\dd{S} - P\dd{V}\), and the second law of thermodynamics.
    We cannot increase \(T\) and \(S\) at the same time as this would cause a spontaneous increase in \(\dd{U}\), which is forbidden by the second law.
    In general if a bulk modulus, \(K\), is negative then this would allow for heat to transfer to work, this violates the Kelvin--Planck statement of the second law.
    Similarly if a heat capacity, \(C\), is negative then it would be possible to move heat from cold to hot, this violates the Clausius statement of the second law.
    
    \subsection{Stress and Strain}
    Stress, \(\sigma_{ij}\), and strain, \(\varepsilon_{ij}\), are three-dimensional second-order tensors related by the three-dimensional fourth order elasticity tensor, \(C_{ijkl}\), by the equation
    \[\sigma_{ij} = C_{ijkl}\varepsilon_{kl}.\]
    \(\sigma_{ij}\) and \(\varepsilon_{kl}\) are conjugate variables but they can be negative as long as the eigenvalues of \(C_{ijkl}\) are positive.
    For example pulling a bar along the \(x\)-axis will cause its length to increase along the \(x\)-axis but its width along the \(y\) and \(z\) axes will decrease.
    The net effect however will be an increase.
    The elastic energy is
    \[\frac{1}{2}C_{ijkl}\varepsilon_{ij}\varepsilon_{kl}\]
    and this must be positive so no matter what the signs of \(\varepsilon_{kl}\) and \(C_{ijkl}\) (and hence the signs of \(\sigma_{ij}\)) the signs must combine in such a way that the energy is positive.
    
    \subsection{Phase Boundary Slope}
    Consider a \((T, P)\)-phase diagram.
    We know that along a phase boundary separating phases 1 and 2 we must have that the Gibbs free energy is equal for both phases, \(g_1 = g_2\).
    Consider two points on the phase boundary, \((T, P)\) and \((T + \dd{T}, P + \dd{P})\).
    Since both points are on the phase boundary we must have
    \[g_1(T, P) = g_2(T, P), \qquad\text{and}\qquad g_1(T + \dd{T}, P + \dd{P}) = g_2(T + \dd{T}, P + \dd{P}).\]
    If we Taylor expand the second condition then we find that
    \[g_1(T, P) + \pdvconst{g_1}{T}{P}\dd{T} + \pdvconst{g_1}{P}{T}\dd{P} + \dotsb = g_2(T, P) + \pdvconst{g_2}{T}{P}\dd{T} + \pdvconst{g_2}{P}{T}\dd{P} + \dotsb.\]
    Noting that \(g_1(T, P) = g_2(T, P)\) appears on both sides so cancels and then rearranging we have
    \[\left[\pdvconst{g_1}{T}{P} - \pdvconst{g_1}{T}{P}\right]\dd{T} = \left[\pdvconst{g_2}{P}{T} - \pdvconst{g_1}{P}{T}\right]\dd{P}.\]
    Now using \(\dd{g} = v\dd{P} - s\dd{T}\) we get
    \[\pdvconst{g}{T}{P} = -s, \qquad\text{and}\qquad \pdvconst{g}{P}{T} = v.\]
    Hence
    \[[-s_1 + s_2]\dd{T} = [v_2 - v_1]\dd{P}\]
    \[\implies \pdvconst{P}{T}{\mathrm{pb}} = \frac{s_2 - s_1}{v_2 - v_1} = \frac{S_2 - S_1}{V_2 - V_1}\]
    where \(\mathrm{pb}\) indicates the derivative is computed at the phase boundary.
    We can rewrite this using the latent heat, \(\ell = T(s_2 - s_1)\), which gives
    \[\pdvconst{P}{T}{\mathrm{pb}} = \frac{\ell}{T(v_2 - v_1)} = \frac{\ell}{T(V_2 - V_1)}.\]
    This is the \define{Clausius--Clapeyron equation}.
    The latent heat, \(\ell\), is positive for a phase transition from a lower temperature phase to a higher temperature phase.
    Suppose that the lower temperature phase is solid and the higher temperature phase is liquid.
    If \(v_2 > v_1\) then
    \[\pdvconst{P}{T}{\mathrm{pb}} > 0\]
    so the solid expands upon melting (most solids).
    If instead \(v_2 < v_1\) then
    \[\pdvconst{P}{T}{\mathrm{pb}} < 0\]
    so the solid contracts upon melting (e.g. water).
    So the gradient of the solid--liquid phase boundary will be positive on a \((T, P)\)-diagram if the solid expands upon melting will be negative if the solid contracts upon melting.
    
    \subsection{Boiling Point}
    Let \(v_v\) be the specific vapour volume (i.e. the volume of one mole of vapour) and \(v_l\) be the specific liquid volume.
    Typically \(v_v \gg v_l\) and therefore we can approximate \(v_v - v_l \approx v_v\).
    For an ideal gas
    \[v_v = \frac{RT}{P}.\]
    The Clausius--Clapeyron equation then becomes
    \[\pdvconst{P}{T}{\mathrm{pb}} = \frac{\ell}{T(v_v - v_l)} \approx \frac{\ell}{Tv_v} = \frac{\ell}{T}\frac{P}{RT} = \frac{\ell P}{RT^2} \implies \frac{1}{P}\pdvconst{P}{T}{\mathrm{pb}} = \frac{\ell}{RT^2}.\]
    We assume that \(\ell\) is approximately constant over the range of interest and integrate this.
    We get
    \[\ln P = -\frac{\ell}{RT} + C\]
    for some constant of integration, \(C\).
    From this we can see that the boiling point changes linearly with \(\ln P\).
    For example at sea level is \SI{100}{\degreeCelsius}, where the pressure is \SI{0.031}{\mega\pascal} whereas at the top of mount Everest the pressure is only \SI{0.031}{\mega\pascal} (\SI{0.31}{\atm}) and water boils at \SI{70}{\degreeCelsius}.
    This makes it impossible to make good tea up a mountain as the water can't be heated above \SI{70}{\degreeCelsius} without a pressurised vessel.
    
    We can be more precise if we use Trouton's rule, that the entropy of vaporisation is \(10.5R\) and the latent heat is \(\ell = 10.5RT_{\vap}\), which is not constant as the boiling point, \(T_{\vap}\), depends on the pressure.
    The molar entropy of liquids at the boiling point is the same (or similar) for all liquids.
    This is because at this point the entropy due to possible molecular arrangements dominates and other effects, such as the entropy of individual molecules, can be ignored.
    We can measure the enthalpy using the fact that \(g = h - Ts\) and \(\Delta g = 0\) across a phase boundary, which means that
    \[\Delta h_{\vap} = T_{\vap}\Delta s_{\vap}.\]
    Therefore the Clausius--Clapeyron equation becomes
    \[\pdvconst{P}{T}{\mathrm{pb}} \approx \frac{\ell}{T}\frac{P}{RT} \approx \frac{10.5P}{T} \implies \frac{P}{P_0} \approx \left(\frac{T}{T_0}\right)^{10.5},\]
    where \(P_0\) and \(T_0\) are constants to be determined.
    
    \subsection{Slopes and Discontinuities}
    The plot of \(g\) on a \((T, P)\)-diagram will always have a negative slope as
    \[\pdvconst{g}{T}{P} = -s < 0.\]
    The slope is steeper for higher temperatures meaning that at higher temperature phases will have lower Gibbs free energy.
    For example a vapour will have a lower Gibbs free energy than a liquid at temperatures higher than the boiling point (they have the same Gibbs free energy at the boiling point).
    This makes sense as we defined the stable phase (which at temperatures higher than the boiling point is the vapour) as the phase with the lowest Gibbs free energy.
    The slope is steeper at higher \(T\) as
    \[\pdvconst[2]{g}{T}{P} = -\pdvconst{s}{T}{P} = -\frac{c_P}{T} < 0.\]
    If \(T_0\) is the melting point and we have a liquid, \(l\), phase and solid, \(s\), phase then \(g_l(T_0) = g_s(T_0)\) so for \(T < T_0\) we will have \(g_s(T) < g_l(T)\) and vice versa.
    
    There will be a discontinuity in the entropy at \(T_0\):
    \[\Delta s = -\Delta \pdvconst{g}{T}{P}\]
    and since \(s_l > s_s\) we expect that \(\Delta s \ne 0\).
    
    Similar logic will tell us that \(g(P)\) is continuous across a transition and has a discontinuity in its slope at \(P_0\):
    \[\Delta v = -\Delta\pdvconst{g}{P}{T}\]
    and since we expect a finite volume change when the phase changes there is no reason for the slope to be continuous.
    
    Consider the following:
    \[\pdvconst{(G/T)}{T}{P} = \frac{1}{T}\pdvconst{G}{T}{P} - \frac{G}{T^2} = \frac{G + TS}{T^2} = -\frac{H}{T^2}.\]
    Integrating then gives
    \[\int_1^2 \dd{\left(\frac{G}{T}\right)} = \frac{G_2}{T_2} - \frac{G_1}{T_1} = -\int_{1}^{2} \frac{\expected{H}}{T^2} \dd{T}.\]
    Since \(P\), \(V\), \(T\), \(U\), and \(H\) are all only dependent on the positions of molecules and on their velocities we can compute these values with a simulation or a lot of quantum mechanics.
    The variables \(S\), \(F\), and \(G\) cannot be directly computed from observed values.
    
    \section{The Third Law of Thermodynamics}
    What is the entropy at \(T = \SI{0}{\kelvin}\)?
    Previously we defined the entropy as
    \[S = \int_{0}^{T} \frac{\dd{Q}}{T} + S_0.\]
    Thus at \(T = \SI{0}{\kelvin}\) \(S = S_0\) so unless \(S_0\) is know the absolute entropy is not a meaningful quantity.
    This is why we work with differences in entropy and ratios of entropies.
    
    \subsection{Nernst's Statement of the Third Law}
    The Nernst heat theorem is
    \begin{displayquote}[--W. Nernst]
        Consider a system undergoing a process between initial and final equilibrium states as a result of external influences, such as pressure.
        The system experiences a change in entropy.
        This change tends to zero as the temperature characterising the process tends to zero.
    \end{displayquote}
    What this says is that \(\Delta S \to 0\) as \(T \to 0\).
    
    This is backed up by experimental evidence.
    For any exothermic isothermal chemical process \(\Delta H\) increases with temperature and \(\Delta G\) decreases with temperature.
    Nernst posited that at \(T = 0\) \(\Delta G = \Delta H\) meaning
    \begin{align*}
        \Delta G &= G_f - G_i\\
        &= \Delta H - \Delta(ST)\\
        &= H_f - H_i - T(S_f - S_i)\\
        &= \Delta H - T\Delta S
    \end{align*}
    So by the observation that
    \[\lim_{T\to 0} \dv{T}(\Delta H - \Delta G) = 0\]
    we have
    \[0 = \lim_{T\to 0}\left[\dv{T}\Delta H - \dv{T} \Delta H + \dv{T}(T\Delta S)\right] = \lim_{T\to 0}\dv{T}(T\Delta S) = \lim_{T\to 0}\Delta S\]
    The fact that \(\Delta S\to 0\) as \(T\to 0\) is a statement of the \define{third law of thermodynamics}.
    
    \subsection{Planck's Statement of the Third Law}
    Max Planck stated the third Law as
    \begin{displayquote}[--M. Planck]
        The entropy of all perfect crystals is the same at absolute zero and may be taken to be zero.
    \end{displayquote}
    This comes in two parts:
    \begin{enumerate}
        \item All perfect crystals have the same entropy because to be a perfect crystal is to have no disorder.
        \item With no other constraints we are free to choose \(S_0 = 0\).
    \end{enumerate}
    This is supported by experimental evidence.
    As well as this if we look at the statistical definition of entropy,
    \[S = \boltzmann\ln W,\]
    we see that since there is only one way to make a perfect crystal since we take atoms to be indistinguishable so permuting different atoms doesn't make a different crystal, only moving atoms which introduces defects, we have \(S = \boltzmann\ln 1 = 0\).
    
    \subsection{Simon's Statement of the Third Law}
    Sir Francis Simon stated the third law as
    \begin{displayquote}[--F. Simon]
        The contribution to the entropy from each aspect of a system which is in thermodynamic equilibrium disappears at absolute zero.
    \end{displayquote}
    That is the configurational entropy, vibrational entropy, magnetic entropy, etc. all vanish as \(T \to 0\).
    
    \subsection{Vanishing Heat Capacity}
    Consider the heat capacity,
    \[C_V = T\pdvconst{S}{T}{V}.\]
    We can then use that
    \[\dd{T}\ln T = \frac{1}{T} \implies \dd{\ln T} = \frac{\dd{T}}{T}\]
    to get
    \[C_V = \pdvconst{S}{\ln T}{V} \approx \frac{\Delta S}{\Delta \ln T}.\]
    Thus letting \(T\to 0\) we have \(\ln T \to -\infty\), and by the third law \(\Delta S \to 0\).
    This means that there are no constraints on \(\Delta \ln T\), importantly \(\Delta \ln T \ne 0\) so \(C_V \to 0\).
    
    This is true for all materials and all heat capacities (\(C_P\), \(C_B\), etc. all tend to zero for any material as \(T \to 0\)).
    From this we can see that the ideal gas heat capacities, such as \(C_V = 3R/2\), fail at low \(T\) as they do not vanish.
    The Schottky heat capacity, \(C_B(T, B = 0) = b/T^2\) also fails at low temperatures.
    This means that Curie's law fails at low temperatures.
    This means that there cannot be paramagnets at \(T = 0\).
    This is because if there are magnetic moments then as \(T\) decreases eventually the magnetic moments overcome thermal effects and align to form permanent magnets at low temperatures.
    
    \subsection{Vanishing Thermal Expansion Coefficients}
    Consider the thermal expansion coefficient,
    \[\beta = \frac{1}{V}\pdvconst{V}{T}{P} = -\frac{1}{V}\pdvconst{S}{P}{T} \approx -\frac{1}{V}\frac{\Delta S}{\Delta P}\]
    where we have used a Maxwell relation in the second equality.
    As \(T\to 0\) \(\Delta S \to 0\) so \(\beta\to 0\).
    Again this is true for any material.
    
    \subsection{Vanishing Slope of the Phase Boundary}
    Consider the Clausius--Clapeyron equation:
    \[\pdvconst{P}{T}{\mathrm{pb}} = \frac{\Delta S}{\Delta V}.\]
    Clearly this tends to zero as \(T\to 0\).
    Therefore all phase boundaries have zero slope at \(T = 0\).
    This has been experimentally observed.
    
    \subsection{Quantum Statement of the Third Law}
    Consider the statistical definition of entropy, \(S = \boltzmann\ln W\).
    A quantised, finite, amount of energy is needed to leave the ground state.
    The ground state in this context being the state corresponding to \(W = 1\), which is a perfect crystal.
    An infinitesimal temperature change cannot provide this finite amount of energy/
    Therefore the infinitesimal processes at \(T = 0\) cannot change \(W\).
    
    The third law really comes into effect when \(\boltzmann T\) is on a similar scale to the quantisation energy.
    Above this temperature there is enough energy that the third law is not particularly important.
    For a diatomic gas \(C_V = 5R/2\) under standard conditions.
    This is because the vibrational states are inaccessible at room temperature as the vibrational quantisation is of a similar scale to \(\boltzmann T\) when \(T\) is room temperature.
    Thus the third law means that the vibrational entropy is zero at room temperature.
    
    For a metal at a low temperature \(c_V \approx c_P = aT + bT^3\).
    We can associate \(aT\) with the conduction of electrons and \(bT^3\) with the lattice vibrations.
    Using
    \[\frac{c_V}{T} = \pdvconst{S}{T}{V} = a + bT^2\]
    and integrating we have
    \[S(T) = aT + \frac{1}{3}bT^3.\]
    Both contributions clearly tend to zero as \(T \to 0\).
    This corresponds to all electron states below the Fermi energy being occupied and all lattice vibrations (which we model as quantum harmonic oscillators) being in the ground state.
    The contribution of this non-zero zero point energy means that the Debye law, that \(c_V \propto T^3\), is not quite correct.
    
    \subsection{Zeno'z Paradox Statement of the Third Law}
    Zeno'z paradox is the idea that it is impossible to do something because to do it we must first do half of it and to do half of it we must first do a quarter of it and so on for an infinite number of steps.
    This isn't usually true but it is when it comes to the third law.
    We can state the third law as
    \begin{displayquote}
        It is impossible to reach absolute zero in a finite number of processes.
    \end{displayquote}
    We saw this when we considered magnetic cooling.
    
    \subsection{Glasses}
    It looks like the entropy of a glass doesn't go to zero as \(T \to 0\).
    This is because a glass will still have entropy due to disorder.
    Decreasing the temperature only decreases the vibrational energy.
    From this we conclude that glasses aren't in equilibrium and over time we expect a glass to crystallise.
    
    \subsection{Statistical Consideration}
    A \define{microstate} is a possible arrangement of particles at a given instant in a given phase.
    A \define{macrostate} is a group of microstates which correspond to the same thermodynamic quantities, such as \(P\), \(V\), \(T\), etc.
    \define{Ergodicity} means that it is possible to move between microstates.
    \define{Equipartition} means that all microstates are equally likely. Note that ergodicity and equipartition do not necessarily hold.
    
    If \(S = \boltzmann\ln W\) then \(S = 0\) means that \(W = 1\) which means that there is only one arrangement.
    For example, if we are considering arrangements of electrons in an insulator then this corresponds to all fermion states below the Fermi temperature being occupied.
    If instead we are considering a Bose condensate then this corresponds to all bosons being in the ground state.
    The third law states that at \(T = 0\) we have \(S = 0\) so \(W = 1\) at \(T = 0\), i.e. there is only one microstate, the ground state, at \(T = 0\).
    This definition of entropy also shows that negative entropy is not possible as \(W \ge 1\) so \(\ln W \ge 0\).
    The fact that \(S = \boltzmann \ln W\) implies that all states are equally likely.
    
    More generally the Gibbs entropy is defined as \(S = -\boltzmann \sum_i p_i\ln p)i\) where \(p_i\) is the probability of finding the system in microstate \(i\).
    This definition of entropy is beyond the scope of this course however.
    
    As an example of this statistical point of view consider a three particle system with two partitions, \(A\) and \(B\).
    Let \(N_A\) be the number of particles in partition \(A\) and \(N_B\) be the number of particles in partition \(B\).
    Then table~\ref{tab:arrangements of three particle, two partition system} shows all possible microstates and number of macrostates.
    \begin{table}[ht]
        \centering
        \begin{tabular}{ll}\hline
            Number of macrostates & Corresponding microstates\\\hline
            \(W(N_A = 0) = 1\) & \((B, B, B)\)\\
            \(W(N_A = 1) = 3\) & \((A, B, B)\), \((B, A, B)\), \((B, B, A)\)\\
            \(W(N_A = 2) = 3\) & \((A, A, B)\), \((A, B, A)\), \((B, A, A)\)\\
            \(W(N_A = 3) = 1\) & \((A, A, A)\)\\\hline
        \end{tabular}
        \caption{The number of microstates and macrostates of a three particle, two part system.}
        \label{tab:arrangements of three particle, two partition system}
    \end{table}
    In general a system of \(N\) particles the number of particles in \(A\) is
    \[W(N_A = k) = {N \choose k} = \frac{N!}{k!(N - k)!}.\]
    The `average' state is far more likely, in this case the `average' state has \(N_A\approx N_B\).
    As \(N\to\infty\) the peak width of a plot of \(W(N_A = k)\) against \(k\) tends to \(1/\sqrt{N}\)\footnote{See the Statistics part of the Fourier analysis and statistics course, \(k\sim\mathrm{Binom}(p=1/2, n=N)\to\mathrm{Norm}(\mu=n/2, \sigma^2=1/N)\).}.
    Here we have assumed that the three particles are distinguishable so that \((A, B, B)\) is note the same microstate as \((B, A, B)\).
    If this is not the case then \(W(N_A = 1) = W(N_A = 2) = 1\).
    
    
    \section{Phase Transitions}
    \subsection{Gibbs Phase Rule}
    The number of independent variables which are needed to fully describe a thermodynamic equilibrium is \(C + 2 - N_P\) where \(C\) is the number of chemical components (the number of chemical species minus constraints from reaction or charge balancing) and \(N_P\) is the number of phases.
    For example:
    \begin{itemize}
        \item Liquid water has \(C = N_P = 1\) so needs \(1 + 2 - 1 = 2\) independent variables.
        We usually choose \(P\) and \(T\).
        \item An ice/water mix has \(C = 1\) and \(N_P = 2\) so needs \(1 + 2 - 2 = 1\) variable.
        This can be seen as a result of an ice/water mix existing on the phase boundary, which is one-dimensional, so specifying \(T\) fixes \(P\) and vice versa.
        \item At the triple point of water we have \(C = 1\) and \(N_P = 3\) so we need \(1 + 2 - 3 = 0\) variables.
        This can be seen as a consequence of the triple point being a unique point and therefore simply by stating `at the triple point' we have already fixed all thermodynamic variables.
        \item A gaseous mixture of \ce{O2}, \ce{H2}, and \ce{H2O} has \(C = 3\) and \(N_P = 1\) so needs \(3 + 2 - 1 = 4\) variables.
        We typically choose \(T\) and \(P\) as well as the amount of \ce{O2} and the amount of \ce{H2}, which we denote \(N_{\ce{O2}}\) and \(N_{\ce{H2}}\) respectively.
        \item If we consider the same mixture as above but now allow for a chemical reaction,
        \[\ce{O2 + 2H2 <=> 2H2O},\]
        then \(C = 3 - 1\) as we remove a chemical component by requiring this equation to balance, i.e. knowing the initial amounts of all species and also \(N_{\ce{O2}}\) and \(N_{\ce{H2}}\) at some later point we can find \(N_{\ce{H2O}}\), the amount of \ce{H2O}.
    \end{itemize}
    \subsection{Ehrenfest's Order of Phase Transitions}
    At a phase boundary \(g_1 = g_2\).
    Ehrenfest classified phase transitions by there \define{order} as follows:
    \begin{enumerate}
        \item First order phase transitions have the following properties:
        \begin{itemize}
            \item Discontinuous state variables, e.g. \(s_1 \ne s_2\) and \(v_1 \ne v_2\).
            \item Discontinuous first derivatives of free energy, e.g.
            \[\pdvconst{g_1}{T}{P} \ne \pdvconst{g_2}{T}{P}.\]
        \end{itemize}
        \item Second order phase transitions have the following properties:
        \begin{itemize}
            \item Continuous state variables, e.g. \(s_1 = s_2\) and \(v_1 = v_2\).
            \item Discontinuous first derivatives of state variables, e.g.
            \[\pdvconst{s_1}{T}{P} \ne \pdvconst{s_2}{T}{P} \implies c_{P,1} \ne c_{P,2},\]
            \[\pdvconst{v_1}{T}{P} \ne \pdvconst{v_2}{T}{P} \implies \beta_1 \ne \beta 2,\]
            and
            \[\pdvconst{v_1}{P}{T} \ne \pdvconst{v_2}{P}{T} \implies K_1 \ne K_2.\]
            \item Continuous first derivatives of free energy, e.g.
            \[\pdvconst{g_1}{T}{P} = \pdvconst{g_2}{T}{P}.\]
            \item Discontinuous second derivatives of free energy, e.g.
            \[\pdvconst[2]{g_1}{T}{P} \ne \pdvconst[2]{g_2}{T}{P}.\]
        \end{itemize}
        \item Third order phase transitions have the following properties:
        \begin{itemize}
            \item Continuous state variables, e.g. \(s_1 = s_2\) and \(v_1 = v_2\).
            \item Continuous first derivatives of state variables, e.g.
            \[\pdvconst{s_1}{T}{P} = \pdvconst{s_2}{T}{P} \implies c_{P,1} = c_{P,2},\]
            \[\pdvconst{v_1}{T}{P} = \pdvconst{v_2}{T}{P} \implies \beta_1 = \beta 2,\]
            and
            \[\pdvconst{v_1}{P}{T} = \pdvconst{v_2}{P}{T} \implies K_1 = K_2.\]
            \item Discontinuous second order first derivatives of state variables, e.g.
            \[\pdv[2]{s_1}{T}{P} \ne \pdv[2]{s_2}{T}{P}, \qquad\text{and}\qquad \pdv{c_{V,1}}{V} \ne \pdv{c_{V,2}}{V}.\]
            \item Continuous first and second order derivatives of free energy, e.g.
            \[\pdvconst{g_1}{T}{P} = \pdvconst{g_1}{T}{P}, \qquad\text{and}\qquad \pdvconst[2]{g_1}{T}{P} = \pdvconst[2]{g_1}{T}{P}.\]
            \item Discontinuous third order derivatives of free energy, e.g.
            \[\pdvconst[3]{g_1}{T}{P} \ne \pdvconst[3]{g_2}{T}{P}.\]
        \end{itemize}
        \item \(n\)th order phase transitions have the following properties:
        \begin{itemize}
            \item Continuous state variables with the first discontinuous derivative being the \((n - 1)\)th derivative.
            \item Continuous derivatives of the free energy with the first discontinuous derivative being the \(n\)th derivative.
        \end{itemize}
    \end{enumerate}
    
    A first order transition is also called a discontinuous transition.
    Consider an isothermal process at first order phase boundary.
    Then
    \[g_1 = g_2, \qquad \pdvconst{g_1}{T}{P} \ne \pdvconst{g_2}{T}{P}, \qquad \pdvconst{g_1}{P}{T} \ne \pdvconst{g_2}{P}{T}, \qquad s_1 \ne s_2, \qquad\text{and}\qquad v_1 \ne v_2.\]
    
    At a second order phase boundary
    \[\pdvconst{g_1}{T}{P} = \pdvconst{g_2}{T}{P}, \qquad\text{and}\qquad \pdvconst{g_1}{P}{T} = \pdvconst{g_2}{P}{T}.\]
    As well \(s_1 = s_2\) so \(\Delta s = 0\) meaning that the latent heat of this transition is \(l = 0\), and \(v_1 = v_2\) so \(\Delta v = 0\).
    \(s\) and \(v\) are related by the Maxwell relation
    \[\pdvconst{s}{P}{T} = -\pdvconst{v}{T}{P}.\]
    Since \(\dd{s} = \dd{v} = 0\) \(\dd{U} = T\dd{S} - P\dd{V} = 0\) also.
    The Clausius--Clapeyron equation fails at second order (or higher) transitions as we have
    \[\pdvconst{P}{T}{\mathrm{pb}} = \frac{\Delta s}{\Delta 
    v} = \frac{0}{0}.\]
    
    There are two types of second order transitions:
    \begin{enumerate}
        \item \define{Critical fluctuations} -- Regions of the system fluctuate into other phases in an uncorrelated way.
        For example parts of a paramagnet may align in some regions to act like a ferromagnet but on average there will still be no magnetisation unless an external field is applied, which means that this aligning is uncorrelated.
        Gradually these regions grow as the magnet is cooled to the Curie temperature and the regions start joining.
        The transitions then become correlated and the symmetry is broken and the magnetic moments align.
        This happens slowly enough that \(\Delta s = 0\) and clearly \(\Delta v = 0\).
        
        \item \define{Coexistence} -- A system consist of both phases at once and parts of the system move between the phases at a constant rate.
    \end{enumerate}
    
    \subsection{Clausius--Clapeyron Second Order Fix}
    Consider the entropy at two points, \(A = (T, P)\) and \(B = (T + \dd{T}, P + \dd{P})\), on a phase boundary between two states, 1 and 2.
    Travelling along the phase boundary \(\Delta S = 0\).
    Therefore
    \[S_1(T, P) = S_2(T, P), \qquad\text{and}\qquad S_1(T + \dd{T}, P + \dd{P}) = S_2(T + \dd{T}, P + \dd{P}).\]
    If we Taylor expand at \(B\) then we have
    \[S_1(T, P) + \pdvconst{S_1}{T}{P}\dd{T} + \pdvconst{S_1}{P}{T}\dd{P} = S_2(T, P) + \pdvconst{S_2}{T}{P}\dd{T} + \pdvconst{S_2}{P}{T}\dd{P}.\]
    Noting that the first terms on either sides are equal, \((\inlinepdv{S}{T})_P = C_P/T\), and \((\inlinepdv{S}{P})_T = -(\inlinepdv{V}{T})_P = -V\beta\) then
    \[\frac{C_{P,1}}{T}\dd{T} - V\beta_1\dd{P} = \frac{C_{P,2}}{T} - V\beta_2\dd{P}\]
    which gives
    \[\pdvconst{P}{T}{\mathrm{pb}} = \frac{C_{P,1} - C_{P,2}}{TV(\beta_1 - \beta_2)} = \frac{c_{P,1} - c_{P,2}}{Tv(\beta_1 - \beta_2)}.\]
    This is known as the first Ehrenfest equation.
    The second Ehrenfest equation can be derived in a similar way if we expand the volume at \(A\) and \(B\) instead of the entropy.
    The second Ehrenfest equation is
    \[\pdvconst{P}{T}{\mathrm{pb}} = \frac{\beta_2 - \beta_1}{\kappa_2 - \kappa_1}.\]
    Hence the slope of the transition line relates \(\Delta\beta\), \(\Delta\kappa\), and \(\Delta C_P\).
    
    In critical regions, \(\pm\Delta T_{\text{crit}}\), around \(T_C\) the heat capacity is given by \(C_V \propto (T - T_C)^{-\alpha}\) for some positive constant \(\alpha\).
    Thus \(C_V\to\infty\) as \(T \to T_C\).
    A similar correlation between magnetic spins has \(\langle S_i, S_j\rangle \propto r^{-\nu}\to\infty\) as \(r\to 0\).
    This power law dependence means that there is no characteristic scale for these quantities, i.e. they can become arbitrarily large.
    The universality conjecture is that \(\alpha\) and \(\nu\) are the same for all materials.
    From this we see that transitions can be characterised by large fluctuations.
    
    \subsection{Ising Ferro Magnet}
    The Ising model is a simple model of a magnetic transition.
    In this model near the Curie temperature
    \[M \propto (T - T_C)^{\beta}\]
    and
    \[\chi = \dv{M}{B} \propto \left(\frac{c}{T - T_C}\right)^{\gamma}.\]
    On a macroscopic scale we see \(M \to 0\) as the magnet is heated from absolute zero to \(T_C\).
    On a microscopic we see neatly aligned spins lose their alignment as thermal effects start to dominate.
    Near the Curie temperature \(\chi\) becomes infinite, another example of large fluctuations at a transition.
    
    \subsection{Metastability}
    The equation of state for a single phase of a material is a surface in \((P, V, T)\)-space.
    The equation of state for a second phase is a different surface in \((P, V, T)\)-space.
    The equation of state for the material is then a combination of the equations of state for each phase choosing whichever has the lowest Gibbs free energy at any given point.
    The material can thus jump discontinuously from one phase to another.
    It is common for a material to have properties that would put it in one phase but to not yet have made the transition, while in the original phase but with properties that should be in the second phase we say that the material is in a \define{metastable} phase.
    For example water can be cooled below \SI{0}{\degreeCelsius} without freezing, called \define{supercooled} water.
    As soon as the water begins to freeze it will all quickly solidify.
    Similarly water can be heated to above \SI{100}{\degreeCelsius} without boiling, called \define{superheated} water.
    As soon as the water begins to boil it will all quickly vaporise.
    
    
    \subsection{Liquid Helium}
    Liquid helium has two phases.
    When cooled down from a gas the helium first condenses to the phase \ce{He I} which acts like a normal fluid.
    When cooled further since \ce{^4He} is bosonic (spin 0) it can form a Bose condensate where all \ce{^4He} atoms are in the same, zero-entropy, state, called a \define{superfluid}.
    This is called \ce{He II}, it has bizarre properties such as no viscosity.
    In reality the transition to \ce{He II} doesn't happen until the helium is cooled below \(T_{\lambda} = \SI{2.2}{\kelvin}\)\footnote{\(\lambda\) is used to denote the point at which \ce{He I} transitions to \ce{He II} because the phase diagram at this point looks a bit like a \(\lambda\).} and not all helium is \ce{^4He} meaning that we end up with coexisting \ce{He I} and \ce{He II}.
    The viscosity, \(\eta\), of this combination is the harmonic average, \(\eta^{-1} = \eta_{\ce{He I}}^{-1} + \eta_{\ce{He II}}^{-1}\).
    
    \subsection{Superconductivity}
    Many materials when cooled below a certain critical temperature, \(T_c\), have no resistance as electrons pair up forming bosons, called Cooper pairs, which can move unimpeded through the material.
    For example pure indium, \ce{In}, is a superconductor below \(T_c = \SI{3.4}{\kelvin}\).
    This is a coexistence type phase transition as electrons slowly pair up.
    
    \section{Chemical Potential}
    Up until now we have considered almost exclusively closed systems.
    In these systems the number of particles is fixed and therefore we ignore it.
    In an open system the number of particles is not fixed.
    To take this into account we modify the central equation:
    \[\dd{U} = T\dd{S} - P\dd{V} + \mu\dd{N}, \qquad\text{where}\qquad \mu = \pdvconst{U}{N}{S, V}.\]
    Here \(N\) is the number of particles and \(\mu\) is the \define{chemical potential}.
    We can view \(\mu\dd{N}\) as another form of work.
    This is not a particularly useful definition as it is difficult to change the number of particles without also changing the entropy.
    It is more convenient to consider the other chemical potentials.
    They have the same definitions,
    \[F = T - TS, \qquad H = U + PV, \qquad\text{and}\qquad G = H - TS = F + PV,\]
    but now in differential form they have an extra term:
    \begin{align*}
        \dd{F} &= \dd{U} - T\dd{S} + S\dd{T}\\
        &= T\dd{S} - P\dd{V} + \mu\dd{N} - T\dd{S} - S\dd{T}\\
        &= \mu\dd{N} - P\dd{V} - S\dd{T},\\
        \dd{H} &= \dd{U} + P\dd{V} + V\dd{P}\\
        &= T\dd{S} - P\dd{V} + \mu\dd{N} + P\dd{V} + V\dd{P}\\
        &= \mu\dd{N} + T\dd{S} + V\dd{P},
        \shortintertext{and}
        \dd{G} &= \dd{H} - T\dd{S} - S\dd{T}\\
        &= T\dd{S} - P\dd{V} + \mu\dd{N} - T\dd{S} - S\dd{T}\\
        &= \mu\dd{N} + V\dd{P} - S\dd{T}.
    \end{align*}
    This gives alternative definitions of \(\mu\):
    \[\mu = \pdvconst{U}{N}{S, V} = \pdvconst{F}{N}{T, V} = \pdvconst{G}{N}{T, P}.\]
    If there are multiple chemical species present then we assign each a different number, \(N_i\), and potential, \(\mu_i\), and in the above equations make the replacement
    \[\mu\dd{N} \rightarrow \sum_i \mu_i\dd{N_i}.\]
    
    Suppose we have a system with \(N\) particles and we change the system to have \(\alpha N\) particles.
    Then all extensive quantities (quantities that depend on the amount of matter) increase by a factor of \(\alpha\).
    Importantly this is true for the Gibbs free energy, i.e. the Gibbs free energy of \(\alpha N\) particles is \(\alpha\) times the Gibbs free energy of \(N\) particles, or as an equation
    \[\alpha G(P, T, N) = G(P, T, \alpha N).\]
    Differentiating both sides with respect to \(\alpha\) we then have
    \begin{align*}
        G(P, T, N) &= \pdvconst{G(P, T, \alpha N)}{\alpha}{P, T}\\
        &= \pdvconst{G(P, T, \alpha N)}{(\alpha N)}{P, T}\pdvconst{(\alpha N)}{\alpha}{P, T}\\
        &= N\pdvconst{G(P, T, \alpha N)}{(\alpha N)}{P, T}\\
        &= N\pdvconst{G(P, T, \alpha N)}{N}{P, T}\pdvconst{N}{\alpha N}{P, T}\\
        &= N\pdvconst{G(P, T, \alpha N)}{N}{P, T}\left[\pdvconst{\alpha N}{N}{P, T}\right]^{-1}\\
        &= \frac{N}{\alpha}\pdvconst{G(P, T, \alpha N)}{N}{P, T}\\
        &= \frac{N}{\alpha}\pdvconst{\alpha G(P, T, N)}{N}{P, T}\\
        &= N\pdvconst{G(P, T, N)}{N}{P, T}\\
        &= N\mu.
    \end{align*}
    So for a pure substance \(G = \mu N\).
    The chemical potential is then \(\mu = G/N\).
    Hence 
    \[\dd{\mu} = \frac{1}{N}\dd{G} + G\dd{\frac{1}{N}}\]
    It can be shown then that
    \[\dd{\mu} = -s\dd{T} + v\dd{P}.\]
    So we can, for a pure substance, write the chemical potential as a function of \(P\) and \(T\) only.
    Also
    \[s = -\pdvconst{\mu}{T}{P, N}, \qquad\text{and}\qquad v = \pdvconst{\mu}{P}{T, N}.\]
    
    \subsection{Equilibration of Two Part System}
    Consider a closed system divided into two parts, \(A\) and \(B\).
    The two parts are separated by a freely moving permeable membrane (i.e. the volume is not constant and particles can move between the sides).
    Since the whole system is isolated total quantities are preserved such as the internal energy, matter, and volume.
    This means that
    \[\dd{U_A} + \dd{U_B} = 0, \qquad\dd{N_A} + \dd{N_B} = 0, \qquad\text{and}\qquad \dd{V_A} + \dd{V_B} = 0.\]
    By the second law we know that
    \[\dd{S_A} + \dd{S_B} \ge 0.\]
    From the central equation we have
    \[\dd{S}(U, V, N) = \frac{1}{T}\dd{U} + \frac{P}{T}\dd{V} - \frac{\mu}{T}\dd{N}.\]
    Splitting each change into the change in the two separate parts and using the fact that the change in \(A\) is equal and opposite to the change in \(B\), e.g. \(\dd{U_A} = -\dd{U_B}\), we have
    \[\left(\frac{1}{T_A} - \frac{1}{T_B}\right)\dd{U_A} + \left(\frac{P_A}{T_A} - \frac{P_B}{T_B}\right)\dd{V_A} - \left(\frac{\mu_A}{T_A} - \frac{\mu_B}{T_B}\right)\dd{N_A} = \dd{S} \ge 0.\]
    At equilibrium we will have \(T_A = T_B\), \(P_A = P_B\), and \(\mu_A = \mu_B\).
    We can then interpret this equation as the three changes that can happen to bring the system into equilibrium:
    \begin{itemize}
        \item The first term corresponds to energy flowing from the hotter section to the cooler section until \(T_A = T_B\), i.e. thermal equilibrium.
        \item The second term corresponds to volume moving from the higher pressure to the lower pressure until \(P_A = P_B\), i.e. mechanical equilibrium.
        \item The third term corresponds to particles flowing from a high chemical potential to a low chemical potential until \(\mu_A = \mu_B\), i.e. chemical equilibrium.
    \end{itemize}
    We see that particles (mass) flow along gradients of chemical potential.
    We can use this as an alternative view of gravity.
    For example planets are made of many layers.
    The formation of these layers can be seen as a result of a chemical potential
    \[\mu = u - Ts + Pv + mgh\]
    where the last term corresponds to gravity and causes a position close to the centre to be more favourable and how favourable depends on the mass of the particle.
    
    An important assumption that we have made here is that non-interacting species in the same system can be treated as independent ideal gases.
    This can be viewed as a consequence of two laws:
    \begin{itemize}
        \item Dalton's law -- The total pressure, \(P\), is equal to the sum of the partial pressures, \(p_i\).
        The partial pressure of a species is the pressure that would occur if that were the only species present in that volume and at the same temperature.
        \item Raoult's law -- Partial pressure is proportional to the concentration meaning
        \[p_iV = N_iRT.\]
    \end{itemize}
    For example we can consider air (ignoring minor constituents) as two ideal gasses.
    One \ce{N2} gas and one \ce{O2} gas with \(N_{\ce{N2}} \approx 4N_{\ce{O2}}\) as there is about four times as much \ce{N2} in the air as there is \ce{O2}.
    This is the same as how we considered photons in a cavity as an infinite number of different systems, one for each wavelength.
    
    \subsection{Solubility}
    Suppose we are dissolving a gas, \(\ce{x}\), in water.
    We wish to know the concentration, \(c_x\), of this gas at equilibrium.
    We do this in an open system with boundary conditions that temperature and pressure are both constant at \(T_0\) and \(P_0\) respectively as well as a constant chemical potential for \(\ce{x}\), \(\mu_x^{(0)}\) which we can achieve by having a constant partial pressure, \(p_x\), for the gas above the water.
    Suppose that the specific enthalpy of solution (i.e. the enthalpy released upon dissolving one mole of \(x\)) is \(\delta h_x\).
    
    By Raoult's law
    \[p_x = P_0\frac{N_x}{N} = P_0c_x.\]
    Then
    \[\mu_x^{(0)} = \mu_x = h - T_0s = \delta h + RT_0\ln\frac{p_x}{P_0}\]
    using the fact that the entropy of a real gas is
    \[s = -R\ln\frac{p_x}{P_0}.\]
    Rearranging this we have
    \[c_x = \exp\left(\frac{\mu_x^{(0)} - \delta h}{RT_0}\right).\]
    If \(\ce{x}\) is insoluble in water then \(\delta h \gg \mu_{x}^{(0)}\), however we still have a non-zero concentration of \(\ce{x}\) in the water.
    If \(\ce{x}\) is soluble in water then \(\delta h < \mu_x^{(0)}\).
    This causes problems because the concentration will then be greater than one.
    This is because the ideal gas assumption is not valid for a soluble gas as there is interaction between the gas and the water.
    
    \subsection{Thermodynamics in Chemistry}
    \textit{This section is non-examinable}
    
    In chemistry molecules can react to form other molecules.
    We can assign a chemical potential for each species.
    For a multiple species system the Gibbs free energy is
    \[\dd{G} = -S\dd{T} + V\dd{P} + \sum_{i} \mu_i\dd{N_i}.\]
    The total Gibbs free energy is also given by
    \[G = \sum_{i} \mu_iN_i \implies \dd{G} = \sum_{i} (N_i\dd{\mu_i} + \mu_i\dd{N_i}).\]
    Combining these two equations for \(\dd{G}\) we have
    \[\sum_{i}N_i\dd{\mu_i} = -S\dd{T} + V\dd{P}.\]
    This is known as the Gibbs--Duhem relation.
    It gives the balance of the concentration of each component.
    
    For a closed system with fixed \(P\) and \(T\) we have \(\dd{P} = \dd{T} = 0\).
    Reactions will occur to minimise the Gibbs free energy which in the case of \(\dd{P} = \dd{T} = 0\) means that at equilibrium
    \[\dd{G} = \sum_{i} \mu_i\dd{N_i} = 0.\]
    The concentrations, \(N_i\), are internal degrees of freedom which we can change to minimise \(G\) even if external degrees of freedom are fixed.
    The values of \(\dd{N_i}\) are constrained by the reaction equation.
    If \(b_i\) moles of \(i\) appear in the left hand side of the reaction equation or \(-b_i\) moles of \(i\) appear in the right hand side of the reaction equation then conservation of matter means that
    \[\sum b_i\dd{N_i} = 0.\]
    For example consider the reaction
    \[\ce{2H2 + O2 <=> 2H2O}\]
    here \(b_{\ce{H2}} = 2\), \(b_{\ce{O2}} = 1\), and \(b_{\ce{H2O}} = -2\) (negative as \ce{H2O} appears on the right hand side) meaning that
    \[2\dd{N_{\ce{H2}}} + \dd{N_{\ce{O2}}} - 2\dd{N_{\ce{H2O}}} = 0.\]
    Combining this with minimising \(G\) and we have at equilibrium that
    \[\sum_i b_i\mu_i = 0.\]
    For our example
    \[2\mu_{\ce{H2}} + \mu_{\ce{O2}} - \mu_{\ce{H2O}}.\]
    So at chemical equilibrium the total chemical potential of the reagents is equal to the total chemical potential of the products.
    Typically we start with more reactants and we react them until equilibrium is achieved.
    
    For an ideal gas with \(N\) held constant we have
    \begin{align*}
        \dd{\mu} &= \frac{1}{N}\dd{G}\\
        &= \dd{g}\\
        &= \pdvconst{(G/N)}{T}{P, N}\dd{T} + \pdvconst{(G/N)}{P}{T, N}\dd{P}\\
        &= \frac{S}{N}\dd{T} + \frac{V}{N}\dd{P}\\
        &= c_P\dd{T} + \frac{RT}{P}\dd{P}.
    \end{align*}
    Integrating over \([0, T]\) and \([P_0, P]\) we get
    \[\mu = c_PT + RT\ln\frac{P}{P_0}.\]
    Notice that \(\mu\) is dependent on \(P \propto N\) so increasing \(N\) at a fixed pressure and temperature will increase \(\mu\).
    
    A dilute solution can be approximated as a mixture of ideal gases.
    For this reason it is called an ideal solution.
    The pressure of the entire solution is then
    \[P = \sum_{i} p_i\]
    and for each individual component
    \[p_iV = N_iRT.\]
    The Gibbs free energy is then
    \[\dd{G} = V\sum_{i} \dd{p_i} - S\dd{T} + \sum_i \mu_i\dd{N_i}.\]
    For an isothermal reaction, \(\dd{T} = 0\), we have
    \[\dd{G} = \sum_i \dd{\mu_i} = \sum_i \frac{RT}{p_i}\dd{p_i}.\]
    Integrating over \([0, \mu_i^{(0)}]\) we get
    \[\mu_i = \mu_i^{(0)} + RT\ln\frac{p_i}{p_i^{(0)}}\]
    where \(\mu_i^{(0)}\) is the chemical potential of species \(i\) at reference pressure \(p_i^{(0)}\) and \(p_i\) is the partial pressure of species \(i\).
    Hence
    \[\sum_i b_i \left[\mu_i^{(0)} + RT\ln\frac{p_i}{p_i^{(0)}}\right] = 0\]
    A bit of rearranging gives
    \[\ln K = \ln\left[\prod_i \left(\frac{p_i}{p_i^{(0)}}\right)^{b_i}\right] = -\frac{1}{RT}\sum_i b_i\mu_i^{(0)}\]
    so
    \[K = \prod_i \left(\frac{p_i}{p_i^{(0)}}\right)^{b_i} = \exp\left[-\frac{1}{RT}\sum_i b_i\mu_i^{(0)}\right].\]
    This is the definition of the equilibrium constant\footnote{Not really a constant, it depends on \(T\)}, \(K\).
    
    At low temperatures (low when comparing \(\boltzmann T\) to the chemical energy)
    \begin{itemize}
        \item \(TS\) is negligible and \(\mu\) is the enthalpy.
        \item \(\sum_i b_i\mu_i\) is then the enthalpy of reaction.
        \item \(K(T)\) looks like the Boltzmann factor, \(\exp(-\Delta E/\boltzmann T)\).
    \end{itemize}
    We can measure \(\mu_i\) \(n\) times (once for each species present) rather than measuring \(K\) \(n!\) times (once for every possible reaction of \(n\) species).
    Clearly this is much easier.
    
    \section{Applications of the Chemical Potential}
    For a system of multiple chemical species the chemical potential of the \(i\)th species is
    \[\mu(T, p_i) = g(T, p_i) = g_0 + RT\ln\frac{p_i}{P_0}.\]
    Here \(g_0\) is the Gibbs free energy at some reference pressure \(P_0\).
    We usually choose \(P_0\) to be the total pressure as then \(p_i/P_0 = x_i\) is the fraction of species \(i\) in the mix, which is one way of measuring the concentration.
    
    The specific entropy of the \(i\)th species is then
    \[s_i = -\pdvconst{g}{T}{P} = R\ln x_i.\]
    The total entropy is then the sum of the entropies weighted by the prevalence of the relevant species\footnote{Sometimes this is given as \(-\sum_i \boltzmann p_i\ln p_i\) where \(p_i\) is the probability of being in the \(i\)th state, which in this case is the same as the ratio of that state to the total number of states.}:
    \[S = -\sum_i x_is_i = \sum x_iR\ln x_i.\]
    This assumes no interactions but is still relatively accurate if there are interactions.
    
    Consider a two component system with substances \(A\) and \(B\).
    The enthalpy is
    \[H = H_0x_Ax_B\]
    where \(H_0\) is some constant characterising the strength of interactions between the substances.
    Notice that if \(x_A\) or \(x_B\) are zero then there is no enthalpy as we don't really have a mixture if the concentration of one component is zero.
    The enthalpy is maximised when \(x_A = x_B = 1/2\), note that \(\sum_i x_i = 1\) from the definition of \(x_i\).
    The Gibbs free energy of this system is
    \[G = H_0x_Ax_B + x_ART\ln x_A + x_BRT\ln x_B = H_0x_A(1 - x_A) + x_ART\ln x_A + (1 - x_A)RT\ln x_A.\]
    Then
    \[\pdvconst{G}{x_A}{T} = \frac{H_0}{RT}(1 - 2x_A) + \ln\frac{x_A}{1 - x_A}.\]
    We seek to minimise Gibbs free energy.
    For this reason we look for extrema of \(G\).
    We find that \((\inlinepdv{G}{x_A})_T = 0\) at \(x_A = 1/2\).
    This is a minimum if \(H_0\) is small or a maximum if \(H_0\) is large.
    In the case that \(H_0\) is large there are two other minima in the interval \([0, 1]\).
    If this is the case then the system minimises the free energy by separating into two phases.
    The phase at the first minima corresponds to a mixture that is mostly substance \(B\) and the phase at the second minima corresponds to a mixture that is mostly substance \(A\).
    These are still mixtures of both substances unless the minima occur at \(x_A = 0, 1\) which doesn't happen.
    One example of a mixture that does this is oil and water.
    We use a convex hull, which is simply a tangent line between the minima, to show the Gibbs free energy when it is minimised.
    This is shown in figure~\ref{fig:gibbs mixture}.
    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.6]{gibbs_of_mixture.png}
        \caption{The Gibbs free energy of a two substance mixture.
        \(x_A\) is the concentration fraction of substance \(A\).
        Plotted for two different values of \(H_0\) to show the two possible shapes.}
        \label{fig:gibbs mixture}
    \end{figure}
    
    At high temperatures the entropy dominates and the system tends towards being an ideal solution, starting out of equilibrium and moving towards an even mix of \(A\) and \(B\).
    The difference in chemical potentials is what causes this motion.
    
    How a mixture separates at a given concentration depends on the curvature of the Gibbs free energy at that concentration.
    If the curvature is negative,
    \[\pdvconst[2]{g}{x_A}{T} < 0,\]
    for example at the maximum of a curve, then separating is spontaneous (i.e. decreases the total Gibbs free energy).
    This is because a splitting the mixture in two and moving part to a lower value of \(x_A\) and part to a higher value of \(x_A\) reduces the net Gibbs free energy.
    On the other hand if the curvature is positive,
    \[\pdvconst[2]{g}{x_A}{T} > 0,\]
    then the same split that decreased the net Gibbs free energy for negative curvature will increase the average Gibbs free energy.
    Suppose, for example, that the mixture is at a concentration that puts it just slightly past the first minimum with a high value of \(H_0\).
    The curvature here is positive.
    The only way for the mixture to separate without increasing the Gibbs free energy is to do it in such a large step that one part of the mixture moves past the maximum and over to the other side.
    This is unlikely to happen so takes a while.
    For this reason the mixture is metastable at this concentration as it will eventually separate but it will take a long time.
    
    The section of the graph with negative curvature is called spinodal and the section with positive curvature is called binodal.
    
    \subsection{Chemical Potential Doing Weird Things}
    The second law says that entropy must increase and that this can happen by moving towards lower chemical potentials.
    However the second law doesn't say how this is to happen and there are some scenarios where something weird happens.
    
    \subsubsection{Ammonia Fountain}
    Ammonia, \ce{NH3}, is very soluble in water.
    Consider the setup in figure~\ref{fig:ammonia fountain}.
    
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{ammonia-fountain}
        \begin{tikzpicture}
            \tikzstyle{apparatus} = [very thick]
            
            \draw[apparatus] (0, 0) -- (0, -2) -- (4, -2) -- (4, 0);
            \draw (0, -0.5) -- (1.95, -0.5);
            \draw (2.05, -0.5) -- (4, -0.5);
            \node[above left] at (4, -2) {\ce{H2O}};
            \draw[color=gray, fill=gray] (2.342020, 1) rectangle (2.05, 0.8);
            \draw[color=gray, fill=gray] (1.657980, 1) rectangle (1.95, 0.8);
            \draw[apparatus] (1.95, -1) -- (1.95, 2);
            \draw[apparatus] (2.05, -1) -- (2.05, 2);
            \draw[apparatus, domain=-70:250, samples=50] plot ({2 + cos(\x)}, {2 + sin(\x)});
            \node at (2, 2.5) {\ce{NH3}};
            
            \draw[apparatus] (2.342020, 1.060307) -- (2.342020, 0.7);
            \draw[apparatus] (1.657980, 1.060307) -- (1.657980, 0.7);
            
            
            \draw[->] (-1, -2) -- (-1, 3) node[left] {\(z\)};
        \end{tikzpicture}
        \caption{The setup for an ammonia fountain.}
        \label{fig:ammonia fountain}
    \end{figure}
    This is called an ammonia fountain because the ammonia dissolves in the water so well that the decrease in Gibbs free energy from this occurring is greater than the increase in Gibbs free energy it would take to move the water to the top flask so the water shoots up the pipe and comes out into the top flask like a fountain.
    The change in Gibbs free energy at constant pressure and temperature is
    \[\dd{g} = \dd{h} - \underbrace{s\dd{T} + v\dd{P}}_{=0} + mg\dd{z}\]
    so as long as \(\dd{h} < mg\dd{z}\) the Gibbs free energy will still decrease when the water moves upwards.
    This same effect will work with any gas and liquid as long as the gas is soluble enough that this inequality holds.
    
    \subsubsection{Liquid Helium Fountain}
    The liquid helium fountain is a similar effect to the ammonia fountain.
    The setup is shown in figure~\ref{fig:liquid He fountain}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{liquid-He-fountain}
        \begin{tikzpicture}
            \tikzstyle{apparatus} = [very thick]
            
            \draw[apparatus] (0, 0) -- (0, -2) -- (4, -2) -- (4, 0);
            \node[above left] at (4, -2) {\small \ce{He II}, \(T_0\), \(P_0\)};
            \fill[pattern=crosshatch dots] (1.5, -0.9) rectangle (2.5, -0.7);
            \draw[apparatus, red] decorate[decoration={name=zigzag}] {(1.5, 0) -- (2.5, 0)};
            \draw[apparatus] (1.9, 1.5) -- (1.5, 1) -- (1.5, -1) (2.5, -1) -- (2.5, 1) -- (2.1, 1.5);
            \draw (0, -0.5) -- (1.5, -0.5);
            \draw (2.5, -0.5) -- (4, -0.5);
            \draw (1.71, 1.3) -- (2.29, 1.3);
            \node at (2, 0.7) {\small \(T_1\), \(P_1\)};
        \end{tikzpicture}
        \caption{The setup for a liquid helium fountain.}
        \label{fig:liquid He fountain}
    \end{figure}
    A bath of \ce{He II}, which is a mixture of \ce{He} and superfluid \ce{He} contains a capsule which has a porous plug that allows only superfluid \ce{He} through.
    The superfluid \ce{He} in the capsule is then heated up shoots out the top of the capsule in a fountain.
    It then falls back down into the bath.
    This process can go on indefinitely as long as we keep supplying heat to the helium.
    Since the superfluid components are in contact through the porous plug they must have the same chemical potential in both the bath and the capsule.
    That is \(\Delta \mu = 0\).
    Since we are heating the gas \(\Delta T \ne 0\) and since \(\Delta \mu = v\Delta P - s\Delta T\) we must have an increase in pressure that counteracts the increase in \(T\), it is this pressure that causes the helium to shoot out into the atmosphere which is at \(P_0 < P_1\).
    
    \subsection{Osmosis}
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{osmosis}
        \begin{tikzpicture}
            \tikzstyle{water} = [opacity=0.5, fill=blue, draw=none]
            \tikzstyle{solute} = [color=red, fill=red]
            \tikzstyle{apparatus} = [very thick]
            
            \draw[water] (0, 0) rectangle (4, 2);
            \draw[apparatus] (0, 0) rectangle (4, 4);
            \draw[apparatus, dashed] (2, 0) -- (2, 4);
            
            \foreach \x / \y in {0.4/0.814, 0.462/0.538, 0.33/1.92, 1.948/0.402, 0.3/1.766, 0.708/1.002, 1.542/0.632, 1.062/1.762, 1.15/1.68, 0.61/1.042, 1.448/1.102, 0.906/0.256, 0.598/1.216, 0.208/1.5, 0.056/0.91, 0.966/0.746, 1.786/1.13, 1.136/0.776, 1.404/0.42, 0.89/0.748, 0.786/0.422, 0.52/0.748, 1.234/1.776, 0.16/1.288, 1.662/0.982, 0.138/0.244, 1.114/1.822, 0.876/0.738, 0.488/1.85, 0.416/0.102, 1.438/1.39, 1.488/1.782, 0.084/0.154, 1.298/0.586, 1.29/0.524, 1.838/0.07, 1.52/1.308, 1.288/0.948, 1.44/0.258, 1.078/1.236, 1.264/0.342, 1.644/0.97, 1.744/1.828, 1.612/0.696, 1.06/1.826, 0.096/0.244, 1.7/1.584, 1.022/0.516, 1.93/1.914, 1.558/1.232, 0.164/1.398, 1.824/0.352, 1.804/0.654, 0.708/1.116, 1.302/1.388, 0.538/1.232, 0.814/1.416, 1.736/0.636, 0.916/1.928, 0.338/0.938, 0.382/0.71, 1.35/1.808, 0.406/1.582, 1.954/1.114, 0.854/1.834, 1.89/0.742, 1.98/1.13, 1.346/1.296, 1.008/1.222, 1.926/0.06, 1.704/0.094, 1.05/1.14, 1.21/0.548, 1.612/0.842, 1.514/1.566, 1.664/0.93, 0.162/1.98, 0.992/0.802, 1.146/1.81, 1.538/0.226, 0.994/0.118, 0.882/1.378, 1.306/0.624, 0.984/1.612, 0.574/1.512, 1.6/1.078, 1.088/0.834, 1.182/0.26, 0.374/1.218, 1.378/0.672, 0.834/1.982, 1.612/0.994, 1.52/0.404, 0.066/1.446, 1.496/1.182, 0.89/1.322, 0.838/0.042, 0.768/1.686, 0.928/0.812, 0.684/0.058} {
                \draw[solute] (\x, \y) circle[radius=0.01cm];
            }
        
            \foreach \x / \y in {2.004/0.138, 2.462/1.114, 2.33/0.876, 3.948/0.488, 2.3/0.416, 2.708/1.438, 3.542/1.488, 3.062/0.064, 3.15/1.298, 2.61/1.29, 3.448/1.838, 2.906/1.52, 2.598/1.288, 2.208/1.44, 2.056/1.078, 2.966/1.264, 3.786/1.644, 3.136/1.744, 3.404/1.612, 2.89/1.06, 2.786/0.076, 2.52/1.7, 3.234/1.022, 2.16/1.93, 3.662/1.558} {
                \draw[solute] (\x, \y) circle[radius=0.01cm];
            }
            
            \begin{scope}[xshift=6cm]
                \draw[water] (0, 0) rectangle (2, 3);
                \draw[water] (2, 0) rectangle (4, 1);
                \draw[apparatus] (0, 0) rectangle (4, 4);
                \draw[apparatus, dashed] (2, 0) -- (2, 4);
                
                \foreach \x / \y in {1.546/0.849, 1.292/1.688, 1.734/0.33, 0.052/2.477, 1.938/0.227, 0.554/0.554, 0.57/0.576, 0.696/0.084, 1.378/1.179, 0.982/0.822, 1.366/0.461, 1.222/0.186, 1.69/2.061, 0.902/0.201, 1.984/0.498, 0.788/0.477, 0.482/1.392, 0.292/0.636, 1.13/0.744, 1.142/2.265, 0.992/1.188, 0.184/0.1, 0.824/1.845, 1.91/1.053, 1.342/1.992, 0.63/2.034, 0.542/1.866, 1.098/0.321, 1.122/1.965, 0.96/1.374, 0.072/2.825, 1.658/0.534, 1.956/0.264, 1.19/1.557, 1.104/0.881, 0.336/0.75, 1.5/2.151, 0.888/0.588, 0.13/1.77, 1.762/2.709, 1.31/2.688, 1.86/0.345, 1.414/0.366, 0.25/1.712, 0.742/0.327, 0.934/2.055, 1.146/1.671, 0.796/0.681, 0.062/2.055, 1.226/2.256, 1.774/2.019, 0.202/0.273, 1.054/0.452, 0.234/0.087, 0.586/0.756, 0.82/2.823, 0.432/2.385, 0.23/1.797, 1.82/1.368, 1.39/2.925, 0.452/0.0569, 0.082/1.302, 1.108/1.188, 1.056/0.939, 1.766/1.014, 0.122/0.156, 1.342/2.52, 1.224/2.844, 0.316/0.192, 0.242/2.403, 0.298/0.096, 0.408/1.341, 1.046/2.676, 1.68/2.295, 1.256/1.023, 0.064/0.168, 0.756/2.583, 1.508/0.315, 0.126/1.866, 0.98/0.186, 1.87/2.394, 1.83/0.096, 0.41/1.833, 1.242/0.828, 0.97/0.869, 0.164/1.578, 1.912/2.832, 1.84/2.594, 0.744/1.688, 1.22/1.335, 0.82/0.954, 1.378/2.931, 1.466/0.792, 1.952/0.536, 0.066/2.181, 0.902/2.763, 0.966/0.327, 1.848/0.189, 0.698/2.7, 1.504/0.189} {
                    \draw[solute] (\x, \y) circle[radius=0.01cm];
                }
                
                \foreach \x / \y in {3.04/0.235, 2.658/0.141, 3.384/0.256, 2.784/0.474, 3.524/0.0, 2.548/0.062, 3.462/0.346, 2.16/0.051, 3.616/0.643, 2.404/0.128, 2.862/0.379, 2.67/0.51, 3.296/0.275, 2.528/0.044, 3.442/0.391, 2.778/0.918, 3.174/0.309, 3.296/0.442, 2.454/0.666, 2.866/0.665, 3.062/0.546, 2.426/0.327, 2.06/0.107, 3.896/0.888, 3.08/0.238} {
                    \draw[solute] (\x, \y) circle[radius=0.01cm];
                }
            \end{scope}
            \node at (2, -0.5) {Before};
            \node at (8, -0.5) {After};
        \end{tikzpicture}
        \caption{Osmosis -- The spontaneous transport of water across a semi-permeable membrane to a region of higher solute concentration.}
        \label{fig:osmosis}
    \end{figure}
    \define{Osmosis} is the spontaneous transport of water across a semi-permeable membrane to a region of higher solute concentration.
    Consider the setup in figure~\ref{fig:osmosis}.
    Here water is placed in a container and divided in two sections, \(A\) on the left and \(B\) on the right, by a semi-permeable membrane which will allow through water and nothing else.
    Sugar is dissolved in the water to a high concentration on the left and a lower concentration on the right.
    The water will, given time, move to the left even though this increases the pressure on this side.
    
    Since the water is in contact with itself the chemical potential of the water in \(A\) must be the same as the chemical potential of the water in \(B\), \(\mu_A^W = \mu_B^W\).
    However the sugar does not have the same chemical potential on both sides, \(\mu_A^S \ne \mu_B^S\).
    The sugar cannot move across the membrane to correct this, but the water can, so it does.
    
    In the simplest case the molecules don't interact with each other and so the chemical potentials are proportional to the pressure as this is the only thing causing change.
    That is \(\mu^W \propto p^W\) and \(p_A^W = p_B^W\).
    The total pressure is \(P = \sum_i p_i\) and the total pressure in \(A\) will be higher than the total pressure in \(B\) by \(p_B^S\).
    
    In a less simple case the molecules can interact and \(\mu^W\) depends on the sugar concentration.
    If the sugar and water attract each other (which they do as sugar is soluble in water) then, considering the difference between terms on left and right, we have \(\Delta\mu^W < 0\) so to have \(\Delta g^W = 0\) we need \(\Delta p^W > 0\).
    
    \subsection{Electrons}
    A metal in its ground state will have all electrons occupying states up to the Fermi energy, \(E_F\).
    If two metals with different Fermi energies are placed in electrical contact then electrons will flow from the metal with the higher Fermi energy to the metal with the lower Fermi energy until the two equilibrate.
    The voltage this creates will be
    \[V = \frac{W_1 - W_2}{e}\]
    where \(e\) is the charge of an electron and \(W_i\) is the work done to move an electron from infinity \((E = 0)\) to the metal \((E = E_F)\) for metal \(i\).
    
    \subsection{Hurricanes}
    Hurricanes behave much like a heat engine.
    The ocean is the hot reservoir (\(T \approx \SI{300}{\kelvin}\)) and the top of the atmosphere is the cold reservoir (\(T \approx \SI{200}{\kelvin}\)).
    The work done is used creating wind.
    The cycle approximates the Carnot cycle:
    \begin{itemize}
        \item Isothermal expansion -- Air spirals towards the eye of the storm and absorbs water.
        \item Adiabatic expansion -- Hot air rises to a lower pressure.
        \item Isothermal compression -- Water vapour condenses and falls as rain releasing latent heat.
        \item Adiabatic compression -- In theory the air drops back to sea level, in practice the air simply leaves the system like an exhaust.
    \end{itemize}
    The initial spinning is caused by the Coriolis force causing the air to spin a little and then the heat engine is self perpetuating causing more and more spinning.
    
    \subsection{Thermoelectric Material}
    A thermoelectric material is one where the chemical potential of the electrons, \(\mu_e\), depends on the temperature.
    If a temperature gradient is applied then the electrons will flow either along the gradient or against it (holes flowing along it) depending on the nature of the material.
    This can be used to make a thermoelectric heat engine.
    Two thermoelectric materials with opposite behaviour are placed between a hot and cold heat reservoir.
    One end of the materials is placed in electrical contact and the other ends are connected by a circuit.
    The thermoelectric material in which the electrons flow with the temperature gradient will push materials from the hot end to the cold end.
    From here they can flow round the circuit into the cold end of the other thermoelectric material which will push the electrons from the cold end to the hot end where they are added back onto the first thermoelectric material for the whole process to start again.
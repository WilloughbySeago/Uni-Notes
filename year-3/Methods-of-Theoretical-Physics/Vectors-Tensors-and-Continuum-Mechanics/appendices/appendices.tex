\section{Column Matrices and Vectors}\label{app:column matrices and vectors}
Often we write things of the form \(A\vv{x} = \vv{y}\) where \(A\in\nxmMatrices{n}{n}{\reals}\) and \(\vv{x}, \vv{y}\in\reals^n\).
Strictly there is no defined multiplication between matrices and vectors.
Instead we consider \(\vv{x}\) and \(\vv{y}\) as elements of \(\nxmMatrices{n}{1}{\reals}\), that is as \(n\times 1\) column matrices.
We then perform the usual matrix multiplication.

The reason that this is possible is that \(\nxmMatrices{n}{1}{\reals}\) is a real vector space of dimension \(1n = n\) (in general \(\nxmMatrices{n}{m}{\reals}\) is a real vector space of dimension \(nm\)).
In this vector space vector addition is given by matrix addition and scalar multiplication is just scalar multiplication of a scalar and matrix.
It can be shown that any two real vector spaces of the same dimension are in fact isomorphic.
What this means is that the two sets have all the same important structural features, the only thing that changes between them is what an element looks like.
An isomorphism between the two sets is trivially given by
\begin{align*}
    \varphi\colon\nxmMatrices{n}{1}{\reals} &\to \reals^n\\
    X = (x_{i1}) &\mapsto \vv{x} = x_{i1}\ve{i} = x_i\ve{i}
\end{align*}
so \(\nxmMatrices{n}{1}{\reals}\cong\reals^n\).
Because of this isomorphism we don't usually bother to distinguish between \(\nxmMatrices{n}{1}{\reals}\) and \(\reals^n\).

\section{Groups}\label{app:groups}
\begin{definition}{Group}{}
    A group, \((G, \cdot)\), is a set, \(G\), and a binary operation, \(\cdot\colon G\times G\to G\) which has the following properties:
    \begin{itemize}
        \item Associativity -- For \(g_1, g_2, g_3\in G\) we have \(g_1(g_2g_3) = (g_1g_2)g_3\).
        \item Identity -- There exists \(e\in G\) such that \(eg = ge = g\) for all \(g\in G\).
        \item Inverse -- For each \(g\in G\) there exists \(g^{-1}\in G\) such that \(gg^{-1} = g^{-1}g = e\).
        \item Closure -- For \(g_1, g_2\in G\) \(g_1g_2\in G\).
    \end{itemize}
\end{definition}
A group is the natural structure to use when talking about symmetry as symmetries (that is transformations that leave some property invariant) naturally form a group structure.
The reason for this is quite simple.
If we have a set of symmetries then clearly doing nothing is a symmetry and therefore in this set, this corresponds to the identity.
Any symmetry we apply can be undone to get back to the original state, this corresponds to the inverse.
Any combination of symmetries is another symmetry, this corresponds to closure.
Finally associativity allows us to combine symmetries and then apply them or apply them one at a time and either way we get the same result.

We have mentioned several groups in the main text.
These are matrix groups, meaning the elements of the groups are matrices and the group operation is matrix multiplication.
This is a natural way to talk about transformations.
The groups we have mentioned are \(\generalLinearGroup(n)\), \(\orthogonalGroup(n)\), and \(\specialOrthogonalGroup(n)\).
These are each subgroups, meaning subsets that are also groups, of the previous group in the list.
In the text we stated that \(\generalLinearGroup(n)\) was `the most general' matrix group in \(n\) dimensions.
This is because if we relax the condition and allow matrices with zero determinant then we no longer necessarily have an inverse meaning that \(\nxmMatrices{n}{n}{\reals}\) is not necessarily a group.
On the other hand \(\orthogonalGroup(n)\) and \(\specialOrthogonalGroup(n)\) are both subgroups of \(\generalLinearGroup(n)\) and therefore are more restricted in some way.

\section{Lie Algebras}\label{app:Lie algebras}
A matrix Lie group is a group \(G\subseteq \generalLinearGroup(n, \complex)\) which is closed in the sense that if \(A_n\in G\) is a sequence of matrices and tends to some matrix \(A\) then either \(A\) in \(G\) or \(A \notin \generalLinearGroup(n, \complex)\).
That is a Lie group is a subgroup of invertible matrices such that any sequence either tends to a matrix in \(G\) or to a matrix that is non-invertible.
Lie groups are continuous groups.

If \(G\subseteq \generalLinearGroup(n, \complex)\) is a Lie group then its Lie algebra, \(\lieAlgebra{g}\), is the set of all matrices \(X\in\nxmMatrices{n}{n}{\complex}\) such that \(e^{tX}\in G\) for all \(t\in\reals\).

If \(\lieAlgebra{g}\) is a Lie algebra then it is a vector space and it is closed under the commutator.
That is if \(X, Y\in\lieAlgebra{g}\) then \([X, Y]\in\lieAlgebra{g}\).

What we said above is all true for matrix Lie groups but there are Lie groups that aren't made of matrices.
For these we need a more general definition.
A Lie algebra is a vector space \(\lieAlgebra{g}\) over some field, \(\numset{F}\), along with a binary operation, \([\cdot, \cdot]\colon\lieAlgebra{g}\times\lieAlgebra{g}\to\lieAlgebra{g}\), called the Lie bracket, which satisfies the following axioms:
\begin{itemize}
    \item Bilinearity -- That is \([\cdot, \cdot]\) is linear in its first and second argument so
    \[[ax + by, z] = a[x, y] + b[y, z], \qquad\text{and}\qquad [z, ax + by] = a[z, x] + b[z, y]\]
    for all \(x, y, z\in\lieAlgebra{g}\) and \(a, b\in\numset{F}\).
    \item Alternativity -- For all \(x\in\lieAlgebra{g}\)
    \[[x, x] = 0.\]
    \item The Jacobi identity -- For all \(x, y, z \in\lieAlgebra{g}\)
    \[[x, [y, z]] + [z, [x, y]] + [y, [z, x]] = 0.\]
\end{itemize}
These combined imply a fourth property, anti-commutativity, where for all \(x, y\in\lieAlgebra{g}\)
\[[x, y] = -[y, x].\]
Alternatively if the fields characteristic is not 2 then anti-commutativity implies alternativity.
Examples of Lie brackets includes the commutator and the cross product.

A Lie algebra as defined this way will then generate a Lie group, \(G\), which is all the elements \(e^{tx}\) for \(x\in\lieAlgebra{g}\) and \(t\in\reals\).

Lie groups have lots of nice properties like continuity and smoothness which make them very useful in physics.
A lot can be learned from a lie group by studying its Lie algebra so Lie algebras are also very common.
For example \(\generalLinearGroup(n, \reals)\), \(\generalLinearGroup(n, \complex)\), \(\orthogonalGroup(n)\), and \(\specialOrthogonalGroup(n)\) are all Lie groups with corresponding Lie algebras \(\lieAlgebra{gl}(n, \reals)\), \(\lieAlgebra{gl}(n, \complex)\), \(\lieAlgebra{o}(n)\), and \(\lieAlgebra{so}(n)\).

\section{Tensors, a Mathematicians Definition}\label{app:tensors}
We defined tensors as objects that transform in a certain way under a rotation.
This definition is often given as
\begin{displayquote}
    A tensor is an object that transforms like a tensor.
\end{displayquote}
Clearly this is not a helpful description unless you already know how a tensor transforms.
There is however, a more precise mathematical definition which can be shown to be equivalent.
This definition, at least as much of it as we need for the way tensors are used in this course,\footnote{The full definition of a tensor allows for it to act on the dual space as well and then we distinguish between co- and contravariant indices} is as follows.
\begin{definition}{Tensor}{}
    A rank \(n\) tensor, \(T\), on a vector space, \(V\), with associated field of scalars \(\mathbb{F}\) is a multi-linear function
    \[T\colon V^n \to \mathbb{F}.\]
\end{definition}
By multi-linear function what we mean is
\[T(v_1, v_2, \dotsc, v_i + cw, \dotsc, v_n) = T(v_1, v_2, \dotsc, v_i, \dotsc, v_n) + cT(v_1, v_2, \dotsc, w, \dotsc, v_n)\]
for any \(i\in\{1, 2, \dotsc, n\}\).
Note that \(v_i, w\in V\) are vectors but we will use a notation in this section where vectors get no special treatment when it comes to typesetting.

So how does this gel with the transformation definition?
First we have to define what we mean by the components of a tensor.
To do this we need a basis, \(\{e_i\}\).
We will denote by \(v^{i}_p\) the \(i\)th component of the vector \(v_p\).
By repeated application of linearity we have
\[T(v_1, \dotsc, v_n) = v_1^{i_1}\dotsm v_n^{i_n}T(e_{i_1}, \dotsc, e_{i_n}) = v_1^{i_1}\dotsm v_n^{i_n}T_{i_1\dotsm i_n}.\]
So when we talk of components of a tensor what we actually mean is the tensor evaluated on the basis vectors:
\[T_{i_1\dotsm i_n} = T(e_{i_1}, \dotsc, e_{i_n}).\]
Under a rotation, \(L\in\specialOrthogonalGroup(d)\) the basis vectors transform as \(e'_i = l_{ij}e_{j}\) to give a new basis, \(\basis' = \{e'_j\}\).
So we have
\begin{align*}
    T'_{i_1\dotsm i_n} &= T(e'_{i_1}, \dotsc, e'_{i_n})\\
    &= T(l_{i_1\alpha_1}e_{\alpha_1}, \dotsc, l_{i_n\alpha_n}e_{\alpha_n})\\
    &= l_{i_1\alpha_1}\dotsm l_{i_n\alpha_n}T(e_{\alpha_1}, \dotsc, e_{\alpha_n})\\
    &= l_{i_1\alpha_1}\dotsm l_{i_n\alpha_n} T_{\alpha_1\dotsm\alpha_n}.
\end{align*}
We see that the components in this definition do indeed transform as tensors.
This and the quotient theorem prove that the two definitions of tensors are indeed equivalent.
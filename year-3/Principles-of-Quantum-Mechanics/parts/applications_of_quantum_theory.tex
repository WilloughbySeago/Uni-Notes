    \part{Applications of Quantum Theory}
    \section{Hidden Variables, EPR Paradox and Bell's Inequality}
    \subsection{Hidden Variables}
    For most of the time up until this point we haven't thought too hard about what anything that we are doing actually means.
    We started by stating that the state of a system is specified by its wave function.
    We then went on to say that observables don't have definite values until we measure them.
    The most we can predict in general is the probability of a given result.
    This assumes (correctly) that there is no pre-existing value of the observable.
    This upset a lot of important physicists in early quantum theory, including Einstein who famously said `God does not play dice'.
    
    Einstein believed that there was an underlying reality in which observables have definite values and we are simply ignorant of this deeper reality.
    This idea, called a hidden variables theory, is similar to the assumption that, given infinite computing power, we could deterministically calculate the future of any system and wouldn't have to resort to statistical mechanics.
    While it turns out that hidden variables theories are wrong in showing so we develop several key ideas.
    
    \subsection{EPR Paradox}
    \subsubsection{Background}
    Recall that for a spin 1/2 system when measuring spin along the \(z\)-axis we can get either \(\pm 1/2\).
    This idea is the basis for the Stern--Gerlach experiment.
    The details of how aren't important here but a Stern--Gerlach experiment splits a beam of particles into two beams according to the spin along a given axis.
    For example, figure~\ref{fig:stern-gerlach z} shows a Stern--Gerlach magnet splitting a beam according to its spin along the \(z\)-axis.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{stern-gerlach}
        \begin{tikzpicture}
            \tikzstyle{experiment} = [draw=gray, very thick, fill=lightgray, rounded corners, top color=lightgray, bottom color=lightgray!50]
            \tikzstyle{beam} = [very thick]
            \shadedraw[experiment] (0, 0) rectangle (3, 2);
            \node at (1.5, 1) {SG (\(z\)-axis)};
            \draw[beam, ->] (-1, 1) -- (0, 1);
            \draw[beam, ->] (3, 1.5) -- (4, 1.5);
            \draw[beam, ->] (3, 0.5) -- (4, 0.5);
            \node[right] at (4, 1.5) {\(m = 1/2\)};
            \node[right] at (4, 0.5) {\(m = -1/2\)};
        \end{tikzpicture}
        \caption{Stern--Gerlach experiment splits a beam according to the spin along the \(z\)-axis.}
        \label{fig:stern-gerlach z}
    \end{figure}
    The two beams leaving this machine correspond to the two eigenstates of \(\operator{S}_z\).
    Recall that \(\operator{S}_z\) can be represented as
    \[
        \operator{S}_z \representation \frac{1}{2}\hbar
        \begin{pmatrix}
            1 & 0\\
            0 & -1
        \end{pmatrix}
        .
    \]
    This has two eigenvectors which have the following representations
    \[
        \ket{m = 1/2} \representation 
        \begin{pmatrix}
            1 \\ 0
        \end{pmatrix}
        , \qquad\text{and}\qquad \ket{m = -1/2} \representation 
        \begin{pmatrix}
            0 \\ 1
        \end{pmatrix}
        .
    \]
    It is equally valid to have a Stern--Gerlach experiment measure along the \(x\)-axis.
    The corresponding operator, \(\operator{S}_x\), has the representation
    \[
        \operator{S}_x \representation \frac{1}{2}\hbar
        \begin{pmatrix}
            0 & 1\\
            1 & 0
        \end{pmatrix}
        .
    \]
    This has two eigenvectors which have the following representations
    \[
        \ket{m_x = 1/2} \representation \frac{1}{\sqrt{2}}
        \begin{pmatrix}
            1 \\ 1
        \end{pmatrix}
        ,\qquad\text{and}\qquad \ket{m_x = -1/2} \representation \frac{1}{\sqrt{2}}
        \begin{pmatrix}
            1 \\ -1
        \end{pmatrix}
        .
    \]
    These can be written as a linear combination of eigenstates of \(\operator{S}_z\):
    \begin{align*}
        \frac{1}{\sqrt{2}} 
        \begin{pmatrix}
            1 \\ 1
        \end{pmatrix}
        &= \frac{1}{\sqrt{2}}
        \begin{pmatrix}
            1 \\ 0
        \end{pmatrix}
        + \frac{1}{\sqrt{2}}
        \begin{pmatrix}
            0 \\ 1
        \end{pmatrix}
        ,\\
        \frac{1}{\sqrt{2}} 
        \begin{pmatrix}
            1 \\ -1
        \end{pmatrix}
        &= \frac{1}{\sqrt{2}}
        \begin{pmatrix}
            1 \\ 0
        \end{pmatrix}
        - \frac{1}{\sqrt{2}}
        \begin{pmatrix}
            0 \\ 1
        \end{pmatrix}
        .
    \end{align*}
    Using a Stern--Gerlach magnet oriented in the \(x\) direction we can select the beam containing only particles in the \(\ket{m_x = -1/2}\) state.
    If we then use a second Stern--Gerlach magnet to measure the \(z\) component of the spin then we find that the probability of measuring \(m = 1/2\) is \(\abs{1/\sqrt{2}}^2 = 1/2\).
    This is shown in figure~\ref{fig:stern-gerlach x then z}
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{double-stern-gerlach}
        \begin{tikzpicture}
            \tikzstyle{experiment} = [draw=gray, very thick, fill=lightgray, rounded corners, top color=lightgray, bottom color=lightgray!50]
            \tikzstyle{block} = [fill=gray, color=gray]
            \tikzstyle{beam} = [very thick]
            \shadedraw[experiment] (0, 0) rectangle (3, 2);
            \node at (1.5, 1) {SG (\(x\)-axis)};
            \draw[beam, ->] (-1, 1) -- (0, 1);
            \draw[beam, ->] (3, 1.5) -- (4, 1.5);
            \draw[beam, ->] (3, 0.5) -- (5, 0.5);
            \draw[block] (4, 1) rectangle (4.3, 2);
            \node[below] at (4, 0.5) {\(m_x = -1/2\)};
            \node[above] at (4.15, 2) {\(m_x = 1/2\)};
            \begin{scope}[xshift=5cm, yshift=-0.5cm]
                \shadedraw[experiment] (0, 0) rectangle (3, 2);
                \node at (1.5, 1) {SG (\(z\)-axis)};
                \draw[beam, ->] (-1, 1) -- (0, 1);
                \draw[beam, ->] (3, 1.5) -- (4, 1.5);
                \draw[beam, ->] (3, 0.5) -- (4, 0.5);
                \node[right] at (4, 1.5) {\(m = 1/2\)};
                \node[right] at (4, 0.5) {\(m = -1/2\)};
            \end{scope}
        \end{tikzpicture}
        \caption{Stern--Gerlach experiment splits a beam according to the spin along the \(z\)-axis.}
        \label{fig:stern-gerlach x then z}
    \end{figure}
    We can more generally measure the spin along some unit vector in the \((x, z)\)-plane, \(\vh{n} = (\sin\vartheta, 0, \cos\vartheta)\).
    The corresponding operator for this measurement is
    \[
        \vecoperator{S}\cdot\vh{n} = \operator{S}_x\sin\vartheta + \operator{S}_z\cos\vartheta \representation \frac{1}{2}
        \begin{pmatrix}
            \cos\vartheta & \sin\vartheta\\
            \sin\vartheta & -\cos\vartheta
        \end{pmatrix}
        .
    \]
    This has eigenvectors
    \[
        \ket{m_{\vartheta} = 1/2} \representation
        \begin{pmatrix}
            \cos \frac{\vartheta}{2}\\
            \sin \frac{\vartheta}{2}
        \end{pmatrix}
        , \qquad\text{and}\qquad
        \ket{m_{\vartheta} = -1/2} \representation
        \begin{pmatrix}
            \sin \frac{\vartheta}{2}\\
            \cos \frac{\vartheta}{2}
        \end{pmatrix}
        .
    \]
    
    \subsubsection{The EPR Thought Experiment}
    In 1935 Einstein, Podolsky, and Rosen published a paper discussing a thought experiment that they thought demonstrated something was missing from quantum mechanics.
    We will discuss this thought experiment, known as the \gls{epr} paradox here.
    
    Consider a process where a particle with no spin decays into a pair of spin 1/2 particles, for example a photon could produce an electron-positron pair.
    To conserve momentum these particles will go off in opposite directions from the point where they were created.
    Suppose that we analyse the two particles with Stern--Gerlach magnets, \(A\) and \(B\), which can be orientated in an arbitrary direction.
    We will place \(A\) and \(B\) sufficiently far apart such that within the time taken for the experiment to occur even light cannot get between from \(A\) to \(B\).
    This means that no information can be shared during the experiment.
    This is shown in figure~\ref{fig:epr setup}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{epr-setup}
        \begin{tikzpicture}
            \tikzstyle{experiment} = [draw=gray, very thick, fill=lightgray, rounded corners, top color=lightgray, bottom color=lightgray!50]
            \tikzstyle{beam} = [very thick]
            \draw[beam, <->] (-3, 1) -- (3, 1);
            \shadedraw[experiment] (3, 0) rectangle (6, 2);
            \shadedraw[experiment] (-3, 0) rectangle (-6, 2);
            \node at (4.5, 1) {SG \(B\)};
            \node at (-4.5, 1) {SG \(A\)};
            \draw[fill=black] (0, 1) circle[radius=0.2cm];
            \node[above] (0, 1) {\(S = 0\)};
        \end{tikzpicture}
        \caption{The setup for the \gls{epr} paradox.}
        \label{fig:epr setup}
    \end{figure}
    Suppose that both experiments are aligned to measure the spin along the \(z\)-axis.
    Conservation of angular momentum means that since we started with \(S = 0\) we must have \(S = 0\) at all times.
    The only way for this to happen is if the two particles have equal and opposite spins along the \(z\)-axis.
    Therefore if we measure the spin at \(A\) and get \(\hbar/2\) we know for a fact that at \(B\) they must measure \(-\hbar/2\).
    At first this seems to be the same as classical mechanics which also has conservation of angular momentum.
    However in quantum mechanics the spin isn't defined until we take a measurement.
    This means that when we make the measurement at \(A\) before the measurement the spin is undefined at both \(A\) and \(B\).
    After the measurement the spin is defined at both \(A\) and \(B\).
    Therefore by making a measurement at \(A\) we necessarily change something at \(B\).
    This raises two questions:
    \begin{enumerate}
        \item Is the fact that the outcome at \(B\) is entirely predictable a consequence of the measurement at \(A\)?
        \item If so how does the information about the measurement at \(A\) get to \(B\)?
    \end{enumerate}
    Suppose instead we had \(A\) measuring along the \(x\)-axis and \(B\) measuring along the \(z\)-axis.
    Then the measurement at \(A\) doesn't affect the outcome of the experiment at \(B\) as both of \(\ket{m_x=\pm1/2}\) are a superposition of equal parts of \(\ket{m = \pm1/2}\).
    Suppose instead that \(B\) measures at an angle \(\vartheta\) to the \(z\)-axis in the \((x, z)\)-plane.
    If \(A\) measured \(-\hbar/2\) as the \(x\) spin then we know that the particle at \(B\) must have an \(x\) component of \(\hbar/2\).
    We can then decompose the \(\hbar/2\) state along the \(z\)-axis in terms of the spin along the \(\vartheta\)-axis:
    \begin{align*}
        \begin{pmatrix}
            1\\ 0
        \end{pmatrix}
        &= \cos\frac{\vartheta}{2}
        \begin{pmatrix}
            \cos\frac{\vartheta}{2}\\\sin\frac{\vartheta}{2}
        \end{pmatrix}
        + \sin\frac{\vartheta}{2}
        \begin{pmatrix}
            \sin\frac{\vartheta}{2}\\
            -\cos\frac{\vartheta}{2}
        \end{pmatrix}
        \\
        &= \cos\frac{\vartheta}{2} \ket{m_{\vartheta} = 1/2} + \sin\frac{\vartheta}{2} \ket{m_{\vartheta} = -1/2}.
    \end{align*}
    So measuring along the \(\vartheta\)-axis \(B\) measures spin-up with probability \(\cos^2(\vartheta/2)\) and spin down with probability \(\sin^2(\vartheta/2)\).
    The measurement at \(B\) is still affected by the measurement at \(A\).
    
    The fact that measurement at \(A\) seems to affect measurements at \(B\) regardless of how far apart the two experiments are seems to imply action at a distance or faster than light information transmission.
    The solution that \gls{epr} suggested was that the particles knew before hand (or rather some hidden variables fixed before hand) the spin they would have when measured along any axis.
    However this was shown to be false by John Bell.
    
    \subsection{Bell's Inequality}
    Bell sought to the following question:
    \begin{enumerate}
        \item Is quantum mechanics a realistic local theory?
        \item If not can we make a prediction or observation that can demonstrate this.
    \end{enumerate}
    To answer these he came up with the Bell inequality.
    
    We start with the same setup as with the \gls{epr} paradox.
    We define three co-planar directions, \(\{\vh{n}\vv{_{i}}\}\), along which we can measure the spin.
    
    \subsubsection{Realistic Theory}
    In a realistic local theory the spin 1/2 particles must individually have definite values, \(\pm\hbar/2\), for each direction we can measure.
    Notice that we don't require measurement of all three directions simultaneously, this can't be done as generally spin operators along different axes don't commute.
    We will denote the two possible outcomes along a given axis by \(+\) and \(-\).
    The spin state of a particle can then be denoted \((\pm, \pm, \pm)\) where, for example, \((+, -, +)\) denotes a particle that will have spin \(\hbar/2\) if measured along \(\vh{n}\vv{_1}\) or \(\vh{n}\vv{_3}\) and \(-\hbar/2\) if measured along \(\vh{n}\vv{_2}\).
    We can denote the spin state of the whole system as \((\pm, \pm, \pm, \pm, \pm, \pm)\) which is simply the state of one particle written immediately after the state of the other particle.
    A generic state can then be written as \((\sigma_1, \sigma_2, \sigma_3, \tau_1, \tau_2, \tau_3)\) where \(\sigma_i\) and \(\tau_i\) represent the spin of the particle at \(A\) and \(B\) respectively when measured along \(\vh{n_i}\).
    We denote by \(f(\sigma_1, \sigma_2, \sigma_3, \tau_1, \tau_2, \tau_3)\) the fraction of particle pairs that are in the state \((\sigma_1, \sigma_2, \sigma_3, \tau_1, \tau_2, \tau_3)\).
    Since ew assume that the net spin is \(0\) we have \(f(\sigma_1, \sigma_2, \sigma_3, \tau_1, \tau_2, \tau_3) = 0\) unless \(\sigma_i = -\tau_i\) for all \(i\).
    This satisfies locality as we can set \(\sigma_i\) and \(\tau_i\) when the particles are first produced and close and then nothing changes when we make a measurement.
    
    \subsubsection{Spin Correlations}
    Denote by \(P_{++}(\vh{n}\vv{_i}, \vh{n}\vv{_j})\) the probability that when measuring along \(\vh{n}\vv{_i}\) (\(\vh{n}\vv{_j}\)) at \(A\) (\(B\)) we get \(+\) (\(+\)).
    Then, for example
    \begin{align*}
        P_{++}(\vh{n}\vv{_1}, \vh{n}\vv{_2}) &= \sum_{\sigma_2, \sigma_3} \sum_{\tau_1, \tau_2} f(+, \sigma_2, \sigma_3, \tau_1, +, \tau_3)\\
        &= f(+, -, +, -, +, -) + f(+, -, -, -, +, +)
    \end{align*}
    where we have used the fact that \(f\) is zero unless \(\sigma_i = -\tau_i\),
    Similarly we have
    \begin{align*}
        P_{++}(\vh{n}\vv{_3}, \vh{n}\vv{_2}) &= f(+, -, +, -, +, -) + f(-, -, +, +, +, -),\\
        P_{++}(\vh{n}\vv{_1}, \vh{n}\vv{_3}) &= f(+, +, -, -, -, +) + f(+, -, -, -, +, +).
    \end{align*}
    Since \(f\) are all non-negative we can add these last two results and we get
    \[P_{++}(\vh{n}\vv{_1}, \vh{n}\vv{_2}) \le P_{++}(\vh{n}\vv{_3}, \vh{n}\vv{_2}) + P_{++}(\vh{n}\vv{_1}, \vh{n}\vv{_3}).\]
    
    \subsubsection{Non-Realistic Theory}
    We can now compute the spin correlations that we get from quantum mechanics.
    Suppose \(\vh{n}\vv{_1}\) corresponds to the \(z\)-axis and at \(B\) we choose to measure the spin along the \(\vh{n}\vv{_2}\) direction which is at an angle \(\vartheta\) to the \(z\)-axis.
    If \(\vartheta_{ij}\) is the angle between \(\vh{n}\vv{_i}\) and \(\vh{n}\vv{_j}\) then
    \[P_{++}(\vh{n}\vv{_i}\vh{n}\vv{_j}) = \frac{1}{2}\sin^2\frac{\vartheta_{ij}}{2}, \qquad\text{and}\qquad P_{-+}(\vh{n}\vv{_i}, \vh{n}\vv{_j}) = \frac{1}{2}\cos^2\frac{\vartheta_{ij}}{2}.\]
    If Bell's inequality is to be satisfied then we must have
    \[\sin^2\frac{\vartheta_{12}}{2} \le \sin^2\frac{\vartheta_{23}}{2} + \sin^2\frac{\vartheta_{13}}{2}.\]
    Consider the case that
    \[\vartheta_{13} = \vartheta_{23} = \frac{\vartheta_{12}}{2}.\]
    Then Bell's inequality becomes
    \[\sin^2\vartheta_{13} \le 2\sin^2\frac{\vartheta_{13}}{2}.\]
    This simplifies to
    \[\cos^2\frac{\vartheta_{13}}{2} \le \frac{1}{2}.\]
    This is not satisfied if \(0 \le \vartheta_{13} \le \pi/2\).
    So if quantum mechanics is correct we would expect Bell's inequality to be violated showing it is not a realistic local theory.
    
    \subsubsection{Experimental Tests of Bell's Inequality}
    Now that we know Bell's inequality can differentiate between a realistic local theory and a non-realistic local theory we need to test to see if Bell's theory is ever violated.
    This was first done not with spins but with linearly polarised photons.
    Photons where polarised at \(A\) and \(B\) in directions \(\vh{n}\vv{_i}\) and \(\vh{n}\vv{_j}\) respectively.
    We define a result to be \(+\) when the polarisation is parallel to \(\vh{n}\vv{_i}\) and \(-\) if it is perpendicular.
    The correlation coefficient is defined as
    \[E(\vh{n}\vv{_i}, \vh{n}\vv{_j}) = P_{++}(\vh{n}\vv{_i}, \vh{n}\vv{_j}) + P_{--}(\vh{n}\vv{_i}, \vh{n}\vv{_j}) - P_{-+}(\vh{n}\vv{_i}, \vh{n}\vv{_j}) - P_{+-}(\vh{n}\vv{_i}, \vh{n}\vv{_j}).\]
    This is the expectation value in the singlet state:
    \[\expected{(\vv{\sigma\cdot\vh{n}\vv{_i}})(\vv{\sigma}\cdot\vh{n}\vv{_j})} = \cos\vartheta_{ij}.\]
    In the original experiment the combination
    \[S = E(\vh{n}\vv{_i}, \vh{n}\vv{_j}) - E(\vh{n}\vv{_i}, \vh{n}\vv{'_j}) + E(\vh{n}\vv{'_i}, \vh{n}\vv{_j}) + E(\vh{n}\vv{'_i}, \vh{n}\vv{'_j}).\]
    Where \(\vh{n}\vv{_i}\), \(\vh{n}\vv{'_i}\) and \(\vh{n}\vv{_j}\), \(\vh{n}\vv{'_j}\) represent two different orientations of the polarises at \(A\) and \(B\) respectively.
    
    In a realistic local theory a Bell inequality for this quantity is
    \[-2 \le S \le 2.\]
    Quantum mechanics on the other hand predicts
    \[-2\sqrt{2} \le S \le 2\sqrt{2}.\]
    Orientations where chosen such that
    \[\vh{n}\vv{_i}\cdot\vh{n}\vv{_j} = \vh{n}\vv{'_i}\cdot\vh{n}\vv{_j} = \vh{n}\vv{'_i}\cdot\vh{n}\vv{'_j} = \cos\varphi, \qquad\text{and}\qquad \vh{n}\vv{_i}\cdot\vh{n}\vv{'_j} = \cos3\varphi.\]
    The value of \(S\) was then determined for various values of \(\varphi\) between \(0^\circ\) and \(90^\circ\).
    The results violated Bell's inequality.
    The maximum violation was at \(\varphi = 22.5^\circ\) where they measured \(S = 2.697 \pm 0.015\).
    
    \section{Quantum Teleportation}
    In the last section we introduced the idea of entangled particles.
    These can be used in many different ways to do things that can't be done classically.
    The first use we will consider is quantum teleportation.
    This allows for information about one quantum state to be transferred to another location.
    No matter is transferred, only information.
    This process is called teleportation because if a second particle is placed into the same state that is sent to the remote location then it as if the particle teleported.
    The process requires both quantum and classical communication and the restriction of the classical communication to be slower than the speed of light means that information cannot be transferred faster than the speed of light.
    
    \subsection{Entanglement}
    In classical computing the most basic unit of information is the bit which can be either 0 or 1.
    In quantum computing the most basic unit of information is the \define{qubit} which is a two state system, which can be described by the Hilbert space \(\hilbert_\text{qubit}\).
    Often we identify one state with 0 and the other with 1.
    As a matter of notation in this section we will refer to two states, \(\ket{+}\) and \(\ket{-}\) although later on we will refer to the same two states as \(\ket{0}\) and \(\ket{1}\) respectively in a different context.
    We will often think of the states as electron spins, for example we may identify spin up with \(\ket{+}\) and spin down with \(\ket{-}\).
    We choose electron spins simply because we are familiar with the mathematics behind them but in practice any two state system could be used.
    In this case it is natural to make the following identification with spinor states:
    \[
        \ket{+} \representation 
        \begin{pmatrix}
            1\\ 0
        \end{pmatrix}
        , \qquad\text{and}\qquad
        \begin{pmatrix}
            0\\ 1
        \end{pmatrix}
        .
    \]
    The main difference from a classical bit is that a qubit is in a quantum superposition of the two pure states.
    A generic qubit state, \(\ket{\psi}\in\hilbert_{\text{qubit}}\), is then
    \[\ket{\psi} = a_-\ket{-} + a_+\ket{+}\]
    for some \(a_-, a_+\in\complex\) such that \(\abs{a_-}^2 + \abs{a_+}^2 = 1\).
    We can parametrise this as
    \[\ket{\psi} = \cos\frac{\vartheta}{2}\ket{+} + e^{i\varphi}\sin\frac{\vartheta}{2}\ket{-}\]
    where \(\vartheta\in[0, \pi]\) and \(\varphi\in[0, 2\pi]\).
    We are free to choose the phase of \(\ket{\psi}\) to make \(a_-\) real.
    When we do this we often represent the state as a point on the Bloch sphere as shown in figure~\ref{fig:bloch sphere 2}.
    \tikzexternaldisable
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{bloch-sphere-qubits}
            \begin{blochsphere}[nested=false, radius=3cm, ball=3d]
                \drawBall[radius=3cm, opacity=0.2];
                \drawBallGrid[radius=3cm, style={opacity=0.3}]{30}{30};
                \drawGreatCircle[style={opacity=0.3}]{0}{0}{0};
                \labelLatLon{plus}{90}{0};
                \labelLatLon{minus}{-90}{90};
                \node[above] at (plus) {\(\ket{+}\)};
                \node[below] at (minus) {\(\ket{-}\)};
                \labelLatLon{point 1}{0}{0};
                \labelLatLon{point 2}{0}{90};
                \draw[->] (0, 0) -- (point 1);
                \draw[->] (0, 0) -- (plus);
                \draw[->] (0, 0) -- (minus);
                \draw[->] (0, 0) -- (point 2);
                \node[right] at (point 1) {\(\frac{1}{\sqrt{2}}(\ket{+} + i\ket{-})\)};
                \node[below left] at (point 2) {\(\frac{1}{\sqrt{2}}(\ket{+} + \ket{-})\)};
                \labelLatLon{psi}{30}{30};
                \draw[very thick, ->] (0, 0) -- (psi);
                \node[above right] at (psi) {\(\ket{\psi}\)};
                \coordinate (projection) at ($(psi) - (0, 1.5)$);
                \draw[dashed] (psi) -- (projection);
                \draw[dashed] (0, 0) -- (projection);
                \begin{scope}
                    \clip (0, 0) -- (plus) -- (psi) -- cycle;
                    \draw (0, 0) circle[radius=0.5cm];
                \end{scope}
                \begin{scope}
                    \clip (0, 0) -- (projection) -- (point 2) -- cycle;
                    \draw (0, 0) circle[x radius=0.5cm, y radius=0.3cm];
                \end{scope}
                \node at (0.3,0.6) {\(\vartheta\)};
                \node at (0.35, -0.45) {\(\varphi\)};
                \node at (-4, 0) {};
            \end{blochsphere}
%        \end{tikzpicture}
        \caption{A Bloch sphere parametrised in terms of \(\vartheta\) and \(\varphi\).}
        \label{fig:bloch sphere 2}
    \end{figure}
    \tikzexternalenable
    The next step up from a single bit is a pair of bits.
    Two bits can represent \(2^2 = 4\) different states, 00, 01, 10, and 11.
    The quantum analogue of a two bit system is a two qubit system which is two two state states described by \(\hilbert = \hilbert_{\text{qubit}} \tensorProd \hilbert_{\text{qubit}}\).
    A generic state, \(\ket{\psi}\in\hilbert\), is then given by
    \[\ket{\psi} = a_{--}\ket{--} + a_{-+}\ket{-+} + a_{+-}\ket{+-} + a_{++}\ket{++}\]
    where \(a_{ij\in\complex}\) such that \(\sum_{ij}\abs{a_{ij}}^2 = 1\).
    Each state is simply \(\ket{ij} = \ket{i}\tensorProd\ket{j}\).
    
    Now that we have two systems together we have the possibility for entanglement.
    Call the two systems \(A\) and \(B\).
    An \define{entangled state} between systems \(A\) and \(B\) is any state that can be written as \(\ket{\psi} = \ket{\psi_A}\tensorProd\ket{\psi_B}\) where \(\ket{\psi_A}\) and \(\ket{\psi_B}\) are the states of \(A\) and \(B\) respectively.
    Note that not every state in \(\hilbert\) can be written this way even though \(\hilbert = \hilbert_A\tensorProd\hilbert_B\).
    When discussing entangled states it is often best to work in a basis of entangled states called the \define{Bell states}:
    \[\ket{\Phi^{\pm}} = \frac{\sqrt{2}}{2}(\ket{++} \pm \ket{--}), \qquad\text{and}\qquad \ket{\Psi^{\pm}} = \frac{\sqrt{2}}{2}(\ket{+-} \pm \ket{-+}).\]
    
    \subsection{No Cloning Theorem}
    \begin{theorem}{No Cloning Theorem}{}
        It is impossible to create an identical copy of an arbitrary \emph{unknown} quantum state.
    \end{theorem}
    Let \(\ket{\chi}\in\hilbert\) be an unknown quantum state that we wish to duplicate.
    The duplicate exists with the original so the entire system is described by states in \(\hilbert\tensorProd\hilbert\).
    Suppose we start with some arbitrary normalised state \(\ket{j}\).
    The system then starts in the state
    \[\ket{j}\tensorProd\ket{\chi} = \ket{j}\ket{\chi} = \ket{j\chi} \in\hilbert\tensorProd\hilbert.\]
    The question is can we somehow act on this state to reach the state \(\ket{\chi}\tensorProd\ket{\chi}\)?
    There are two possible things that we can do to change states.
    First we can observe them but this collapses the state and we lose information and, in general, alter \(\ket{\chi}\), so this option isn't useful here.
    The second thing we can do is control the Hamiltonian of the system so that the system evolves with some unitary time evolution operator, \(\operator{U}(t)\), which evolves the system into the desired state.
    This allows for an alternate framing of the no cloning theorem:
    \addtocounter{theoremCounter}{-1}
    \begin{theorem}{No Cloning Theorem (Alternate Formulation)}{}
        There is no unitary operator, \(\operator{U}\), on \(\hilbert\tensorProd\hilbert\) with the action
        \[\operator{U}(\ket{j}\tensorProd\ket{\chi}) = e^{i\varphi}\ket{\chi}\tensorProd\ket{\chi}.\]
        Where \(\ket{\chi}\in\hilbert\) is an \emph{unknown} state and \(e^{i\varphi}\) is some arbitrary phase.
    \end{theorem}
    \begin{proof}
        Suppose that there exists a unitary operator, \(\operator{U}\), on \(\hilbert\tensorProd\hilbert\) such that
        \begin{align*}
            \operator{U}(\ket{j}\tensorProd\ket{\chi}) &= \exp[-i\Theta(j, \chi)](\ket{\chi}\tensorProd\ket{\chi}),
            \shortintertext{and}
            \operator{U}(\ket{j}\tensorProd\ket{\xi}) &= \exp[-i\Theta(j, \xi)](\ket{\xi}\tensorProd\ket{\xi}),
        \end{align*}
        where \(\ket{\chi}, \ket{\xi}\in\hilbert\) are arbitrary states, and \(\Theta(j, \chi)\) and \(\Theta(j, \xi)\) are phases.
        Using \(\operator{U}\hermit\operator{U} = \ident\) we have
        \[\bra{j}\operator{U}\hermit\operator{U}\ket{j} = \braket{j}{j} = 1.\]
        Thus
        \begin{align*}
            \braket{\chi}{\xi} &= \bra{\chi}\tensorProd\ket{j} \operator{U}\hermit\operator{U} \ket{j} \ket{\xi}\\
            &= \exp[i(\Theta(j, \chi) - \Theta(j, \xi))] (\bra{\chi}\tensorProd\bra{\chi}) (\ket{\xi}\tensorProd \ket{\xi})\\
            &= \exp[i(\Theta(j, \chi) - \Theta(j, \xi))] (\braket{\chi}{\xi})^2.
        \end{align*}
        Consider now \(\abs{\braket{\chi}{\xi}}\).
        Since the exponential term is unitary we have \(\abs{\braket{\chi}{\xi}} = \abs{\braket{\chi}{\xi}}^2\) which means either \(\abs{\braket{\chi}{\xi}} = 1\) or \(\abs{\braket{\chi}{\xi}} = 0\).
        Therefore \(\ket{\chi}\) and \(\ket{\xi}\) are either parallel or orthogonal.
        This contradicts the assumption that \(\ket{\chi}\) and \(\ket{\xi}\) are arbitrary states so we conclude that no such operator can exist.
    \end{proof}
    The no cloning theorem means that quantum teleportation is one of the only way to get information about an unknown state from \(A\) to \(B\).
    We could simply move the particle from \(A\) to \(B\) of course but doing this without disturbing the state becomes increasingly difficult as distances and complexities increase.
    One could imagine taking all pertinent information about a state and sending it classically to \(B\) and recreating the state there but this violates the no cloning theorem.
    There is a corollary theorem, called the \define{no teleportation theorem}, which states that an arbitrary quantum system cannot be converted into a system of classical bits.
    If this were possible then we could violate the no cloning theorem.
    Note that the word arbitrary is important here.
    We will see in the next section that quantum teleportation doesn't work with arbitrary states.
    
    The no cloning theorem can be seen, like weird things in quantum mechanics, as a corollary of the uncertainty principle.
    If we could clone an arbitrary state then we could make a large number of copies and perform multiple measurements and measure conjugate variables to arbitrary precision violating the uncertainty relationship between them.
    
    \subsection{Quantum Teleportation}
    Suppose we have two observers, Alice\footnote{\url{https://en.wikipedia.org/wiki/Alice_and_Bob}} or \(A\), and Bob or \(B\).
    Alice has a particle in quantum state \(\ket{\chi}\).
    This may be an electron with a particular spin or a photon with a given polarisation, the details aren't important.
    Alice may not know the precise state of this system.
    Bob, at another location, has an identical particle.
    The goal is for Alice to send complete information about her state, \(\ket{\chi}\), to Bob to input into their identical particle so that Bob ends up with a particle in an identical state.
    This is the process of \define{quantum teleportation}.
    Information is the only thing transferred in quantum teleportation.
    No matter is transferred.
    
    To understand the process of quantum teleportation we will take the simplest case where \(\ket{\chi}\) is a single qubit which we will imagine to be a spin 1/2 electron.
    Quantum teleportation two entangled particles.
    Suppose that they are in the Bell state
    \[\ket{\Psi^-_{AB}} = \frac{\sqrt{2}}{2}[\ket{+_A-_B} - \ket{{-_A}{+_B}}].\]
    Here subscripts denote which observer starts with the relevant particle.
    The details of how this pair of particles are created are not important.
    It could be that Alice or Bob produced them at the start of the experiment or some third party, Charlie, could have provided them.
    The important thing is that the particles are entangled and the entangled state is known.
    Sharing this entangled state between Alice and Bob is called establishing a \define{quantum channel} between them.
    There is also a classical channel such as a phone line, messaging service or bicycle messenger between them, again the details aren't important as long as Alice can somehow send at least two classical bits to Bob.
    
    To recap Alice now has two particles.
    The original particle she wishes to send information about to Bob, which is in state \(\ket{\chi}\), and her entangled particle in state \(\ket{\chi_A}\).
    The most general state that \(\ket{\chi}\) can be in is
    \[\ket{\chi} = c\ket{+} + d\ket{-}\]
    where \(\abs{c}^2 + \abs{d}^2 = 1\).
    The state of the entire, three particle system is
    \begin{align*}
        \ket{\Xi} &= \ket{\chi} \tensorProd \ket{\Psi^-_{AB}}\\
        &= [c\ket{+} + d\ket{-}] \tensorProd \frac{\sqrt{2}}{2}[\ket{+_A-_B} - \ket{-_A+_B}]\\
        &= [c\ket{+} + d\ket{-}] \tensorProd \frac{\sqrt{2}}{2}[\ket{+_A}\tensorProd\ket{-_B} - \ket{-_A}\tensorProd\ket{+_B}]\\
        &= \frac{c\sqrt{2}}{2}[\ket{+}\tensorProd\ket{+_A}\tensorProd\ket{-_B} + \ket{+}\tensorProd\ket{-_A}\tensorProd\ket{+_B}]\\
        &+ \frac{d\sqrt{2}}{2} [\ket{-}\tensorProd\ket{+_A} \tensorProd \ket{-_B} + \ket{-}\tensorProd\ket{-_A}\tensorProd\ket{+_B}].
    \end{align*}
    We can rewrite the state of the two particles that Alice has in terms of Bell states.
    For example
    \[\ket{+}\tensorProd\ket{+_A} = \frac{\sqrt{2}}{2}[\ket{\Phi^+_{A}} + \ket{\Phi^-_{A}}].\]
    If we do this for all other pairs of states then we have
    \begin{align*}
        \ket{\Xi} &= \frac{1}{2}\left[\ket{\Psi^-_{A}} \tensorProd (-c\ket{+_B} - d\ket{-_B}) + \ket{\Psi^+_A} \tensorProd (-c\ket{+_B} + d\ket{-_B}) \right.\stepcounter{equation}\tag{\theequation}\label{eqn:quantum teleportation state}\\
        &\left.+ \ket{\Phi^-_A} \tensorProd (c\ket{-_B} + d\ket{+_B}) + \ket{\Phi^+_A} \tensorProd (c\ket{-_B} - d\ket{+_B})\right].
    \end{align*}
    Both Alice and Bob have enough information to write this expansion.
    Notice that the states of \(B\) are now similar in form to the state \(\ket{\chi}\), in fact the first state is identical to the state of \(\ket{\chi}\) up to total phase of \(-1\).
    The other states are related to \(\ket{\chi}\) in a simple way.
    Simple here meaning that there is a unitary transformation that will take any given state to \(\ket{\chi}\).
    
    At this point Alice makes an observation of her particles and this collapses the wave function.
    Alice will observe one of the four Bell states, \(\ket{\Psi^{\pm}_A}\) and \(\ket{\Phi^{\pm}_A}\).
    Whichever she observes she will send this information to Bob via the classical channel.
    This requires two classical bits of information as there are four Bell states.
    Once Bob receives this information they will know exactly what state their particle is in because they too know the above expansion.
    Bob can then perform the appropriate unitary transformation and move their state to coincide with the state \(\ket{\chi}\).
    So we have sufficiently teleported \(\ket{\chi}\) from Alice to Bob.
    
    Notice that it was necessary for Alice to make an observation which disturbs the state of the particle she started with and so the particle hasn't been cloned as we don't now have two copies.
    Another important feature is that it is not necessary for Alice to know anything about \(\ket{\chi}\).
    If this was required then Alice would have to make measurements which would collapse \(\ket{\chi}\) before it could be teleported.
    If Alice does know the state of her particle then there is really no reason to use quantum teleportation as she can simply send this information classically to Bob.
    However, we will see later that quantum teleportation can be used as the basis for secure communication of information so it may still be useful even if Alice knows the state of her particle.
    
    \section{Secure Communication and Superdense Coding}
    \subsection{Secure Communication}
    One important application of quantum teleportation is securely sending information from one place to another without a third party being able to listen in.
    There are many procedures that have been developed with this goal in mind.
    In this context a procedure is called a \define{protocol}.
    The simplest protocol uses quantum teleportation to send a message that can only be understood if you have one of the entangled particles.
    
    We will consider the protocol called quantum secure direct communication.
    Suppose Alice wants to send a binary message to Bob.
    If she used a classical channel then she risks a third party, Eve, listening in.
    Instead Alice and Bob set up \(N\) pairs of entangled particles for a message of \(N\,\si{bits}\).
    Alice then sets up \(N\) particles in states \(\ket{\chi_i}\) being either \(\ket{+}\) or \(\ket{-}\) to match the \(i\)th bit of the message.
    Alice and Bob then follow the normal quantum teleportation procedure to send information about \(\ket{\chi_i}\) to Bob.
    This protocol is secure because while Eve could listen in to the classical channel and find out what state Alice measures without access to the quantum channel, that is the entangled pair, that knowledge does not provide enough information to know what state \(\ket{\chi_i}\) is.
    This protocol can actually be made more efficient as Bob only cares if \(\ket{\chi_i}\) represents \(+\) or \(-\) and therefore Alice only needs to send one classical bit as the overall phase is not important.
    
    For example, suppose Alice wants to send a \(+\) bit of data to Bob.
    She starts by preparing her state, 
    \[\ket{\chi} = c\ket{+} + d\ket{-},\]
    with \(c = 1\) and \(d = 0\).
    The state of the overall system is given in equation~\ref{eqn:quantum teleportation state}.
    If Alice measures either of \(\ket{\Psi^{\pm}_A}\) and tells Bob she measured one of the \(\Psi\) states then Bob's measurement of their state will give \(+\) and they will know that \(c = 1\).
    Similarly if Alice measures either of \(\ket{\Phi^{\pm}_A}\) and tells Bob she measured one of the \(\Phi\) states then Bob's measurement of their state will give \(-\) and again they will know that \(c = 1\).
    
    Problems that arise when this protocol is used mostly stem from the fact that it is only as secure as the quantum channel is.
    For example if Alice is producing the entangled pairs and sending Bob their entangled particle and Eve can intercept the particle then she can listen in on the classical channel and use the entangled particle to decode the message in the same way Bob would.
    Eve can then send the entangled particle on to Bob to avoid suspicion.
    However there is no way that Eve can do this without making measurements of the particles and interfering with the entanglement.
    Therefore Alice and Bob should test the entangled particles they use to ensure that the correlation still exists to know that no one has interfered.
    
    Another protocol, called remote state preparation, works as follows.
    Suppose Alice wishes to send Bob the state
    \[\ket{\chi} = c\ket{+} + d\ket{-}\]
    and she knows the values of \(c\) and \(d\).
    As usual Alice and Bob must start of sharing a pair of entangled particles.
    Suppose they start in state
    \[\ket{\Psi_{AB}^-} = \frac{\sqrt{2}}{2}(\ket{+_A-_B} - \ket{-_A+_B}).\]
    This state can be expressed in a different basis made of \(\ket{\chi}\) and a state \(\ket{\chi^\perp}\), which is perpendicular to \(\ket{\chi}\).
    For example
    \[\ket{\chi^\perp} = d^*\ket{+} - c^*\ket{-}.\]
    In this basis
    \[\ket{\Psi^-_{AB}} = \frac{\sqrt{2}}{2}(-\ket{\chi_A\chi_B^\perp} + \ket{\chi_A^\perp\chi_B}).\]
    Alice measures her state in this basis.
    If she measures \(\chi_A^\perp\) then Bob will have a particle in the desired state \(\chi_B\).
    If she measures \(\chi_A\) then Bob will have a particle in the state \(\chi_B^\perp\).
    Alice sends one bit along a classical channel to tell Bob which state she measured and then Bob knows which state they have.
    This protocol is also secure in the sense that Eve needs access to both the classical and quantum channels to eavesdrop and even if she does Alice and Bob can tell that their communication has been interfered with.
    
    \subsection{Superdense Coding}
    Superdense coding is a protocol that uses only a quantum channel to communicate classical information.
    In this section we will consider two states, \(\ket{0}\) and \(\ket{1}\), and suppose Alice has a two bit message that she wishes to send Bob.
    Alice and Bob start with an entangled state, for example
    \[\ket{\Phi^+} = \frac{\sqrt{2}}{2}(\ket{00} + \ket{11}),\]
    where the first bit corresponds to the state of Alice's particles and the second to the state of Bob's particle.
    If the message that Alice wants to send is the two bits \((m, n)\) then the protocol is as follows:
    \begin{enumerate}
        \item If \(m = 1\) Alice applies the unitary transformation \(\operator{\sigma}_z\tensorProd\ident\) to the state.
        \item If \(n - 1\) Alice applies the unitary transformation \(\operator{\sigma}_z\tensorProd\ident\) to the state.
        \item Alice sends her particle to Bob without allowing it to change state.
        \item Bob applies the control NOT, or CNOT, operation, \(\operator{C}\), to the state.
        \item Bob applies the Hadamard operation, \(\operator{H}_d\tensorProd\ident\), to the state.
        \item The system will now be in the state \(\ket{mn}\) and Bob simply makes measurements to find out the message.
    \end{enumerate}
    The operators mentioned above have matrix representations.
    There are two representations that we use in this protocol.
    The first is the two dimensional tensor product representation where each state is simply a superposition of tensor products of states.
    Three operators above, \(\operator{\sigma}_z\), \(\operator{\sigma}_x\) and \(\operator{H}_d\) are given in the two dimensional representation:
    \[
        \operator{\sigma}_z \representation
        \begin{pmatrix}
            1 & 0\\
            0 & -1
        \end{pmatrix}
        , \qquad \operator{\sigma}_x \representation
        \begin{pmatrix}
            0 & 1\\
            1 & 0
        \end{pmatrix}
        , \qquad\text{and}\qquad \operator{H}_d \representation \frac{\sqrt{2}}{2}
        \begin{pmatrix}
            1 & 1\\
            1 & -1
        \end{pmatrix}
        .
    \]
    Note that \(\sigma_i\) are simply the Pauli matrices.
    The third operator, \(\operator{C}\), is best described in the four dimensional representation where
    \[
        \lambda\ket{00} + \mu\ket{01} + \nu\ket{10} + \xi\ket{11} \representation
        \begin{pmatrix}
            \lambda\\ \mu\\ \nu\\ \xi
        \end{pmatrix}
        .
    \]
    In this representation
    \[\operator{C} \representation \CNOT\]
    
    Suppose Alice wants to send the message \((m, n) = (1, 1)\) to Bob.
    In the tensor product representation the starting state of the system is
    \[
        \ket{\Phi^+} \representation \frac{\sqrt{2}}{2} \left[\twoVec{1}{0} \tensorProd \twoVec{1}{0} + \twoVec{0}{1} \tensorProd \twoVec{0}{1}\right].
    \]
    Since \(m = 1\) Alice starts by applying \(\operator{\sigma}_z \tensorProd \ident\):
    \begin{align*}
        (\operator{\sigma}_z \tensorProd \ident)\ket{\Phi^+} &\representation \left[\twoMat{1}{0}{0}{-1}\tensorProd \ident\right]\frac{\sqrt{2}}{2} \left[\twoVec{1}{0} \tensorProd \twoVec{1}{0} + \twoVec{0}{1} \tensorProd \twoVec{0}{1}\right]\\
        &= \frac{\sqrt{2}}{2}\left[\twoMat{1}{0}{0}{-1}\twoVec{1}{0} \tensorProd \ident \twoVec{1}{0} + \twoMat{1}{0}{0}{-1}\twoVec{0}{1} \tensorProd \ident \twoVec{0}{1}\right]\\
        &= \frac{\sqrt{2}}{2}\left[\twoVec{1}{0} \tensorProd \twoVec{1}{0} + \twoVec{0}{-1} \tensorProd \twoVec{0}{1}\right].
    \end{align*}
    Next since \(n = 1\) Alice applies \(\operator{\sigma}_x\tensorProd\ident\):
    \begin{align*}
        (\operator{\sigma}_x\tensorProd\ident)(\operator{\sigma}_z \tensorProd \ident)\ket{\Phi^+} &= (\operator{\sigma}_x\operator{\sigma}_z\tensorProd\ident)\ket{\Phi^+}\\
        &\representation \left[\twoMat{0}{1}{1}{0}\tensorProd\ident\right] \frac{\sqrt{2}}{2} \left[\twoVec{1}{0} \tensorProd \twoVec{1}{0} + \twoVec{0}{-1} \tensorProd \twoVec{0}{1}\right]\\
        &= \frac{\sqrt{2}}{2} \left[\twoMat{0}{1}{1}{0} \twoVec{1}{0} \tensorProd \ident \twoVec{1}{0} + \twoMat{0}{1}{1}{0} \twoVec{0}{-1} \tensorProd \ident \twoVec{0}{1}\right]\\
        &= \frac{\sqrt{2}}{2} \left[\twoVec{0}{1} \tensorProd \twoVec{1}{0} + \twoVec{-1}{0} \tensorProd \twoVec{0}{1}\right]\\
    \end{align*}
    Alice then sends this particle to Bob.
    First Bob applies the CNOT operator which is best expressed in the four dimensional representation.
    In this representation the state of the system is
    \[(\operator{\sigma}_x\operator{\sigma}_z\tensorProd\ident)\ket{\Phi^+} \representation \frac{\sqrt{2}}{2}\fourVec{0}{-1}{1}{1}.\]
    So Bob applying the CNOT gives
    \begin{align*}
        \operator{C}(\operator{\sigma}_x\operator{\sigma}_z\tensorProd\ident)\ket{\Phi^+} &\representation \frac{\sqrt{2}}{2}\CNOT\fourVec{0}{-1}{1}{0}\\
        &= \frac{\sqrt{2}}{2}\fourVec{0}{-1}{0}{1}.
    \end{align*}
    Moving back to the two dimensional tensor product representation we have
    \[\operator{C}(\operator{\sigma}_x\operator{\sigma}_z\tensorProd\ident)\ket{\Phi^+} \representation \frac{\sqrt{2}}{2}\left[\twoVec{0}{1} \tensorProd \twoVec{0}{1} + \twoVec{-1}{0} \tensorProd \twoVec{0}{1} \right].\]
    The last step is for Bob to apply \(\operator{H}_d\tensorProd\ident\):
    \begin{align*}
        (\operator{H}_d\tensorProd\ident)\operator{C}(\operator{\sigma}_x\operator{\sigma}_z\tensorProd\ident)\ket{\Phi^+} &\representation \left[\frac{\sqrt{2}}{2} \twoMat{1}{1}{1}{-1}\tensorProd\ident\right] \frac{\sqrt{2}}{2} \left[\twoVec{0}{1} \tensorProd \twoVec{0}{1} + \twoVec{-1}{0} \tensorProd \twoVec{0}{1} \right]\\
        &= \frac{1}{2}\left[\twoMat{1}{1}{1}{-1} \twoVec{0}{1} \tensorProd \ident \twoVec{0}{1} + \twoMat{1}{1}{1}{-1} \twoVec{-1}{0} \tensorProd \ident \twoVec{0}{1} \right]\\
        &= \frac{1}{2}\left[\twoVec{1}{-1} \tensorProd \twoVec{0}{1} + \twoVec{-1}{-1} \tensorProd \twoVec{0}{1} \right]\\
        &= \frac{1}{2} \twoVec{0}{-2}\tensorProd \twoVec{0}{2}\\
        &= \twoVec{0}{-1}\tensorProd \twoVec{0}{1}\\
        &\leftarrow -\ket{11}.
    \end{align*}
    So when Bob measures the state they will find it is, up to a phase, \(\ket{11}\) which corresponds to \((m, n) = (1, 1)\).
    
    \section{Quantum Computing}
    \subsection{Classical Computing}
    A classical computer deals with binary information.
    Everything it does can be boiled down to ones and zeros.
    These can be combined and manipulated in many ways.
    This gives rise to Boolean algebra.
    Any given operation can be expressed in a truth table which simply lists all possible inputs and the corresponding outputs.
    For example the addition of two bits is given by
    \begin{center}
        \begin{tabular}{cc|cc|l}\hline
            \(a\) & \(b\) & \(s\) & \(c\) & Equation \\\hline
            0 & 0 & 0 & 0 & \(0 + 0 = 0\)\\
            0 & 1 & 1 & 0 & \(0 + 1 = 1\)\\
            1 & 0 & 1 & 0 & \(1 + 0 = 1\)\\
            1 & 1 & 0 & 1 & \(1 + 1 = 10_2 = 2_{10}\)\\\hline
        \end{tabular}
    \end{center}
    Here \(a\) and \(b\) are the two inputs, \(s\) is the sum and \(c\) is a bit that carries over to the next position.
    Many operations can be in classical computing can be implemented through the logic gates NOT, AND, OR, XOR, NAND, NOR, and XNOR.
    The truth tables for these gates and the algebraic notation are shown in table~\ref{tab:truth tables}.
    \begin{table}[ht]
        \centering
        \begin{subtable}{0.2\textwidth}
            \centering
            \begin{tabular}{c|c}\hline
                \(a\) & \(\neg a\)\\\hline
                0 & 1\\
                1 & 0\\\hline
            \end{tabular}
            \caption{NOT}
        \end{subtable}
        \begin{subtable}{0.2\textwidth}
            \centering
            \begin{tabular}{cc|c}\hline
                \(a\) & \(b\) & \(a \wedge b\)\\\hline
                0 & 0 & 0\\
                0 & 1 & 0\\
                1 & 0 & 0\\
                1 & 1 & 1\\\hline
            \end{tabular}
            \caption{AND}
        \end{subtable}
        \begin{subtable}{0.2\textwidth}
            \centering
            \begin{tabular}{cc|c}\hline
                \(a\) & \(b\) & \(a \vee b\)\\\hline
                0 & 0 & 0\\
                0 & 1 & 1\\
                1 & 0 & 1\\
                1 & 1 & 1\\\hline
            \end{tabular}
            \caption{OR}
        \end{subtable}
        \begin{subtable}{0.2\textwidth}
            \centering
            \begin{tabular}{cc|c}\hline
                \(a\) & \(b\) & \(a \oplus b\)\\\hline
                0 & 0 & 0\\
                0 & 1 & 1\\
                1 & 0 & 1\\
                1 & 1 & 0\\\hline
            \end{tabular}
            \caption{XOR}
        \end{subtable}
        \begin{subtable}{0.3\textwidth}
            \centering
            \begin{tabular}{cc|c}\hline
                \(a\) & \(b\) & \(\neg(a \wedge b)\)\\\hline
                0 & 0 & 1\\
                0 & 1 & 1\\
                1 & 0 & 1\\
                1 & 1 & 0\\\hline
            \end{tabular}
            \caption{NAND}
        \end{subtable}
        \begin{subtable}{0.3\textwidth}
            \centering
            \begin{tabular}{cc|c}\hline
                \(a\) & \(b\) & \(\neg(a \vee b)\)\\\hline
                0 & 0 & 1\\
                0 & 1 & 0\\
                1 & 0 & 0\\
                1 & 1 & 0\\\hline
            \end{tabular}
            \caption{NOR}
        \end{subtable}
    \begin{subtable}{0.3\textwidth}
        \centering
        \begin{tabular}{cc|c}\hline
            \(a\) & \(b\) & \(\neg(a \oplus b)\)\\\hline
            0 & 0 & 1\\
            0 & 1 & 0\\
            1 & 0 & 0\\
            1 & 1 & 1\\\hline
        \end{tabular}
        \caption{XNOR}
    \end{subtable}
    \caption{The truth tables for the logic gates NOT, AND, OR, XOR, NAND, NOR, and XNOR.}
    \label{tab:truth tables}
    \end{table}
    A computer is then simply a large collection of gates arranged to do more complicated things.
    These can be represented with circuit diagrams where each gate has a unique symbol, shown in figure~\ref{fig:logic gates}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{classical-logic-gates}
        \begin{tikzpicture}
            \node[not port] (NOT) at (0, 0) {};
            \node[and port] (AND) at (3, 0) {};
            \node[or port] (OR) at (6, 0) {};
            \node[xor port] (XOR) at (9, 0) {};
            \node[nand port] (NAND) at (1.5, -2) {};
            \node[nor port] (NOR) at (4.5, -2) {};
            \node[xnor port] (XNOR) at (7.5, -2) {};
            \node at ($(NOT) - (0.5, 1)$) {NOT};
            \node at ($(AND) - (0.5, 1)$) {AND};
            \node at ($(OR) - (0.5, 1)$) {OR};
            \node at ($(XOR) - (0.5, 1)$) {XOR};
            \node at ($(NAND) - (0.5, 1)$) {NAND};
            \node at ($(NOR) - (0.5, 1)$) {NOR};
            \node at ($(XNOR) - (0.5, 1)$) {XNOR};
        \end{tikzpicture}
        \caption{Circuit diagram symbols for NOT, AND, OR, XOR, NAND, NOR, and XNOR.}
        \label{fig:logic gates}
    \end{figure}
    For example the addition operation defined above, called a half adder, can be constructed from an AND gate and an XOR gate as shown in figure~\ref{fig:half adder}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{classical-half-adder}
        \begin{tikzpicture}
            \node[and port] (AND) at (0, 0) {};
            \node[xor port] (XOR) at (0, 2) {};
            \coordinate (a) at ($(XOR.in 1) - (2, 0)$) {};
            \coordinate (b) at ($(XOR.in 2) - (2, 0)$) {};
            \coordinate (a branch) at ($(a)!.6!(XOR.in 1)$) {};
            \coordinate (b branch) at ($(b)!.4!(XOR.in 2)$) {};
            \coordinate (a right) at (a branch|-AND.in 1) {};
            \coordinate (b right) at (b branch|-AND.in 2) {};
            \coordinate (s) at ($(XOR.out) + (1, 0)$) {};
            \coordinate (c) at ($(AND.out) + (1, 0)$) {};
            \node[left] at (a) {\(a\)};
            \node[left] at (b) {\(b\)};
            \draw (a) to[short] (XOR.in 1);
            \draw (b) to[short] (XOR.in 2);
            \draw (a branch) to[short, *-] (a right) to[short] (AND.in 1);
            \draw (b branch) to[short, *-] (b right) to[short] (AND.in 2);
            \draw (XOR.out) to[short] (s) node[right] {\(s\)};
            \draw (AND.out) to[short] (c) node[right] {\(c\)};
        \end{tikzpicture}
        \caption{Half adder}
        \label{fig:half adder}
    \end{figure}
    Comparing the truth tables XOR and AND we see that the outputs correspond to \(s\) and \(c\) respectively from the half adder.
    
    \subsection{Quantum Computing}
    The field of quantum computing essentially follows the ideas of classical computing but instead of electric currents representing bits we work with state vectors.
    Instead of gates and switches we evolve the state vector to the desired output.
    
    Quantum computing first arose from a practical issue.
    Suppose we have a binary gate with two inputs and one output.
    Because this gate is not reversible some information is necessarily lost when it is used.
    The most efficient possible case would be to have one electron go into each input and then have one electron come out and the other dissipate away.
    The loss of information here leads to an entropy increase of \(\ln 2\).
    If the system is at temperature \(T\) then this leads to generation of heat \(\boltzmann T\ln 2\).
    In a real computer far more than one electron is used and this simply increases the heat released.
    While better designs can reduce the heat output of a computer there is still an intrinsic heat release due to information loss that simply cannot be removed.
    Another concern with classical computing is the breakdown of Moore's law.
    Moore's law, in its simplest form, states that the density of transistors on a chip doubles approximately every two years.
    The problem is that there is a quantum limit to this.
    At some point transistors will becomes so small that quantum effects such as tunnelling cause real problems and transistors simply can't be made smaller.
    Since almost all systems, including logic gates, are implemented with transistors this causes some real problems for the future of computing.
    
    One solution to these problems was the suggestion that computers should be designed so that every operation is reversible.
    It was then suggested that quantum mechanics could be exploited to do this and so quantum computing was birthed.
    Another motivation was that quantum computers could be used to simulate quantum mechanical systems realistically in a way that simply isn't possible with classical computers.
    Many algorithms have been designed to work with quantum computers exploiting quantum mechanics to perform certain tasks orders of magnitude faster than is possible with classical computers.
    
    The basic quantum computer works with a set of \(n\)-qubits.
    A full description then uses states in a Hilbert space of dimension \(2^n\).
    The \(n\)-qubits are formed into tensor product states and these sates are acted on by unitary operators leading to various superpositions of the \(2^n\) basis states.
    The idea is then to use these superpositions to cancel out certain states and leave behind only the desired output.
    The goal is to find a combination of basic operations that leads to something computationally useful.
    This is then called a quantum algorithm.
    
    \subsection{Quantum Registers and Logic Gates}
    In a classical computer the bits of data are held in a register and then the data is accessed and combined using logic gates.
    The language used in quantum computing is similar to this however the implementation is not.
    A quantum register is a state vector.
    For example a one qubit register is simply the state of the qubit, \(\ket{\psi} = a_0\ket{0} + a_1\ket{1}\).
    A two qubit register is state of the two qubit system,
    \[\ket{\psi} = a_{00}\ket{00} + a_{01}\ket{01} + a_{10}\ket{10} + a_{11}\ket{11}.\]
    Higher qubit registers are defined similarly.
    Note that a quantum register of multiple qubits is more complicated than simply multiple copies of a single qubit register as it allows for entanglement.
    
    A standard way to think about quantum computing algorithms is to write the quantum register in the form
    \[\sum_{c, t}a_{ct}\ket{c}_n\tensorProd\ket{t}_m.\]
    Here the first terms, \(c\), are called the control or input register and the second terms, \(t\), are called the target or output register.
    These names don't necessarily match with the actual implementation of the algorithm but we aren't concerned about that here.
    The subscripts \(m\) and \(n\) are simply the number of qubits which is also the length of the binary number needed to specify a state.
    For example \(\ket{c}_4\) is an input register that can take one of \(2^4 = 16\) states labelled from 0 to 15.
    
    A quantum logic gate is an operator that acts on the quantum register.
    Since we are interested in reversible computations most of the operators we consider are unitary and therefore preserve the norm of the quantum register.
    Since the time evolution operator is unitary we can think of these gates as simply controlling the Hamiltonian so that the state evolves in a desirable way.
    Non-unitary operators are also sometimes considered.
    For example making a measurement is generally non-unitary.
    It is also possible that interactions with the environment will cause decoherence in a real life application.
    It is possible that non-unitary operators could be used to reverse these effects.
    Since we are only going to look at introductory quantum computing we will consider all operators to be unitary.
    
    One operator that we have already seen is the Hadamard operator, \(\operator{H}_d\).
    This acts on a single qubit.
    In the two dimensional spinor representation this gate is given by
    \[\operator{H}_d\representation \frac{\sqrt{2}}{2}\twoMat{1}{1}{1}{-1}.\]
    This can be written more abstractly as
    \[\operator{H}_d = \frac{\sqrt{2}}{2}(\ketbra{0}{0} + \ketbra{0}{1} + \ketbra{1}{0} - \ketbra{1}{1}).\]
    Gates can also be represented diagrammaticality as a labelled box:
    \begin{center}
        \tikzsetnextfilename{quantum-gate-1}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \matrix[row sep=0.4cm] {
            \node (in) {}; &[0.5cm]
            \node[operator] {\(\operator{H}_d\)}; &[0.5cm]
            \node (out) {.}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in) -- (out);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    The action of this operator on, for example, the state \(\ket{0}\), is then given by
    \begin{center}
        \tikzsetnextfilename{quantum-gate-2}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \matrix[row sep=0.4cm] {
                \node (in) {\(\ket{0}\)}; &[0.5cm]
                \node[operator] {\(\operator{H}_d\)}; &[0.5cm]
                \node (out) {\(\frac{\sqrt{2}}{2}(\ket{0} + \ket{1})\).}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in) -- (out);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    Other gates that we have met are the Pauli gates which in the two dimensional spinor representation are represented by the Pauli spin matrices,
    \[\sigma_x = \twoMat{0}{1}{1}{0}, \qquad \sigma_y = \twoMat{0}{-i}{i}{0}, \qquad\text{and}\qquad \sigma_z = \twoMat{1}{0}{0}{-1}.\]
    These are written more abstractly as
    \[\operator{\sigma}_x = \operator{X} = \ketbra{1}{0} + \ketbra{0}{1}, \qquad \operator{\sigma}_y = \operator{Y} = i\ketbra{1}{0} - i\ketbra{0}{1}, \qquad\text{and}\qquad \operator{\sigma}_z = \operator{Z} = \ketbra{0}{0} - \ketbra{1}{1}.\]
    Or diagrammatically:
    \begin{center}
        \tikzsetnextfilename{quantum-gate-3}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \matrix[row sep=0.4cm] {
                \node (x in) {}; &[0.5cm]
                \node[operator] {\(\operator{X}\)}; &[0.5cm]
                \node (x out) {,}; &&
                \node (y in) {}; &[0.5cm]
                \node[operator] {\(\operator{Y}\)}; &[0.5cm]
                \node (y out) {,}; & and &
                \node (z in) {}; &[0.5cm]
                \node[operator] {\(\operator{Z}\)}; &[0.5cm]
                \node (z out) {.}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (x in) -- (x out);
                \draw[thick] (y in) -- (y out);
                \draw[thick] (z in) -- (z out);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    These can be generalised further.
    Fore example a generalisation of the Pauli-\(Z\) gate is the \(R_\varphi\) gate which in the two dimensional spinor representation is
    \[\operator{R}_\varphi \representation \twoMat{1}{0}{0}{e^{i\varphi}},\]
    or more abstractly
    \[\operator{R}_\varphi = \ketbra{0}{0} + e^{i\varphi}\ketbra{1}{1},\]
    or diagrammatically
    \begin{center}
        \tikzsetnextfilename{quantum-gate-4}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \matrix[row sep=0.4cm] {
                \node (in) {}; &[0.5cm]
                \node[operator] {\(\operator{R}_\varphi\)}; &[0.5cm]
                \node (out) {.}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in) -- (out);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    There are also gates that act on two or more qubits at once.
    For example the CNOT gate represented by
    \[\operator{C}\representation\CNOT,\]
    or more abstractly
    \[\operator{C} = \ketbraResize{00}{00} + \ketbra{01}{01} + \ketbra{10}{11} + \ketbra{11}{10},\]
    or diagrammatically
    \begin{center}
        \tikzsetnextfilename{quantum-gate-5}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \matrix[row sep=0.4cm] {
                \node (in) {}; &[0.5cm]
                \node[operator] {\(\operator{C}\)}; &[0.5cm]
                \node (out) {.}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in) -- (out);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    This gate can also be written in a more detailed form as
    \begin{center}
        \tikzsetnextfilename{quantum-gate-6}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \tikzstyle{phase} = [draw, fill, shape=circle, minimum size=5pt, inner sep=0pt]
            \matrix[row sep=0.4cm, column sep=0.6cm] {
                % First row
                \node[left] (in1) {\(\ket{c}\)}; &[0.5cm]
                \node[phase] (p) {}; &[0.5cm]
                \node[right] (out1) {\(\ket{c}\)}; \\
                % Second row
                \node[left] (in2) {\(\ket{t}\)}; &[0.5cm]
                \node[operator] (C) {\(\operator{C}\)}; &[0.5cm]
                \node[right] (out2) {\(\ket{t\oplus c}\).}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in1) -- (out1);
                \draw[thick] (in2) -- (out2);
                \draw[thick] (p) -- (C);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    Here \(\oplus\) is simply addition modulo two which is the same as XOR.
    We can think of CNOT as the reversible version of XOR as the output is \(c\) and \(t \oplus c\) which gives us enough information to determine \(c\) and \(t\).
    This is also sometimes written as
    \begin{center}
        \tikzsetnextfilename{quantum-gate-7}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \tikzstyle{phase} = [draw, fill, shape=circle, minimum size=5pt, inner sep=0pt]
            \matrix[row sep=0.4cm, column sep=0.6cm] {
                % First row
                \node[left] (in1) {\(\ket{c}\)}; &[0.5cm]
                \node[phase] (p) {}; &[0.5cm]
                \node[right] (out1) {\(\ket{c}\)}; \\
                % Second row
                \node[left] (in2) {\(\ket{t}\)}; &[0.5cm]
                \node[operator] (X) {\(\operator{X}\)}; &[0.5cm]
                \node[right] (out2) {\(\ket{t\oplus c}\).}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in1) -- (out1);
                \draw[thick] (in2) -- (out2);
                \draw[thick] (p) -- (X);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    as we can view \(\operator{C}\) as a block matrix:
    \[\operator{C}\representation \twoMat{\ident}{0}{0}{\operator{X}}.\]
    Note that the order of operators in a diagram is left to right in the order we would think of a circuit passing through gates whereas the order of operators in a product is right to left as with matrix multiplication.
    Hence
    \begin{center}
        \tikzsetnextfilename{quantum-gate-8}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \matrix[row sep=0.4cm, column sep=0.8cm] {
                \node (in) {\(\ket{\psi}\)}; &%[0.5cm]
                \node[operator] {\(\operator{A}\)}; &%[0.5cm]
                \node[operator] {\(\operator{B}\)}; &%[0.5cm]
                \node[right] (out) {\(\operator{B}\operator{A}\ket{\psi}\).}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in) -- (out);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    
    \section{Deutsch Algorithm}
    The Deutsch algorithm is one of the simplest quantum computing algorithms and provides useful insight into the key features of many quantum computing algorithms.
    The Deutsch algorithm aims to answer the following question: Given a function, \(f\colon\{1, 0\} \to \{1, 0\}\), that is a function from a bit to a bit, is \(f(0) = f(1)\), that is, is \(f\) constant?
    To answer this question classically we would need to compute \(f(0)\) and \(f(1)\) and compare the results.
    This corresponds to two measurements.
    The Deutsch algorithm gives us a way to make one measurement and it will tell us if \(f\) is constant or not.
    It should be noted that the actual value is not known after this algorithm, just if it is the same for both inputs or different.
    
    A variable that we wish to investigate in quantum computing is often encoded in an operator, \(\operator{O}\), usually called the \define{oracle}.
    The exact specifics of how this is done depend on the object we want to study.
    In this case we wish to know about \(f\) and we define the oracle as an operator on the register with the action
    \[\operator{O}_f (\ket{c}\tensorProd\ket{t}) = \ket{c}\tensorProd\ket{c \oplus f(t)},\]
    where \(\oplus\) is addition modulo 2.
    This can be represented diagrammatically as
    \begin{center}
        \tikzsetnextfilename{deutsch-algorithm-quantum-gate-1}
        \begin{tikzpicture}
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \tikzstyle{phase} = [draw, fill, shape=circle, minimum size=5pt, inner sep=0pt]
            \matrix[row sep=0.4cm, column sep=0.6cm] {
                % First row
                \node[left] (in1) {\(\ket{c}\)}; &[0.5cm]
                \node[phase] (p) {}; &[0.5cm]
                \node[right] (out1) {\(\ket{c}\)}; \\
                % Second row
                \node[left] (in2) {\(\ket{t}\)}; &[0.5cm]
                \node[operator] (O) {\(\operator{O}_f\)}; &[0.5cm]
                \node[right] (out2) {\(\ket{t\oplus f(c)}\).}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in1) -- (out1);
                \draw[thick] (in2) -- (out2);
                \draw[thick] (p) -- (O);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{center}
    Since the domain and codomain of \(f\) are small we can exactly specify all four possible functions:
    \[
        \begin{array}{c|cc}\hline
            f_i & f_i(0) & f_i(1)\\\hline
            f_1 & 0 & 0\\
            f_2 & 0 & 1\\
            f_3 & 1 & 0\\
            f_4 & 1 & 1\\\hline
        \end{array}
    \]
    We can encode each of these into an oracle, \(\operator{O}_{f_i}\).
    For example the action of \(\operator{O}_{f_2}\) is
    \begin{align*}
        \operator{O}_{f_2}\ket{0}\tensorProd\ket{0} &= \ket{0}\tensorProd\ket{0 \oplus f_2(0)} = \ket{0}\tensorProd\ket{0 \oplus 0} = \ket{0}\tensorProd\ket{0},\\
        \operator{O}_{f_2}\ket{0}\tensorProd\ket{1} &= \ket{0}\tensorProd\ket{1 \oplus f_2(0)} = \ket{0}\tensorProd\ket{1 \oplus 0} = \ket{0}\tensorProd\ket{1},\\
        \operator{O}_{f_2}\ket{1}\tensorProd\ket{0} &= \ket{1}\tensorProd\ket{0 \oplus f_2(1)} = \ket{1}\tensorProd\ket{0 \oplus 1} = \ket{1}\tensorProd\ket{1},\\
        \operator{O}_{f_2}\ket{1}\tensorProd\ket{1} &= \ket{1}\tensorProd\ket{1 \oplus f_2(1)} = \ket{1}\tensorProd\ket{1 \oplus 1} = \ket{1}\tensorProd\ket{0}.
    \end{align*}
    Notice that this is the same action as \(\operator{C}\).
    
    The next step is the actual algorithm which is just a list of operators to apply and a final measurement.
    The process for finding the appropriate operators is mostly trial and error and not very illuminating so we will just give the final algorithm.
    Start with the register \(\ket{0}\tensorProd\ket{0}\), it is common for quantum algorithms to start with a state of the form \(\ket{0}\tensorProd\dotsb\tensorProd\ket{0}\).
    Compute
    \[(\operator{H}_d\tensorProd\ident)\operator{O}_{f} (\operator{H}_d\tensorProd\operator{H}_d) (\operator{X}\tensorProd\operator{X}) (\ket{0}\tensorProd\ket{0}).\]
    Measure the state.
    If you get \(\ket{c} = \ket{0}\) then the function is constant.
    If \(\ket{c} = \ket{1}\) then the function is not constant.
    We will now carry out this algorithm.
    
    First we need to compute
    \[(\operator{X}\tensorProd\operator{X})(\ket{0}\tensorProd\ket{0}).\]
    We do this in the two dimensional tensor product representation:
    \begin{align*}
        \operator{X}\ket{0} &\representation \twoMat{0}{1}{1}{0}\twoVec{1}{0}\\
        &= \twoVec{0}{1}\\
        &\represents \ket{1}
    \end{align*}
    So,
    \[(\operator{X}\tensorProd\operator{X})(\ket{0}\tensorProd\ket{0}) = (\operator{X}\ket{0})\tensorProd(\operator{X}\ket{0}) = \ket{1}\tensorProd\ket{1}.\]
    Next we compute
    \[(\operator{H}_d\tensorProd\operator{H}_d)(\ket{1}\tensorProd\ket{1}),\]
    again in the two dimensional tensor product representation we have
    \begin{align*}
        \operator{H}_d\ket{1} &\representation \frac{\sqrt{2}}{2}\twoMat{1}{1}{1}{-1}\twoVec{0}{1}\\
        &= \frac{\sqrt{2}}{2}\twoVec{1}{-1}\\
        &= \frac{\sqrt{2}}{2}\left[\twoVec{1}{0} - \twoVec{0}{1}\right]\\
        &\represents \frac{\sqrt{2}}{2}(\ket{0} - \ket{1}).
    \end{align*}
    So,
    \begin{align*}
        (\operator{H}_d\tensorProd\operator{H}_d) (\ket{1}\tensorProd\ket{1}) &= (\operator{H}_d\ket{1})\tensorProd (\operator{H}_d\tensorProd\ket{1})\\
        &= \frac{1}{2}(\ket{0} - \ket{1})\tensorProd(\ket{0} - \ket{1})\\
        &= \frac{1}{2}(\ket{0}\tensorProd\ket{0} - \ket{0}\tensorProd\ket{1} - \ket{1}\tensorProd\ket{0} + \ket{1}\tensorProd\ket{1}).
    \end{align*}
    Next we act with the oracle:
    \begin{align*}
        \ket{\psi} &= \operator{O}_f \frac{1}{2}(\ket{0}\tensorProd\ket{0} - \ket{0}\tensorProd\ket{1} - \ket{1}\tensorProd\ket{0} + \ket{1}\tensorProd\ket{1})\\
        &= \frac{1}{2}(\ket{0}\tensorProd\ket{0\oplus f(0)} - \ket{0}\tensorProd\ket{1\oplus f(0)} - \ket{1}\tensorProd\ket{0\oplus f(1)} + \ket{1}\tensorProd\ket{1\oplus f(1)})\\
        &= \frac{1}{2}[\ket{0}\tensorProd(\ket{0\oplus f(0)} - \ket{1\oplus f(0)}) + \ket{1}\tensorProd (-\ket{0\oplus f(1)} + \ket{1\oplus f(1)})].
    \end{align*}
    Finally we apply the Hadamard operator to only the control state.
    In the two dimensional tensor product representation we have
    \begin{align*}
        \operator{H}_d\ket{0} &\representation \frac{\sqrt{2}}{2}\twoMat{1}{1}{1}{-1}\twoVec{1}{0} = \twoVec{1}{1} = \twoVec{1}{0} + \twoVec{0}{1} \represents \frac{\sqrt{2}}{2}(\ket{0} + \ket{1})\\
        \operator{H}_d\ket{1} &\representation \frac{\sqrt{2}}{2}\twoMat{1}{1}{1}{-1}\twoVec{0}{1} = \twoVec{1}{-1} = \twoVec{1}{0} - \twoVec{0}{1} \represents \frac{\sqrt{2}}{2}(\ket{0} - \ket{1})\\
    \end{align*}
    and so,
    \begin{align*}
        (\operator{H}_d\tensorProd\ident)\ket{\psi} &= \frac{\sqrt{2}}{4}[(\ket{0} + \ket{1})\tensorProd(\ket{0\oplus f(0)} - \ket{1\oplus f(0)}) + (\ket{0} - \ket{1})\tensorProd (-\ket{0\oplus f(1)} + \ket{1\oplus f(1)})]\\
        &= \frac{\sqrt{2}}{4} \ket{0} \tensorProd (\ket{0\oplus f(0)} - \ket{1 \oplus f(0)} - \ket{0 \oplus f(1)} + \ket{1 \oplus f(1)})\\
        &+ \frac{\sqrt{2}}{4}\ket{1} \tensorProd (\ket{0\oplus f(0)} - \ket{1\oplus f(0)} + \ket{0\oplus f(1)} - \ket{1\oplus f(1)})\\
        &= \frac{\sqrt{2}}{4} \ket{0} \tensorProd (\ket{f(0)} - \ket{1 \oplus f(0)} - \ket{f(1)} + \ket{1 \oplus f(1)})\\
        &+ \frac{\sqrt{2}}{4}\ket{1} \tensorProd (\ket{f(0)} - \ket{1\oplus f(0)} + \ket{f(1)} - \ket{1\oplus f(1)})\\
    \end{align*}
    Now consider the case when \(f\) is constant so \(f(0) = f(1)\).
    We then have
    \begin{align*}
        (\operator{H}_d\tensorProd\ident)\ket{\psi} &= \frac{\sqrt{2}}{4}\ket{0} \tensorProd (\ket{f(0)} - \ket{1\oplus f(0)} - \ket{f(0)} + \ket{1\oplus f(0)})\\
        &+ \frac{\sqrt{2}}{4}\ket{1}\tensorProd (\ket{f(0)} - \ket{1 \oplus f(0)} + \ket{f(0)} - \ket{1 \oplus f(0)})\\
        &= \frac{\sqrt{2}}{2}\ket{1}\tensorProd (\ket{f(0)} - \ket{1 \oplus f(0)}).
    \end{align*}
    So if we measure this state we will find \(\ket{c} = \ket{1}\).
    If instead \(f(0) \ne f(1)\) then \(1 \oplus f(0) = 0\oplus f(1)\) and \(0 \oplus f(0) = 1\oplus f(1)\).
    Also \(f(0) = 1 \oplus f(1)\) and \(f(1) = 1 \oplus f(0)\).
    Thus
    \begin{align*}
        (\operator{H}_d\tensorProd\ident)\ket{\psi} &= \frac{\sqrt{2}}{4} \ket{0} \tensorProd (\ket{f(0)} - \ket{1 \oplus f(0)} - \ket{1\oplus f(0)} + \ket{f(0)})\\
        &+ \frac{\sqrt{2}}{4}\ket{1} \tensorProd (\ket{f(0)} - \ket{1\oplus f(0)} + \ket{1\oplus f(0)} - \ket{f(0)})\\
        &= \frac{\sqrt{2}}{2}\ket{0} \tensorProd (\ket{f(0)} - \ket{1 \oplus f(0)}).
    \end{align*}
    So if upon measuring the register we get \(\ket{c} = \ket{0}\) we know that \(f(0) \ne f(1)\).
    
    \subsection{Constructing The Oracle}
    When defining a quantum algorithm one should show that all operators are unitary and, if possible, that they can be constructed from elementary gates such as \(\operator{C}\), \(\operator{X}\), etc.
    This is useful as it would allow a general quantum computer to use the algorithm without needing special hardware.
    The only new operator in the Deutsch algorithm is the oracle.
    It is fairly simple to show directly that the oracles can be constructed as follows:
    \begin{align*}
        \operator{O}_{f_1} &= \ident,\\
        \operator{O}_{f_2} &= \operator{C},\\
        \operator{O}_{f_3} &= \operator{C} (\ident \tensorProd \operator{X}),\\
        \operator{O}_{f_4}&= (\operator{I}\tensorProd\operator{X}).
    \end{align*}
    We can construct \(4\times 4\) matrices representing these.
    To do so we need to define the tensor product of two \(2\times 2\) matrices.
    Let \(A\) and \(B\) be \(2\times 2\) matrices with components \(a_{ij}\) and \(b_{ij}\).
    Then
    \[
        A\tensorProd B = \twoMat{a_{11}B}{a_{12}B}{a_{21}B}{a_{22}B} =
        \begin{pmatrix}
            a_{11}b_{11} & a_{11}b_{12} & a_{12}b_{11} & a_{12}b_{12}\\
            a_{11}b_{21} & a_{11}b_{21} & a_{12}b_{21} & a_{12}b_{21}\\
            a_{21}b_{11} & a_{21}b_{12} & a_{22}b_{11} & a_{22}b_{12}\\
            a_{21}b_{21} & a_{21}b_{21} & a_{22}b_{21} & a_{22}b_{21}
        \end{pmatrix}
        .
    \]
    This generalises to higher dimensional matrices as well.
    This allows us to write terms like \(\ident\tensorProd\operator{X}\) as \(4\times 4\) matrices.
    Thus the four oracles in this representation and as quantum circuits are given by
    \begin{align*}
        \operator{O}_{f_1} &= \ident\\
        &\representation
        \begin{pmatrix}
            1 & 0 & 0 & 0\\
            0 & 1 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        \\
        &= \tikzsetnextfilename{deutsch-algorithm-quantum-gate-2}
        \begin{tikzpicture}[baseline=(current bounding box.center)]
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \tikzstyle{phase} = [draw, fill, shape=circle, minimum size=5pt, inner sep=0pt]
            \matrix[row sep=0.2cm, column sep=0.6cm] {
                % First Row
                \node[left] (in1) {\(\ket{c}\)}; \pgfmatrixnextcell
                \node[right] (out1) {\(\ket{c}\)}; \\
                % Second Row
                \node[left] (in2) {\(\ket{t}\)}; \pgfmatrixnextcell
                \node[right] (out2) {\(\ket{t \oplus f_1(c)}\)}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in1) -- (out1);
                \draw[thick] (in2) -- (out2);
            \end{pgfonlayer}
        \end{tikzpicture}
        \\
        \operator{O}_{f_2} &= \operator{C}\\
        &\representation \CNOT\\
        &= \tikzsetnextfilename{deutsch-algorithm-quantum-gate-3}
        \begin{tikzpicture}[baseline=(current bounding box.center)]
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \tikzstyle{phase} = [draw, fill, shape=circle, minimum size=5pt, inner sep=0pt]
            \matrix[row sep=0.2cm, column sep=0.6cm] {
                % First Row
                \node[left] (in1) {\(\ket{c}\)}; \pgfmatrixnextcell
                \node[phase] (P) {}; \pgfmatrixnextcell
                \node[right] (out1) {\(\ket{c}\)}; \\
                % Second Row
                \node[left] (in2) {\(\ket{t}\)}; \pgfmatrixnextcell
                \node[operator] (X) {\(\operator{X}\)}; \pgfmatrixnextcell
                \node[right] (out2) {\(\ket{t \oplus f_2(c)}\)}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in1) -- (out1);
                \draw[thick] (in2) -- (out2);
                \draw[thick] (P) -- (X);
            \end{pgfonlayer}
        \end{tikzpicture}
        \\
        \operator{O}_{f_3} &= \operator{C}(\ident \tensorProd \operator{X})\\
        &\representation
        \begin{pmatrix}
            0 & 1 & 0 & 0\\
            1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 0 & 0 & 1
        \end{pmatrix}
        \\
        &= \tikzsetnextfilename{deutsch-algorithm-quantum-gate-4}
        \begin{tikzpicture}[baseline=(current bounding box.center)]
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \tikzstyle{phase} = [draw, fill, shape=circle, minimum size=5pt, inner sep=0pt]
            \matrix[row sep=0.2cm, column sep=0.6cm] {
                % First Row
                \node[left] (in1) {\(\ket{c}\)}; \pgfmatrixnextcell
                \node[left] {}; \pgfmatrixnextcell
                \node[phase] (P) {}; \pgfmatrixnextcell
                \node[right] (out1) {\(\ket{c}\)}; \\
                % Second Row
                \node[left] (in2) {\(\ket{t}\)}; \pgfmatrixnextcell
                \node[operator] {\(\operator{X}\)}; \pgfmatrixnextcell
                \node[operator] (X) {\(\operator{X}\)}; \pgfmatrixnextcell
                \node[right] (out2) {\(\ket{t \oplus f_3(c)}\)}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in1) -- (out1);
                \draw[thick] (in2) -- (out2);
                \draw[thick] (P) -- (X);
            \end{pgfonlayer}
        \end{tikzpicture}
        \\
        \operator{O}_{f_4} &= \ident\tensorProd\operator{X}\\
        &\representation
        \begin{pmatrix}
            0 & 1 & 0 & 0\\
            1 & 0 & 0 & 0\\
            0 & 0 & 0 & 1\\
            0 & 0 & 1 & 0
        \end{pmatrix}
        \\
        &= \tikzsetnextfilename{deutsch-algorithm-quantum-gate-5}
        \begin{tikzpicture}[baseline=(current bounding box.center)]
            \tikzstyle{operator} = [draw, fill=white, minimum size=1.5em, thick]
            \tikzstyle{phase} = [draw, fill, shape=circle, minimum size=5pt, inner sep=0pt]
            \matrix[row sep=0.2cm, column sep=0.6cm] {
                % First Row
                \node[left] (in1) {\(\ket{c}\)}; \pgfmatrixnextcell
                \node {}; \pgfmatrixnextcell
                \node[right] (out1) {\(\ket{c}\)}; \\
                % Second Row
                \node[left] (in2) {\(\ket{t}\)}; \pgfmatrixnextcell
                \node[operator] {\(\operator{X}\)}; \pgfmatrixnextcell
                \node[right] (out2) {\(\ket{t \oplus f_4(c)}\)}; \\
            };
            \begin{pgfonlayer}{background}
                \draw[thick] (in1) -- (out1);
                \draw[thick] (in2) -- (out2);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{align*}
    
    \section{Grover's Algorithm}
    Grover's algorithm can be stated in many ways.
    It can be thought of as a search algorithm which finds a given state in a quantum register or as a way to probe an unknown oracle and find the state that it picks out.
    The algorithm starts with the state \(\ket{0}^{\tensorProd N}\) where
    \[\ket{0}^{\tensorProd N} = \underbrace{\ket{0}\tensorProd\ket{0} \tensorProd\dotsb\tensorProd \ket{0}}_{N~\text{times}},\]
    and \(N\) is the number of things we have to search through.
    Then the Hadamard operator is applied to each term which results in the state \(\operator{H}_d^{\tensorProd N} \ket{0}^{\tensorProd N}\) which is a linear combination of all basis states such that all states have equal probability.
    We will consider for an example here the case \(n = 3\).
    The starting state is then \(\ket{000}\) and the first step gives us
    \[\ket{\psi} = (\operator{H}_d\tensorProd \operator{H}_d\tensorProd \operator{H}_d)\ket{000} = \frac{\sqrt{2}}{4}(\ket{000} + \ket{001} + \ket{010} + \ket{100} + \ket{011} + \ket{101} + \ket{110} + \ket{111}).\]
    Suppose we have an oracle that picks out the state \(\ket{s}\).
    The Grover algorithm gives us a way to move the system towards this state in a unitary manner.
    This is important as it means that operations are reversible and we can continue to use the state for further computations after the algorithm.
    Compare this to the normal way we pick out a state by simply taking the inner product with \(\bra{s}\).
    
    The next step of the Grover algorithm is to apply the oracle, \(\operator{O}\), which flips the sign of the state, \(\ket{s}\), which it is singling out.
    This oracle is given by
    \[\operator{O} = \ident - 2\ketbra{s}{s}.\]
    This has a diagonal matrix representation of ones down the diagonal
    except for the entry associated with \(\ket{s}\) which has a \(-1\).
    
    Suppose for our example that \(\ket{s} = \ket{110}\).
    Then the action of the oracle is
    \[\ket{\psi_-} = \operator{O}\ket{\psi} = \frac{\sqrt{2}}{4}(\ket{000} + \ket{001} + \ket{010} + \ket{100} + \ket{011} + \ket{101} - \ket{110} + \ket{111}).\]
    The only change is that the coefficient of \(\ket{s} = \ket{110}\) is now negative.
    
    The next step of the algorithm is to apply the Grover operation,
    \[\operator{G} = 2\ketbra{\psi}{\psi} - \ident\]
    which for our example gives us
    \[\ket{\psi_d} = \frac{\sqrt{2}}{8}(\ket{000} + \ket{001} + \ket{010} + \ket{100} + \ket{011} + \ket{101} + 5\ket{110} + \ket{111}).\]
    Notice that the net effect of \(\operator{G}\operator{O}\) is to increase the probability of measuring the register in the state \(\ket{s}\).
    It turns out that applying \(\operator{G}\operator{O}\) iteratively \(\sqrt{N}\) times will cause \(\ket{s}\) to have its maximum amplitude.
    So if we do this then measure the register it is likely that we will measure the state \(\ket{s}\).
    
    We can better understand this algorithm by considering the action of \(\operator{O}\) and \(\operator{G}\) on the desired state \(\ket{s}\) and the initial state \(\ket{\psi} = \operator{H}_d^{\tensorProd N}\ket{0}^{\tensorProd N}\):
    \begin{align*}
        \operator{O}\ket{s} &= -\ket{s},\\
        \operator{O}\ket{\psi} &= \ket{\psi} - \frac{2}{\sqrt{N}}\ket{s},\\
        \operator{G}\ket{s} &= \frac{2}{\sqrt{N}}\ket{\psi} - \ket{s},\\
        \operator{G}\ket{\psi} &= \ket{\psi}.
    \end{align*}
    We see that the action of both operators always returns a linear combination of \(\ket{s}\) and \(\ket{\psi}\).
    This means that the action of \(\operator{G}\operator{O}\) on \(\ket{\psi}\) will be another state in the two dimensional subspace spanned by \(\ket{\psi}\) and \(\ket{s}\).
    
    We can write an arbitrary state, \(\ket{u}\in\hilbert\), as
    \[\ket{u} = a\ket{s_\perp} + b\ket{s}\]
    where \(\ket{s_\perp}\) is some state perpendicular to \(\ket{s}\) so \(\braket{s}{s_\perp} = 0\).
    Note that this is not a basis for the whole space as another state, \(\ket{v}\in\hilbert\) may require a different \(\ket{s_\perp}\) but all states can be decomposed this way.
    Similarly we can write an arbitrary state, \(\ket{u'}\in\hilbert\) as
    \[\ket{u'} = a\ket{\psi} + b\ket{\psi_\perp}\]
    where \(\ket{\psi_\perp}\) is perpendicular to \(\ket{\psi}\) so \(\braket{\psi}{\psi_\perp} = 0\).
    
    The first step of Grover's algorithm is to act on \(\ket{\psi}\) with \(\operator{O}\).
    We can decompose \(\ket{\psi}\) in terms of \(\ket{s}\) and \(\ket{s_\perp}\).
    First note that since each coefficient in \(\ket{\psi}\) is equal for a properly normalised state we must have \(\braket{s}{\psi} = N^{-1/2}\) and so
    \begin{align*}
        \operator{O}\ket{\psi} &= \operator{O}\left(\sqrt{\frac{N-1}{N}}\ket{s_\perp} + \frac{1}{\sqrt{N}}\ket{s}\right)\\
        &= \sqrt{\frac{N - 1}{N}}\ket{s_\perp} - \frac{1}{\sqrt{N}}\ket{s}.
    \end{align*}
    We can consider this geometrically as a reflection of \(\ket{\psi}\) in the \(\ket{s}\) axis, see figure~\ref{fig:action of O in grover algorithm as a reflection}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{grover-algorithm-action-of-O}
        \begin{tikzpicture}
            \tikzset{axis/.style={very thick}}
            \tikzset{vector/.style={ultra thick, ->, >=latex}}
            \draw[axis] (0, -3) -- (0, 3) node[above] {\(\ket{s}\)};
            \draw[axis] (0, 0) -- (3, 0) node[right] {\(\ket{s_\perp}\)};
            \draw[vector] (0, 0) -- (3, 2) node[right] {\(\ket{\psi}\)};
            \draw[vector] (0, 0) -- (3, -2) node[right] {\(\operator{O}\ket{\psi}\)};
            \begin{scope}
                \clip (0, 0) -- (3, 2) -- (3, 0) -- cycle;
                \draw (0, 0) circle [radius=0.6cm];
            \end{scope}
            \begin{scope}
                \clip (0, 0) -- (3, -2) -- (3, 0) -- cycle;
                \draw (0, 0) circle [radius=0.4cm];
            \end{scope}
            \node at (0.7, 0.2) {\(\vartheta\)};
            \node at (0.55, -0.17) {\(\vartheta\)};
        \end{tikzpicture}
        \caption{The action of \(\operator{O}\) on \(\ket{\psi}\) can be viewed as a reflection in the \(\ket{s}\) axis.}
        \label{fig:action of O in grover algorithm as a reflection}
    \end{figure}
    Notice that if \(\ket{\psi}\) makes an angle\footnote{for some definition of angles in a non-Euclidean complex vector space} of \(\vartheta\) to \(\ket{s_{\perp}}\) then the action of \(\operator{O}\) is a rotation through \(-2\vartheta\).
    
    The action of \(\operator{G}\) can be similarly interpreted as a reflection in the line defined by the original state \(\ket{\psi}\).
    This can also be viewed as a rotation through \(4\vartheta\) and so \(\operator{G}\operator{O}\) has a net result of a rotation of \(2\vartheta\) towards \(\ket{s}\).
    
    Each iterative application of \(\operator{G}\operator{O}\) rotates the state \(2\vartheta\) towards \(\ket{s}\).
    At some point we will overshoot \(\ket{s}\) and start moving away again.
    We need to know how many times to apply \(\operator{G}\operator{O}\) to maximise the probability of measuring the register to be in state \(\ket{s}\).
    
    The initial angle, \(\vartheta\), is related to the original state as shown in figure~\ref{fig:relation between s, psi, and theta}.
    \begin{figure}[ht]
        \centering
        \tikzsetnextfilename{grover-algorithm-psi-decomposed-to-s-and-s-perpendicular}
        \begin{tikzpicture}
            \tikzset{vector/.style={ultra thick, ->, >=latex}}
            \draw[vector] (0, 0) -- (3, 0) node[midway, below] {\(\sqrt{\frac{N-1}{N}}\ket{s_\perp}\)};
            \draw[vector] (3, 0) -- (3, 1) node[midway, right] {\(\frac{1}{\sqrt{N}}\ket{s}\)};
            \draw[vector] (0, 0) -- (3, 1) node[midway, above] {\(\ket{\psi}\)};
            \begin{scope}
                \clip (0, 0) -- (3, 0) -- (3, 1) -- cycle;
                \draw (0, 0) circle [radius=0.7cm];
            \end{scope}
            \node at (1, 0.16) {\(\vartheta\)};
        \end{tikzpicture}
        \caption{\(\ket{\psi}\) decomposed into \(\ket{s_\perp}\) and \(\ket{s}\) components.}
        \label{fig:relation between s, psi, and theta}
    \end{figure}
    We see that
    \[\sin\vartheta = \frac{1}{\sqrt{N}}.\]
    For large \(N\) \(\vartheta\) must be small so we have \(\vartheta\approx N^{-1/2}\).
    Similarly \(\ket{\psi}\) is almost entirely aligned with \(\ket{s_\perp}\) so we need to rotate it through an angle of \(\pi/2\) to be aligned in the \(\ket{s}\) direction.
    Since each iteration moves us an angle \(2\vartheta \approx 2/\sqrt{N}\) we need \(\pi/(4\vartheta) = \pi\sqrt{N}/4\) (or the closest integer to this) iterations  to reach maximal probability of measuring \(\ket{s}\).
    
    To implement this algorithm one needs to be able to construct \(\operator{O}\) and \(\operator{G}\) from basic gates.
    This can be done but is not very illuminating and quite involved so we won't do it here.
    
    We mentioned at the start of this section that the Grover algorithm can be seen as a search algorithm.
    The context for this is as follows: suppose we have a quantum register, \(\ket{\varphi}\), and we wish to know if a given state, \(\ket{s}\), is in this register.
    Then we use the Grover algorithm as described above and after \(\sqrt{N}\) iterations we measure the register and we expect to measure \(\ket{s}\) with high probability\footnote{with high probability is used here in the technical sense that an event with probability on \(N\) occurs with high probability iff that probability goes to one as \(N\to\infty\).} if \(\ket{s}\) is in \(\ket{\varphi}\).
    From this we conclude that the Grover algorithm, when thought of as a search algorithm searching \(N\) qubits, is \(\order(\sqrt{N})\) since we need to apply \(\operator{G}\operator{O}\) \(\order(\sqrt{N})\) times.
    This is much faster than similar classical search algorithms which are \(\order(N)\).
    
    Quantum computing is still a developing field.
    One of the key features is the ability to explore multiple solutions at once by studying superpositions of states.
    For example a system of \(n\) spin 1/2 particles has \(N = 2^n\) possible states so by studying 10 particles we can study \(2^{10} = 1024\) different states.
    The number of different states grows exponentially with the size of the system.
    If each state corresponds with a possible solution then we can test many solutions at once.
    
    One hope that is held for quantum computing is the ability to solve problems that fall in a class called \NPcomplexity problems.
    \NPcomplexity stands for non-deterministic polynomial time which, roughly speaking, means that once a solution is found we can check that this is a solution in polynomial time.
    That is for a given \NPcomplexity problem there is an algorithm that will return true or false for a given solution and that algorithm is \(\order(N^n)\) for some integer \(n\) and input size \(N\).
    While a solution for a \NPcomplexity problem can be found in polynomial time whether a solution can be found in polynomial time for all \NPcomplexity problems is an open question\footnote{see  \href{https://en.wikipedia.org/wiki/P_versus_NP_problem}{\Pcomplexity vs. \NPcomplexity}}.
    
    For an example of an \NPcomplexity problem consider the task of finding a particular number in a set of all \(n\)-digit numbers which are randomly ordered.
    If we were to simply check every number then the time take grows exponentially with \(n\) (as there are \(\order(10^n)\) \(n\)-digit numbers).
    On the other hand if someone has already found the number in the list then we simply need to check each digit to check it is the correct number and the number of digits grows linearly with \(n\) (as the number of digits \emph{is} \(n\)).
    
    It is possible that a quantum computing algorithm exists which allows us to solve \NPcomplexity problems faster.
    Suppose we have a system of \(n\) spin 1/2 particles.
    If we can somehow find a correspondence between the \(2^n\) states with set of solutions and apply a quantum algorithm to the system which enhances the probability amplitude of the correct answer it is possible that this would lead to an answer faster than any classical algorithm can find a solution.
    Grover's algorithm is the most simple example where this is possible.
    We manipulate the probability amplitudes to find the desired state in \(\order(\sqrt{N})\) as opposed to the classical \(\order(N)\) algorithms.
    
    \section{Information Theory}
    \subsection{Classical Information Theory}
    Information theory is the study of messages and communications with the goal of developing efficient communication.
    The formal meaning of this is as follows.
    A (formal) \define{language} is a set of \(N\) symbols, \(\Sigma = \{s_1, \dotsc, s_N\}\).
    These can be arranged into a \define{message}, \(X\), which is an ordered set of symbols of length \(M_X\), \(X = s_{i}^{1}s_{j}^{2}\dotsm s_{k}^{M_X}\) where superscripts denote position in the message and subscripts take values from 1 to \(N\) such that \(s_i\in\Sigma\).
    At this point there is an important decision to make.
    We need to decide what we care about.
    If we only care about a few possible messages then we can be very efficient by simply assigning each a number and sending that number.
    If we want to be able to send arbitrary messages then  we need to consider the entire language.
    In the following this decision reveals itself whenever there is a sum or we count something we need to choose to sum over or count the part of the language that we care about.
    
    If we denote by \(M_i^X\) then number of times the symbol \(s_i\) appears in the message \(X\) then the frequency, or probability, of \(s_i\) is \(f_i = p_i = M_i^X/M_X\).
    We say that the \define{information} of the symbol \(s_i\) with probability \(p_i\) is
    \[I(p_i) = -\log_2p_i.\]
    Notice that this logarithm is taken in base 2.
    This is because we want to measure information in bits.
    The information of a symbol is a useful quantity when we are studying a large number of symbols.
    Roughly the information characterises how much we learn when we are sent a particular symbol.
    In the extreme case of a language with only one symbol or where we restrict messages to be only one message the information is zero as we know what every message is before it is sent.
    In a language with two symbols with equal probability the information of either symbol is 1 as we learn one thing when the message is sent (we learn which symbol was sent).
    
    The \define{Shannon entropy} of a message is the average information of all symbols:
    \[\shannonEntropy = -\sum_i p_i\log_2 p_i.\]
    This has units of bits per symbol.
    The Shannon source coding theorem states that the Shannon entropy is the minimum number of bits needed to encode the message.
    
    For a first example suppose we have a language of two symbols, \(\{A, B\}\).
    Suppose that the probability of \(A\) is \(p_A\).
    Then the Shannon entropy is
    \[\shannonEntropy = -p_A\log_2p_A - (1 - p_A)\log_2p_A.\]
    This is zero in the limits when \(p_A = 0, 1\) and peaks at \(p_A = 0.5\) with \(\shannonEntropy = \SI{1}{\bitsSI/\symbolSI}\)\footnote{This function may be familiar from the entropy in statistical mechanics}.
    The case of \(p_A = 0.5\) is equivalent to choosing randomly between \(A\) and \(B\) and the Shannon source coding theorem simply states that it requires one bit of information to tell someone which one we chose.
    
    For a more realistic (but only slightly) example we need more symbols.
    Consider a language of four symbols, \(\{A, B, C, D\}\), with  probabilities \(0.5\), \(0.25\), \(0.125\), and \(0.125\) respectively.
    The Shannon entropy of this language is
    \[\shannonEntropy = -0.5\log_20.5 - 0.25\log_20.25 - 0.125\log_20.125 - 0.125\log_20.125 = \SI{1.75}{\bitsSI/\symbolSI}.\]
    So we need at least \(\SI{1.75}{\bitsSI/\symbolSI}\) to encode a character in this language.
    For example consider the following encoding:
    \begin{center}
        \begin{tabular}{cc}\hline
            Symbol & Encoding\\\hline
            \(A\) & 1\\
            \(B\) & 01\\
            \(C\) & 001\\
            \(D\) & 000\\\hline
        \end{tabular}
    \end{center}
    This encoding seems a bit odd at first, why not just use \(0\) for \(A\), 1 for \(B\), \(10\) for \(C\) and \(11\) for \(D\)?
    Because then there is ambiguity, for example \(10\) could be either \(AB\) or \(C\).
    Computers don't have this issue as they work with fixed blocks of bits or use an extra symbol to denote the end of a symbol but this is not the most efficient way to encode.
    Notice that the information of each of the symbols above is equal to the number of bits in the encoding.
    Suppose we want to encode the message \(AABABACD\).
    Then our encoded message is \(11011011001000\).
    This is \(\SI{14}{\bitsSI}\) and as the message is 8 symbols long the information per symbol is \(\SI{14}{\bitsSI}/\SI{8}{\symbolSI} = \SI{1.75}{\bitsSI/\symbolSI}\).
    So we have achieved maximum efficiency in our encoding.
    Note that this limit is for the \emph{average} message.
    If we want to send \(AAAA\) the this can be done in four bits so the Shannon entropy is \(\SI{1}{\bitsSI/\symbolSI}\).
    Similarly if we want to send \(CDCD\) then this requires \(\SI{12}{\bitsSI}\) so the Shannon entropy is \(\SI{3}{\bitsSI/\symbolSI}\).
    
    The idea when encoding a language is that we want the highest frequency bit to have the shortest possible encoding, hence why we chose \(A\) to have a single bit encoding in the example above.
    There is one way that we can improve on this information rate.
    We can encode words.
    Suppose that the string we encoded before, \(AABABACD\), is representative of all messages in the language.
    Notice that \(AA\) appears once, \(BA\) appears twice, and \(CD\) appears once.
    The Shannon entropy of \(\{AA, BA, CD\}\) is \(\SI{1.5}{\bitsSI/\symbolSI}\).
    If we encode \(AB\) as \(1\), \(AA\) as \(01\) and \(CD\) as \(001\) then we can encode our message as \(0111001\) which is \(\SI{7}{bits}\) for 14 symbols\footnote{the hit, quantum physics, sequel to seven brides for seven brothers featuring hit songs such as barn raise(ing and lowering operators) and bless your beautiful hide(rogen atom)}, so one bit better than before.
    However there are now messages we can't encode, for example \(BACCDB\).
    
    \subsection{Quantum Information Theory}
    \subsubsection{Density Matrix}\label{sec:density matrix}
    The stats that we have dealt with so far are what are called \define{pure states}.
    This means that they are a linear combination of basis states.
    The other type of state is a \define{mixed state}.
    These are collections of pure states with some classical probability associated with them.
    These most commonly arise when we aren't certain what pure state a system is in but we do know what the probability of a given state is.
    The probabilities here are classical in that they aren't inherent to the system but are due to our lack of knowledge of the system.
    
    To account for both possibilities we define the \define{density matrix}:
    \[\operator{\rho} = \sum_i p_i\ketbra{\chi_i}{\chi_i}\]
    where \(p_i\) is the classical probability of the state \(\ket{\chi_i}\) from the mixed state.
    The density matrix for a pure state, \(\ket{\chi}\), is simply the projection operator, \(\ketbra{\chi}{\chi}\).
    Some useful properties of the density matrix are:
    \begin{itemize}
        \item \emph{Hermitian} -- This follows since \(\hermit\) distributes over addition and \((\ketbra{\chi}{\chi})\hermit = \ketbra{\chi}{\chi}\).
        
        \item \emph{Trace property} -- The trace of the density matrix is \(\Tr(\operator{\rho}) = 1\).
        To see this let \(\{\ket{\varphi_i}\}\) be a complete set of states.
        Then
        \begin{align*}
            \Tr(\operator{\rho}) &= \sum_{j} \rho_{jj}\\
            &= \sum_{j} \bra{\varphi_j}\operator{\rho}\ket{\varphi_j}\\
            &= \sum_{ij} \bra{\varphi_j} p_i\ket{\chi_i} \braket{\chi_i}{\varphi_j}\\
            &= \sum_i p_i \sum_j \braket{\varphi_j}{\chi_i} \braket{\chi_i}{\varphi_j}\\
            &= \sum_i p_i \sum_j \braket{\chi_i}{\varphi_j} \braket{\varphi_j}{\chi_i}\\
            &= \sum_i p_i \braket{\chi_i}{\chi_i}\\
            &= \sum_i p_i\\
            &= 1.
        \end{align*}
        
        \item \emph{Positive-semidefinite} -- For all \(\ket{\varphi}\in\hilbert\)
        \[\bra{\varphi}\operator{\rho}\ket{\varphi} = \sum_i p_i \braket{\varphi}{\chi_i}\braket{\chi_i}{\varphi} = \sum_i p_i \abs{\braket{\varphi}{\chi_i}}^2 \ge 0.\]
        
        \item \emph{Eigenvalues lie in \([0, 1]\)} -- Let \(\alpha_i\) be the eigenvalues of \(\operator{\rho}\).
        Since \(\operator{\rho}\) is Hermitian it only has real eigenvalues.
        Since \(\operator{\rho}\) is positive-semidefinite it's eigenvalues are non-negative.
        In the eigenbasis of \(\operator{\rho}\) it is diagonal with it's eigenvalues on the diagonal.
        The trace is invariant under change of basis so the trace is 1 meaning \(\sum_i \alpha_i = 1\).
        The only way for positive real numbers to sum to one is if all of those numbers are less than 1.
        
        \item \emph{Trace property for \(\operator{\rho}^2\)} -- \(\Tr(\operator{\rho}^2) \le 1\).
        In the eigenbasis of \(\operator{\rho}\) we have \(\Tr(\operator{\rho}^2) = \sum_i\alpha_i^2\).
        By the triangle inequality
        \[1 = \left[\sum_i \alpha_i\right] \ge \sum_i \alpha_i\]
    \end{itemize}
    Consider now the special case of a pure state.
    Then \(\operator{\rho}^2 = \sum_i\alpha_i^2\ketbra{\alpha_i}{\alpha_i}\) so \(\operator{\rho}^2 = \operator{\rho}\).
    Hence if \(\Tr(\operator{\rho}^2) = 1\) or \(\operator{\rho}^2 = \operator{\rho}\) then \(\operator{\rho}\) represents a pure state.
    
    \subsubsection{Von Neumann Entropy}
    The \define{von Neummann entropy} is defined as
    \[\vonNeumannEntropy = -\Tr(\operator{\rho}\log_2\operator{\rho}),\]
    where \(\log_2\) is applied element wise.
    It is, in many ways, the quantum analogue of the Shannon entropy.
    Consider, for example, the case of a mixed state where each state, \(\ket{\varphi_i}\), appears with probability \(p_i\), and \(\{\ket{\varphi_i}\}\) are mutually orthogonal.
    In this case we can express the density matrix as a diagonal matrix in the \(\{\ket{\varphi_i}\}\) basis.
    In this case
    \begin{align*}
        \vonNeumannEntropy &= -\Tr[\operator{\rho}\log_2\operator{\rho}]\\
        &= -\Tr \left[ 
            \begin{pmatrix}
                p_1    & 0      & \dots  & 0   \\
                0      & p_2    & \dots  & 0   \\
                \vdots & \vdots & \ddots & 0   \\
                0      & 0      & \dots  & p_N
            \end{pmatrix}
            \begin{pmatrix}
                \log_2p_1 & 0         & \dots  & 0         \\
                0         & \log_2p_2 & \dots  & 0         \\
                \vdots    & \vdots    & \ddots & 0         \\
                0         & 0         & \dots  & \log_2p_N
            \end{pmatrix}
        \right]\\
        &= - \Tr
        \begin{pmatrix}
            p_1\log_2p_1    & 0               & \dots  & 0            \\
            0               & p_2\log_2p_2    & \dots  & 0            \\
            \vdots          & \vdots          & \ddots & 0            \\
            0               & 0               & \dots  & p_N\log_2p_N
        \end{pmatrix}
        \\
        &= \sum_i p_i\log_2 p_i\\
        &= \shannonEntropy.
    \end{align*}
    So for orthogonal mixed states the von Neumann entropy reduces to the Shannon entropy.
    Consider instead the case of a pure state, \(\ket{\varphi}\).
    The density matrix is the \(\operator{\rho} = \ketbra{\varphi}{\varphi}\).
    In this case \(\operator{\rho}\) can be expressed as a matrix that is zero everywhere apart from the diagonal element corresponding to \(\ket{\varphi}\) in some basis including \(\ket{\varphi}\) as a basis vector.
    In this slot it will be 1.
    Since \(\log_2 1 = 0\) the von Neumann entropy will be zero.
    This is analogous to the case of a language of a single symbol.
    
    The von Neumann entropy starts to differ from the Shannon entropy when we consider a mixture of non-orthogonal quantum states.
    For example consider the set of two spin states, one spin up along the \(z\)-axis, \(\ket{+}\), and one spin up along the \(y\) axis, \(\ket{y} = \sqrt{2}(\ket{-} + i\ket{+})/2\).
    Suppose both states occur with equal probability in the mixed state.
    The density matrix in the \(\{\ket{+}, \ket{-}\}\) basis is then
    \begin{align*}
        \operator{\rho} &= 0.5\ketbra{+}{+} + 0.5\left[\frac{\sqrt{2}}{2}(i\ket{+} + \ket{-})\right] \left[\frac{\sqrt{2}}{2}(-i\ket{+} + \ket{-})\right]\\
        &= \left( 0.5 + i\frac{1}{2} \right)\ketbra{+}{+} + 0.5i\frac{1}{2}\ketbra{+}{-} - 0.5i\frac{1}{2}\ketbra{-}{+} + 0.5\frac{1}{2}\ketbra{-}{-}\\
        &= 
        \begin{pmatrix}
            0.75   & 0.25i\\
            -0.25i & 0.25
        \end{pmatrix}
        .
    \end{align*}
    The eigenvalues of this matrix are \(\alpha_1 = 0.146\) and \(\alpha_2 = 0.854\).
    The von Neumann entropy is then
    \[\vonNeumannEntropy = -0.146\log_20.146 - 0.854\log_20.854 = \SI{0.601}{\bitsSI}.\]
    This differs from the Shannon entropy for two symbols which occur with equal probability in which case \(\shannonEntropy = \SI{1}{\bitsSI/\symbolSI}\).
    In general \(\vonNeumannEntropy \le \shannonEntropy\).
    Equality holds only when all states in the mixed state are orthogonal.
    
    \subsubsection{Quantum Information Theory}
    In quantum mechanics the information is encoded into the quantum states.
    We have seen this already with quantum computing.
    Quantum information theory seeks to find out how much information a state can possibly hold.
    In the quantum case states replace the language so suppose we have a collection of states, \(\{\ket{i}\}\), which may or may not be orthogonal.
    Consider the ensemble where each of these states exists with probability \(p_i\).
    The density matrix for this ensemble is
    \[\operator{\rho} = \sum_i p_i \ketbra{i}{i}.\]
    The collection of all possible messages of \(n\) states is then given by the tensor product density matrix:
    \[\operator{\rho}^{\tensorProd n} = \underbrace{\operator{\rho} \tensorProd\operator{\rho} \tensorProd \dotsm \tensorProd \operator{\rho}}.\]
    The naive way to send \(n\) quantum states is to use a Hilbert space of dimension \(n\).
    However the quantum analogue of the the Shannon source coding theorem, which is called the Schumacher coding theorem, tells us that the smallest dimension of the Hilbert space needed to send \(n\) quantum states satisfies
    \[\log_2(\dim\hilbert) = n\vonNeumannEntropy(\operator{\rho}).\]
    For example suppose we have the two orthogonal states \(\ket{+}\) and \(\ket{-}\) and they appear with equal probability.
    Then
    \[\operator{\rho} = 0.5\ketbra{+}{+} + 0.5\ketbra{-}{-},\]
    and the von Neumann entropy is
    \[\vonNeumannEntropy = 2(-0.5\log_20.5) = 1.\]
    So each state we wish to send requires the Hilbert space to have an extra dimension.
    This is not that different from the classical case.
    We can simply assign one state in the Hilbert space for each state.
    We can extend this to states including many orthogonal states such as \(\ket{+--+}\).
    
    Again the difference arises when we have states which aren't orthogonal.
    In this case the von Neumann entropy is less than the Shannon entropy.
    This means that we can compress the data more than is allowed classically.
    This is because by virtue of the states being non-orthogonal they have some overlap in information and this has no classical counterpart.
    
    One way to view this overlap is explained in the following example.
    Consider the case of the two, non-orthogonal states \(\ket{+}\) and \(\ket{y}\) defined in the previous section.
    Suppose that we wish to send \(\ket{+}\) but instead \(\ket{y}\) is sent.
    All is not yet lost as \(\ket{y} = 2(i\ket{+} + \ket{-})/2\) still contains \(\ket{+}\) with probability \(1/2\) of measuring \(\ket{+}\).
    The amount in which two states, \(\ket{\varphi}\) and \(\ket{\psi}\), overlap is measured by the \define{fidelity}, \(F\), defined to be
    \[F(\ket{\varphi}, \ket{\psi}) = \abs{\braket{\varphi}{\psi}}.\]
    More generally we can define the fidelity of a pure state, \(\ket{\varphi}\), and a density matrix, \(\operator{\rho}\), as
    \[F(\ket{\varphi}, \operator{\rho}) = \sqrt{\bra{\varphi}\operator{\rho}\ket{\varphi}}.\]
    If \(\operator{\rho}\) represents the pure state \(\ket{\psi}\) then this reduces to the case of two pure states.
    
    The fidelity can be seen as a measure of how close the information is to the information intended.
    For example with our \(\ket{+}\) and \(\ket{y}\) density matrix the eigenvalues are
    \begin{align*}
        \ket{+'} &= \cos\frac{\pi}{8}\ket{+} - i\sin\frac{\pi}{8}\ket{-},\\
        \ket{-'} &= \sin\frac{\pi}{8}\ket{+} + i\cos\frac{\pi}{8}\ket{-}.
    \end{align*}
    The corresponding eigenvalues are \(\cos^2(\pi/8)\) and \(\sin^2(\pi/2)\) respectively.
    The fidelities between the primed and un-primed states are
    \begin{align*}
        F(\ket{+}, \ket{+'}) &= F(\ket{y}, \ket{+'}) = \cos\frac{\pi}{8} = 0.924,\\
        F(\ket{+}, \ket{-'}) &= F(\ket{y}, \ket{-'}) = \sin\frac{\pi}{8} = 0.0.383.
    \end{align*}
    Suppose we don't know which of the two un-primed states was sent.
    If we simply guess \(\ket{+'}\) then this still has considerable overlap with the actual state, whether it is \(\ket{+}\) or \(\ket{y}\).
    This means that \(\ket{+'}\) may be a good enough estimate of the state if we don't need to be 100\% accurate.
    Classically we cannot make this sort of approximation.
    The symbol sent is either correct or incorrect.
    There is no overlap.
    
    \subsubsection{Entanglement Entropy}
    We can use the entropy to measure how much a system is entangled.
    Recall that a two particle system is entangled if it is in a state which \emph{cannot} be written as a tensor product of state vectors of the initial single particle Hilbert spaces.
    For example if the state is \(\ket{+-}\) then we can write this as \(\ket{+}\tensorProd\ket{-}\) so the state is not entangled.
    Any two particle state that \emph{cannot} be written as
    \[(a_-\ket{-} + a_+\ket{+})\tensorProd(b_-\ket{-} + b_+\ket{+})\]
    for \(a_-, a_+, b_-, b_+\in\complex\) is entangled.
    
    Suppose our system can be split into two subsystems, \(A\) and \(B\).
    These may be individual particles but they could also be more complicated systems.
    The Hilbert space describing the system is then \(\hilbert = \hilbert_A \tensorProd \hilbert_B\) where \(\hilbert_A\) and \(\hilbert_B\) are the Hilbert spaces describing subsystems \(A\) and \(B\) respectively.
    Let \(\ket{\Psi}\in\hilbert\) be a pure state of the combined system.
    We wish to know if this state is entangled between the two subsystems, and if so to what degree.
    Note that the existence of the two subsystems is critical here. 
    It makes no sense to ask how entangled a given state is, only how entangled a subsystem is when it is in this state.
    As mentioned at the start of the section this is measured by the entropy:
    \[S = -\Tr[\operator{\rho}_A\log_2\operator{\rho}_A]\]
    which we call the \define{entanglement entropy of subsystem \(\bm{A}\)}.
    Here \(\operator{\rho}_A\) is the reduced density matrix of subsystem \(A\) which is defined by tracing out the states of subsystem \(B\):
    \[\operator{\rho}_A = \Tr_B[\operator{\rho}_{AB}]\]
    where \(\operator{\rho}_{AB}\) is the density matrix of the entire system, for a pure state \(\operator{\rho}_{AB} = \ketbra{\Psi}{\Psi}\).
    Also \(\Tr_B[\operator{\rho}_{AB}]\) is the \define{partial trace} with respect to subsystem \(B\).
    It is defined as
    \[\Tr_B(\operator{P}\tensorProd\operator{Q}) = \Tr{}[\operator{Q}]\operator{P}\]
    where \(\operator{P}\) is some operator on \(\hilbert_A\) and \(\operator{Q}\) is some operator on \(\hilbert_B\) and \(\Tr\) is the normal trace defined on a single operator.
    
    \begin{example}
        Suppose that \(\ket{\Psi} = \ket{\Psi_A}\tensorProd\ket{\Psi_B}\), i.e. the system is \emph{not} entangled.
        Then 
        \[\operator{\rho}_{AB} = \ketbra{\Psi}{\Psi} = (\ket{\Psi_A}\tensorProd\ket{\Psi_B})(\bra{\Psi_A}\tensorProd\bra{\Psi_B}) = \ketbra{\Psi_A}{\Psi_A}\tensorProd\ketbra{\Psi_B}{\Psi_B}.\]
        Hence
        \[\operator{\rho}_A = \Tr_B[\ketbra{\Psi}{\Psi}] = \Tr[\ketbra{\Psi_B}{\Psi_B}]\ketbra{\Psi_A}{\Psi_A} = \ketbra{\Psi_A}{\Psi_A}.\]
        Since \(\ketbra{\Psi_B}{\Psi_B}\) has a matrix representation with zeros everywhere apart from the diagonal entry corresponding to the state \(\ket{\Psi_B}\) where it is one and so it has unit trace.
        The entanglement entropy is then
        \[S = -\Tr[\operator{\rho}_A\log_2\operator{\rho}_A] = 0.\]
        As with \(\ketbra{\Psi_B}{\Psi_B}\) the entries in \(\operator{\rho}_A\) are either one or zero\footnote{while \(\log_20\) is undefined we are multiplying by zero and so seemingly this doesn't matter.}.
        Since \(\log_21 = 0\) this means that \(\log_2\operator{\rho}_A\) is the zero matrix and so the entanglement entropy is zero, as it should be for an state that isn't entangled.
    \end{example}
    
    \begin{example}
        Consider a two state system with two spin states, \(\ket{+}\) and \(\ket{-}\).
        Suppose the system is in the pure state\footnote{we use \(\pm\) for a general result but this is not a mixed state, the state is \(+\) or \(-\) and we choose which.}
        \[\ket{\Psi} = \frac{\sqrt{2}}{2} (\ket{+-} \pm \ket{-+}).\]
        The density matrix for the entire system is \(\operator{\rho}_{AB} = \ketbra{\Psi}{\Psi}\).
        We can write this as a matrix in a basis where the rows and columns correspond to \(\ket{++}\), \(\ket{-+}\), \(\ket{+-}\), and \(\ket{--}\), in that order.
        This is the same four-dimensional basis we have used when talking about quantum computing.
        In this basis
        \[
            \operator{\rho}_{AB} =
            \begin{pmatrix}
                0 & 0 & 0 & 0 \\
                0 & 1/2 & \pm 1/2 & 0\\
                0 & \pm 1/2 & 1/2 & 0\\
                0 & 0 & 0 & 0
            \end{pmatrix}
            .
        \]
        The density matrix for particle \(A\) is then
        \begin{align*}
            \operator{\rho}_A &= \Tr_B[\operator{\rho}_{AB}]\\
            &= \bra{+B}\operator{\rho}_{AB}\ket{+_B} + \bra{-_B}\operator{\rho}_{AB}\ket{-_B}\\
            &= \frac{1}{2}(\ketbra{-_A}{-_A} + \ketbra{+_A}{+_A}).
        \end{align*}
        As a matrix with rows and columns corresponding to \(\ket{+_A}\) and \(\ket{-_A}\) in that order this can be written as
        \[
            \operator{\rho}_A = 
            \begin{pmatrix}
                1/2 & 0\\
                0 & 1/2
            \end{pmatrix}
        \]
        Hence
        \[S = -\Tr[\operator{\rho}_A] = -\frac{1}{2}\log_2\frac{1}{2} - \frac{1}{2}\log_2\frac{1}{2} = 1.\]
        We say that the system is in a maximally entangled state as 1 is the highest entropy a two particle state can have.
    \end{example}
    
    \subsubsection{Purification}
    The notion of \define{purification} is that for a given density matrix we can find a Hilbert space such that the density matrix represents a pure state in that Hilbert space.
    That is given a system \(A\) with density matrix \(\operator{\rho}_A\) we can introduce another system, \(R\), and find a pure state, \(\ket{AR}\), in the joint system such that \(\operator{\rho}_A = \Tr_R[\ketbra{AR}{AR}]\).
    
    To show this is true recall that \(\operator{\rho}_A\) is Hermitian and therefore can be diagonalised.
    Let \(\{\ket{i_A}\}\) be a basis such that \(\operator{\rho}_A\) is diagonal.
    Then
    \[\operator{\rho}_A = \sum_i p_i\ketbra{i_A}{i_A}\]
    where \(p_i\) is the classical probability that the system is in the state \(\ket{i_A}\).
    Suppose that \(\{\ket{i_R}\}\) is an orthonormal basis for system \(R\).
    Let 
    \[\ket{AR} = \sum_i \sqrt{p_i}\ket{i_Ai_R}.\]
    Then the reduced density matrix for the system, which we get by taking the partial trace with respect to \(R\), is
    \begin{align*}
        \Tr_R[\ketbra{AR}{AR}] &= \sum_{ij} \sqrt{p_ip_j}\ketbra{i_A}{j_A}\Tr{}[\ketbra{i_R}{j_R}]\\
        &= \sum_{ij}\sqrt{p_ip_j}\ketbra{i_A}{j_A}\delta_{ij}\\
        &= \sum_i p_i\ketbra{i_A}{i_A}\\
        &= \operator{\rho}_A.
    \end{align*}
    The state \(\ket{AR}\) is called the purification of \(\operator{\rho}_A\).
    
    \subsubsection{Klein's Inequality}\label{sec:klein's inequality}
    Let \(\operator{\rho}\) and \(\operator{\sigma}\) be density matrices which are diagonal in the bases \(\{\ket{i}\}\) and \(\{\ket{j}\}\) respectively.
    Then
    \[\operator{\rho} = \sum_i p_i\ketbra{i}{i}, \qquad\text{and}\qquad \operator{\sigma} = \sum_j q_j\ketbra{j}{j}.\]
    The \define{relative entropy} of \(\operator{\rho}\) with respect to \(\operator{\sigma}\) is a measure of how close \(\operator{\rho}\) and \(\operator{\sigma}\) are.
    It is defined as
    \[\vonNeumannEntropy(\operator{\rho}\|\operator{\sigma}) = \Tr[\operator{\rho}\log_2\operator{\rho} - \operator{\rho}\log_2\operator{\sigma}] = \sum_i p_i\log_2p_i - \sum_i \bra{i}\operator{\rho}\log_2\operator{\sigma}\ket{i}.\]
    The Klein inequality simply states that this is non-negative, that is
    \[\vonNeumannEntropy(\operator{\rho}\|\operator{\sigma}) \ge 0,\]
    or equivalently,
    \[\vonNeumannEntropy(\operator{\rho}) \le -\Tr[\operator{\rho}\log_2\operator{\sigma}].\]
    
    To see why this should be true first note that
    \[\bra{i}\log_2\operator{\sigma}\ket{i} = \bra{i}\sum_j\log_2q_j\ket{j}\braket{j}{i} = \sum_jP_{ij}\log_2q_j\]
    where \(P_{ij} = \braket{i}{j}\braket{j}{i} = \abs{\braket{i}{j}}^2 \ge 0\).
    For complete, orthogonal basis sets \(P_{ij}\) satisfies
    \begin{align*}
        \sum_i P_{ij} &= \sum_i \braket{i}{j}\braket{j}{i}\\
        &= \sum_i \braket{j}{i}\braket{i}{j}\\
        &= \braket{j}{j}\\
        &= 1,
    \end{align*}
    and similarly \(\sum_j P_{ij} = 1\).
    Substituting this into the definition of the relative entropy we have
    \[\vonNeumannEntropy(\operator{\rho}\|\operator{\sigma}) = \sum_{i} p_i \left[ p_i\log_2 p_i - \sum_j P_{ij}\log_2q_j \right].\]
    It can be shown that \(\log_2\) is a concave function and therefore satisfies
    \[\log_2[(1 - a)x + ay] \ge (1 - a)\log_2x + a\log_2y\]
    for \(a\in[0, 1]\).
    Hence
    \[\sum_j P_{ij}\log_2q_j \le \log_2 r_i < 0\]
    where
    \[r_i = \sum_j P_{ij}q_j.\]
    Equality occurs if for some value of \(j\) we have \(P_{ij} = 1\).
    In this case all other \(P_{ij} = 0\).
    Using this we have
    \[\vonNeumannEntropy(\operator{\rho}\|\operator{\sigma}) \ge \sum_i p_i \log_2 \frac{p_i}{r_i} = -\sum_i p_i \log_2 \frac{r_i}{p_i}.\]
    Now we use the following identity for transforming between bases:
    \[-\log_2(x)\ln 2 = -\ln x \ge 1 - x\]
    which is valid for \(x > 0\) with equality only when \(x = 1\).
    Finally
    \[\vonNeumannEntropy(\operator{\rho}\|\operator{\sigma}) \ge - \frac{1}{\ln 2} \sum_i p_i\left[ 1 - \frac{r_i}{p_i} \right] = \frac{1}{\ln 2} \sum_i (r_i - p_i) = 0.\]
    
    \subsubsection{Mathematical Properties of von Neumann Entropy}
    The von Neumann entropy has many nice properties that aid in calculation or provide some insight.
    We list some here:
    \begin{enumerate}
        \item \emph{Invariance under unitary transformation of the density matrix.}
        
        To show this we first need a result about logarithms of matrices.
        The \(\log\) of a matrix is defined to be the function such that if \(\exp(A) = B\) then \(A = \log(B)\), i.e. the inverse of exponentiation.
        If \(B\) is Hermitian we can diagonalise it; meaning that we can find a diagonal matrix \(D\) such that \(UDU\hermit = B\) for some unitary transformation \(U\).
        Hence \(\exp(A) = UDU\hermit\) and \(A = \log(UDU\hermit)\).
        Recall that \(\exp(A)\) is defined through a power series,
        \[\exp(A) = \sum_{n=0}^{\infty} \frac{A^n}{n!}.\]
        Hence
        \begin{align*}
            U\hermit\exp(A)U &= U\hermit\left[ \sum_{n=0}^{\infty} \frac{A^n}{n!} \right]U\\
            &= \sum_{n=0}^{\infty} \frac{1}{n!} U\hermit A^nU.
        \end{align*}
        Now we use
        \[(U\hermit AU)^n = \underbrace{U\hermit AUU\hermit AU \dotsm U\hermit AU}_{n~\text{times}} = U\hermit\underbrace{A\ident A\ident \dotsm \ident A}_{n~\text{times}} U = U\hermit A^nU\]
        and so
        \[U\hermit\exp(A)U = \sum_{n=0}^{\infty} \frac{1}{n!}U\hermit A^nU = \sum_{n=0}^{\infty} \frac{1}{n!}(U\hermit AU)^n = \exp(UAU\hermit).\]
        Hence
        \[U\hermit\exp(A)U = \exp(U\hermit AU) = D \implies U\hermit AU = \log(D)\]
        and so
        \[UU\hermit AUU\hermit = A = U\log(D)U\hermit = \log(B) = \log(UDU\hermit).\]
        
        Now let \(\operator{U}\) be an arbitrary unitary operator.
        Then
        \begin{align*}
            \vonNeumannEntropy(\operator{U}\operator{\rho}\operator{U}\hermit) &= -\Tr[\operator{U}\operator{\rho}\operator{U}\hermit \log_2 (\operator{U}\operator{\rho}\operator\hermit)]\\
            &= -\Tr[\operator{U}\operator{\rho}\operator{U}\hermit \operator{U} \log_2 (\operator{\rho})\operator\hermit]\\
            &= -\Tr[\operator{U}\operator{\rho}\log_2 (\operator{\rho})\operator\hermit]\\
            &= -\Tr[\operator{U}\hermit\operator{U}\operator{\rho}\log_2\operator{\rho}]\\
            &= -\Tr[\operator{\rho}\log_2\operator{\rho}]\\
            &= \vonNeumannEntropy(\operator{\rho}).
        \end{align*}
        Here we have also used the cyclic property of the trace:
        \[\Tr[ABC] = \Tr[CAB].\]
        
        \item \emph{Positivity: \(\vonNeumannEntropy(\operator{\rho}) \ge 0\).}
        
        Since \(\operator{\rho}\) is Hermitian it can be diagonalised.
        Recall from section~\ref{sec:density matrix} that the eigenvalues, \(\alpha_i\), of \(\operator{\rho}\) all lie in the interval \([0, 1]\).
        Hence \(\log_2\alpha_i \le 0\) with equality only when \(\alpha_i = 1\).
        In the eigenbasis of \(\operator{\rho}\) the trace is simply the sum over the eigenvalues and hence
        \[\vonNeumannEntropy(\operator{\rho}) = -\sum_i \alpha_i \log_2\alpha_i \ge 0\]
        with equality only when \(\alpha_i = 1\) which means we are in a pure state.
        
        \item \emph{Maximum: \(\vonNeumannEntropy \le \vonNeumannEntropy^{\max} = \log_2 D\) where \(D\) is the number of non-zero eigenvalues of the density matrix.}
        
        In the eigenbasis of \(\operator{\rho}\) the entropy is
        \[-\sum_i \alpha_i \log_2 \alpha_i.\]
        We know from section~\ref{sec:density matrix} that
        \[\sum_i \alpha_i = 1.\]
        It can then be shown that \(\vonNeumannEntropy(\operator{\rho})\) is maximised when all eigenvalues have the same value.
        If \(D\) is the dimensionality of \(\operator{\rho}\) then this occurs when \(\alpha_i = 1/S\) and hence
        \[\vonNeumannEntropy(\operator{\rho}) = - \sum_{i=1}^D \alpha_i\log_2\alpha_i=\log_2 D.\]
        
        \item \emph{Subadditivity: \(\vonNeumannEntropy(\operator{\rho}_{AB}) \le \vonNeumannEntropy(\operator{\rho}_A) + \vonNeumannEntropy(\operator{\rho}_B)\).}
        
        That is the von Neumann entropy of the combined system of \(A\) and \(B\) is always at most the von Neumann entropy of the two individual systems.
        We will show this for the simplest case when \(\hilbert_{AB} = \hilbert_A \tensorProd \hilbert_B\) where \(\hilbert_{AB}\) is the Hilbert space for the whole system and \(\hilbert_A\) and \(\hilbert_B\) are the Hilbert spaces for the two subsystems.
        In this case \(\operator{\rho}_{AB} = \operator{\rho}_A \tensorProd \operator{\rho}_B\).
        Let \(\operator{U}_i\) be the unitary transformation that diagonalises \(\operator{\rho}_i\).
        Then \(\operator{U}_A\tensorProd\operator{U}_B\) diagonalises \(\operator{\rho}_A \tensorProd \operator{\rho}_B\).
        In this diagonal basis
        \begin{align*}
            \vonNeumannEntropy(\operator{\rho}_{AB}) &= - \sum_{ij}p_i^{(A)}p_j^{(B)}\log_2 (p_i^{(A)}p_j^{(B)})\\
            &= -\sum_{ij}p_i^{(A)}p_j^{(B)}[\log_2 p_i^{(A)} + \log_2 p_j^{(B)}]\\
            &= -\sum_{ij} p_i^{(A)}p_j^{(B)}\log_2 p_i^{(A)} -\sum_{ij} p_i^{(A)}p_j^{(B)}\log_2 p_j^{(B)}\\
            &= -\sum_{i} p_i^{(A)}\log_2 p_i^{(A)} - \sum_{ij} p_j^{(B)}\log_2 p_j^{(B)}
        \end{align*}
        where we have used the fact that \(p_i^{(j)}\) are probabilities so sum to 1.
        In the case where \(\hilbert_A\) and \(\hilbert_B\) are more correlated then this can only decrease the number of states available and hence the entropy.
        For example if there is a potential between the two particles then the particles will be limited to certain states and won't be able to access states that they could when they were free.
        Hence if \(\hilbert_{AB} \subset \hilbert_A \tensorProd \hilbert_B\) then we find that the joint entropy is less than the sum of the individual entropies.
        
        We can prove this more generally using Klein's inequality (section~\ref{sec:klein's inequality}).
        Let \(\operator{\rho} = \operator{\rho}_{AB}\) and \(\operator{\sigma} = \operator{\rho}_A\tensorProd\operator{\rho}_B\).
        Then by Klein's inequality
        \begin{align*}
            \vonNeumannEntropy(\operator{\rho}_{AB}) &\le
            -\Tr[\operator{\rho}\log_2\operator{\sigma}]\\
            &= -\Tr[\operator{\rho}_{AB}(\log_2\operator{\rho}_A + \log_2\operator{\rho}_B)]\\
            &= -\Tr[\operator{\rho}_A\log_2\operator{\rho}_A] - \Tr[\operator{\rho}_B\log_2 \operator{\rho}_B]\\
            &= \vonNeumannEntropy(\operator{\rho}_A) + \vonNeumannEntropy(\operator{\rho}_B).
        \end{align*}
        Equality holds when \(\operator{\rho}_{AB} = \operator{\rho}_A \tensorProd\operator{\rho}_B\).
        
        \item \emph{Concavity: For \(\beta_i \ge 0\) such that \(\sum_i \beta_{i=1}^{n} = 1\) we have}
        \begin{multline*}
            \vonNeumannEntropy\left( \sum_{i=1}^{n}\beta_i\operator{\rho}_i \right) = \vonNeumannEntropy(\beta_1\operator{\rho}_1 + \beta_2\operator{\rho}_2 + \dotsb + \beta_n\operator{\rho}_n) \\\ge \beta_1\vonNeumannEntropy(\operator{\rho}_1) + \beta_2\vonNeumannEntropy(\operator{\rho}_2) + \dotsb + \beta_n\vonNeumannEntropy(\operator{\rho}_n) = \sum_{i=1}^{n}\beta_i \vonNeumannEntropy(\operator{\rho}_i).
        \end{multline*}
        
        Sums of density matrices occur when we have an ensemble of ensembles.
        Intuitively the entropy is larger for the ensemble of density matrices as we know less about the system.
        On the other hand we may know a lot about each individual system and so the sum of entropies is lower.
        
        To show this mathematically let \(\operator{\rho}_A = \sum_j\beta_j\operator{\rho}_j\) and \(\operator{\rho}_B = \sum_j\beta_j\ketbra{j}{j}\) for some orthonormal basis, \(\{\ket{j}\}\).
        The joint state is then \(\operator{\rho}_{AB} = \sum_j\beta_j\operator{\rho}_j\tensorProd\ketbra{j}{j}\).
        Notice then that
        \[\vonNeumannEntropy(\operator{\rho}_{AB}) = \shannonEntropy(\{\beta_j\}) + \sum_j \beta_j \vonNeumannEntropy(\operator{\rho}_j)\]
        and concavity then follows from subadditivity.
        
        \item \emph{Triangle inequality: \(\vonNeumannEntropy(\operator{\rho}_{AB}) \ge \abs{\vonNeumannEntropy(\operator{\rho}_A}) - \vonNeumannEntropy(\operator{\rho}_B)\).}
        
        In the case \(\hilbert_{AB} = \hilbert_A\tensorProd\hilbert_B\) equality holds which follows from the discussion in the section on subadditivity.
        In general let \(R\) be a system that purifies \(A\) and \(B\).
        Recall that this means that we create a pure state \(\ket{AR}\) such that \(\operator{\rho}_A = \Tr_R[\ketbra{AR}{AR}]\).
        This can be done for any system.
        By construction the system \(ABR\) is in a pure state which means that
        \[\vonNeumannEntropy(\operator{\rho}_{AR}) = \vonNeumannEntropy(\operator{\rho}_B)\]
        and
        \[\vonNeumannEntropy(\operator{\rho}_R) = \vonNeumannEntropy(\operator{\rho}_{AB}).\]
        Applying subadditivity to systems \(A\) and \(R\) we have
        \[\vonNeumannEntropy(\operator{\rho}_R) + \vonNeumannEntropy(\operator{\rho}_A) \ge \vonNeumannEntropy(\operator{\rho}_{AR}).\]
        Combining these statements we have
        \[\vonNeumannEntropy(\operator{\rho}_{AB}) \ge \vonNeumannEntropy(\operator{\rho}_B) - \vonNeumannEntropy(\operator{\rho}_A).\]
        Since all of this is symmetric in \(A\) and \(B\) we also have
        \[\vonNeumannEntropy(\operator{\rho}_{AB}) \ge \vonNeumannEntropy(\operator{\rho}_A) - \vonNeumannEntropy(\operator{\rho}_B).\]
        These can then be combined into the triangle inequality.
        
        \item \emph{Strong subadditivity: \(\vonNeumannEntropy(\operator{\rho}_{ABC}) + \vonNeumannEntropy(\operator{\rho}_B) \le \vonNeumannEntropy(\operator{\rho}_{AB}) + \vonNeumannEntropy(\operator{\rho}_{BC})\).}
        
        The proof of is not given.
    \end{enumerate}

    \subsection{Other Measures of Information}
    Suppose we have two classical languages, \(A\) and \(B\).
    For a language \(J = A, B\) denote by \(p_i^{(J)}\) the probability of the \(i\)th symbol in the language.
    The Shannon entropy for each language is given by
    \[\shannonEntropy(J) = -\sum_{i}p_i^{(J)}\log_2p_i^{(J)}.\]
    We can imagine joining both languages together to get a new language \(AB\) with entropy
    \[\shannonEntropy(A, B) = -\sum p_i^{(AB)}\log_2p_i^{(AB)}\]
    where \(p_i^{(AB)}\) is the probability of the \(i\)th symbol in the joint language.
    If \(A\) and \(B\) are completely independent then \(i\) would simply index over all possible product states, \(j_Ak_B\), of the two sub-languages.
    However if the two languages are not independent then this is not the case.
    We define the \define{mutual information} of the joint system to be
    \[I_{\mathrm{Sh}}(A, B) = \shannonEntropy(A) + \shannonEntropy(B) - \shannonEntropy(A, B).\]
    This measures the contribution to the information of both systems and then removes the excess information that is over counted since it is provided by both systems\footnote{cf. \(\abs{A\union B} = \abs{A} + \abs{B} - \abs{A\intersect B}\) and \(P(A\vee B) = P(A) + P(B) - P(A\wedge B)\).}.
    For completely independent systems \(I_{\mathrm{Sh}} = 0\).
    
    Another measure that we could use is the \define{conditional entropy} of a system which is defined as
    \[\shannonEntropy(A|B) = \sum_{i_B} p_{i_B}^{(B)}\shannonEntropy(A|B = i_B)\]
    where
    \[\shannonEntropy(A|B = i_B) = -\sum_{i_A}p(A_{i_A}|B_{i_B})\log_2 p(A_{i_A}|B_{i_B}).\]
    Here \(p(X|Y)\) has the usual meaning in probability of `probability of \(X\) given that \(Y\) is true'.
    The conditional entropy of \(A\) as defined above measures the information content of \(A\) when \(B\) is in all possible states.
    In classical probability theory \(\shannonEntropy(A|B) = \shannonEntropy(A, B) - \shannonEntropy(B)\).
    This allows for an equivalent definition of the mutual information in classical probability theory:
    \[J_{\mathrm{Sh}}(A:B) = \shannonEntropy(A) - \shannonEntropy(A|B).\]
    
    In quantum information theory probability distributions are replaced with density matrices and Shannon entropies are replaced with von Neumann entropies.
    The joint entropy is then \(\vonNeumannEntropy(\operator{\rho}_{AB})\) where \(\operator{\rho}_{AB}\) is the density matrix for the combined system of \(A\) and \(B\).
    The \define{quantum mutual information} then has an analogous definition to its classical counterpart:
    \[I_{\mathrm{vN}}(A, B) = \vonNeumannEntropy(\operator{\rho}_A) + \vonNeumannEntropy(\operator{\rho}_B) - \vonNeumannEntropy(\operator{\rho}_{AB}).\]
    If \(I_{\mathrm{vN}}(A, B) \ne 0\) then we say that \(A\) and \(B\) are \define{correlated}.
    
    Knowing the information necessarily requires measurements to be made and therefore in quantum information theory we have to consider the collapse of the wave function.
    To find the conditional entropy in quantum mechanics we need a well defined notion of a measurement.
    The most commonly used are called the von Neumann measurements or local projective measurements.
    Suppose that we measure subsystem \(B\) and find it in the state \(\ket{j_B}\).
    The probability of this is \(p_{j_B} = \Tr[\operator{\rho}_{AB|j_B}]\) where
    \[\operator{\rho}_{AB|j_B} = \frac{1}{p_{j_B}}(\ident_A\tensorProd \operator{\Pi}_{j_B})\operator{\rho}_{AB}(\ident_A\tensorProd\operator{\Pi}_{j_B})\]
    where
    \[\operator{\Pi}_{j_B} = \ketbra{j_B}{j_B}.\]
    The conditional entropy for \(A\) when \(B\) is measured in all possible states is then
    \[\vonNeumannEntropy(\operator{\rho}_{AB}|\{\operator{\Pi}_{j_B}\}) = \sum_{j_B} p_{j_B}\vonNeumannEntropy(\operator{\rho}_{A|j_B})\]
    where
    \[\operator{\rho}_{A|j_B} = \Tr_B[\operator{\rho}_{AB|j_B}].\]
    Clearly this is dependent on which set of projection operators, \(\{\operator{\Pi}_{j_B}\}\), we choose.
    This means that what we measure \(B\) as will effect the conditional entropy of \(A\).
    
    We can now define the other form of mutual information:
    \[J_{\mathrm{vN}}(\operator{\rho}_{AB}) = \vonNeumannEntropy(\operator{\rho}_A) - \vonNeumannEntropy(\operator{\rho}_{AB}|\{\operator{\Pi}_{j_B}\}).\]
    The \define{quantum discord} is defined as
    \[D_{AB}(\operator{\rho}_{AB}) = I_{\mathrm{vN}}(\operator{\rho_{AB}}) - \max_{\{\operator{\Pi}_{j_B}\}} [J_{\mathrm{vN}}(\operator{\rho}_{AB})].\]
    That is it measures the difference between the two types of mutual information when \(\{\operator{\Pi}_{j_B}\}\) are chosen to maximise \(J_{\mathrm{vN}}\).
    Classically both measures of information are equivalent and the quantum discord is zero.
    This is no longer true in the quantum case.
    Roughly we can think of the quantum discord as a measure of how `quantum' a system is.
    Another way to think of it is as the purely quantum correction to the definition of the mutual information as \(I_{\mathrm{vN}}\) accounts for both quantum and classical effects whereas \(J_{\mathrm{vN}}\) accounts only for the classical effects and so there difference is simply the quantum effect.
    
    
    Much discussion about the philosophy of quantum mechanics centres around these measures of information and what we interpret them to mean.
    However we won't discuss them further here.